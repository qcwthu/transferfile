nohup: ignoring input
Task: amazon_polarity, Checkpoint: models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1
03/23/2022 16:02:19 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/23/2022 16:02:19 - INFO - __main__ - models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity
Output directory () already exists and is not empty.
03/23/2022 16:02:19 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/23/2022 16:02:19 - INFO - __main__ - models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity
03/23/2022 16:02:22 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/23/2022 16:02:22 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/23/2022 16:02:22 - INFO - __main__ - args.device: cuda:0
03/23/2022 16:02:22 - INFO - __main__ - Using 2 gpus
03/23/2022 16:02:22 - INFO - __main__ - args.device: cuda:1
03/23/2022 16:02:22 - INFO - __main__ - Using 2 gpus
03/23/2022 16:02:22 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
03/23/2022 16:02:22 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
03/23/2022 16:02:27 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.5, bsz=8 ...
03/23/2022 16:02:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:02:28 - INFO - __main__ - Printing 3 examples
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:02:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:02:28 - INFO - __main__ - Printing 3 examples
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:02:28 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:02:28 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:02:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:02:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:02:28 - INFO - __main__ - Printing 3 examples
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:02:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:02:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:02:28 - INFO - __main__ - Printing 3 examples
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/23/2022 16:02:28 - INFO - __main__ - ['positive']
03/23/2022 16:02:28 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:02:28 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:02:28 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:02:28 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:02:28 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:02:45 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:02:45 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:02:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:02:48 - INFO - __main__ - Starting training!
03/23/2022 16:02:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:02:55 - INFO - __main__ - Starting training!
03/23/2022 16:02:59 - INFO - __main__ - Step 10 Global step 10 Train loss 0.49 on epoch=4
03/23/2022 16:03:02 - INFO - __main__ - Step 20 Global step 20 Train loss 0.34 on epoch=9
03/23/2022 16:03:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.25 on epoch=14
03/23/2022 16:03:07 - INFO - __main__ - Step 40 Global step 40 Train loss 0.14 on epoch=19
03/23/2022 16:03:10 - INFO - __main__ - Step 50 Global step 50 Train loss 0.12 on epoch=24
03/23/2022 16:03:11 - INFO - __main__ - Global step 50 Train loss 0.27 Classification-F1 0.9687194525904204 on epoch=24
03/23/2022 16:03:11 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9687194525904204 on epoch=24, global_step=50
03/23/2022 16:03:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.08 on epoch=29
03/23/2022 16:03:16 - INFO - __main__ - Step 70 Global step 70 Train loss 0.06 on epoch=34
03/23/2022 16:03:19 - INFO - __main__ - Step 80 Global step 80 Train loss 0.03 on epoch=39
03/23/2022 16:03:22 - INFO - __main__ - Step 90 Global step 90 Train loss 0.05 on epoch=44
03/23/2022 16:03:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.02 on epoch=49
03/23/2022 16:03:25 - INFO - __main__ - Global step 100 Train loss 0.05 Classification-F1 0.9687194525904204 on epoch=49
03/23/2022 16:03:28 - INFO - __main__ - Step 110 Global step 110 Train loss 0.05 on epoch=54
03/23/2022 16:03:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.07 on epoch=59
03/23/2022 16:03:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.01 on epoch=64
03/23/2022 16:03:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.04 on epoch=69
03/23/2022 16:03:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.01 on epoch=74
03/23/2022 16:03:40 - INFO - __main__ - Global step 150 Train loss 0.04 Classification-F1 1.0 on epoch=74
03/23/2022 16:03:40 - INFO - __main__ - Saving model with best Classification-F1: 0.9687194525904204 -> 1.0 on epoch=74, global_step=150
03/23/2022 16:03:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.01 on epoch=79
03/23/2022 16:03:45 - INFO - __main__ - Step 170 Global step 170 Train loss 0.01 on epoch=84
03/23/2022 16:03:48 - INFO - __main__ - Step 180 Global step 180 Train loss 0.01 on epoch=89
03/23/2022 16:03:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.02 on epoch=94
03/23/2022 16:03:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.01 on epoch=99
03/23/2022 16:03:54 - INFO - __main__ - Global step 200 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=99
03/23/2022 16:03:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.00 on epoch=104
03/23/2022 16:04:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.02 on epoch=109
03/23/2022 16:04:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.00 on epoch=114
03/23/2022 16:04:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.00 on epoch=119
03/23/2022 16:04:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.04 on epoch=124
03/23/2022 16:04:09 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 16:04:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 16:04:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 16:04:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.03 on epoch=139
03/23/2022 16:04:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 16:04:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 16:04:24 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 16:04:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.01 on epoch=154
03/23/2022 16:04:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.01 on epoch=159
03/23/2022 16:04:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.02 on epoch=164
03/23/2022 16:04:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 16:04:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 16:04:38 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 1.0 on epoch=174
03/23/2022 16:04:41 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 16:04:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.03 on epoch=184
03/23/2022 16:04:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 16:04:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.01 on epoch=194
03/23/2022 16:04:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 16:04:53 - INFO - __main__ - Global step 400 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=199
03/23/2022 16:04:56 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 16:04:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 16:05:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 16:05:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 16:05:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 16:05:08 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 1.0 on epoch=224
03/23/2022 16:05:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.01 on epoch=229
03/23/2022 16:05:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 16:05:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 16:05:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 16:05:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 16:05:22 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 1.0 on epoch=249
03/23/2022 16:05:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 16:05:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
03/23/2022 16:05:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 16:05:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 16:05:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/23/2022 16:05:37 - INFO - __main__ - Global step 550 Train loss 0.01 Classification-F1 1.0 on epoch=274
03/23/2022 16:05:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 16:05:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 16:05:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 16:05:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 16:05:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 16:05:52 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 1.0 on epoch=299
03/23/2022 16:05:55 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 16:05:58 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 16:06:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/23/2022 16:06:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/23/2022 16:06:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 16:06:07 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 1.0 on epoch=324
03/23/2022 16:06:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 16:06:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 16:06:16 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 16:06:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 16:06:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 16:06:22 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 1.0 on epoch=349
03/23/2022 16:06:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/23/2022 16:06:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 16:06:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 16:06:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 16:06:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
03/23/2022 16:06:37 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=374
03/23/2022 16:06:40 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 16:06:43 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 16:06:46 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 16:06:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 16:06:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 16:06:52 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 1.0 on epoch=399
03/23/2022 16:06:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 16:06:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 16:07:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 16:07:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 16:07:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 16:07:07 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 16:07:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 16:07:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 16:07:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 16:07:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 16:07:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 16:07:22 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 1.0 on epoch=449
03/23/2022 16:07:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 16:07:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 16:07:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 16:07:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 16:07:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 16:07:37 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 1.0 on epoch=474
03/23/2022 16:07:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 16:07:42 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 16:07:45 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 16:07:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 16:07:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 16:07:51 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=499
03/23/2022 16:07:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 16:07:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 16:08:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 16:08:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 16:08:06 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 16:08:06 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 1.0 on epoch=524
03/23/2022 16:08:09 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 16:08:12 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 16:08:15 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 16:08:18 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 16:08:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 16:08:21 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 1.0 on epoch=549
03/23/2022 16:08:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 16:08:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 16:08:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 16:08:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 16:08:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/23/2022 16:08:36 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 1.0 on epoch=574
03/23/2022 16:08:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 16:08:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 16:08:45 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 16:08:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 16:08:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 16:08:51 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 1.0 on epoch=599
03/23/2022 16:08:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 16:08:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 16:09:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 16:09:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 16:09:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 16:09:06 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 1.0 on epoch=624
03/23/2022 16:09:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 16:09:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 16:09:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 16:09:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 16:09:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 16:09:21 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 1.0 on epoch=649
03/23/2022 16:09:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 16:09:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 16:09:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 16:09:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 16:09:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 16:09:36 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 1.0 on epoch=674
03/23/2022 16:09:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 16:09:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 16:09:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 16:09:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
03/23/2022 16:09:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 16:09:51 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 1.0 on epoch=699
03/23/2022 16:09:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 16:09:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 16:10:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 16:10:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 16:10:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 16:10:06 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 1.0 on epoch=724
03/23/2022 16:10:09 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 16:10:12 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 16:10:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 16:10:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/23/2022 16:10:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 16:10:21 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 1.0 on epoch=749
03/23/2022 16:10:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 16:10:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 16:10:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 16:10:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 16:10:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 16:10:36 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 1.0 on epoch=774
03/23/2022 16:10:39 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 16:10:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 16:10:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 16:10:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 16:10:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 16:10:51 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 1.0 on epoch=799
03/23/2022 16:10:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 16:10:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 16:11:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 16:11:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 16:11:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 16:11:06 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 1.0 on epoch=824
03/23/2022 16:11:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 16:11:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 16:11:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 16:11:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 16:11:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 16:11:21 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 1.0 on epoch=849
03/23/2022 16:11:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 16:11:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 16:11:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 16:11:33 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 16:11:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 16:11:37 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 1.0 on epoch=874
03/23/2022 16:11:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 16:11:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 16:11:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 16:11:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 16:11:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 16:11:52 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 1.0 on epoch=899
03/23/2022 16:11:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 16:11:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 16:12:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 16:12:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 16:12:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 16:12:07 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 1.0 on epoch=924
03/23/2022 16:12:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 16:12:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 16:12:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 16:12:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 16:12:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 16:12:22 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 1.0 on epoch=949
03/23/2022 16:12:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 16:12:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 16:12:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 16:12:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 16:12:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 16:12:37 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 1.0 on epoch=974
03/23/2022 16:12:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 16:12:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 16:12:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 16:12:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 16:12:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 16:12:52 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 1.0 on epoch=999
03/23/2022 16:12:52 - INFO - __main__ - save last model!
03/23/2022 16:12:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 16:12:52 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 16:12:52 - INFO - __main__ - Printing 3 examples
03/23/2022 16:12:52 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 16:12:52 - INFO - __main__ - ['negative']
03/23/2022 16:12:52 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 16:12:52 - INFO - __main__ - ['negative']
03/23/2022 16:12:52 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 16:12:52 - INFO - __main__ - ['negative']
03/23/2022 16:12:52 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:12:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:12:53 - INFO - __main__ - Printing 3 examples
03/23/2022 16:12:53 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/23/2022 16:12:53 - INFO - __main__ - ['positive']
03/23/2022 16:12:53 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/23/2022 16:12:53 - INFO - __main__ - ['positive']
03/23/2022 16:12:53 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/23/2022 16:12:53 - INFO - __main__ - ['positive']
03/23/2022 16:12:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 16:12:53 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:12:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:12:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:12:53 - INFO - __main__ - Printing 3 examples
03/23/2022 16:12:53 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/23/2022 16:12:53 - INFO - __main__ - ['positive']
03/23/2022 16:12:53 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/23/2022 16:12:53 - INFO - __main__ - ['positive']
03/23/2022 16:12:53 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/23/2022 16:12:53 - INFO - __main__ - ['positive']
03/23/2022 16:12:53 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:12:53 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:12:53 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:12:53 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:12:54 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 16:13:11 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:13:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:13:12 - INFO - __main__ - Starting training!
03/23/2022 16:13:24 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_100_0.5_8_predictions.txt
03/23/2022 16:13:24 - INFO - __main__ - Classification-F1 on test data: 0.9590
03/23/2022 16:13:24 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.5, bsz=8, dev_performance=1.0, test_performance=0.9589966787309773
03/23/2022 16:13:24 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.4, bsz=8 ...
03/23/2022 16:13:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:13:25 - INFO - __main__ - Printing 3 examples
03/23/2022 16:13:25 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/23/2022 16:13:25 - INFO - __main__ - ['positive']
03/23/2022 16:13:25 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/23/2022 16:13:25 - INFO - __main__ - ['positive']
03/23/2022 16:13:25 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/23/2022 16:13:25 - INFO - __main__ - ['positive']
03/23/2022 16:13:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:13:25 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:13:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:13:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:13:25 - INFO - __main__ - Printing 3 examples
03/23/2022 16:13:25 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/23/2022 16:13:25 - INFO - __main__ - ['positive']
03/23/2022 16:13:25 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/23/2022 16:13:25 - INFO - __main__ - ['positive']
03/23/2022 16:13:25 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/23/2022 16:13:25 - INFO - __main__ - ['positive']
03/23/2022 16:13:25 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:13:25 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:13:25 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:13:44 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:13:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:13:45 - INFO - __main__ - Starting training!
03/23/2022 16:13:49 - INFO - __main__ - Step 10 Global step 10 Train loss 0.66 on epoch=4
03/23/2022 16:13:52 - INFO - __main__ - Step 20 Global step 20 Train loss 0.28 on epoch=9
03/23/2022 16:13:55 - INFO - __main__ - Step 30 Global step 30 Train loss 0.30 on epoch=14
03/23/2022 16:13:58 - INFO - __main__ - Step 40 Global step 40 Train loss 0.18 on epoch=19
03/23/2022 16:14:01 - INFO - __main__ - Step 50 Global step 50 Train loss 0.17 on epoch=24
03/23/2022 16:14:02 - INFO - __main__ - Global step 50 Train loss 0.32 Classification-F1 1.0 on epoch=24
03/23/2022 16:14:02 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 1.0 on epoch=24, global_step=50
03/23/2022 16:14:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.10 on epoch=29
03/23/2022 16:14:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.08 on epoch=34
03/23/2022 16:14:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.09 on epoch=39
03/23/2022 16:14:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.07 on epoch=44
03/23/2022 16:14:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.04 on epoch=49
03/23/2022 16:14:16 - INFO - __main__ - Global step 100 Train loss 0.07 Classification-F1 0.9372549019607843 on epoch=49
03/23/2022 16:14:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.03 on epoch=54
03/23/2022 16:14:22 - INFO - __main__ - Step 120 Global step 120 Train loss 0.04 on epoch=59
03/23/2022 16:14:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.01 on epoch=64
03/23/2022 16:14:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.01 on epoch=69
03/23/2022 16:14:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.07 on epoch=74
03/23/2022 16:14:31 - INFO - __main__ - Global step 150 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 16:14:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.03 on epoch=79
03/23/2022 16:14:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.01 on epoch=84
03/23/2022 16:14:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.01 on epoch=89
03/23/2022 16:14:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.08 on epoch=94
03/23/2022 16:14:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.01 on epoch=99
03/23/2022 16:14:46 - INFO - __main__ - Global step 200 Train loss 0.03 Classification-F1 1.0 on epoch=99
03/23/2022 16:14:49 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 16:14:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.01 on epoch=109
03/23/2022 16:14:55 - INFO - __main__ - Step 230 Global step 230 Train loss 0.01 on epoch=114
03/23/2022 16:14:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.01 on epoch=119
03/23/2022 16:15:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.01 on epoch=124
03/23/2022 16:15:01 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 1.0 on epoch=124
03/23/2022 16:15:04 - INFO - __main__ - Step 260 Global step 260 Train loss 0.02 on epoch=129
03/23/2022 16:15:07 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 16:15:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 16:15:12 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 16:15:15 - INFO - __main__ - Step 300 Global step 300 Train loss 0.01 on epoch=149
03/23/2022 16:15:16 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 1.0 on epoch=149
03/23/2022 16:15:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 16:15:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.02 on epoch=159
03/23/2022 16:15:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 16:15:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 16:15:30 - INFO - __main__ - Step 350 Global step 350 Train loss 0.01 on epoch=174
03/23/2022 16:15:31 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=174
03/23/2022 16:15:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 16:15:37 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 16:15:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 16:15:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 16:15:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 16:15:46 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=199
03/23/2022 16:15:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 16:15:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 16:15:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 16:15:57 - INFO - __main__ - Step 440 Global step 440 Train loss 0.02 on epoch=219
03/23/2022 16:16:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 16:16:01 - INFO - __main__ - Global step 450 Train loss 0.01 Classification-F1 1.0 on epoch=224
03/23/2022 16:16:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 16:16:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 16:16:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 16:16:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 16:16:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 16:16:16 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 1.0 on epoch=249
03/23/2022 16:16:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 16:16:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 16:16:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 16:16:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 16:16:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/23/2022 16:16:30 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 1.0 on epoch=274
03/23/2022 16:16:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 16:16:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 16:16:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 16:16:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 16:16:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 16:16:45 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 16:16:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 16:16:51 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 16:16:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 16:16:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
03/23/2022 16:16:59 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 16:17:00 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 16:17:03 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 16:17:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 16:17:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 16:17:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 16:17:14 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 16:17:15 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 1.0 on epoch=349
03/23/2022 16:17:18 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 16:17:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 16:17:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 16:17:26 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 16:17:29 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 16:17:30 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 1.0 on epoch=374
03/23/2022 16:17:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 16:17:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 16:17:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 16:17:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 16:17:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 16:17:44 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=399
03/23/2022 16:17:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 16:17:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 16:17:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
03/23/2022 16:17:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 16:17:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 16:17:59 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 16:18:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 16:18:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 16:18:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 16:18:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/23/2022 16:18:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 16:18:14 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 16:18:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 16:18:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 16:18:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 16:18:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 16:18:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 16:18:29 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 16:18:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 16:18:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 16:18:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 16:18:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 16:18:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 16:18:44 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=499
03/23/2022 16:18:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/23/2022 16:18:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 16:18:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 16:18:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 16:18:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/23/2022 16:18:58 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 1.0 on epoch=524
03/23/2022 16:19:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 16:19:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 16:19:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 16:19:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 16:19:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 16:19:13 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 1.0 on epoch=549
03/23/2022 16:19:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 16:19:19 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 16:19:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 16:19:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 16:19:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 16:19:28 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 16:19:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 16:19:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 16:19:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 16:19:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 16:19:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 16:19:43 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/23/2022 16:19:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 16:19:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 16:19:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 16:19:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 16:19:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 16:19:58 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 16:20:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 16:20:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 16:20:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 16:20:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 16:20:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 16:20:13 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 16:20:16 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 16:20:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 16:20:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 16:20:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 16:20:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 16:20:28 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 16:20:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 16:20:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 16:20:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 16:20:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 16:20:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 16:20:43 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 16:20:46 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 16:20:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 16:20:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 16:20:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 16:20:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 16:20:58 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 1.0 on epoch=724
03/23/2022 16:21:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 16:21:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 16:21:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 16:21:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 16:21:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 16:21:13 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 1.0 on epoch=749
03/23/2022 16:21:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 16:21:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 16:21:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 16:21:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 16:21:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 16:21:28 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/23/2022 16:21:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 16:21:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 16:21:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 16:21:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 16:21:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 16:21:43 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 1.0 on epoch=799
03/23/2022 16:21:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 16:21:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 16:21:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 16:21:54 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 16:21:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 16:21:58 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 1.0 on epoch=824
03/23/2022 16:22:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 16:22:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 16:22:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 16:22:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 16:22:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 16:22:13 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 1.0 on epoch=849
03/23/2022 16:22:15 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 16:22:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 16:22:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 16:22:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 16:22:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 16:22:28 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 1.0 on epoch=874
03/23/2022 16:22:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 16:22:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 16:22:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 16:22:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 16:22:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 16:22:42 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 16:22:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 16:22:48 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 16:22:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 16:22:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 16:22:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 16:22:57 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 1.0 on epoch=924
03/23/2022 16:23:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 16:23:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 16:23:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 16:23:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 16:23:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 16:23:12 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 1.0 on epoch=949
03/23/2022 16:23:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 16:23:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 16:23:21 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 16:23:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 16:23:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/23/2022 16:23:27 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 1.0 on epoch=974
03/23/2022 16:23:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 16:23:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 16:23:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 16:23:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 16:23:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 16:23:42 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 1.0 on epoch=999
03/23/2022 16:23:42 - INFO - __main__ - save last model!
03/23/2022 16:23:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 16:23:42 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 16:23:42 - INFO - __main__ - Printing 3 examples
03/23/2022 16:23:42 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 16:23:42 - INFO - __main__ - ['negative']
03/23/2022 16:23:42 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 16:23:42 - INFO - __main__ - ['negative']
03/23/2022 16:23:42 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 16:23:42 - INFO - __main__ - ['negative']
03/23/2022 16:23:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:23:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:23:43 - INFO - __main__ - Printing 3 examples
03/23/2022 16:23:43 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/23/2022 16:23:43 - INFO - __main__ - ['positive']
03/23/2022 16:23:43 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/23/2022 16:23:43 - INFO - __main__ - ['positive']
03/23/2022 16:23:43 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/23/2022 16:23:43 - INFO - __main__ - ['positive']
03/23/2022 16:23:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 16:23:43 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:23:43 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:23:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:23:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:23:43 - INFO - __main__ - Printing 3 examples
03/23/2022 16:23:43 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/23/2022 16:23:43 - INFO - __main__ - ['positive']
03/23/2022 16:23:43 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/23/2022 16:23:43 - INFO - __main__ - ['positive']
03/23/2022 16:23:43 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/23/2022 16:23:43 - INFO - __main__ - ['positive']
03/23/2022 16:23:43 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:23:43 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:23:43 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:23:44 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 16:23:58 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:23:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:23:59 - INFO - __main__ - Starting training!
03/23/2022 16:24:08 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_100_0.4_8_predictions.txt
03/23/2022 16:24:08 - INFO - __main__ - Classification-F1 on test data: 0.9479
03/23/2022 16:24:08 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.4, bsz=8, dev_performance=1.0, test_performance=0.9479398184301052
03/23/2022 16:24:08 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.3, bsz=8 ...
03/23/2022 16:24:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:24:09 - INFO - __main__ - Printing 3 examples
03/23/2022 16:24:09 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/23/2022 16:24:09 - INFO - __main__ - ['positive']
03/23/2022 16:24:09 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/23/2022 16:24:09 - INFO - __main__ - ['positive']
03/23/2022 16:24:09 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/23/2022 16:24:09 - INFO - __main__ - ['positive']
03/23/2022 16:24:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:24:09 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:24:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:24:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:24:09 - INFO - __main__ - Printing 3 examples
03/23/2022 16:24:09 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/23/2022 16:24:09 - INFO - __main__ - ['positive']
03/23/2022 16:24:09 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/23/2022 16:24:09 - INFO - __main__ - ['positive']
03/23/2022 16:24:09 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/23/2022 16:24:09 - INFO - __main__ - ['positive']
03/23/2022 16:24:09 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:24:09 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:24:10 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:24:25 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:24:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:24:25 - INFO - __main__ - Starting training!
03/23/2022 16:24:29 - INFO - __main__ - Step 10 Global step 10 Train loss 0.55 on epoch=4
03/23/2022 16:24:32 - INFO - __main__ - Step 20 Global step 20 Train loss 0.33 on epoch=9
03/23/2022 16:24:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.20 on epoch=14
03/23/2022 16:24:37 - INFO - __main__ - Step 40 Global step 40 Train loss 0.21 on epoch=19
03/23/2022 16:24:40 - INFO - __main__ - Step 50 Global step 50 Train loss 0.14 on epoch=24
03/23/2022 16:24:41 - INFO - __main__ - Global step 50 Train loss 0.29 Classification-F1 1.0 on epoch=24
03/23/2022 16:24:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 1.0 on epoch=24, global_step=50
03/23/2022 16:24:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.16 on epoch=29
03/23/2022 16:24:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.12 on epoch=34
03/23/2022 16:24:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.12 on epoch=39
03/23/2022 16:24:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.07 on epoch=44
03/23/2022 16:24:55 - INFO - __main__ - Step 100 Global step 100 Train loss 0.10 on epoch=49
03/23/2022 16:24:56 - INFO - __main__ - Global step 100 Train loss 0.12 Classification-F1 1.0 on epoch=49
03/23/2022 16:24:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.04 on epoch=54
03/23/2022 16:25:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.03 on epoch=59
03/23/2022 16:25:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.04 on epoch=64
03/23/2022 16:25:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.02 on epoch=69
03/23/2022 16:25:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.04 on epoch=74
03/23/2022 16:25:10 - INFO - __main__ - Global step 150 Train loss 0.03 Classification-F1 1.0 on epoch=74
03/23/2022 16:25:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.01 on epoch=79
03/23/2022 16:25:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.01 on epoch=84
03/23/2022 16:25:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.01 on epoch=89
03/23/2022 16:25:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.01 on epoch=94
03/23/2022 16:25:24 - INFO - __main__ - Step 200 Global step 200 Train loss 0.02 on epoch=99
03/23/2022 16:25:25 - INFO - __main__ - Global step 200 Train loss 0.01 Classification-F1 1.0 on epoch=99
03/23/2022 16:25:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 16:25:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.02 on epoch=109
03/23/2022 16:25:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.02 on epoch=114
03/23/2022 16:25:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.01 on epoch=119
03/23/2022 16:25:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.02 on epoch=124
03/23/2022 16:25:40 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 1.0 on epoch=124
03/23/2022 16:25:43 - INFO - __main__ - Step 260 Global step 260 Train loss 0.01 on epoch=129
03/23/2022 16:25:46 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 16:25:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.01 on epoch=139
03/23/2022 16:25:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.01 on epoch=144
03/23/2022 16:25:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.01 on epoch=149
03/23/2022 16:25:55 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 16:25:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.02 on epoch=154
03/23/2022 16:26:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 16:26:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.01 on epoch=164
03/23/2022 16:26:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 16:26:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 16:26:10 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 1.0 on epoch=174
03/23/2022 16:26:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 16:26:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 16:26:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 16:26:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.01 on epoch=194
03/23/2022 16:26:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.04 on epoch=199
03/23/2022 16:26:24 - INFO - __main__ - Global step 400 Train loss 0.01 Classification-F1 1.0 on epoch=199
03/23/2022 16:26:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 16:26:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 16:26:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.04 on epoch=214
03/23/2022 16:26:36 - INFO - __main__ - Step 440 Global step 440 Train loss 0.01 on epoch=219
03/23/2022 16:26:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.01 on epoch=224
03/23/2022 16:26:39 - INFO - __main__ - Global step 450 Train loss 0.01 Classification-F1 1.0 on epoch=224
03/23/2022 16:26:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
03/23/2022 16:26:45 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 16:26:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 16:26:51 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 16:26:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 16:26:54 - INFO - __main__ - Global step 500 Train loss 0.02 Classification-F1 1.0 on epoch=249
03/23/2022 16:26:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 16:27:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 16:27:03 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 16:27:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 16:27:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 16:27:09 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 1.0 on epoch=274
03/23/2022 16:27:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 16:27:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 16:27:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 16:27:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 16:27:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 16:27:24 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 1.0 on epoch=299
03/23/2022 16:27:27 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/23/2022 16:27:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/23/2022 16:27:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/23/2022 16:27:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.05 on epoch=319
03/23/2022 16:27:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
03/23/2022 16:27:39 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 1.0 on epoch=324
03/23/2022 16:27:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 16:27:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 16:27:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 16:27:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 16:27:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 16:27:54 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 16:27:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 16:27:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 16:28:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 16:28:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 16:28:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 16:28:08 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=374
03/23/2022 16:28:11 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 16:28:14 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 16:28:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 16:28:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 16:28:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 16:28:23 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=399
03/23/2022 16:28:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 16:28:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 16:28:32 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 16:28:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 16:28:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 16:28:38 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 16:28:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 16:28:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 16:28:47 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 16:28:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 16:28:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 16:28:53 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 16:28:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 16:28:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 16:29:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
03/23/2022 16:29:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 16:29:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 16:29:08 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 16:29:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 16:29:14 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 16:29:16 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 16:29:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 16:29:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 16:29:23 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 1.0 on epoch=499
03/23/2022 16:29:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 16:29:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 16:29:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 16:29:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 16:29:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 16:29:38 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 1.0 on epoch=524
03/23/2022 16:29:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 16:29:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 16:29:46 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 16:29:49 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 16:29:52 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 16:29:53 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 1.0 on epoch=549
03/23/2022 16:29:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 16:29:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 16:30:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 16:30:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/23/2022 16:30:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 16:30:08 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 1.0 on epoch=574
03/23/2022 16:30:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 16:30:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/23/2022 16:30:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 16:30:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 16:30:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 16:30:22 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 1.0 on epoch=599
03/23/2022 16:30:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 16:30:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 16:30:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 16:30:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 16:30:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 16:30:37 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 1.0 on epoch=624
03/23/2022 16:30:40 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 16:30:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 16:30:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 16:30:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 16:30:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 16:30:52 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 1.0 on epoch=649
03/23/2022 16:30:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 16:30:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 16:31:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 16:31:04 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 16:31:06 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=674
03/23/2022 16:31:07 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 1.0 on epoch=674
03/23/2022 16:31:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/23/2022 16:31:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/23/2022 16:31:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 16:31:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 16:31:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 16:31:22 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 1.0 on epoch=699
03/23/2022 16:31:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 16:31:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 16:31:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 16:31:33 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 16:31:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 16:31:37 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 1.0 on epoch=724
03/23/2022 16:31:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 16:31:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 16:31:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 16:31:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 16:31:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 16:31:52 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 1.0 on epoch=749
03/23/2022 16:31:55 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 16:31:57 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 16:32:00 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/23/2022 16:32:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 16:32:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 16:32:07 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 1.0 on epoch=774
03/23/2022 16:32:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 16:32:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 16:32:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 16:32:18 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 16:32:21 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 16:32:22 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 1.0 on epoch=799
03/23/2022 16:32:24 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 16:32:27 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 16:32:30 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
03/23/2022 16:32:33 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 16:32:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 16:32:36 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 1.0 on epoch=824
03/23/2022 16:32:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 16:32:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 16:32:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 16:32:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 16:32:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 16:32:51 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 1.0 on epoch=849
03/23/2022 16:32:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 16:32:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 16:33:00 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 16:33:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 16:33:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 16:33:06 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 1.0 on epoch=874
03/23/2022 16:33:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 16:33:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 16:33:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 16:33:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 16:33:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 16:33:21 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 1.0 on epoch=899
03/23/2022 16:33:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 16:33:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 16:33:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 16:33:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 16:33:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 16:33:36 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 1.0 on epoch=924
03/23/2022 16:33:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 16:33:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 16:33:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 16:33:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 16:33:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 16:33:51 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 1.0 on epoch=949
03/23/2022 16:33:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 16:33:57 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 16:34:00 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 16:34:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 16:34:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 16:34:06 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 1.0 on epoch=974
03/23/2022 16:34:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 16:34:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 16:34:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 16:34:17 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 16:34:20 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 16:34:21 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 1.0 on epoch=999
03/23/2022 16:34:21 - INFO - __main__ - save last model!
03/23/2022 16:34:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 16:34:21 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 16:34:21 - INFO - __main__ - Printing 3 examples
03/23/2022 16:34:21 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 16:34:21 - INFO - __main__ - ['negative']
03/23/2022 16:34:21 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 16:34:21 - INFO - __main__ - ['negative']
03/23/2022 16:34:21 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 16:34:21 - INFO - __main__ - ['negative']
03/23/2022 16:34:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:34:22 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:34:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:34:22 - INFO - __main__ - Printing 3 examples
03/23/2022 16:34:22 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/23/2022 16:34:22 - INFO - __main__ - ['positive']
03/23/2022 16:34:22 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/23/2022 16:34:22 - INFO - __main__ - ['positive']
03/23/2022 16:34:22 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/23/2022 16:34:22 - INFO - __main__ - ['positive']
03/23/2022 16:34:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 16:34:22 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:34:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:34:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:34:22 - INFO - __main__ - Printing 3 examples
03/23/2022 16:34:22 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/23/2022 16:34:22 - INFO - __main__ - ['positive']
03/23/2022 16:34:22 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/23/2022 16:34:22 - INFO - __main__ - ['positive']
03/23/2022 16:34:22 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/23/2022 16:34:22 - INFO - __main__ - ['positive']
03/23/2022 16:34:22 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:34:22 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:34:22 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:34:23 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 16:34:41 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:34:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:34:42 - INFO - __main__ - Starting training!
03/23/2022 16:34:47 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_100_0.3_8_predictions.txt
03/23/2022 16:34:47 - INFO - __main__ - Classification-F1 on test data: 0.9540
03/23/2022 16:34:47 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.3, bsz=8, dev_performance=1.0, test_performance=0.953988220984572
03/23/2022 16:34:47 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.2, bsz=8 ...
03/23/2022 16:34:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:34:48 - INFO - __main__ - Printing 3 examples
03/23/2022 16:34:48 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/23/2022 16:34:48 - INFO - __main__ - ['positive']
03/23/2022 16:34:48 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/23/2022 16:34:48 - INFO - __main__ - ['positive']
03/23/2022 16:34:48 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/23/2022 16:34:48 - INFO - __main__ - ['positive']
03/23/2022 16:34:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:34:48 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:34:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:34:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:34:48 - INFO - __main__ - Printing 3 examples
03/23/2022 16:34:48 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/23/2022 16:34:48 - INFO - __main__ - ['positive']
03/23/2022 16:34:48 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/23/2022 16:34:48 - INFO - __main__ - ['positive']
03/23/2022 16:34:48 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/23/2022 16:34:48 - INFO - __main__ - ['positive']
03/23/2022 16:34:48 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:34:48 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:34:48 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:35:07 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:35:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:35:08 - INFO - __main__ - Starting training!
03/23/2022 16:35:12 - INFO - __main__ - Step 10 Global step 10 Train loss 0.62 on epoch=4
03/23/2022 16:35:14 - INFO - __main__ - Step 20 Global step 20 Train loss 0.38 on epoch=9
03/23/2022 16:35:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.34 on epoch=14
03/23/2022 16:35:20 - INFO - __main__ - Step 40 Global step 40 Train loss 0.24 on epoch=19
03/23/2022 16:35:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.25 on epoch=24
03/23/2022 16:35:24 - INFO - __main__ - Global step 50 Train loss 0.37 Classification-F1 0.9375 on epoch=24
03/23/2022 16:35:24 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9375 on epoch=24, global_step=50
03/23/2022 16:35:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.21 on epoch=29
03/23/2022 16:35:29 - INFO - __main__ - Step 70 Global step 70 Train loss 0.17 on epoch=34
03/23/2022 16:35:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.16 on epoch=39
03/23/2022 16:35:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.16 on epoch=44
03/23/2022 16:35:38 - INFO - __main__ - Step 100 Global step 100 Train loss 0.11 on epoch=49
03/23/2022 16:35:39 - INFO - __main__ - Global step 100 Train loss 0.16 Classification-F1 0.9372549019607843 on epoch=49
03/23/2022 16:35:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.09 on epoch=54
03/23/2022 16:35:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.09 on epoch=59
03/23/2022 16:35:47 - INFO - __main__ - Step 130 Global step 130 Train loss 0.07 on epoch=64
03/23/2022 16:35:50 - INFO - __main__ - Step 140 Global step 140 Train loss 0.06 on epoch=69
03/23/2022 16:35:53 - INFO - __main__ - Step 150 Global step 150 Train loss 0.06 on epoch=74
03/23/2022 16:35:53 - INFO - __main__ - Global step 150 Train loss 0.07 Classification-F1 1.0 on epoch=74
03/23/2022 16:35:53 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 1.0 on epoch=74, global_step=150
03/23/2022 16:35:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.03 on epoch=79
03/23/2022 16:35:59 - INFO - __main__ - Step 170 Global step 170 Train loss 0.06 on epoch=84
03/23/2022 16:36:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.02 on epoch=89
03/23/2022 16:36:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.05 on epoch=94
03/23/2022 16:36:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.01 on epoch=99
03/23/2022 16:36:08 - INFO - __main__ - Global step 200 Train loss 0.04 Classification-F1 1.0 on epoch=99
03/23/2022 16:36:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.02 on epoch=104
03/23/2022 16:36:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.06 on epoch=109
03/23/2022 16:36:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.05 on epoch=114
03/23/2022 16:36:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.01 on epoch=119
03/23/2022 16:36:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.05 on epoch=124
03/23/2022 16:36:23 - INFO - __main__ - Global step 250 Train loss 0.04 Classification-F1 0.9372549019607843 on epoch=124
03/23/2022 16:36:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.01 on epoch=129
03/23/2022 16:36:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.02 on epoch=134
03/23/2022 16:36:32 - INFO - __main__ - Step 280 Global step 280 Train loss 0.01 on epoch=139
03/23/2022 16:36:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.01 on epoch=144
03/23/2022 16:36:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.01 on epoch=149
03/23/2022 16:36:38 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 1.0 on epoch=149
03/23/2022 16:36:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.01 on epoch=154
03/23/2022 16:36:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.02 on epoch=159
03/23/2022 16:36:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.01 on epoch=164
03/23/2022 16:36:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.01 on epoch=169
03/23/2022 16:36:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.02 on epoch=174
03/23/2022 16:36:53 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 1.0 on epoch=174
03/23/2022 16:36:56 - INFO - __main__ - Step 360 Global step 360 Train loss 0.01 on epoch=179
03/23/2022 16:36:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.01 on epoch=184
03/23/2022 16:37:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.01 on epoch=189
03/23/2022 16:37:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 16:37:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 16:37:08 - INFO - __main__ - Global step 400 Train loss 0.01 Classification-F1 1.0 on epoch=199
03/23/2022 16:37:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 16:37:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
03/23/2022 16:37:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 16:37:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 16:37:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.01 on epoch=224
03/23/2022 16:37:23 - INFO - __main__ - Global step 450 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 16:37:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.01 on epoch=229
03/23/2022 16:37:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 16:37:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.02 on epoch=239
03/23/2022 16:37:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 16:37:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 16:37:38 - INFO - __main__ - Global step 500 Train loss 0.01 Classification-F1 1.0 on epoch=249
03/23/2022 16:37:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 16:37:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 16:37:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 16:37:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 16:37:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 16:37:52 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 1.0 on epoch=274
03/23/2022 16:37:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/23/2022 16:37:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 16:38:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 16:38:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/23/2022 16:38:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/23/2022 16:38:07 - INFO - __main__ - Global step 600 Train loss 0.01 Classification-F1 1.0 on epoch=299
03/23/2022 16:38:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 16:38:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
03/23/2022 16:38:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/23/2022 16:38:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 16:38:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 16:38:22 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 1.0 on epoch=324
03/23/2022 16:38:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 16:38:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 16:38:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/23/2022 16:38:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 16:38:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 16:38:37 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=349
03/23/2022 16:38:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/23/2022 16:38:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 16:38:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 16:38:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 16:38:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
03/23/2022 16:38:52 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=374
03/23/2022 16:38:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
03/23/2022 16:38:57 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 16:39:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 16:39:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 16:39:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 16:39:07 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 1.0 on epoch=399
03/23/2022 16:39:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 16:39:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 16:39:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 16:39:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 16:39:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 16:39:22 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 1.0 on epoch=424
03/23/2022 16:39:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 16:39:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 16:39:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 16:39:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 16:39:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 16:39:36 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 1.0 on epoch=449
03/23/2022 16:39:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 16:39:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 16:39:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 16:39:47 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 16:39:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 16:39:51 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=474
03/23/2022 16:39:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 16:39:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 16:39:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 16:40:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 16:40:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 16:40:06 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 1.0 on epoch=499
03/23/2022 16:40:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 16:40:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/23/2022 16:40:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 16:40:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
03/23/2022 16:40:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 16:40:20 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 1.0 on epoch=524
03/23/2022 16:40:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/23/2022 16:40:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 16:40:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 16:40:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 16:40:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 16:40:35 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 1.0 on epoch=549
03/23/2022 16:40:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 16:40:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 16:40:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 16:40:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 16:40:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 16:40:50 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 1.0 on epoch=574
03/23/2022 16:40:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 16:40:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 16:40:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 16:41:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 16:41:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=599
03/23/2022 16:41:05 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 1.0 on epoch=599
03/23/2022 16:41:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 16:41:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/23/2022 16:41:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 16:41:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 16:41:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/23/2022 16:41:19 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 1.0 on epoch=624
03/23/2022 16:41:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 16:41:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 16:41:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 16:41:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 16:41:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 16:41:34 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 1.0 on epoch=649
03/23/2022 16:41:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 16:41:40 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 16:41:42 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 16:41:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 16:41:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 16:41:49 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 1.0 on epoch=674
03/23/2022 16:41:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 16:41:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/23/2022 16:41:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 16:42:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 16:42:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 16:42:03 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 16:42:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 16:42:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 16:42:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 16:42:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 16:42:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 16:42:18 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 1.0 on epoch=724
03/23/2022 16:42:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 16:42:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 16:42:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 16:42:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 16:42:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 16:42:33 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 1.0 on epoch=749
03/23/2022 16:42:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 16:42:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 16:42:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 16:42:44 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 16:42:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 16:42:48 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 1.0 on epoch=774
03/23/2022 16:42:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 16:42:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 16:42:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 16:42:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/23/2022 16:43:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 16:43:03 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 16:43:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 16:43:08 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 16:43:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 16:43:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 16:43:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 16:43:18 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/23/2022 16:43:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 16:43:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 16:43:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 16:43:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 16:43:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 16:43:32 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=849
03/23/2022 16:43:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 16:43:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 16:43:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 16:43:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 16:43:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 16:43:47 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 1.0 on epoch=874
03/23/2022 16:43:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 16:43:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 16:43:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 16:43:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 16:44:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
03/23/2022 16:44:02 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 1.0 on epoch=899
03/23/2022 16:44:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 16:44:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 16:44:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 16:44:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 16:44:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 16:44:17 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 1.0 on epoch=924
03/23/2022 16:44:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 16:44:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 16:44:25 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 16:44:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 16:44:31 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 16:44:31 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 1.0 on epoch=949
03/23/2022 16:44:34 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=954
03/23/2022 16:44:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 16:44:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 16:44:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 16:44:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 16:44:46 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 1.0 on epoch=974
03/23/2022 16:44:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 16:44:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 16:44:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 16:44:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 16:45:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
03/23/2022 16:45:01 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=999
03/23/2022 16:45:01 - INFO - __main__ - save last model!
03/23/2022 16:45:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 16:45:01 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 16:45:01 - INFO - __main__ - Printing 3 examples
03/23/2022 16:45:01 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 16:45:01 - INFO - __main__ - ['negative']
03/23/2022 16:45:01 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 16:45:01 - INFO - __main__ - ['negative']
03/23/2022 16:45:01 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 16:45:01 - INFO - __main__ - ['negative']
03/23/2022 16:45:01 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:45:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:45:01 - INFO - __main__ - Printing 3 examples
03/23/2022 16:45:01 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/23/2022 16:45:01 - INFO - __main__ - ['negative']
03/23/2022 16:45:01 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/23/2022 16:45:01 - INFO - __main__ - ['negative']
03/23/2022 16:45:01 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/23/2022 16:45:01 - INFO - __main__ - ['negative']
03/23/2022 16:45:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 16:45:02 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:45:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:45:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:45:02 - INFO - __main__ - Printing 3 examples
03/23/2022 16:45:02 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/23/2022 16:45:02 - INFO - __main__ - ['negative']
03/23/2022 16:45:02 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/23/2022 16:45:02 - INFO - __main__ - ['negative']
03/23/2022 16:45:02 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/23/2022 16:45:02 - INFO - __main__ - ['negative']
03/23/2022 16:45:02 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:45:02 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:45:02 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:45:02 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:45:03 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 16:45:20 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:45:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:45:21 - INFO - __main__ - Starting training!
03/23/2022 16:45:27 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_100_0.2_8_predictions.txt
03/23/2022 16:45:27 - INFO - __main__ - Classification-F1 on test data: 0.9510
03/23/2022 16:45:27 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.2, bsz=8, dev_performance=1.0, test_performance=0.950999558996031
03/23/2022 16:45:27 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.5, bsz=8 ...
03/23/2022 16:45:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:45:28 - INFO - __main__ - Printing 3 examples
03/23/2022 16:45:28 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/23/2022 16:45:28 - INFO - __main__ - ['negative']
03/23/2022 16:45:28 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/23/2022 16:45:28 - INFO - __main__ - ['negative']
03/23/2022 16:45:28 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/23/2022 16:45:28 - INFO - __main__ - ['negative']
03/23/2022 16:45:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:45:28 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:45:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:45:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:45:28 - INFO - __main__ - Printing 3 examples
03/23/2022 16:45:28 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/23/2022 16:45:28 - INFO - __main__ - ['negative']
03/23/2022 16:45:28 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/23/2022 16:45:28 - INFO - __main__ - ['negative']
03/23/2022 16:45:28 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/23/2022 16:45:28 - INFO - __main__ - ['negative']
03/23/2022 16:45:28 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:45:28 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:45:28 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:45:47 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:45:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:45:48 - INFO - __main__ - Starting training!
03/23/2022 16:45:53 - INFO - __main__ - Step 10 Global step 10 Train loss 0.34 on epoch=4
03/23/2022 16:45:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.15 on epoch=9
03/23/2022 16:45:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.12 on epoch=14
03/23/2022 16:46:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.07 on epoch=19
03/23/2022 16:46:05 - INFO - __main__ - Step 50 Global step 50 Train loss 0.04 on epoch=24
03/23/2022 16:46:06 - INFO - __main__ - Global step 50 Train loss 0.15 Classification-F1 0.9687194525904204 on epoch=24
03/23/2022 16:46:06 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9687194525904204 on epoch=24, global_step=50
03/23/2022 16:46:09 - INFO - __main__ - Step 60 Global step 60 Train loss 0.03 on epoch=29
03/23/2022 16:46:12 - INFO - __main__ - Step 70 Global step 70 Train loss 0.02 on epoch=34
03/23/2022 16:46:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.02 on epoch=39
03/23/2022 16:46:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.01 on epoch=44
03/23/2022 16:46:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.01 on epoch=49
03/23/2022 16:46:22 - INFO - __main__ - Global step 100 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=49
03/23/2022 16:46:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.00 on epoch=54
03/23/2022 16:46:27 - INFO - __main__ - Step 120 Global step 120 Train loss 0.00 on epoch=59
03/23/2022 16:46:30 - INFO - __main__ - Step 130 Global step 130 Train loss 0.03 on epoch=64
03/23/2022 16:46:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.00 on epoch=69
03/23/2022 16:46:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.00 on epoch=74
03/23/2022 16:46:37 - INFO - __main__ - Global step 150 Train loss 0.01 Classification-F1 0.9375 on epoch=74
03/23/2022 16:46:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.00 on epoch=79
03/23/2022 16:46:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.00 on epoch=84
03/23/2022 16:46:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.00 on epoch=89
03/23/2022 16:46:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.00 on epoch=94
03/23/2022 16:46:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.00 on epoch=99
03/23/2022 16:46:53 - INFO - __main__ - Global step 200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=99
03/23/2022 16:46:56 - INFO - __main__ - Step 210 Global step 210 Train loss 0.00 on epoch=104
03/23/2022 16:46:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.00 on epoch=109
03/23/2022 16:47:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.00 on epoch=114
03/23/2022 16:47:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.00 on epoch=119
03/23/2022 16:47:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.00 on epoch=124
03/23/2022 16:47:08 - INFO - __main__ - Global step 250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 16:47:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 16:47:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 16:47:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 16:47:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 16:47:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 16:47:24 - INFO - __main__ - Global step 300 Train loss 0.00 Classification-F1 0.9375 on epoch=149
03/23/2022 16:47:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 16:47:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 16:47:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 16:47:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 16:47:38 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 16:47:39 - INFO - __main__ - Global step 350 Train loss 0.00 Classification-F1 0.9375 on epoch=174
03/23/2022 16:47:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 16:47:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 16:47:48 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 16:47:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 16:47:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 16:47:54 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=199
03/23/2022 16:47:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 16:48:00 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 16:48:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 16:48:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 16:48:09 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 16:48:10 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 16:48:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 16:48:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 16:48:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 16:48:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 16:48:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
03/23/2022 16:48:25 - INFO - __main__ - Global step 500 Train loss 0.01 Classification-F1 0.9375 on epoch=249
03/23/2022 16:48:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 16:48:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 16:48:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 16:48:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 16:48:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 16:48:41 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 16:48:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 16:48:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 16:48:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/23/2022 16:48:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 16:48:56 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 16:48:57 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 16:49:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 16:49:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 16:49:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 16:49:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 16:49:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 16:49:13 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 16:49:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 16:49:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 16:49:22 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 16:49:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/23/2022 16:49:28 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 16:49:28 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 16:49:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 16:49:35 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 16:49:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 16:49:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 16:49:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 16:49:44 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9375 on epoch=374
03/23/2022 16:49:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 16:49:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 16:49:53 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 16:49:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 16:50:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 16:50:00 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=399
03/23/2022 16:50:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 16:50:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 16:50:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 16:50:12 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 16:50:15 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 16:50:16 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 16:50:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 16:50:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 16:50:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 16:50:28 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 16:50:31 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 16:50:32 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 16:50:34 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 16:50:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 16:50:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 16:50:43 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 16:50:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 16:50:47 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 16:50:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 16:50:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 16:50:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 16:50:59 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 16:51:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 16:51:02 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=499
03/23/2022 16:51:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 16:51:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 16:51:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 16:51:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 16:51:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 16:51:18 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/23/2022 16:51:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 16:51:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 16:51:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 16:51:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 16:51:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 16:51:34 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=549
03/23/2022 16:51:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/23/2022 16:51:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 16:51:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 16:51:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 16:51:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 16:51:49 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 16:51:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 16:51:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 16:51:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 16:52:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 16:52:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 16:52:04 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/23/2022 16:52:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 16:52:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 16:52:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 16:52:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 16:52:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 16:52:20 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 16:52:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 16:52:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 16:52:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 16:52:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 16:52:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 16:52:35 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 16:52:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 16:52:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 16:52:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 16:52:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 16:52:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 16:52:51 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 16:52:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 16:52:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 16:53:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 16:53:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 16:53:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 16:53:06 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9375 on epoch=699
03/23/2022 16:53:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 16:53:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 16:53:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 16:53:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 16:53:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 16:53:22 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9375 on epoch=724
03/23/2022 16:53:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 16:53:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 16:53:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 16:53:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 16:53:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 16:53:37 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/23/2022 16:53:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 16:53:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 16:53:46 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 16:53:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 16:53:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 16:53:53 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9375 on epoch=774
03/23/2022 16:53:56 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 16:53:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 16:54:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 16:54:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 16:54:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 16:54:09 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9375 on epoch=799
03/23/2022 16:54:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 16:54:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 16:54:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 16:54:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 16:54:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 16:54:25 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9375 on epoch=824
03/23/2022 16:54:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 16:54:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 16:54:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 16:54:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 16:54:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=849
03/23/2022 16:54:41 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.9375 on epoch=849
03/23/2022 16:54:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 16:54:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 16:54:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 16:54:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 16:54:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 16:54:57 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/23/2022 16:55:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 16:55:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 16:55:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 16:55:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 16:55:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 16:55:13 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 16:55:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 16:55:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 16:55:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 16:55:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 16:55:28 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 16:55:28 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/23/2022 16:55:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 16:55:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 16:55:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 16:55:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 16:55:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 16:55:44 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/23/2022 16:55:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 16:55:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 16:55:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 16:55:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 16:56:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 16:56:00 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/23/2022 16:56:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 16:56:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 16:56:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 16:56:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 16:56:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 16:56:16 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/23/2022 16:56:16 - INFO - __main__ - save last model!
03/23/2022 16:56:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 16:56:16 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 16:56:16 - INFO - __main__ - Printing 3 examples
03/23/2022 16:56:16 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 16:56:16 - INFO - __main__ - ['negative']
03/23/2022 16:56:16 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 16:56:16 - INFO - __main__ - ['negative']
03/23/2022 16:56:16 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 16:56:16 - INFO - __main__ - ['negative']
03/23/2022 16:56:16 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:56:17 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:56:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:56:18 - INFO - __main__ - Printing 3 examples
03/23/2022 16:56:18 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/23/2022 16:56:18 - INFO - __main__ - ['negative']
03/23/2022 16:56:18 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/23/2022 16:56:18 - INFO - __main__ - ['negative']
03/23/2022 16:56:18 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/23/2022 16:56:18 - INFO - __main__ - ['negative']
03/23/2022 16:56:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 16:56:18 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:56:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:56:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:56:18 - INFO - __main__ - Printing 3 examples
03/23/2022 16:56:18 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/23/2022 16:56:18 - INFO - __main__ - ['negative']
03/23/2022 16:56:18 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/23/2022 16:56:18 - INFO - __main__ - ['negative']
03/23/2022 16:56:18 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/23/2022 16:56:18 - INFO - __main__ - ['negative']
03/23/2022 16:56:18 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:56:18 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:56:18 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:56:18 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 16:56:33 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:56:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:56:34 - INFO - __main__ - Starting training!
03/23/2022 16:56:41 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_13_0.5_8_predictions.txt
03/23/2022 16:56:41 - INFO - __main__ - Classification-F1 on test data: 0.9247
03/23/2022 16:56:42 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.5, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9247380130233341
03/23/2022 16:56:42 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.4, bsz=8 ...
03/23/2022 16:56:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:56:43 - INFO - __main__ - Printing 3 examples
03/23/2022 16:56:43 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/23/2022 16:56:43 - INFO - __main__ - ['negative']
03/23/2022 16:56:43 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/23/2022 16:56:43 - INFO - __main__ - ['negative']
03/23/2022 16:56:43 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/23/2022 16:56:43 - INFO - __main__ - ['negative']
03/23/2022 16:56:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 16:56:43 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:56:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 16:56:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 16:56:43 - INFO - __main__ - Printing 3 examples
03/23/2022 16:56:43 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/23/2022 16:56:43 - INFO - __main__ - ['negative']
03/23/2022 16:56:43 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/23/2022 16:56:43 - INFO - __main__ - ['negative']
03/23/2022 16:56:43 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/23/2022 16:56:43 - INFO - __main__ - ['negative']
03/23/2022 16:56:43 - INFO - __main__ - Tokenizing Input ...
03/23/2022 16:56:43 - INFO - __main__ - Tokenizing Output ...
03/23/2022 16:56:43 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 16:57:01 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 16:57:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 16:57:02 - INFO - __main__ - Starting training!
03/23/2022 16:57:06 - INFO - __main__ - Step 10 Global step 10 Train loss 0.38 on epoch=4
03/23/2022 16:57:09 - INFO - __main__ - Step 20 Global step 20 Train loss 0.25 on epoch=9
03/23/2022 16:57:12 - INFO - __main__ - Step 30 Global step 30 Train loss 0.14 on epoch=14
03/23/2022 16:57:15 - INFO - __main__ - Step 40 Global step 40 Train loss 0.10 on epoch=19
03/23/2022 16:57:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.07 on epoch=24
03/23/2022 16:57:19 - INFO - __main__ - Global step 50 Train loss 0.19 Classification-F1 1.0 on epoch=24
03/23/2022 16:57:19 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 1.0 on epoch=24, global_step=50
03/23/2022 16:57:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.04 on epoch=29
03/23/2022 16:57:25 - INFO - __main__ - Step 70 Global step 70 Train loss 0.05 on epoch=34
03/23/2022 16:57:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.03 on epoch=39
03/23/2022 16:57:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.01 on epoch=44
03/23/2022 16:57:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.01 on epoch=49
03/23/2022 16:57:34 - INFO - __main__ - Global step 100 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=49
03/23/2022 16:57:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.01 on epoch=54
03/23/2022 16:57:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.01 on epoch=59
03/23/2022 16:57:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.01 on epoch=64
03/23/2022 16:57:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.01 on epoch=69
03/23/2022 16:57:50 - INFO - __main__ - Step 150 Global step 150 Train loss 0.01 on epoch=74
03/23/2022 16:57:50 - INFO - __main__ - Global step 150 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=74
03/23/2022 16:57:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.00 on epoch=79
03/23/2022 16:57:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.00 on epoch=84
03/23/2022 16:57:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.01 on epoch=89
03/23/2022 16:58:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.01 on epoch=94
03/23/2022 16:58:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.00 on epoch=99
03/23/2022 16:58:06 - INFO - __main__ - Global step 200 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=99
03/23/2022 16:58:09 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 16:58:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.00 on epoch=109
03/23/2022 16:58:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.01 on epoch=114
03/23/2022 16:58:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.00 on epoch=119
03/23/2022 16:58:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.00 on epoch=124
03/23/2022 16:58:22 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 16:58:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 16:58:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 16:58:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 16:58:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 16:58:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 16:58:38 - INFO - __main__ - Global step 300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 16:58:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 16:58:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 16:58:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 16:58:50 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 16:58:53 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 16:58:54 - INFO - __main__ - Global step 350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=174
03/23/2022 16:58:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 16:59:00 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 16:59:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 16:59:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 16:59:09 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 16:59:10 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=199
03/23/2022 16:59:13 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 16:59:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 16:59:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 16:59:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 16:59:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 16:59:26 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 16:59:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 16:59:32 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 16:59:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 16:59:38 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 16:59:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 16:59:41 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=249
03/23/2022 16:59:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 16:59:48 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 16:59:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 16:59:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 16:59:57 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 16:59:57 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 17:00:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 17:00:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 17:00:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 17:00:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 17:00:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 17:00:13 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 17:00:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 17:00:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 17:00:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 17:00:25 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 17:00:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 17:00:29 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 17:00:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 17:00:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 17:00:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 17:00:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 17:00:44 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 17:00:45 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.906158357771261 on epoch=349
03/23/2022 17:00:48 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 17:00:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 17:00:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 17:00:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 17:01:00 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 17:01:01 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=374
03/23/2022 17:01:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 17:01:07 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 17:01:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 17:01:13 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
03/23/2022 17:01:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 17:01:17 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=399
03/23/2022 17:01:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 17:01:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 17:01:26 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 17:01:29 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 17:01:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 17:01:33 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 17:01:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 17:01:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 17:01:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 17:01:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 17:01:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 17:01:48 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 17:01:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 17:01:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 17:01:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 17:02:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 17:02:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 17:02:04 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 17:02:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 17:02:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 17:02:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 17:02:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 17:02:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 17:02:20 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=499
03/23/2022 17:02:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 17:02:26 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 17:02:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 17:02:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 17:02:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 17:02:36 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=524
03/23/2022 17:02:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 17:02:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 17:02:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 17:02:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 17:02:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 17:02:52 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=549
03/23/2022 17:02:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 17:02:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 17:03:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 17:03:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 17:03:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 17:03:08 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=574
03/23/2022 17:03:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 17:03:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 17:03:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 17:03:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 17:03:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 17:03:24 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=599
03/23/2022 17:03:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 17:03:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 17:03:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 17:03:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 17:03:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
03/23/2022 17:03:39 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 17:03:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 17:03:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 17:03:48 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/23/2022 17:03:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 17:03:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 17:03:55 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 17:03:58 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 17:04:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 17:04:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 17:04:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 17:04:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 17:04:11 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 17:04:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 17:04:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 17:04:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 17:04:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 17:04:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 17:04:27 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 17:04:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 17:04:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 17:04:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 17:04:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 17:04:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 17:04:43 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 17:04:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 17:04:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 17:04:52 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 17:04:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 17:04:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 17:04:59 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 1.0 on epoch=749
03/23/2022 17:05:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 17:05:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 17:05:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 17:05:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 17:05:14 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 17:05:15 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/23/2022 17:05:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 17:05:21 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 17:05:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 17:05:27 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 17:05:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 17:05:30 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 17:05:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 17:05:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 17:05:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 17:05:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 17:05:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 17:05:46 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=824
03/23/2022 17:05:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 17:05:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 17:05:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 17:05:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 17:06:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 17:06:02 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=849
03/23/2022 17:06:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 17:06:08 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 17:06:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 17:06:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 17:06:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 17:06:18 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=874
03/23/2022 17:06:21 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 17:06:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 17:06:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 17:06:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 17:06:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 17:06:34 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 17:06:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 17:06:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 17:06:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 17:06:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 17:06:49 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 17:06:49 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=924
03/23/2022 17:06:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 17:06:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 17:06:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 17:07:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 17:07:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 17:07:05 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=949
03/23/2022 17:07:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 17:07:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 17:07:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 17:07:17 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 17:07:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 17:07:21 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/23/2022 17:07:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 17:07:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 17:07:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 17:07:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 17:07:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 17:07:37 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/23/2022 17:07:37 - INFO - __main__ - save last model!
03/23/2022 17:07:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 17:07:37 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 17:07:37 - INFO - __main__ - Printing 3 examples
03/23/2022 17:07:37 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 17:07:37 - INFO - __main__ - ['negative']
03/23/2022 17:07:37 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 17:07:37 - INFO - __main__ - ['negative']
03/23/2022 17:07:37 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 17:07:37 - INFO - __main__ - ['negative']
03/23/2022 17:07:37 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:07:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:07:37 - INFO - __main__ - Printing 3 examples
03/23/2022 17:07:37 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/23/2022 17:07:37 - INFO - __main__ - ['negative']
03/23/2022 17:07:37 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/23/2022 17:07:37 - INFO - __main__ - ['negative']
03/23/2022 17:07:37 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/23/2022 17:07:37 - INFO - __main__ - ['negative']
03/23/2022 17:07:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 17:07:37 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:07:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:07:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:07:37 - INFO - __main__ - Printing 3 examples
03/23/2022 17:07:37 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/23/2022 17:07:37 - INFO - __main__ - ['negative']
03/23/2022 17:07:37 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/23/2022 17:07:37 - INFO - __main__ - ['negative']
03/23/2022 17:07:37 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/23/2022 17:07:37 - INFO - __main__ - ['negative']
03/23/2022 17:07:37 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:07:37 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:07:37 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:07:37 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:07:38 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 17:07:52 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:07:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:07:53 - INFO - __main__ - Starting training!
03/23/2022 17:08:02 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_13_0.4_8_predictions.txt
03/23/2022 17:08:02 - INFO - __main__ - Classification-F1 on test data: 0.9318
03/23/2022 17:08:02 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.4, bsz=8, dev_performance=1.0, test_performance=0.9318156294620654
03/23/2022 17:08:02 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.3, bsz=8 ...
03/23/2022 17:08:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:08:03 - INFO - __main__ - Printing 3 examples
03/23/2022 17:08:03 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/23/2022 17:08:03 - INFO - __main__ - ['negative']
03/23/2022 17:08:03 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/23/2022 17:08:03 - INFO - __main__ - ['negative']
03/23/2022 17:08:03 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/23/2022 17:08:03 - INFO - __main__ - ['negative']
03/23/2022 17:08:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:08:03 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:08:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:08:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:08:03 - INFO - __main__ - Printing 3 examples
03/23/2022 17:08:03 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/23/2022 17:08:03 - INFO - __main__ - ['negative']
03/23/2022 17:08:03 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/23/2022 17:08:03 - INFO - __main__ - ['negative']
03/23/2022 17:08:03 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/23/2022 17:08:03 - INFO - __main__ - ['negative']
03/23/2022 17:08:03 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:08:03 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:08:03 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:08:22 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:08:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:08:23 - INFO - __main__ - Starting training!
03/23/2022 17:08:26 - INFO - __main__ - Step 10 Global step 10 Train loss 0.49 on epoch=4
03/23/2022 17:08:29 - INFO - __main__ - Step 20 Global step 20 Train loss 0.27 on epoch=9
03/23/2022 17:08:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.19 on epoch=14
03/23/2022 17:08:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.14 on epoch=19
03/23/2022 17:08:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.11 on epoch=24
03/23/2022 17:08:39 - INFO - __main__ - Global step 50 Train loss 0.24 Classification-F1 1.0 on epoch=24
03/23/2022 17:08:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 1.0 on epoch=24, global_step=50
03/23/2022 17:08:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.06 on epoch=29
03/23/2022 17:08:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.05 on epoch=34
03/23/2022 17:08:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.05 on epoch=39
03/23/2022 17:08:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.03 on epoch=44
03/23/2022 17:08:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.05 on epoch=49
03/23/2022 17:08:55 - INFO - __main__ - Global step 100 Train loss 0.05 Classification-F1 1.0 on epoch=49
03/23/2022 17:08:58 - INFO - __main__ - Step 110 Global step 110 Train loss 0.03 on epoch=54
03/23/2022 17:09:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.04 on epoch=59
03/23/2022 17:09:04 - INFO - __main__ - Step 130 Global step 130 Train loss 0.01 on epoch=64
03/23/2022 17:09:07 - INFO - __main__ - Step 140 Global step 140 Train loss 0.05 on epoch=69
03/23/2022 17:09:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.01 on epoch=74
03/23/2022 17:09:10 - INFO - __main__ - Global step 150 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 17:09:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.05 on epoch=79
03/23/2022 17:09:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.00 on epoch=84
03/23/2022 17:09:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.01 on epoch=89
03/23/2022 17:09:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.01 on epoch=94
03/23/2022 17:09:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.00 on epoch=99
03/23/2022 17:09:26 - INFO - __main__ - Global step 200 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=99
03/23/2022 17:09:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 17:09:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.01 on epoch=109
03/23/2022 17:09:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.00 on epoch=114
03/23/2022 17:09:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.01 on epoch=119
03/23/2022 17:09:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.00 on epoch=124
03/23/2022 17:09:41 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 17:09:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 17:09:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.01 on epoch=134
03/23/2022 17:09:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 17:09:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 17:09:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.03 on epoch=149
03/23/2022 17:09:57 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 17:10:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 17:10:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 17:10:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 17:10:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 17:10:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 17:10:13 - INFO - __main__ - Global step 350 Train loss 0.00 Classification-F1 1.0 on epoch=174
03/23/2022 17:10:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 17:10:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 17:10:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 17:10:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.01 on epoch=194
03/23/2022 17:10:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 17:10:28 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=199
03/23/2022 17:10:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 17:10:34 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 17:10:37 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 17:10:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 17:10:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 17:10:44 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 17:10:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 17:10:50 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 17:10:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 17:10:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 17:10:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 17:11:00 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=249
03/23/2022 17:11:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 17:11:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 17:11:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 17:11:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 17:11:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 17:11:15 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 17:11:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 17:11:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 17:11:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/23/2022 17:11:27 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 17:11:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 17:11:31 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 17:11:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 17:11:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 17:11:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/23/2022 17:11:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 17:11:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 17:11:47 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 1.0 on epoch=324
03/23/2022 17:11:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 17:11:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 17:11:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 17:11:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 17:12:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 17:12:03 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 17:12:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 17:12:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 17:12:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 17:12:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 17:12:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 17:12:18 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=374
03/23/2022 17:12:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 17:12:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/23/2022 17:12:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 17:12:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 17:12:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 17:12:34 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 1.0 on epoch=399
03/23/2022 17:12:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 17:12:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 17:12:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 17:12:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 17:12:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 17:12:50 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 17:12:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 17:12:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 17:12:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 17:13:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 17:13:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 17:13:05 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 17:13:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 17:13:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 17:13:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 17:13:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 17:13:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 17:13:21 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 17:13:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 17:13:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 17:13:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
03/23/2022 17:13:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 17:13:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 17:13:37 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=499
03/23/2022 17:13:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 17:13:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 17:13:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 17:13:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 17:13:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 17:13:52 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/23/2022 17:13:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 17:13:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 17:14:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 17:14:05 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 17:14:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 17:14:08 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/23/2022 17:14:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 17:14:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 17:14:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 17:14:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 17:14:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 17:14:24 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 17:14:27 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 17:14:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 17:14:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 17:14:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 17:14:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 17:14:40 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=599
03/23/2022 17:14:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 17:14:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 17:14:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 17:14:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 17:14:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 17:14:55 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 17:14:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 17:15:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 17:15:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 17:15:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 17:15:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 17:15:11 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=649
03/23/2022 17:15:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 17:15:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 17:15:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 17:15:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 17:15:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 17:15:27 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 17:15:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 17:15:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 17:15:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 17:15:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 17:15:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 17:15:42 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 17:15:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 17:15:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 17:15:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 17:15:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 17:15:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 17:15:58 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 17:16:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 17:16:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 17:16:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 17:16:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 17:16:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 17:16:14 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/23/2022 17:16:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 17:16:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 17:16:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 17:16:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 17:16:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 17:16:29 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=774
03/23/2022 17:16:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 17:16:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 17:16:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 17:16:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 17:16:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 17:16:45 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 17:16:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 17:16:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 17:16:54 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 17:16:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 17:17:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 17:17:01 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/23/2022 17:17:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 17:17:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 17:17:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 17:17:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 17:17:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 17:17:16 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/23/2022 17:17:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 17:17:22 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 17:17:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 17:17:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 17:17:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 17:17:32 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/23/2022 17:17:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 17:17:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 17:17:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 17:17:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 17:17:47 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 17:17:48 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=899
03/23/2022 17:17:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 17:17:54 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 17:17:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 17:18:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=919
03/23/2022 17:18:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 17:18:04 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=924
03/23/2022 17:18:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 17:18:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 17:18:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 17:18:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 17:18:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 17:18:19 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=949
03/23/2022 17:18:22 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 17:18:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 17:18:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 17:18:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 17:18:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 17:18:35 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=974
03/23/2022 17:18:38 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 17:18:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 17:18:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 17:18:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 17:18:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 17:18:50 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=999
03/23/2022 17:18:50 - INFO - __main__ - save last model!
03/23/2022 17:18:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 17:18:51 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 17:18:51 - INFO - __main__ - Printing 3 examples
03/23/2022 17:18:51 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 17:18:51 - INFO - __main__ - ['negative']
03/23/2022 17:18:51 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 17:18:51 - INFO - __main__ - ['negative']
03/23/2022 17:18:51 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 17:18:51 - INFO - __main__ - ['negative']
03/23/2022 17:18:51 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:18:51 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:18:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:18:52 - INFO - __main__ - Printing 3 examples
03/23/2022 17:18:52 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/23/2022 17:18:52 - INFO - __main__ - ['negative']
03/23/2022 17:18:52 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/23/2022 17:18:52 - INFO - __main__ - ['negative']
03/23/2022 17:18:52 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/23/2022 17:18:52 - INFO - __main__ - ['negative']
03/23/2022 17:18:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 17:18:52 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:18:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:18:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:18:52 - INFO - __main__ - Printing 3 examples
03/23/2022 17:18:52 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/23/2022 17:18:52 - INFO - __main__ - ['negative']
03/23/2022 17:18:52 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/23/2022 17:18:52 - INFO - __main__ - ['negative']
03/23/2022 17:18:52 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/23/2022 17:18:52 - INFO - __main__ - ['negative']
03/23/2022 17:18:52 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:18:52 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:18:52 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:18:52 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 17:19:10 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:19:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:19:11 - INFO - __main__ - Starting training!
03/23/2022 17:19:15 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_13_0.3_8_predictions.txt
03/23/2022 17:19:15 - INFO - __main__ - Classification-F1 on test data: 0.9419
03/23/2022 17:19:17 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.3, bsz=8, dev_performance=1.0, test_performance=0.9419247344558548
03/23/2022 17:19:17 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.2, bsz=8 ...
03/23/2022 17:19:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:19:18 - INFO - __main__ - Printing 3 examples
03/23/2022 17:19:18 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/23/2022 17:19:18 - INFO - __main__ - ['negative']
03/23/2022 17:19:18 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/23/2022 17:19:18 - INFO - __main__ - ['negative']
03/23/2022 17:19:18 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/23/2022 17:19:18 - INFO - __main__ - ['negative']
03/23/2022 17:19:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:19:18 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:19:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:19:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:19:18 - INFO - __main__ - Printing 3 examples
03/23/2022 17:19:18 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/23/2022 17:19:18 - INFO - __main__ - ['negative']
03/23/2022 17:19:18 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/23/2022 17:19:18 - INFO - __main__ - ['negative']
03/23/2022 17:19:18 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/23/2022 17:19:18 - INFO - __main__ - ['negative']
03/23/2022 17:19:18 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:19:18 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:19:18 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:19:33 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:19:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:19:33 - INFO - __main__ - Starting training!
03/23/2022 17:19:37 - INFO - __main__ - Step 10 Global step 10 Train loss 0.56 on epoch=4
03/23/2022 17:19:40 - INFO - __main__ - Step 20 Global step 20 Train loss 0.33 on epoch=9
03/23/2022 17:19:43 - INFO - __main__ - Step 30 Global step 30 Train loss 0.26 on epoch=14
03/23/2022 17:19:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.18 on epoch=19
03/23/2022 17:19:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.17 on epoch=24
03/23/2022 17:19:49 - INFO - __main__ - Global step 50 Train loss 0.30 Classification-F1 0.9054187192118226 on epoch=24
03/23/2022 17:19:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9054187192118226 on epoch=24, global_step=50
03/23/2022 17:19:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.14 on epoch=29
03/23/2022 17:19:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.09 on epoch=34
03/23/2022 17:19:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.09 on epoch=39
03/23/2022 17:20:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.07 on epoch=44
03/23/2022 17:20:04 - INFO - __main__ - Step 100 Global step 100 Train loss 0.06 on epoch=49
03/23/2022 17:20:05 - INFO - __main__ - Global step 100 Train loss 0.09 Classification-F1 1.0 on epoch=49
03/23/2022 17:20:05 - INFO - __main__ - Saving model with best Classification-F1: 0.9054187192118226 -> 1.0 on epoch=49, global_step=100
03/23/2022 17:20:08 - INFO - __main__ - Step 110 Global step 110 Train loss 0.04 on epoch=54
03/23/2022 17:20:11 - INFO - __main__ - Step 120 Global step 120 Train loss 0.02 on epoch=59
03/23/2022 17:20:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.02 on epoch=64
03/23/2022 17:20:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.02 on epoch=69
03/23/2022 17:20:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.02 on epoch=74
03/23/2022 17:20:20 - INFO - __main__ - Global step 150 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 17:20:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.02 on epoch=79
03/23/2022 17:20:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.01 on epoch=84
03/23/2022 17:20:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.02 on epoch=89
03/23/2022 17:20:32 - INFO - __main__ - Step 190 Global step 190 Train loss 0.01 on epoch=94
03/23/2022 17:20:35 - INFO - __main__ - Step 200 Global step 200 Train loss 0.01 on epoch=99
03/23/2022 17:20:36 - INFO - __main__ - Global step 200 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=99
03/23/2022 17:20:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.03 on epoch=104
03/23/2022 17:20:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.01 on epoch=109
03/23/2022 17:20:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.01 on epoch=114
03/23/2022 17:20:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.01 on epoch=119
03/23/2022 17:20:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.01 on epoch=124
03/23/2022 17:20:51 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 17:20:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 17:20:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.01 on epoch=134
03/23/2022 17:21:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 17:21:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 17:21:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.01 on epoch=149
03/23/2022 17:21:06 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 17:21:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 17:21:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.01 on epoch=159
03/23/2022 17:21:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 17:21:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 17:21:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.01 on epoch=174
03/23/2022 17:21:22 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=174
03/23/2022 17:21:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 17:21:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.01 on epoch=184
03/23/2022 17:21:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 17:21:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 17:21:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.01 on epoch=199
03/23/2022 17:21:37 - INFO - __main__ - Global step 400 Train loss 0.01 Classification-F1 0.9375 on epoch=199
03/23/2022 17:21:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 17:21:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 17:21:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 17:21:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 17:21:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 17:21:53 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 17:21:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 17:21:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 17:22:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 17:22:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 17:22:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 17:22:09 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=249
03/23/2022 17:22:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 17:22:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 17:22:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 17:22:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 17:22:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 17:22:24 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 17:22:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 17:22:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 17:22:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 17:22:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 17:22:39 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 17:22:40 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 17:22:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 17:22:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 17:22:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 17:22:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 17:22:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 17:22:55 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 17:22:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 17:23:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
03/23/2022 17:23:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 17:23:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 17:23:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 17:23:10 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 17:23:13 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 17:23:16 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 17:23:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 17:23:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 17:23:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/23/2022 17:23:26 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=374
03/23/2022 17:23:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 17:23:32 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 17:23:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 17:23:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 17:23:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 17:23:41 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9375 on epoch=399
03/23/2022 17:23:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 17:23:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 17:23:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 17:23:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 17:23:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 17:23:57 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9375 on epoch=424
03/23/2022 17:24:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 17:24:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 17:24:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 17:24:09 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 17:24:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 17:24:12 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 17:24:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 17:24:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 17:24:21 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 17:24:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 17:24:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 17:24:28 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9375 on epoch=474
03/23/2022 17:24:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 17:24:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 17:24:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 17:24:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 17:24:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 17:24:43 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9375 on epoch=499
03/23/2022 17:24:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 17:24:49 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 17:24:52 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 17:24:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 17:24:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 17:24:58 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/23/2022 17:25:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 17:25:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 17:25:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 17:25:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 17:25:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 17:25:14 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/23/2022 17:25:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 17:25:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 17:25:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 17:25:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 17:25:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
03/23/2022 17:25:29 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 17:25:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 17:25:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 17:25:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 17:25:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 17:25:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 17:25:45 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/23/2022 17:25:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 17:25:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 17:25:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 17:25:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 17:25:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 17:26:00 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=624
03/23/2022 17:26:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 17:26:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 17:26:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 17:26:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 17:26:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 17:26:15 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 17:26:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 17:26:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 17:26:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 17:26:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 17:26:30 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 17:26:31 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 17:26:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 17:26:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 17:26:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 17:26:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 17:26:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/23/2022 17:26:46 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 17:26:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 17:26:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 17:26:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 17:26:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 17:27:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 17:27:01 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 17:27:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 17:27:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 17:27:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 17:27:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 17:27:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 17:27:17 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.906158357771261 on epoch=749
03/23/2022 17:27:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 17:27:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 17:27:26 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 17:27:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 17:27:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 17:27:33 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/23/2022 17:27:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 17:27:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 17:27:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 17:27:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 17:27:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 17:27:48 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 17:27:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 17:27:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 17:27:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 17:28:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 17:28:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 17:28:04 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/23/2022 17:28:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 17:28:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 17:28:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 17:28:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 17:28:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 17:28:20 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9375 on epoch=849
03/23/2022 17:28:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 17:28:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 17:28:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 17:28:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 17:28:35 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 17:28:36 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/23/2022 17:28:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 17:28:42 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 17:28:45 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 17:28:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 17:28:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 17:28:52 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 17:28:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 17:28:58 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 17:29:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 17:29:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 17:29:07 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 17:29:08 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/23/2022 17:29:11 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 17:29:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 17:29:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 17:29:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 17:29:23 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 17:29:23 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/23/2022 17:29:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 17:29:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 17:29:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 17:29:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 17:29:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 17:29:39 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/23/2022 17:29:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 17:29:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 17:29:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 17:29:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 17:29:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 17:29:55 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/23/2022 17:29:55 - INFO - __main__ - save last model!
03/23/2022 17:29:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 17:29:55 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 17:29:55 - INFO - __main__ - Printing 3 examples
03/23/2022 17:29:55 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 17:29:55 - INFO - __main__ - ['negative']
03/23/2022 17:29:55 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 17:29:55 - INFO - __main__ - ['negative']
03/23/2022 17:29:55 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 17:29:55 - INFO - __main__ - ['negative']
03/23/2022 17:29:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:29:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:29:56 - INFO - __main__ - Printing 3 examples
03/23/2022 17:29:56 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/23/2022 17:29:56 - INFO - __main__ - ['positive']
03/23/2022 17:29:56 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/23/2022 17:29:56 - INFO - __main__ - ['positive']
03/23/2022 17:29:56 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/23/2022 17:29:56 - INFO - __main__ - ['positive']
03/23/2022 17:29:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 17:29:56 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:29:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:29:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:29:56 - INFO - __main__ - Printing 3 examples
03/23/2022 17:29:56 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/23/2022 17:29:56 - INFO - __main__ - ['positive']
03/23/2022 17:29:56 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/23/2022 17:29:56 - INFO - __main__ - ['positive']
03/23/2022 17:29:56 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/23/2022 17:29:56 - INFO - __main__ - ['positive']
03/23/2022 17:29:56 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:29:56 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:29:56 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:29:56 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:29:57 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 17:30:14 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:30:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:30:15 - INFO - __main__ - Starting training!
03/23/2022 17:30:20 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_13_0.2_8_predictions.txt
03/23/2022 17:30:20 - INFO - __main__ - Classification-F1 on test data: 0.9399
03/23/2022 17:30:21 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.2, bsz=8, dev_performance=1.0, test_performance=0.9399132347109226
03/23/2022 17:30:21 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.5, bsz=8 ...
03/23/2022 17:30:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:30:22 - INFO - __main__ - Printing 3 examples
03/23/2022 17:30:22 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/23/2022 17:30:22 - INFO - __main__ - ['positive']
03/23/2022 17:30:22 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/23/2022 17:30:22 - INFO - __main__ - ['positive']
03/23/2022 17:30:22 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/23/2022 17:30:22 - INFO - __main__ - ['positive']
03/23/2022 17:30:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:30:22 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:30:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:30:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:30:22 - INFO - __main__ - Printing 3 examples
03/23/2022 17:30:22 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/23/2022 17:30:22 - INFO - __main__ - ['positive']
03/23/2022 17:30:22 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/23/2022 17:30:22 - INFO - __main__ - ['positive']
03/23/2022 17:30:22 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/23/2022 17:30:22 - INFO - __main__ - ['positive']
03/23/2022 17:30:22 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:30:22 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:30:22 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:30:37 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:30:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:30:37 - INFO - __main__ - Starting training!
03/23/2022 17:30:41 - INFO - __main__ - Step 10 Global step 10 Train loss 0.52 on epoch=4
03/23/2022 17:30:44 - INFO - __main__ - Step 20 Global step 20 Train loss 0.28 on epoch=9
03/23/2022 17:30:47 - INFO - __main__ - Step 30 Global step 30 Train loss 0.20 on epoch=14
03/23/2022 17:30:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.15 on epoch=19
03/23/2022 17:30:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.10 on epoch=24
03/23/2022 17:30:54 - INFO - __main__ - Global step 50 Train loss 0.25 Classification-F1 0.9687194525904204 on epoch=24
03/23/2022 17:30:54 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9687194525904204 on epoch=24, global_step=50
03/23/2022 17:30:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.05 on epoch=29
03/23/2022 17:30:59 - INFO - __main__ - Step 70 Global step 70 Train loss 0.06 on epoch=34
03/23/2022 17:31:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.03 on epoch=39
03/23/2022 17:31:05 - INFO - __main__ - Step 90 Global step 90 Train loss 0.06 on epoch=44
03/23/2022 17:31:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.01 on epoch=49
03/23/2022 17:31:09 - INFO - __main__ - Global step 100 Train loss 0.04 Classification-F1 0.9375 on epoch=49
03/23/2022 17:31:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.01 on epoch=54
03/23/2022 17:31:15 - INFO - __main__ - Step 120 Global step 120 Train loss 0.01 on epoch=59
03/23/2022 17:31:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.01 on epoch=64
03/23/2022 17:31:21 - INFO - __main__ - Step 140 Global step 140 Train loss 0.02 on epoch=69
03/23/2022 17:31:24 - INFO - __main__ - Step 150 Global step 150 Train loss 0.01 on epoch=74
03/23/2022 17:31:25 - INFO - __main__ - Global step 150 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=74
03/23/2022 17:31:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.00 on epoch=79
03/23/2022 17:31:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.04 on epoch=84
03/23/2022 17:31:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.01 on epoch=89
03/23/2022 17:31:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.00 on epoch=94
03/23/2022 17:31:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.00 on epoch=99
03/23/2022 17:31:40 - INFO - __main__ - Global step 200 Train loss 0.01 Classification-F1 0.9375 on epoch=99
03/23/2022 17:31:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.00 on epoch=104
03/23/2022 17:31:46 - INFO - __main__ - Step 220 Global step 220 Train loss 0.00 on epoch=109
03/23/2022 17:31:49 - INFO - __main__ - Step 230 Global step 230 Train loss 0.00 on epoch=114
03/23/2022 17:31:52 - INFO - __main__ - Step 240 Global step 240 Train loss 0.00 on epoch=119
03/23/2022 17:31:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.04 on epoch=124
03/23/2022 17:31:56 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=124
03/23/2022 17:31:59 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 17:32:02 - INFO - __main__ - Step 270 Global step 270 Train loss 0.01 on epoch=134
03/23/2022 17:32:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 17:32:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 17:32:11 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 17:32:12 - INFO - __main__ - Global step 300 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=149
03/23/2022 17:32:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 17:32:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 17:32:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.01 on epoch=164
03/23/2022 17:32:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 17:32:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 17:32:28 - INFO - __main__ - Global step 350 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=174
03/23/2022 17:32:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 17:32:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 17:32:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 17:32:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 17:32:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 17:32:43 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=199
03/23/2022 17:32:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 17:32:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 17:32:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 17:32:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 17:32:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 17:32:59 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=224
03/23/2022 17:33:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 17:33:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 17:33:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 17:33:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 17:33:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 17:33:15 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=249
03/23/2022 17:33:18 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 17:33:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 17:33:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 17:33:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 17:33:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 17:33:31 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=274
03/23/2022 17:33:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
03/23/2022 17:33:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 17:33:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 17:33:43 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 17:33:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 17:33:46 - INFO - __main__ - Global step 600 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=299
03/23/2022 17:33:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 17:33:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 17:33:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 17:33:58 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 17:34:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 17:34:02 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=324
03/23/2022 17:34:05 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 17:34:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 17:34:11 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 17:34:14 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
03/23/2022 17:34:17 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 17:34:18 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=349
03/23/2022 17:34:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 17:34:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
03/23/2022 17:34:27 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 17:34:30 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 17:34:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 17:34:34 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=374
03/23/2022 17:34:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 17:34:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 17:34:43 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 17:34:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 17:34:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 17:34:50 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=399
03/23/2022 17:34:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 17:34:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/23/2022 17:34:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 17:35:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 17:35:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 17:35:06 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 17:35:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 17:35:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 17:35:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 17:35:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 17:35:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 17:35:22 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=449
03/23/2022 17:35:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 17:35:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 17:35:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 17:35:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 17:35:37 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 17:35:37 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 17:35:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 17:35:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 17:35:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 17:35:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 17:35:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 17:35:53 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=499
03/23/2022 17:35:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 17:35:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 17:36:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 17:36:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 17:36:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 17:36:09 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=524
03/23/2022 17:36:12 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 17:36:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 17:36:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 17:36:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 17:36:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 17:36:26 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=549
03/23/2022 17:36:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 17:36:32 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 17:36:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 17:36:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 17:36:41 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 17:36:42 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=574
03/23/2022 17:36:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 17:36:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 17:36:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=589
03/23/2022 17:36:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 17:36:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 17:36:58 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.9372549019607843 on epoch=599
03/23/2022 17:37:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 17:37:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 17:37:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 17:37:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 17:37:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 17:37:14 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 17:37:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 17:37:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 17:37:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 17:37:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 17:37:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 17:37:30 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 17:37:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 17:37:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 17:37:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 17:37:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 17:37:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 17:37:46 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=674
03/23/2022 17:37:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 17:37:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 17:37:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 17:37:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 17:38:02 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
03/23/2022 17:38:03 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 17:38:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 17:38:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 17:38:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 17:38:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 17:38:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 17:38:19 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 17:38:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 17:38:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 17:38:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 17:38:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 17:38:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 17:38:35 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=749
03/23/2022 17:38:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 17:38:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 17:38:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 17:38:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 17:38:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 17:38:51 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=774
03/23/2022 17:38:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 17:38:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 17:39:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 17:39:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 17:39:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 17:39:07 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=799
03/23/2022 17:39:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 17:39:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 17:39:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 17:39:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 17:39:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 17:39:23 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=824
03/23/2022 17:39:26 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 17:39:29 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 17:39:32 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 17:39:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 17:39:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 17:39:39 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=849
03/23/2022 17:39:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 17:39:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 17:39:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 17:39:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 17:39:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 17:39:55 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=874
03/23/2022 17:39:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 17:40:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 17:40:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 17:40:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 17:40:10 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 17:40:11 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 17:40:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 17:40:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 17:40:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 17:40:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 17:40:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 17:40:28 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/23/2022 17:40:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 17:40:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 17:40:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 17:40:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 17:40:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 17:40:44 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/23/2022 17:40:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 17:40:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 17:40:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 17:40:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 17:40:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 17:41:00 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=974
03/23/2022 17:41:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 17:41:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 17:41:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 17:41:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 17:41:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 17:41:16 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=999
03/23/2022 17:41:16 - INFO - __main__ - save last model!
03/23/2022 17:41:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 17:41:16 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 17:41:16 - INFO - __main__ - Printing 3 examples
03/23/2022 17:41:16 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 17:41:16 - INFO - __main__ - ['negative']
03/23/2022 17:41:16 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 17:41:16 - INFO - __main__ - ['negative']
03/23/2022 17:41:16 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 17:41:16 - INFO - __main__ - ['negative']
03/23/2022 17:41:16 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:41:17 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:41:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:41:17 - INFO - __main__ - Printing 3 examples
03/23/2022 17:41:17 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/23/2022 17:41:17 - INFO - __main__ - ['positive']
03/23/2022 17:41:17 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/23/2022 17:41:17 - INFO - __main__ - ['positive']
03/23/2022 17:41:17 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/23/2022 17:41:17 - INFO - __main__ - ['positive']
03/23/2022 17:41:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 17:41:17 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:41:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:41:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:41:17 - INFO - __main__ - Printing 3 examples
03/23/2022 17:41:17 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/23/2022 17:41:17 - INFO - __main__ - ['positive']
03/23/2022 17:41:17 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/23/2022 17:41:17 - INFO - __main__ - ['positive']
03/23/2022 17:41:17 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/23/2022 17:41:17 - INFO - __main__ - ['positive']
03/23/2022 17:41:17 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:41:17 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:41:17 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:41:18 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 17:41:36 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:41:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:41:36 - INFO - __main__ - Starting training!
03/23/2022 17:41:51 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_21_0.5_8_predictions.txt
03/23/2022 17:41:51 - INFO - __main__ - Classification-F1 on test data: 0.9420
03/23/2022 17:41:51 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.5, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9419544923219805
03/23/2022 17:41:51 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.4, bsz=8 ...
03/23/2022 17:41:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:41:52 - INFO - __main__ - Printing 3 examples
03/23/2022 17:41:52 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/23/2022 17:41:52 - INFO - __main__ - ['positive']
03/23/2022 17:41:52 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/23/2022 17:41:52 - INFO - __main__ - ['positive']
03/23/2022 17:41:52 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/23/2022 17:41:52 - INFO - __main__ - ['positive']
03/23/2022 17:41:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:41:52 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:41:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:41:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:41:52 - INFO - __main__ - Printing 3 examples
03/23/2022 17:41:52 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/23/2022 17:41:52 - INFO - __main__ - ['positive']
03/23/2022 17:41:52 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/23/2022 17:41:52 - INFO - __main__ - ['positive']
03/23/2022 17:41:52 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/23/2022 17:41:52 - INFO - __main__ - ['positive']
03/23/2022 17:41:52 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:41:52 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:41:52 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:42:11 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:42:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:42:12 - INFO - __main__ - Starting training!
03/23/2022 17:42:16 - INFO - __main__ - Step 10 Global step 10 Train loss 0.61 on epoch=4
03/23/2022 17:42:19 - INFO - __main__ - Step 20 Global step 20 Train loss 0.28 on epoch=9
03/23/2022 17:42:22 - INFO - __main__ - Step 30 Global step 30 Train loss 0.21 on epoch=14
03/23/2022 17:42:25 - INFO - __main__ - Step 40 Global step 40 Train loss 0.13 on epoch=19
03/23/2022 17:42:28 - INFO - __main__ - Step 50 Global step 50 Train loss 0.13 on epoch=24
03/23/2022 17:42:29 - INFO - __main__ - Global step 50 Train loss 0.27 Classification-F1 0.906158357771261 on epoch=24
03/23/2022 17:42:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.906158357771261 on epoch=24, global_step=50
03/23/2022 17:42:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.10 on epoch=29
03/23/2022 17:42:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.05 on epoch=34
03/23/2022 17:42:38 - INFO - __main__ - Step 80 Global step 80 Train loss 0.05 on epoch=39
03/23/2022 17:42:41 - INFO - __main__ - Step 90 Global step 90 Train loss 0.02 on epoch=44
03/23/2022 17:42:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.04 on epoch=49
03/23/2022 17:42:45 - INFO - __main__ - Global step 100 Train loss 0.05 Classification-F1 0.9372549019607843 on epoch=49
03/23/2022 17:42:45 - INFO - __main__ - Saving model with best Classification-F1: 0.906158357771261 -> 0.9372549019607843 on epoch=49, global_step=100
03/23/2022 17:42:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.01 on epoch=54
03/23/2022 17:42:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.04 on epoch=59
03/23/2022 17:42:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.01 on epoch=64
03/23/2022 17:42:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.01 on epoch=69
03/23/2022 17:43:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.00 on epoch=74
03/23/2022 17:43:00 - INFO - __main__ - Global step 150 Train loss 0.02 Classification-F1 0.9372549019607843 on epoch=74
03/23/2022 17:43:03 - INFO - __main__ - Step 160 Global step 160 Train loss 0.00 on epoch=79
03/23/2022 17:43:06 - INFO - __main__ - Step 170 Global step 170 Train loss 0.01 on epoch=84
03/23/2022 17:43:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.00 on epoch=89
03/23/2022 17:43:12 - INFO - __main__ - Step 190 Global step 190 Train loss 0.00 on epoch=94
03/23/2022 17:43:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.00 on epoch=99
03/23/2022 17:43:16 - INFO - __main__ - Global step 200 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=99
03/23/2022 17:43:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 17:43:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.04 on epoch=109
03/23/2022 17:43:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.01 on epoch=114
03/23/2022 17:43:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.02 on epoch=119
03/23/2022 17:43:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.00 on epoch=124
03/23/2022 17:43:32 - INFO - __main__ - Global step 250 Train loss 0.02 Classification-F1 0.9372549019607843 on epoch=124
03/23/2022 17:43:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 17:43:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.01 on epoch=134
03/23/2022 17:43:41 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 17:43:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 17:43:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 17:43:48 - INFO - __main__ - Global step 300 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=149
03/23/2022 17:43:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 17:43:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.01 on epoch=159
03/23/2022 17:43:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 17:44:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 17:44:03 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 17:44:04 - INFO - __main__ - Global step 350 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=174
03/23/2022 17:44:07 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 17:44:10 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 17:44:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 17:44:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 17:44:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 17:44:20 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=199
03/23/2022 17:44:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 17:44:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 17:44:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 17:44:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 17:44:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 17:44:35 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=224
03/23/2022 17:44:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 17:44:41 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 17:44:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 17:44:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 17:44:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 17:44:51 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=249
03/23/2022 17:44:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 17:44:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 17:45:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 17:45:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 17:45:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 17:45:07 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=274
03/23/2022 17:45:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 17:45:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 17:45:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 17:45:19 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 17:45:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 17:45:23 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=299
03/23/2022 17:45:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 17:45:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 17:45:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 17:45:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
03/23/2022 17:45:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 17:45:39 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=324
03/23/2022 17:45:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 17:45:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 17:45:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 17:45:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 17:45:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 17:45:54 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=349
03/23/2022 17:45:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 17:46:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 17:46:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 17:46:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
03/23/2022 17:46:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 17:46:10 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.906158357771261 on epoch=374
03/23/2022 17:46:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 17:46:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/23/2022 17:46:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 17:46:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 17:46:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 17:46:26 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=399
03/23/2022 17:46:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 17:46:32 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 17:46:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 17:46:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 17:46:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 17:46:41 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=424
03/23/2022 17:46:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 17:46:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 17:46:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 17:46:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 17:46:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 17:46:57 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=449
03/23/2022 17:47:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 17:47:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 17:47:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 17:47:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 17:47:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 17:47:13 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=474
03/23/2022 17:47:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 17:47:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 17:47:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 17:47:25 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 17:47:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 17:47:29 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=499
03/23/2022 17:47:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 17:47:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 17:47:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 17:47:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 17:47:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 17:47:44 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=524
03/23/2022 17:47:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 17:47:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 17:47:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 17:47:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 17:47:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 17:48:00 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=549
03/23/2022 17:48:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
03/23/2022 17:48:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 17:48:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 17:48:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 17:48:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 17:48:16 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=574
03/23/2022 17:48:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 17:48:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 17:48:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 17:48:28 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 17:48:31 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 17:48:32 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=599
03/23/2022 17:48:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 17:48:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 17:48:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 17:48:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 17:48:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 17:48:47 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=624
03/23/2022 17:48:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
03/23/2022 17:48:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 17:48:56 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 17:48:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 17:49:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 17:49:03 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=649
03/23/2022 17:49:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 17:49:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 17:49:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
03/23/2022 17:49:15 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 17:49:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 17:49:19 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=674
03/23/2022 17:49:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 17:49:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 17:49:28 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 17:49:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 17:49:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 17:49:35 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=699
03/23/2022 17:49:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 17:49:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 17:49:44 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 17:49:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 17:49:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 17:49:50 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=724
03/23/2022 17:49:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 17:49:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 17:49:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 17:50:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 17:50:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 17:50:06 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=749
03/23/2022 17:50:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 17:50:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 17:50:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 17:50:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 17:50:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 17:50:22 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=774
03/23/2022 17:50:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 17:50:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 17:50:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 17:50:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 17:50:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 17:50:38 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=799
03/23/2022 17:50:41 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 17:50:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 17:50:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 17:50:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 17:50:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 17:50:53 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=824
03/23/2022 17:50:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 17:50:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 17:51:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 17:51:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 17:51:08 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 17:51:09 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=849
03/23/2022 17:51:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 17:51:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 17:51:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 17:51:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 17:51:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 17:51:25 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=874
03/23/2022 17:51:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 17:51:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 17:51:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 17:51:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 17:51:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 17:51:40 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=899
03/23/2022 17:51:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 17:51:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 17:51:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 17:51:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 17:51:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 17:51:56 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=924
03/23/2022 17:51:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 17:52:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 17:52:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 17:52:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 17:52:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 17:52:11 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=949
03/23/2022 17:52:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 17:52:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 17:52:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 17:52:23 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 17:52:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 17:52:27 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=974
03/23/2022 17:52:30 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 17:52:33 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 17:52:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 17:52:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 17:52:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 17:52:43 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=999
03/23/2022 17:52:43 - INFO - __main__ - save last model!
03/23/2022 17:52:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 17:52:43 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 17:52:43 - INFO - __main__ - Printing 3 examples
03/23/2022 17:52:43 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 17:52:43 - INFO - __main__ - ['negative']
03/23/2022 17:52:43 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 17:52:43 - INFO - __main__ - ['negative']
03/23/2022 17:52:43 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 17:52:43 - INFO - __main__ - ['negative']
03/23/2022 17:52:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:52:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:52:43 - INFO - __main__ - Printing 3 examples
03/23/2022 17:52:43 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/23/2022 17:52:43 - INFO - __main__ - ['positive']
03/23/2022 17:52:43 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/23/2022 17:52:43 - INFO - __main__ - ['positive']
03/23/2022 17:52:43 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/23/2022 17:52:43 - INFO - __main__ - ['positive']
03/23/2022 17:52:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 17:52:43 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:52:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:52:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:52:43 - INFO - __main__ - Printing 3 examples
03/23/2022 17:52:43 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/23/2022 17:52:43 - INFO - __main__ - ['positive']
03/23/2022 17:52:43 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/23/2022 17:52:43 - INFO - __main__ - ['positive']
03/23/2022 17:52:43 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/23/2022 17:52:43 - INFO - __main__ - ['positive']
03/23/2022 17:52:43 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:52:43 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:52:43 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:52:43 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:52:44 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 17:52:59 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:52:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:52:59 - INFO - __main__ - Starting training!
03/23/2022 17:53:08 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_21_0.4_8_predictions.txt
03/23/2022 17:53:08 - INFO - __main__ - Classification-F1 on test data: 0.9410
03/23/2022 17:53:08 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.4, bsz=8, dev_performance=0.9372549019607843, test_performance=0.9409503392352969
03/23/2022 17:53:08 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.3, bsz=8 ...
03/23/2022 17:53:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:53:09 - INFO - __main__ - Printing 3 examples
03/23/2022 17:53:09 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/23/2022 17:53:09 - INFO - __main__ - ['positive']
03/23/2022 17:53:09 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/23/2022 17:53:09 - INFO - __main__ - ['positive']
03/23/2022 17:53:09 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/23/2022 17:53:09 - INFO - __main__ - ['positive']
03/23/2022 17:53:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 17:53:09 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:53:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 17:53:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 17:53:09 - INFO - __main__ - Printing 3 examples
03/23/2022 17:53:09 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/23/2022 17:53:09 - INFO - __main__ - ['positive']
03/23/2022 17:53:09 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/23/2022 17:53:09 - INFO - __main__ - ['positive']
03/23/2022 17:53:09 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/23/2022 17:53:09 - INFO - __main__ - ['positive']
03/23/2022 17:53:09 - INFO - __main__ - Tokenizing Input ...
03/23/2022 17:53:09 - INFO - __main__ - Tokenizing Output ...
03/23/2022 17:53:09 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 17:53:25 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 17:53:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 17:53:25 - INFO - __main__ - Starting training!
03/23/2022 17:53:29 - INFO - __main__ - Step 10 Global step 10 Train loss 0.62 on epoch=4
03/23/2022 17:53:32 - INFO - __main__ - Step 20 Global step 20 Train loss 0.34 on epoch=9
03/23/2022 17:53:35 - INFO - __main__ - Step 30 Global step 30 Train loss 0.22 on epoch=14
03/23/2022 17:53:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.22 on epoch=19
03/23/2022 17:53:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.13 on epoch=24
03/23/2022 17:53:41 - INFO - __main__ - Global step 50 Train loss 0.30 Classification-F1 0.906158357771261 on epoch=24
03/23/2022 17:53:41 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.906158357771261 on epoch=24, global_step=50
03/23/2022 17:53:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.13 on epoch=29
03/23/2022 17:53:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.07 on epoch=34
03/23/2022 17:53:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.05 on epoch=39
03/23/2022 17:53:53 - INFO - __main__ - Step 90 Global step 90 Train loss 0.04 on epoch=44
03/23/2022 17:53:56 - INFO - __main__ - Step 100 Global step 100 Train loss 0.05 on epoch=49
03/23/2022 17:53:57 - INFO - __main__ - Global step 100 Train loss 0.07 Classification-F1 0.9372549019607843 on epoch=49
03/23/2022 17:53:57 - INFO - __main__ - Saving model with best Classification-F1: 0.906158357771261 -> 0.9372549019607843 on epoch=49, global_step=100
03/23/2022 17:54:00 - INFO - __main__ - Step 110 Global step 110 Train loss 0.03 on epoch=54
03/23/2022 17:54:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.03 on epoch=59
03/23/2022 17:54:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.02 on epoch=64
03/23/2022 17:54:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.02 on epoch=69
03/23/2022 17:54:12 - INFO - __main__ - Step 150 Global step 150 Train loss 0.01 on epoch=74
03/23/2022 17:54:12 - INFO - __main__ - Global step 150 Train loss 0.02 Classification-F1 0.9375 on epoch=74
03/23/2022 17:54:13 - INFO - __main__ - Saving model with best Classification-F1: 0.9372549019607843 -> 0.9375 on epoch=74, global_step=150
03/23/2022 17:54:15 - INFO - __main__ - Step 160 Global step 160 Train loss 0.02 on epoch=79
03/23/2022 17:54:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.07 on epoch=84
03/23/2022 17:54:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.01 on epoch=89
03/23/2022 17:54:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.02 on epoch=94
03/23/2022 17:54:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.01 on epoch=99
03/23/2022 17:54:28 - INFO - __main__ - Global step 200 Train loss 0.03 Classification-F1 0.9372549019607843 on epoch=99
03/23/2022 17:54:31 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 17:54:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.01 on epoch=109
03/23/2022 17:54:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.00 on epoch=114
03/23/2022 17:54:40 - INFO - __main__ - Step 240 Global step 240 Train loss 0.03 on epoch=119
03/23/2022 17:54:43 - INFO - __main__ - Step 250 Global step 250 Train loss 0.00 on epoch=124
03/23/2022 17:54:43 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=124
03/23/2022 17:54:46 - INFO - __main__ - Step 260 Global step 260 Train loss 0.01 on epoch=129
03/23/2022 17:54:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 17:54:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 17:54:55 - INFO - __main__ - Step 290 Global step 290 Train loss 0.03 on epoch=144
03/23/2022 17:54:58 - INFO - __main__ - Step 300 Global step 300 Train loss 0.02 on epoch=149
03/23/2022 17:54:59 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 0.9375 on epoch=149
03/23/2022 17:55:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.10 on epoch=154
03/23/2022 17:55:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 17:55:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 17:55:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 17:55:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 17:55:14 - INFO - __main__ - Global step 350 Train loss 0.02 Classification-F1 0.9372549019607843 on epoch=174
03/23/2022 17:55:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 17:55:20 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 17:55:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 17:55:26 - INFO - __main__ - Step 390 Global step 390 Train loss 0.02 on epoch=194
03/23/2022 17:55:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 17:55:30 - INFO - __main__ - Global step 400 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=199
03/23/2022 17:55:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.01 on epoch=204
03/23/2022 17:55:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 17:55:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 17:55:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.01 on epoch=219
03/23/2022 17:55:45 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 17:55:45 - INFO - __main__ - Global step 450 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 17:55:45 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=224, global_step=450
03/23/2022 17:55:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 17:55:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 17:55:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 17:55:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 17:56:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
03/23/2022 17:56:01 - INFO - __main__ - Global step 500 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=249
03/23/2022 17:56:04 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 17:56:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 17:56:10 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 17:56:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 17:56:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 17:56:16 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=274
03/23/2022 17:56:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 17:56:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/23/2022 17:56:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.13 on epoch=289
03/23/2022 17:56:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 17:56:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 17:56:32 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 17:56:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 17:56:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 17:56:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 17:56:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 17:56:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 17:56:48 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=324
03/23/2022 17:56:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 17:56:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 17:56:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 17:56:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 17:57:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 17:57:03 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=349
03/23/2022 17:57:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 17:57:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 17:57:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 17:57:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 17:57:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 17:57:19 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=374
03/23/2022 17:57:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/23/2022 17:57:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 17:57:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 17:57:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 17:57:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 17:57:34 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=399
03/23/2022 17:57:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 17:57:40 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 17:57:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 17:57:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 17:57:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 17:57:50 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=424
03/23/2022 17:57:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 17:57:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 17:57:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 17:58:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 17:58:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 17:58:05 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=449
03/23/2022 17:58:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 17:58:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 17:58:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 17:58:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 17:58:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 17:58:21 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=474
03/23/2022 17:58:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 17:58:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 17:58:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 17:58:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 17:58:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
03/23/2022 17:58:36 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.906158357771261 on epoch=499
03/23/2022 17:58:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 17:58:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 17:58:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 17:58:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 17:58:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 17:58:52 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=524
03/23/2022 17:58:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 17:58:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 17:59:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 17:59:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 17:59:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 17:59:07 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=549
03/23/2022 17:59:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 17:59:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 17:59:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 17:59:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 17:59:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 17:59:23 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=574
03/23/2022 17:59:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 17:59:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 17:59:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/23/2022 17:59:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 17:59:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 17:59:39 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=599
03/23/2022 17:59:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 17:59:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 17:59:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 17:59:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 17:59:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 17:59:54 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=624
03/23/2022 17:59:57 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 18:00:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/23/2022 18:00:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 18:00:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 18:00:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 18:00:10 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=649
03/23/2022 18:00:13 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 18:00:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 18:00:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 18:00:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 18:00:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 18:00:25 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=674
03/23/2022 18:00:28 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 18:00:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 18:00:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 18:00:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 18:00:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 18:00:41 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=699
03/23/2022 18:00:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 18:00:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 18:00:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 18:00:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 18:00:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 18:00:57 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=724
03/23/2022 18:01:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 18:01:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 18:01:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 18:01:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 18:01:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 18:01:12 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=749
03/23/2022 18:01:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 18:01:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 18:01:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 18:01:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 18:01:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 18:01:28 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=774
03/23/2022 18:01:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 18:01:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 18:01:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 18:01:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 18:01:42 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 18:01:43 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 18:01:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 18:01:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 18:01:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 18:01:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 18:01:58 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 18:01:59 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=824
03/23/2022 18:02:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/23/2022 18:02:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 18:02:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 18:02:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
03/23/2022 18:02:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 18:02:14 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=849
03/23/2022 18:02:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 18:02:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 18:02:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 18:02:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 18:02:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 18:02:30 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=874
03/23/2022 18:02:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 18:02:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 18:02:39 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 18:02:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 18:02:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 18:02:45 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=899
03/23/2022 18:02:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 18:02:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 18:02:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 18:02:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 18:03:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 18:03:01 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=924
03/23/2022 18:03:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/23/2022 18:03:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 18:03:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 18:03:12 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 18:03:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 18:03:16 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=949
03/23/2022 18:03:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/23/2022 18:03:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 18:03:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 18:03:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 18:03:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 18:03:32 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=974
03/23/2022 18:03:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 18:03:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 18:03:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 18:03:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 18:03:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 18:03:47 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=999
03/23/2022 18:03:47 - INFO - __main__ - save last model!
03/23/2022 18:03:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 18:03:47 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 18:03:47 - INFO - __main__ - Printing 3 examples
03/23/2022 18:03:47 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 18:03:47 - INFO - __main__ - ['negative']
03/23/2022 18:03:47 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 18:03:47 - INFO - __main__ - ['negative']
03/23/2022 18:03:47 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 18:03:47 - INFO - __main__ - ['negative']
03/23/2022 18:03:47 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:03:48 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:03:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:03:48 - INFO - __main__ - Printing 3 examples
03/23/2022 18:03:48 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/23/2022 18:03:48 - INFO - __main__ - ['positive']
03/23/2022 18:03:48 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/23/2022 18:03:48 - INFO - __main__ - ['positive']
03/23/2022 18:03:48 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/23/2022 18:03:48 - INFO - __main__ - ['positive']
03/23/2022 18:03:48 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 18:03:48 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:03:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:03:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:03:48 - INFO - __main__ - Printing 3 examples
03/23/2022 18:03:48 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/23/2022 18:03:48 - INFO - __main__ - ['positive']
03/23/2022 18:03:48 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/23/2022 18:03:48 - INFO - __main__ - ['positive']
03/23/2022 18:03:48 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/23/2022 18:03:48 - INFO - __main__ - ['positive']
03/23/2022 18:03:48 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:03:48 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:03:48 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:03:49 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 18:04:07 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:04:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:04:08 - INFO - __main__ - Starting training!
03/23/2022 18:04:13 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_21_0.3_8_predictions.txt
03/23/2022 18:04:13 - INFO - __main__ - Classification-F1 on test data: 0.9460
03/23/2022 18:04:13 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.3, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9459824983294587
03/23/2022 18:04:13 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.2, bsz=8 ...
03/23/2022 18:04:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:04:14 - INFO - __main__ - Printing 3 examples
03/23/2022 18:04:14 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/23/2022 18:04:14 - INFO - __main__ - ['positive']
03/23/2022 18:04:14 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/23/2022 18:04:14 - INFO - __main__ - ['positive']
03/23/2022 18:04:14 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/23/2022 18:04:14 - INFO - __main__ - ['positive']
03/23/2022 18:04:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:04:14 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:04:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:04:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:04:14 - INFO - __main__ - Printing 3 examples
03/23/2022 18:04:14 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/23/2022 18:04:14 - INFO - __main__ - ['positive']
03/23/2022 18:04:14 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/23/2022 18:04:14 - INFO - __main__ - ['positive']
03/23/2022 18:04:14 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/23/2022 18:04:14 - INFO - __main__ - ['positive']
03/23/2022 18:04:14 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:04:14 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:04:14 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:04:31 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:04:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:04:32 - INFO - __main__ - Starting training!
03/23/2022 18:04:35 - INFO - __main__ - Step 10 Global step 10 Train loss 0.74 on epoch=4
03/23/2022 18:04:38 - INFO - __main__ - Step 20 Global step 20 Train loss 0.34 on epoch=9
03/23/2022 18:04:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.27 on epoch=14
03/23/2022 18:04:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.22 on epoch=19
03/23/2022 18:04:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.19 on epoch=24
03/23/2022 18:04:48 - INFO - __main__ - Global step 50 Train loss 0.35 Classification-F1 0.906158357771261 on epoch=24
03/23/2022 18:04:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.906158357771261 on epoch=24, global_step=50
03/23/2022 18:04:51 - INFO - __main__ - Step 60 Global step 60 Train loss 0.16 on epoch=29
03/23/2022 18:04:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.13 on epoch=34
03/23/2022 18:04:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.09 on epoch=39
03/23/2022 18:05:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.13 on epoch=44
03/23/2022 18:05:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.08 on epoch=49
03/23/2022 18:05:04 - INFO - __main__ - Global step 100 Train loss 0.12 Classification-F1 0.906158357771261 on epoch=49
03/23/2022 18:05:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.09 on epoch=54
03/23/2022 18:05:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.04 on epoch=59
03/23/2022 18:05:13 - INFO - __main__ - Step 130 Global step 130 Train loss 0.07 on epoch=64
03/23/2022 18:05:16 - INFO - __main__ - Step 140 Global step 140 Train loss 0.03 on epoch=69
03/23/2022 18:05:19 - INFO - __main__ - Step 150 Global step 150 Train loss 0.05 on epoch=74
03/23/2022 18:05:20 - INFO - __main__ - Global step 150 Train loss 0.06 Classification-F1 0.906158357771261 on epoch=74
03/23/2022 18:05:22 - INFO - __main__ - Step 160 Global step 160 Train loss 0.05 on epoch=79
03/23/2022 18:05:25 - INFO - __main__ - Step 170 Global step 170 Train loss 0.05 on epoch=84
03/23/2022 18:05:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.04 on epoch=89
03/23/2022 18:05:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.03 on epoch=94
03/23/2022 18:05:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.02 on epoch=99
03/23/2022 18:05:35 - INFO - __main__ - Global step 200 Train loss 0.04 Classification-F1 0.9375 on epoch=99
03/23/2022 18:05:35 - INFO - __main__ - Saving model with best Classification-F1: 0.906158357771261 -> 0.9375 on epoch=99, global_step=200
03/23/2022 18:05:38 - INFO - __main__ - Step 210 Global step 210 Train loss 0.08 on epoch=104
03/23/2022 18:05:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.03 on epoch=109
03/23/2022 18:05:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.04 on epoch=114
03/23/2022 18:05:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.01 on epoch=119
03/23/2022 18:05:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.01 on epoch=124
03/23/2022 18:05:51 - INFO - __main__ - Global step 250 Train loss 0.03 Classification-F1 0.9375 on epoch=124
03/23/2022 18:05:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.02 on epoch=129
03/23/2022 18:05:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.01 on epoch=134
03/23/2022 18:06:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.02 on epoch=139
03/23/2022 18:06:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.01 on epoch=144
03/23/2022 18:06:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.04 on epoch=149
03/23/2022 18:06:06 - INFO - __main__ - Global step 300 Train loss 0.02 Classification-F1 0.906158357771261 on epoch=149
03/23/2022 18:06:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.01 on epoch=154
03/23/2022 18:06:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.01 on epoch=159
03/23/2022 18:06:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.01 on epoch=164
03/23/2022 18:06:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.01 on epoch=169
03/23/2022 18:06:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 18:06:22 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=174
03/23/2022 18:06:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.07 on epoch=179
03/23/2022 18:06:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.01 on epoch=184
03/23/2022 18:06:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 18:06:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 18:06:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.01 on epoch=199
03/23/2022 18:06:38 - INFO - __main__ - Global step 400 Train loss 0.02 Classification-F1 0.906158357771261 on epoch=199
03/23/2022 18:06:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 18:06:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.01 on epoch=209
03/23/2022 18:06:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.01 on epoch=214
03/23/2022 18:06:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 18:06:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 18:06:53 - INFO - __main__ - Global step 450 Train loss 0.01 Classification-F1 0.9375 on epoch=224
03/23/2022 18:06:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 18:06:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 18:07:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 18:07:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 18:07:08 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 18:07:09 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=249
03/23/2022 18:07:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 18:07:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 18:07:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 18:07:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 18:07:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/23/2022 18:07:25 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=274
03/23/2022 18:07:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 18:07:31 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 18:07:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 18:07:37 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/23/2022 18:07:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 18:07:40 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=299
03/23/2022 18:07:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 18:07:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 18:07:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 18:07:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 18:07:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 18:07:56 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=324
03/23/2022 18:07:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 18:08:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 18:08:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 18:08:08 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 18:08:11 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/23/2022 18:08:12 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=349
03/23/2022 18:08:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/23/2022 18:08:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 18:08:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 18:08:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 18:08:27 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 18:08:28 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=374
03/23/2022 18:08:28 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=374, global_step=750
03/23/2022 18:08:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 18:08:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 18:08:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 18:08:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 18:08:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 18:08:43 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=399
03/23/2022 18:08:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 18:08:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 18:08:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 18:08:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 18:08:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/23/2022 18:08:59 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=424
03/23/2022 18:09:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 18:09:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 18:09:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 18:09:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 18:09:14 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 18:09:15 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=449
03/23/2022 18:09:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 18:09:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 18:09:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 18:09:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 18:09:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/23/2022 18:09:30 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=474
03/23/2022 18:09:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 18:09:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 18:09:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 18:09:42 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/23/2022 18:09:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 18:09:46 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=499
03/23/2022 18:09:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 18:09:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 18:09:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 18:09:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 18:10:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 18:10:02 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=524
03/23/2022 18:10:05 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
03/23/2022 18:10:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 18:10:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 18:10:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 18:10:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
03/23/2022 18:10:18 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=549
03/23/2022 18:10:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 18:10:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 18:10:27 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 18:10:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 18:10:33 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 18:10:33 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 18:10:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 18:10:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 18:10:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 18:10:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 18:10:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 18:10:49 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=599
03/23/2022 18:10:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 18:10:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 18:10:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 18:11:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 18:11:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 18:11:05 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=624
03/23/2022 18:11:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 18:11:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 18:11:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 18:11:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 18:11:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 18:11:21 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=649
03/23/2022 18:11:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 18:11:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 18:11:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 18:11:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 18:11:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 18:11:36 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=674
03/23/2022 18:11:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
03/23/2022 18:11:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 18:11:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 18:11:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 18:11:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 18:11:52 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=699
03/23/2022 18:11:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 18:11:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 18:12:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/23/2022 18:12:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 18:12:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 18:12:08 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=724
03/23/2022 18:12:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 18:12:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 18:12:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
03/23/2022 18:12:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 18:12:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 18:12:24 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=749
03/23/2022 18:12:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 18:12:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 18:12:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 18:12:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 18:12:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 18:12:39 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=774
03/23/2022 18:12:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 18:12:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 18:12:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 18:12:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 18:12:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 18:12:55 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=799
03/23/2022 18:12:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 18:13:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 18:13:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 18:13:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 18:13:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 18:13:11 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=824
03/23/2022 18:13:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 18:13:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 18:13:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 18:13:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 18:13:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 18:13:27 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=849
03/23/2022 18:13:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 18:13:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 18:13:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 18:13:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 18:13:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=874
03/23/2022 18:13:43 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=874
03/23/2022 18:13:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 18:13:48 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 18:13:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 18:13:54 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 18:13:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 18:13:58 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=899
03/23/2022 18:14:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 18:14:04 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 18:14:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 18:14:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 18:14:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 18:14:14 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=924
03/23/2022 18:14:17 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 18:14:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 18:14:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 18:14:26 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 18:14:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 18:14:30 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=949
03/23/2022 18:14:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
03/23/2022 18:14:36 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 18:14:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 18:14:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 18:14:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 18:14:46 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=974
03/23/2022 18:14:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 18:14:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 18:14:55 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 18:14:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 18:15:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 18:15:02 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=999
03/23/2022 18:15:02 - INFO - __main__ - save last model!
03/23/2022 18:15:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 18:15:02 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 18:15:02 - INFO - __main__ - Printing 3 examples
03/23/2022 18:15:02 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 18:15:02 - INFO - __main__ - ['negative']
03/23/2022 18:15:02 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 18:15:02 - INFO - __main__ - ['negative']
03/23/2022 18:15:02 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 18:15:02 - INFO - __main__ - ['negative']
03/23/2022 18:15:02 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:15:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:15:02 - INFO - __main__ - Printing 3 examples
03/23/2022 18:15:02 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/23/2022 18:15:02 - INFO - __main__ - ['negative']
03/23/2022 18:15:02 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/23/2022 18:15:02 - INFO - __main__ - ['negative']
03/23/2022 18:15:02 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/23/2022 18:15:02 - INFO - __main__ - ['negative']
03/23/2022 18:15:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 18:15:02 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:15:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:15:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:15:02 - INFO - __main__ - Printing 3 examples
03/23/2022 18:15:02 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/23/2022 18:15:02 - INFO - __main__ - ['negative']
03/23/2022 18:15:02 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/23/2022 18:15:02 - INFO - __main__ - ['negative']
03/23/2022 18:15:02 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/23/2022 18:15:02 - INFO - __main__ - ['negative']
03/23/2022 18:15:02 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:15:02 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:15:02 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:15:02 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:15:03 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 18:15:17 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:15:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:15:18 - INFO - __main__ - Starting training!
03/23/2022 18:15:31 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_21_0.2_8_predictions.txt
03/23/2022 18:15:31 - INFO - __main__ - Classification-F1 on test data: 0.9409
03/23/2022 18:15:31 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.2, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9409191182729155
03/23/2022 18:15:31 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.5, bsz=8 ...
03/23/2022 18:15:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:15:32 - INFO - __main__ - Printing 3 examples
03/23/2022 18:15:32 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/23/2022 18:15:32 - INFO - __main__ - ['negative']
03/23/2022 18:15:32 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/23/2022 18:15:32 - INFO - __main__ - ['negative']
03/23/2022 18:15:32 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/23/2022 18:15:32 - INFO - __main__ - ['negative']
03/23/2022 18:15:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:15:32 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:15:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:15:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:15:32 - INFO - __main__ - Printing 3 examples
03/23/2022 18:15:32 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/23/2022 18:15:32 - INFO - __main__ - ['negative']
03/23/2022 18:15:32 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/23/2022 18:15:32 - INFO - __main__ - ['negative']
03/23/2022 18:15:32 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/23/2022 18:15:32 - INFO - __main__ - ['negative']
03/23/2022 18:15:32 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:15:32 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:15:32 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:15:47 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:15:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:15:48 - INFO - __main__ - Starting training!
03/23/2022 18:15:54 - INFO - __main__ - Step 10 Global step 10 Train loss 0.43 on epoch=4
03/23/2022 18:15:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.27 on epoch=9
03/23/2022 18:16:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.16 on epoch=14
03/23/2022 18:16:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.13 on epoch=19
03/23/2022 18:16:06 - INFO - __main__ - Step 50 Global step 50 Train loss 0.10 on epoch=24
03/23/2022 18:16:07 - INFO - __main__ - Global step 50 Train loss 0.22 Classification-F1 1.0 on epoch=24
03/23/2022 18:16:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 1.0 on epoch=24, global_step=50
03/23/2022 18:16:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.11 on epoch=29
03/23/2022 18:16:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.05 on epoch=34
03/23/2022 18:16:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.08 on epoch=39
03/23/2022 18:16:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.03 on epoch=44
03/23/2022 18:16:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.07 on epoch=49
03/23/2022 18:16:23 - INFO - __main__ - Global step 100 Train loss 0.07 Classification-F1 0.9687194525904204 on epoch=49
03/23/2022 18:16:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.04 on epoch=54
03/23/2022 18:16:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.01 on epoch=59
03/23/2022 18:16:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.03 on epoch=64
03/23/2022 18:16:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.02 on epoch=69
03/23/2022 18:16:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.01 on epoch=74
03/23/2022 18:16:40 - INFO - __main__ - Global step 150 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 18:16:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.02 on epoch=79
03/23/2022 18:16:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.01 on epoch=84
03/23/2022 18:16:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.01 on epoch=89
03/23/2022 18:16:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.01 on epoch=94
03/23/2022 18:16:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.03 on epoch=99
03/23/2022 18:16:56 - INFO - __main__ - Global step 200 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=99
03/23/2022 18:16:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.03 on epoch=104
03/23/2022 18:17:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.01 on epoch=109
03/23/2022 18:17:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.01 on epoch=114
03/23/2022 18:17:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.02 on epoch=119
03/23/2022 18:17:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.00 on epoch=124
03/23/2022 18:17:12 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 18:17:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.01 on epoch=129
03/23/2022 18:17:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 18:17:22 - INFO - __main__ - Step 280 Global step 280 Train loss 0.01 on epoch=139
03/23/2022 18:17:25 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 18:17:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 18:17:29 - INFO - __main__ - Global step 300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 18:17:32 - INFO - __main__ - Step 310 Global step 310 Train loss 0.01 on epoch=154
03/23/2022 18:17:35 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 18:17:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 18:17:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.02 on epoch=169
03/23/2022 18:17:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.01 on epoch=174
03/23/2022 18:17:45 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=174
03/23/2022 18:17:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 18:17:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 18:17:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 18:17:57 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 18:18:01 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 18:18:01 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=199
03/23/2022 18:18:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 18:18:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 18:18:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 18:18:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.01 on epoch=219
03/23/2022 18:18:17 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 18:18:18 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=224
03/23/2022 18:18:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 18:18:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 18:18:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 18:18:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 18:18:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 18:18:34 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=249
03/23/2022 18:18:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 18:18:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 18:18:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 18:18:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 18:18:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 18:18:50 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9375 on epoch=274
03/23/2022 18:18:53 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
03/23/2022 18:18:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/23/2022 18:19:00 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 18:19:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 18:19:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 18:19:07 - INFO - __main__ - Global step 600 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 18:19:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 18:19:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 18:19:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 18:19:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 18:19:22 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 18:19:23 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 18:19:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 18:19:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 18:19:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 18:19:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 18:19:39 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 18:19:40 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=349
03/23/2022 18:19:43 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 18:19:46 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 18:19:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 18:19:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/23/2022 18:19:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 18:19:56 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=374
03/23/2022 18:19:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 18:20:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 18:20:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 18:20:09 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 18:20:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 18:20:13 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=399
03/23/2022 18:20:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 18:20:19 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 18:20:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 18:20:25 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 18:20:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 18:20:29 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=424
03/23/2022 18:20:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 18:20:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
03/23/2022 18:20:39 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 18:20:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 18:20:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 18:20:46 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=449
03/23/2022 18:20:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 18:20:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 18:20:55 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 18:20:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 18:21:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
03/23/2022 18:21:02 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=474
03/23/2022 18:21:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 18:21:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 18:21:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 18:21:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 18:21:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 18:21:19 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=499
03/23/2022 18:21:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 18:21:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 18:21:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 18:21:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 18:21:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 18:21:35 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=524
03/23/2022 18:21:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 18:21:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 18:21:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 18:21:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/23/2022 18:21:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 18:21:52 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9054187192118226 on epoch=549
03/23/2022 18:21:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 18:21:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 18:22:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 18:22:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 18:22:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 18:22:09 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9054187192118226 on epoch=574
03/23/2022 18:22:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 18:22:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 18:22:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 18:22:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 18:22:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 18:22:26 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/23/2022 18:22:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 18:22:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 18:22:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 18:22:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 18:22:41 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 18:22:42 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 18:22:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 18:22:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 18:22:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 18:22:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 18:22:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 18:22:59 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=649
03/23/2022 18:23:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 18:23:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 18:23:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 18:23:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 18:23:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 18:23:16 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 18:23:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 18:23:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 18:23:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 18:23:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 18:23:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 18:23:32 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 18:23:35 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 18:23:39 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 18:23:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 18:23:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 18:23:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 18:23:49 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 18:23:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 18:23:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 18:23:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 18:24:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 18:24:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 18:24:06 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/23/2022 18:24:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 18:24:12 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 18:24:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 18:24:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 18:24:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 18:24:23 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/23/2022 18:24:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 18:24:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 18:24:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 18:24:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 18:24:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 18:24:40 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 18:24:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 18:24:46 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 18:24:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 18:24:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 18:24:56 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 18:24:56 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/23/2022 18:25:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=829
03/23/2022 18:25:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 18:25:06 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 18:25:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 18:25:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 18:25:13 - INFO - __main__ - Global step 1700 Train loss 0.04 Classification-F1 0.9687194525904204 on epoch=849
03/23/2022 18:25:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 18:25:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 18:25:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 18:25:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 18:25:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 18:25:30 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/23/2022 18:25:33 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
03/23/2022 18:25:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 18:25:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 18:25:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 18:25:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 18:25:47 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 18:25:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 18:25:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 18:25:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 18:26:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 18:26:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 18:26:04 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/23/2022 18:26:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 18:26:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 18:26:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 18:26:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 18:26:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 18:26:20 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/23/2022 18:26:23 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 18:26:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 18:26:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 18:26:33 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 18:26:36 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 18:26:37 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/23/2022 18:26:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 18:26:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 18:26:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 18:26:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 18:26:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 18:26:54 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9054187192118226 on epoch=999
03/23/2022 18:26:54 - INFO - __main__ - save last model!
03/23/2022 18:26:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 18:26:54 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 18:26:54 - INFO - __main__ - Printing 3 examples
03/23/2022 18:26:54 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 18:26:54 - INFO - __main__ - ['negative']
03/23/2022 18:26:54 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 18:26:54 - INFO - __main__ - ['negative']
03/23/2022 18:26:54 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 18:26:54 - INFO - __main__ - ['negative']
03/23/2022 18:26:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:26:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:26:54 - INFO - __main__ - Printing 3 examples
03/23/2022 18:26:54 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/23/2022 18:26:54 - INFO - __main__ - ['negative']
03/23/2022 18:26:54 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/23/2022 18:26:54 - INFO - __main__ - ['negative']
03/23/2022 18:26:54 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/23/2022 18:26:54 - INFO - __main__ - ['negative']
03/23/2022 18:26:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 18:26:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:26:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:26:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:26:54 - INFO - __main__ - Printing 3 examples
03/23/2022 18:26:54 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/23/2022 18:26:54 - INFO - __main__ - ['negative']
03/23/2022 18:26:54 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/23/2022 18:26:54 - INFO - __main__ - ['negative']
03/23/2022 18:26:54 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/23/2022 18:26:54 - INFO - __main__ - ['negative']
03/23/2022 18:26:54 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:26:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:26:54 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:26:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:26:55 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 18:27:09 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:27:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:27:10 - INFO - __main__ - Starting training!
03/23/2022 18:27:23 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_42_0.5_8_predictions.txt
03/23/2022 18:27:23 - INFO - __main__ - Classification-F1 on test data: 0.9288
03/23/2022 18:27:23 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.5, bsz=8, dev_performance=1.0, test_performance=0.9288291187140324
03/23/2022 18:27:23 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.4, bsz=8 ...
03/23/2022 18:27:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:27:24 - INFO - __main__ - Printing 3 examples
03/23/2022 18:27:24 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/23/2022 18:27:24 - INFO - __main__ - ['negative']
03/23/2022 18:27:24 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/23/2022 18:27:24 - INFO - __main__ - ['negative']
03/23/2022 18:27:24 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/23/2022 18:27:24 - INFO - __main__ - ['negative']
03/23/2022 18:27:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:27:24 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:27:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:27:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:27:24 - INFO - __main__ - Printing 3 examples
03/23/2022 18:27:24 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/23/2022 18:27:24 - INFO - __main__ - ['negative']
03/23/2022 18:27:24 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/23/2022 18:27:24 - INFO - __main__ - ['negative']
03/23/2022 18:27:24 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/23/2022 18:27:24 - INFO - __main__ - ['negative']
03/23/2022 18:27:24 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:27:24 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:27:24 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:27:43 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:27:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:27:44 - INFO - __main__ - Starting training!
03/23/2022 18:27:47 - INFO - __main__ - Step 10 Global step 10 Train loss 0.45 on epoch=4
03/23/2022 18:27:51 - INFO - __main__ - Step 20 Global step 20 Train loss 0.27 on epoch=9
03/23/2022 18:27:54 - INFO - __main__ - Step 30 Global step 30 Train loss 0.19 on epoch=14
03/23/2022 18:27:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.16 on epoch=19
03/23/2022 18:28:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.12 on epoch=24
03/23/2022 18:28:01 - INFO - __main__ - Global step 50 Train loss 0.24 Classification-F1 1.0 on epoch=24
03/23/2022 18:28:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 1.0 on epoch=24, global_step=50
03/23/2022 18:28:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.07 on epoch=29
03/23/2022 18:28:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.08 on epoch=34
03/23/2022 18:28:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.05 on epoch=39
03/23/2022 18:28:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.07 on epoch=44
03/23/2022 18:28:17 - INFO - __main__ - Step 100 Global step 100 Train loss 0.04 on epoch=49
03/23/2022 18:28:17 - INFO - __main__ - Global step 100 Train loss 0.06 Classification-F1 0.9687194525904204 on epoch=49
03/23/2022 18:28:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.05 on epoch=54
03/23/2022 18:28:24 - INFO - __main__ - Step 120 Global step 120 Train loss 0.03 on epoch=59
03/23/2022 18:28:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.02 on epoch=64
03/23/2022 18:28:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.02 on epoch=69
03/23/2022 18:28:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.02 on epoch=74
03/23/2022 18:28:34 - INFO - __main__ - Global step 150 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 18:28:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.02 on epoch=79
03/23/2022 18:28:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.02 on epoch=84
03/23/2022 18:28:43 - INFO - __main__ - Step 180 Global step 180 Train loss 0.02 on epoch=89
03/23/2022 18:28:47 - INFO - __main__ - Step 190 Global step 190 Train loss 0.03 on epoch=94
03/23/2022 18:28:50 - INFO - __main__ - Step 200 Global step 200 Train loss 0.00 on epoch=99
03/23/2022 18:28:51 - INFO - __main__ - Global step 200 Train loss 0.02 Classification-F1 1.0 on epoch=99
03/23/2022 18:28:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 18:28:57 - INFO - __main__ - Step 220 Global step 220 Train loss 0.01 on epoch=109
03/23/2022 18:29:00 - INFO - __main__ - Step 230 Global step 230 Train loss 0.06 on epoch=114
03/23/2022 18:29:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.01 on epoch=119
03/23/2022 18:29:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.01 on epoch=124
03/23/2022 18:29:07 - INFO - __main__ - Global step 250 Train loss 0.02 Classification-F1 1.0 on epoch=124
03/23/2022 18:29:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 18:29:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 18:29:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 18:29:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 18:29:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 18:29:24 - INFO - __main__ - Global step 300 Train loss 0.00 Classification-F1 1.0 on epoch=149
03/23/2022 18:29:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.01 on epoch=154
03/23/2022 18:29:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 18:29:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 18:29:36 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 18:29:39 - INFO - __main__ - Step 350 Global step 350 Train loss 0.07 on epoch=174
03/23/2022 18:29:40 - INFO - __main__ - Global step 350 Train loss 0.02 Classification-F1 1.0 on epoch=174
03/23/2022 18:29:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 18:29:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 18:29:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 18:29:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 18:29:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 18:29:57 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 1.0 on epoch=199
03/23/2022 18:30:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 18:30:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 18:30:06 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 18:30:09 - INFO - __main__ - Step 440 Global step 440 Train loss 0.01 on epoch=219
03/23/2022 18:30:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 18:30:13 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 18:30:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 18:30:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 18:30:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 18:30:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.01 on epoch=244
03/23/2022 18:30:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
03/23/2022 18:30:30 - INFO - __main__ - Global step 500 Train loss 0.01 Classification-F1 1.0 on epoch=249
03/23/2022 18:30:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 18:30:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 18:30:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 18:30:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 18:30:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 18:30:46 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 1.0 on epoch=274
03/23/2022 18:30:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 18:30:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 18:30:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 18:30:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 18:31:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 18:31:03 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 18:31:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/23/2022 18:31:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/23/2022 18:31:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 18:31:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 18:31:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 18:31:19 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 1.0 on epoch=324
03/23/2022 18:31:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 18:31:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/23/2022 18:31:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
03/23/2022 18:31:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 18:31:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.07 on epoch=349
03/23/2022 18:31:36 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 18:31:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 18:31:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 18:31:45 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 18:31:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
03/23/2022 18:31:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 18:31:52 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 1.0 on epoch=374
03/23/2022 18:31:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 18:31:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 18:32:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 18:32:05 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 18:32:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 18:32:09 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 1.0 on epoch=399
03/23/2022 18:32:12 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 18:32:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 18:32:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 18:32:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 18:32:25 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 18:32:25 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 1.0 on epoch=424
03/23/2022 18:32:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 18:32:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 18:32:35 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/23/2022 18:32:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 18:32:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 18:32:42 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 1.0 on epoch=449
03/23/2022 18:32:45 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 18:32:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 18:32:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 18:32:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 18:32:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 18:32:59 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 1.0 on epoch=474
03/23/2022 18:33:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 18:33:05 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/23/2022 18:33:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 18:33:11 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 18:33:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 18:33:15 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 1.0 on epoch=499
03/23/2022 18:33:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 18:33:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/23/2022 18:33:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 18:33:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 18:33:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 18:33:32 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=524
03/23/2022 18:33:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 18:33:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 18:33:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 18:33:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 18:33:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 18:33:48 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 1.0 on epoch=549
03/23/2022 18:33:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 18:33:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 18:33:58 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 18:34:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 18:34:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 18:34:05 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 18:34:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 18:34:11 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 18:34:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 18:34:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 18:34:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 18:34:21 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 1.0 on epoch=599
03/23/2022 18:34:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 18:34:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 18:34:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 18:34:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 18:34:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 18:34:38 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 1.0 on epoch=624
03/23/2022 18:34:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 18:34:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 18:34:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 18:34:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/23/2022 18:34:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 18:34:54 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 1.0 on epoch=649
03/23/2022 18:34:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 18:35:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 18:35:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 18:35:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 18:35:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 18:35:11 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 1.0 on epoch=674
03/23/2022 18:35:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 18:35:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 18:35:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 18:35:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 18:35:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 18:35:27 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 1.0 on epoch=699
03/23/2022 18:35:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 18:35:33 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 18:35:37 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 18:35:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 18:35:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 18:35:44 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 1.0 on epoch=724
03/23/2022 18:35:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/23/2022 18:35:50 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 18:35:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 18:35:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 18:35:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 18:36:00 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 1.0 on epoch=749
03/23/2022 18:36:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 18:36:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 18:36:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 18:36:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 18:36:16 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 18:36:17 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 1.0 on epoch=774
03/23/2022 18:36:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 18:36:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 18:36:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 18:36:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 18:36:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 18:36:33 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 1.0 on epoch=799
03/23/2022 18:36:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 18:36:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 18:36:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=814
03/23/2022 18:36:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 18:36:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 18:36:50 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 1.0 on epoch=824
03/23/2022 18:36:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 18:36:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=834
03/23/2022 18:36:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 18:37:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 18:37:05 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 18:37:06 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 1.0 on epoch=849
03/23/2022 18:37:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 18:37:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=859
03/23/2022 18:37:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 18:37:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 18:37:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 18:37:23 - INFO - __main__ - Global step 1750 Train loss 0.02 Classification-F1 1.0 on epoch=874
03/23/2022 18:37:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 18:37:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 18:37:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
03/23/2022 18:37:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 18:37:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 18:37:39 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 1.0 on epoch=899
03/23/2022 18:37:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 18:37:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 18:37:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 18:37:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 18:37:55 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 18:37:56 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 1.0 on epoch=924
03/23/2022 18:37:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 18:38:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 18:38:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 18:38:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 18:38:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 18:38:12 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 1.0 on epoch=949
03/23/2022 18:38:15 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 18:38:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 18:38:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 18:38:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 18:38:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 18:38:29 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 1.0 on epoch=974
03/23/2022 18:38:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 18:38:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 18:38:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 18:38:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 18:38:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 18:38:45 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 1.0 on epoch=999
03/23/2022 18:38:45 - INFO - __main__ - save last model!
03/23/2022 18:38:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 18:38:45 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 18:38:45 - INFO - __main__ - Printing 3 examples
03/23/2022 18:38:45 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 18:38:45 - INFO - __main__ - ['negative']
03/23/2022 18:38:45 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 18:38:45 - INFO - __main__ - ['negative']
03/23/2022 18:38:45 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 18:38:45 - INFO - __main__ - ['negative']
03/23/2022 18:38:45 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:38:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:38:46 - INFO - __main__ - Printing 3 examples
03/23/2022 18:38:46 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/23/2022 18:38:46 - INFO - __main__ - ['negative']
03/23/2022 18:38:46 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/23/2022 18:38:46 - INFO - __main__ - ['negative']
03/23/2022 18:38:46 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/23/2022 18:38:46 - INFO - __main__ - ['negative']
03/23/2022 18:38:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 18:38:46 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:38:46 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:38:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:38:46 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:38:46 - INFO - __main__ - Printing 3 examples
03/23/2022 18:38:46 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/23/2022 18:38:46 - INFO - __main__ - ['negative']
03/23/2022 18:38:46 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/23/2022 18:38:46 - INFO - __main__ - ['negative']
03/23/2022 18:38:46 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/23/2022 18:38:46 - INFO - __main__ - ['negative']
03/23/2022 18:38:46 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:38:46 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:38:46 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:38:47 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 18:39:01 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:39:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:39:02 - INFO - __main__ - Starting training!
03/23/2022 18:39:10 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_42_0.4_8_predictions.txt
03/23/2022 18:39:10 - INFO - __main__ - Classification-F1 on test data: 0.9470
03/23/2022 18:39:11 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.4, bsz=8, dev_performance=1.0, test_performance=0.9469808600904926
03/23/2022 18:39:11 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.3, bsz=8 ...
03/23/2022 18:39:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:39:12 - INFO - __main__ - Printing 3 examples
03/23/2022 18:39:12 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/23/2022 18:39:12 - INFO - __main__ - ['negative']
03/23/2022 18:39:12 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/23/2022 18:39:12 - INFO - __main__ - ['negative']
03/23/2022 18:39:12 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/23/2022 18:39:12 - INFO - __main__ - ['negative']
03/23/2022 18:39:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:39:12 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:39:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:39:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:39:12 - INFO - __main__ - Printing 3 examples
03/23/2022 18:39:12 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/23/2022 18:39:12 - INFO - __main__ - ['negative']
03/23/2022 18:39:12 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/23/2022 18:39:12 - INFO - __main__ - ['negative']
03/23/2022 18:39:12 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/23/2022 18:39:12 - INFO - __main__ - ['negative']
03/23/2022 18:39:12 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:39:12 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:39:12 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:39:27 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:39:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:39:28 - INFO - __main__ - Starting training!
03/23/2022 18:39:31 - INFO - __main__ - Step 10 Global step 10 Train loss 0.51 on epoch=4
03/23/2022 18:39:35 - INFO - __main__ - Step 20 Global step 20 Train loss 0.33 on epoch=9
03/23/2022 18:39:38 - INFO - __main__ - Step 30 Global step 30 Train loss 0.20 on epoch=14
03/23/2022 18:39:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.14 on epoch=19
03/23/2022 18:39:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.15 on epoch=24
03/23/2022 18:39:45 - INFO - __main__ - Global step 50 Train loss 0.27 Classification-F1 1.0 on epoch=24
03/23/2022 18:39:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 1.0 on epoch=24, global_step=50
03/23/2022 18:39:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.14 on epoch=29
03/23/2022 18:39:51 - INFO - __main__ - Step 70 Global step 70 Train loss 0.11 on epoch=34
03/23/2022 18:39:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.10 on epoch=39
03/23/2022 18:39:57 - INFO - __main__ - Step 90 Global step 90 Train loss 0.05 on epoch=44
03/23/2022 18:40:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.07 on epoch=49
03/23/2022 18:40:01 - INFO - __main__ - Global step 100 Train loss 0.09 Classification-F1 1.0 on epoch=49
03/23/2022 18:40:04 - INFO - __main__ - Step 110 Global step 110 Train loss 0.05 on epoch=54
03/23/2022 18:40:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.08 on epoch=59
03/23/2022 18:40:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.07 on epoch=64
03/23/2022 18:40:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.04 on epoch=69
03/23/2022 18:40:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.03 on epoch=74
03/23/2022 18:40:18 - INFO - __main__ - Global step 150 Train loss 0.06 Classification-F1 1.0 on epoch=74
03/23/2022 18:40:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.03 on epoch=79
03/23/2022 18:40:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.05 on epoch=84
03/23/2022 18:40:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.05 on epoch=89
03/23/2022 18:40:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.01 on epoch=94
03/23/2022 18:40:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.02 on epoch=99
03/23/2022 18:40:34 - INFO - __main__ - Global step 200 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=99
03/23/2022 18:40:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.02 on epoch=104
03/23/2022 18:40:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.05 on epoch=109
03/23/2022 18:40:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.01 on epoch=114
03/23/2022 18:40:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.03 on epoch=119
03/23/2022 18:40:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.01 on epoch=124
03/23/2022 18:40:51 - INFO - __main__ - Global step 250 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 18:40:54 - INFO - __main__ - Step 260 Global step 260 Train loss 0.01 on epoch=129
03/23/2022 18:40:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.03 on epoch=134
03/23/2022 18:41:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.01 on epoch=139
03/23/2022 18:41:03 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 18:41:06 - INFO - __main__ - Step 300 Global step 300 Train loss 0.01 on epoch=149
03/23/2022 18:41:07 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 1.0 on epoch=149
03/23/2022 18:41:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 18:41:14 - INFO - __main__ - Step 320 Global step 320 Train loss 0.01 on epoch=159
03/23/2022 18:41:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 18:41:20 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 18:41:23 - INFO - __main__ - Step 350 Global step 350 Train loss 0.01 on epoch=174
03/23/2022 18:41:24 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 1.0 on epoch=174
03/23/2022 18:41:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 18:41:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 18:41:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.01 on epoch=189
03/23/2022 18:41:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 18:41:39 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 18:41:40 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 1.0 on epoch=199
03/23/2022 18:41:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.01 on epoch=204
03/23/2022 18:41:47 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 18:41:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 18:41:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.06 on epoch=219
03/23/2022 18:41:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 18:41:57 - INFO - __main__ - Global step 450 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 18:42:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 18:42:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 18:42:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 18:42:09 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 18:42:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 18:42:13 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 1.0 on epoch=249
03/23/2022 18:42:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 18:42:20 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 18:42:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
03/23/2022 18:42:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 18:42:29 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 18:42:30 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 18:42:33 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 18:42:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 18:42:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 18:42:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 18:42:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 18:42:46 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 1.0 on epoch=299
03/23/2022 18:42:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 18:42:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 18:42:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 18:42:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 18:43:02 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 18:43:03 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 1.0 on epoch=324
03/23/2022 18:43:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 18:43:09 - INFO - __main__ - Step 670 Global step 670 Train loss 0.07 on epoch=334
03/23/2022 18:43:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 18:43:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 18:43:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 18:43:19 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 1.0 on epoch=349
03/23/2022 18:43:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 18:43:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 18:43:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 18:43:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
03/23/2022 18:43:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 18:43:36 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 1.0 on epoch=374
03/23/2022 18:43:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/23/2022 18:43:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 18:43:45 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 18:43:48 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 18:43:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 18:43:52 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 1.0 on epoch=399
03/23/2022 18:43:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 18:43:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 18:44:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 18:44:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 18:44:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 18:44:09 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 1.0 on epoch=424
03/23/2022 18:44:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 18:44:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/23/2022 18:44:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 18:44:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 18:44:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 18:44:25 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 1.0 on epoch=449
03/23/2022 18:44:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 18:44:31 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
03/23/2022 18:44:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 18:44:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 18:44:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 18:44:42 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 1.0 on epoch=474
03/23/2022 18:44:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
03/23/2022 18:44:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
03/23/2022 18:44:51 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 18:44:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 18:44:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 18:44:58 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 1.0 on epoch=499
03/23/2022 18:45:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
03/23/2022 18:45:05 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 18:45:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 18:45:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 18:45:14 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 18:45:15 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 1.0 on epoch=524
03/23/2022 18:45:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 18:45:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 18:45:24 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 18:45:27 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 18:45:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/23/2022 18:45:31 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/23/2022 18:45:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 18:45:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 18:45:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 18:45:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/23/2022 18:45:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 18:45:48 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 1.0 on epoch=574
03/23/2022 18:45:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 18:45:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 18:45:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 18:46:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 18:46:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 18:46:04 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 1.0 on epoch=599
03/23/2022 18:46:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 18:46:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 18:46:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/23/2022 18:46:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 18:46:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 18:46:21 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 1.0 on epoch=624
03/23/2022 18:46:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 18:46:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 18:46:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 18:46:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 18:46:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 18:46:38 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 1.0 on epoch=649
03/23/2022 18:46:41 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 18:46:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 18:46:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 18:46:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 18:46:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
03/23/2022 18:46:54 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 1.0 on epoch=674
03/23/2022 18:46:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 18:47:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 18:47:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 18:47:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 18:47:09 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 18:47:10 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 1.0 on epoch=699
03/23/2022 18:47:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 18:47:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 18:47:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 18:47:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 18:47:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 18:47:26 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 18:47:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 18:47:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 18:47:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 18:47:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 18:47:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 18:47:43 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 1.0 on epoch=749
03/23/2022 18:47:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/23/2022 18:47:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 18:47:52 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 18:47:55 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 18:47:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 18:47:59 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 1.0 on epoch=774
03/23/2022 18:48:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 18:48:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 18:48:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 18:48:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 18:48:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 18:48:15 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 1.0 on epoch=799
03/23/2022 18:48:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 18:48:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 18:48:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 18:48:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 18:48:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 18:48:31 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 1.0 on epoch=824
03/23/2022 18:48:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 18:48:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 18:48:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 18:48:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 18:48:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 18:48:48 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/23/2022 18:48:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 18:48:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 18:48:57 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 18:49:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 18:49:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 18:49:04 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 1.0 on epoch=874
03/23/2022 18:49:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 18:49:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 18:49:13 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 18:49:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 18:49:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 18:49:20 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 1.0 on epoch=899
03/23/2022 18:49:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 18:49:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 18:49:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 18:49:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 18:49:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 18:49:37 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 1.0 on epoch=924
03/23/2022 18:49:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 18:49:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 18:49:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 18:49:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 18:49:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 18:49:53 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 1.0 on epoch=949
03/23/2022 18:49:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 18:49:59 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 18:50:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 18:50:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 18:50:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 18:50:09 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 1.0 on epoch=974
03/23/2022 18:50:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 18:50:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=984
03/23/2022 18:50:19 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 18:50:22 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 18:50:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 18:50:26 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 1.0 on epoch=999
03/23/2022 18:50:26 - INFO - __main__ - save last model!
03/23/2022 18:50:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 18:50:26 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 18:50:26 - INFO - __main__ - Printing 3 examples
03/23/2022 18:50:26 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 18:50:26 - INFO - __main__ - ['negative']
03/23/2022 18:50:26 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 18:50:26 - INFO - __main__ - ['negative']
03/23/2022 18:50:26 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 18:50:26 - INFO - __main__ - ['negative']
03/23/2022 18:50:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:50:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:50:26 - INFO - __main__ - Printing 3 examples
03/23/2022 18:50:26 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/23/2022 18:50:26 - INFO - __main__ - ['negative']
03/23/2022 18:50:26 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/23/2022 18:50:26 - INFO - __main__ - ['negative']
03/23/2022 18:50:26 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/23/2022 18:50:26 - INFO - __main__ - ['negative']
03/23/2022 18:50:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 18:50:26 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:50:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:50:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:50:26 - INFO - __main__ - Printing 3 examples
03/23/2022 18:50:26 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/23/2022 18:50:26 - INFO - __main__ - ['negative']
03/23/2022 18:50:26 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/23/2022 18:50:26 - INFO - __main__ - ['negative']
03/23/2022 18:50:26 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/23/2022 18:50:26 - INFO - __main__ - ['negative']
03/23/2022 18:50:26 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:50:26 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:50:26 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:50:26 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:50:27 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 18:50:41 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:50:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:50:42 - INFO - __main__ - Starting training!
03/23/2022 18:50:51 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_42_0.3_8_predictions.txt
03/23/2022 18:50:51 - INFO - __main__ - Classification-F1 on test data: 0.9410
03/23/2022 18:50:51 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.3, bsz=8, dev_performance=1.0, test_performance=0.9409503392352969
03/23/2022 18:50:51 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.2, bsz=8 ...
03/23/2022 18:50:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:50:52 - INFO - __main__ - Printing 3 examples
03/23/2022 18:50:52 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/23/2022 18:50:52 - INFO - __main__ - ['negative']
03/23/2022 18:50:52 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/23/2022 18:50:52 - INFO - __main__ - ['negative']
03/23/2022 18:50:52 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/23/2022 18:50:52 - INFO - __main__ - ['negative']
03/23/2022 18:50:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 18:50:52 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:50:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 18:50:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 18:50:52 - INFO - __main__ - Printing 3 examples
03/23/2022 18:50:52 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/23/2022 18:50:52 - INFO - __main__ - ['negative']
03/23/2022 18:50:52 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/23/2022 18:50:52 - INFO - __main__ - ['negative']
03/23/2022 18:50:52 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/23/2022 18:50:52 - INFO - __main__ - ['negative']
03/23/2022 18:50:52 - INFO - __main__ - Tokenizing Input ...
03/23/2022 18:50:52 - INFO - __main__ - Tokenizing Output ...
03/23/2022 18:50:52 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 18:51:11 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 18:51:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 18:51:12 - INFO - __main__ - Starting training!
03/23/2022 18:51:16 - INFO - __main__ - Step 10 Global step 10 Train loss 0.65 on epoch=4
03/23/2022 18:51:19 - INFO - __main__ - Step 20 Global step 20 Train loss 0.31 on epoch=9
03/23/2022 18:51:22 - INFO - __main__ - Step 30 Global step 30 Train loss 0.27 on epoch=14
03/23/2022 18:51:25 - INFO - __main__ - Step 40 Global step 40 Train loss 0.22 on epoch=19
03/23/2022 18:51:28 - INFO - __main__ - Step 50 Global step 50 Train loss 0.22 on epoch=24
03/23/2022 18:51:29 - INFO - __main__ - Global step 50 Train loss 0.33 Classification-F1 0.9687194525904204 on epoch=24
03/23/2022 18:51:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9687194525904204 on epoch=24, global_step=50
03/23/2022 18:51:32 - INFO - __main__ - Step 60 Global step 60 Train loss 0.15 on epoch=29
03/23/2022 18:51:35 - INFO - __main__ - Step 70 Global step 70 Train loss 0.17 on epoch=34
03/23/2022 18:51:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.14 on epoch=39
03/23/2022 18:51:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.15 on epoch=44
03/23/2022 18:51:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.09 on epoch=49
03/23/2022 18:51:46 - INFO - __main__ - Global step 100 Train loss 0.14 Classification-F1 1.0 on epoch=49
03/23/2022 18:51:46 - INFO - __main__ - Saving model with best Classification-F1: 0.9687194525904204 -> 1.0 on epoch=49, global_step=100
03/23/2022 18:51:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.07 on epoch=54
03/23/2022 18:51:52 - INFO - __main__ - Step 120 Global step 120 Train loss 0.10 on epoch=59
03/23/2022 18:51:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.08 on epoch=64
03/23/2022 18:51:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.05 on epoch=69
03/23/2022 18:52:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.08 on epoch=74
03/23/2022 18:52:02 - INFO - __main__ - Global step 150 Train loss 0.08 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 18:52:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.05 on epoch=79
03/23/2022 18:52:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.03 on epoch=84
03/23/2022 18:52:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.05 on epoch=89
03/23/2022 18:52:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.04 on epoch=94
03/23/2022 18:52:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.03 on epoch=99
03/23/2022 18:52:19 - INFO - __main__ - Global step 200 Train loss 0.04 Classification-F1 1.0 on epoch=99
03/23/2022 18:52:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.04 on epoch=104
03/23/2022 18:52:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.07 on epoch=109
03/23/2022 18:52:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.04 on epoch=114
03/23/2022 18:52:32 - INFO - __main__ - Step 240 Global step 240 Train loss 0.04 on epoch=119
03/23/2022 18:52:35 - INFO - __main__ - Step 250 Global step 250 Train loss 0.02 on epoch=124
03/23/2022 18:52:36 - INFO - __main__ - Global step 250 Train loss 0.04 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 18:52:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.03 on epoch=129
03/23/2022 18:52:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.03 on epoch=134
03/23/2022 18:52:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.01 on epoch=139
03/23/2022 18:52:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.03 on epoch=144
03/23/2022 18:52:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.01 on epoch=149
03/23/2022 18:52:52 - INFO - __main__ - Global step 300 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 18:52:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.01 on epoch=154
03/23/2022 18:52:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.02 on epoch=159
03/23/2022 18:53:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.04 on epoch=164
03/23/2022 18:53:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.01 on epoch=169
03/23/2022 18:53:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.01 on epoch=174
03/23/2022 18:53:09 - INFO - __main__ - Global step 350 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=174
03/23/2022 18:53:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.01 on epoch=179
03/23/2022 18:53:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 18:53:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.01 on epoch=189
03/23/2022 18:53:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 18:53:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.02 on epoch=199
03/23/2022 18:53:26 - INFO - __main__ - Global step 400 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=199
03/23/2022 18:53:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
03/23/2022 18:53:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
03/23/2022 18:53:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.01 on epoch=214
03/23/2022 18:53:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.01 on epoch=219
03/23/2022 18:53:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.03 on epoch=224
03/23/2022 18:53:43 - INFO - __main__ - Global step 450 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 18:53:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.01 on epoch=229
03/23/2022 18:53:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.03 on epoch=234
03/23/2022 18:53:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 18:53:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.01 on epoch=244
03/23/2022 18:53:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/23/2022 18:53:59 - INFO - __main__ - Global step 500 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=249
03/23/2022 18:54:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 18:54:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 18:54:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
03/23/2022 18:54:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
03/23/2022 18:54:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/23/2022 18:54:16 - INFO - __main__ - Global step 550 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 18:54:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/23/2022 18:54:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 18:54:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 18:54:29 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/23/2022 18:54:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 18:54:33 - INFO - __main__ - Global step 600 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 18:54:36 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/23/2022 18:54:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 18:54:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/23/2022 18:54:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/23/2022 18:54:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 18:54:50 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 18:54:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/23/2022 18:54:56 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 18:54:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.02 on epoch=339
03/23/2022 18:55:03 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 18:55:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 18:55:07 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 18:55:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/23/2022 18:55:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
03/23/2022 18:55:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 18:55:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 18:55:22 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 18:55:23 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=374
03/23/2022 18:55:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 18:55:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/23/2022 18:55:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 18:55:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 18:55:39 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 18:55:40 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=399
03/23/2022 18:55:43 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 18:55:46 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 18:55:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 18:55:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 18:55:56 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 18:55:57 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 18:56:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 18:56:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 18:56:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 18:56:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 18:56:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 18:56:14 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 18:56:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 18:56:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 18:56:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 18:56:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 18:56:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 18:56:30 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 18:56:34 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 18:56:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 18:56:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
03/23/2022 18:56:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 18:56:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 18:56:47 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=499
03/23/2022 18:56:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 18:56:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 18:56:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 18:57:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
03/23/2022 18:57:03 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 18:57:04 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=524
03/23/2022 18:57:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 18:57:10 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 18:57:14 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/23/2022 18:57:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 18:57:20 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 18:57:21 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/23/2022 18:57:24 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 18:57:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 18:57:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/23/2022 18:57:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 18:57:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 18:57:38 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 18:57:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 18:57:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 18:57:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 18:57:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 18:57:54 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 18:57:55 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/23/2022 18:57:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 18:58:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 18:58:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 18:58:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 18:58:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 18:58:11 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 18:58:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 18:58:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 18:58:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 18:58:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 18:58:27 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
03/23/2022 18:58:28 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 18:58:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 18:58:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 18:58:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 18:58:41 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 18:58:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 18:58:45 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 18:58:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 18:58:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 18:58:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 18:58:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 18:59:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 18:59:02 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 18:59:05 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 18:59:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 18:59:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 18:59:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 18:59:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 18:59:18 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 18:59:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 18:59:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 18:59:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
03/23/2022 18:59:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/23/2022 18:59:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 18:59:35 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=749
03/23/2022 18:59:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 18:59:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 18:59:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 18:59:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 18:59:51 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 18:59:52 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/23/2022 18:59:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 18:59:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 19:00:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 19:00:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 19:00:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 19:00:09 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 19:00:12 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 19:00:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 19:00:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 19:00:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 19:00:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 19:00:25 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/23/2022 19:00:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 19:00:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 19:00:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 19:00:38 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 19:00:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 19:00:42 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/23/2022 19:00:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 19:00:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 19:00:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 19:00:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 19:00:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 19:00:59 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/23/2022 19:01:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 19:01:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 19:01:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 19:01:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 19:01:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 19:01:15 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 19:01:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 19:01:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 19:01:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 19:01:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 19:01:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 19:01:32 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/23/2022 19:01:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 19:01:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 19:01:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 19:01:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 19:01:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 19:01:49 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/23/2022 19:01:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 19:01:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 19:01:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 19:02:02 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 19:02:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 19:02:06 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/23/2022 19:02:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 19:02:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 19:02:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 19:02:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 19:02:22 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 19:02:22 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/23/2022 19:02:22 - INFO - __main__ - save last model!
03/23/2022 19:02:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 19:02:22 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 19:02:22 - INFO - __main__ - Printing 3 examples
03/23/2022 19:02:22 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 19:02:22 - INFO - __main__ - ['negative']
03/23/2022 19:02:22 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 19:02:22 - INFO - __main__ - ['negative']
03/23/2022 19:02:22 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 19:02:22 - INFO - __main__ - ['negative']
03/23/2022 19:02:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:02:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:02:23 - INFO - __main__ - Printing 3 examples
03/23/2022 19:02:23 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/23/2022 19:02:23 - INFO - __main__ - ['negative']
03/23/2022 19:02:23 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/23/2022 19:02:23 - INFO - __main__ - ['negative']
03/23/2022 19:02:23 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/23/2022 19:02:23 - INFO - __main__ - ['negative']
03/23/2022 19:02:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 19:02:23 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:02:23 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:02:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:02:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:02:23 - INFO - __main__ - Printing 3 examples
03/23/2022 19:02:23 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/23/2022 19:02:23 - INFO - __main__ - ['negative']
03/23/2022 19:02:23 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/23/2022 19:02:23 - INFO - __main__ - ['negative']
03/23/2022 19:02:23 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/23/2022 19:02:23 - INFO - __main__ - ['negative']
03/23/2022 19:02:23 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:02:23 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:02:23 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:02:24 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 19:02:42 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:02:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:02:43 - INFO - __main__ - Starting training!
03/23/2022 19:02:47 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_42_0.2_8_predictions.txt
03/23/2022 19:02:47 - INFO - __main__ - Classification-F1 on test data: 0.9420
03/23/2022 19:02:48 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.2, bsz=8, dev_performance=1.0, test_performance=0.9419767907162866
03/23/2022 19:02:48 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.5, bsz=8 ...
03/23/2022 19:02:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:02:49 - INFO - __main__ - Printing 3 examples
03/23/2022 19:02:49 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/23/2022 19:02:49 - INFO - __main__ - ['negative']
03/23/2022 19:02:49 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/23/2022 19:02:49 - INFO - __main__ - ['negative']
03/23/2022 19:02:49 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/23/2022 19:02:49 - INFO - __main__ - ['negative']
03/23/2022 19:02:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:02:49 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:02:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:02:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:02:49 - INFO - __main__ - Printing 3 examples
03/23/2022 19:02:49 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/23/2022 19:02:49 - INFO - __main__ - ['negative']
03/23/2022 19:02:49 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/23/2022 19:02:49 - INFO - __main__ - ['negative']
03/23/2022 19:02:49 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/23/2022 19:02:49 - INFO - __main__ - ['negative']
03/23/2022 19:02:49 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:02:49 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:02:49 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:03:07 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:03:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:03:08 - INFO - __main__ - Starting training!
03/23/2022 19:03:12 - INFO - __main__ - Step 10 Global step 10 Train loss 0.54 on epoch=4
03/23/2022 19:03:15 - INFO - __main__ - Step 20 Global step 20 Train loss 0.27 on epoch=9
03/23/2022 19:03:18 - INFO - __main__ - Step 30 Global step 30 Train loss 0.16 on epoch=14
03/23/2022 19:03:21 - INFO - __main__ - Step 40 Global step 40 Train loss 0.14 on epoch=19
03/23/2022 19:03:24 - INFO - __main__ - Step 50 Global step 50 Train loss 0.09 on epoch=24
03/23/2022 19:03:25 - INFO - __main__ - Global step 50 Train loss 0.24 Classification-F1 0.9687194525904204 on epoch=24
03/23/2022 19:03:25 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9687194525904204 on epoch=24, global_step=50
03/23/2022 19:03:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.04 on epoch=29
03/23/2022 19:03:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.03 on epoch=34
03/23/2022 19:03:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.03 on epoch=39
03/23/2022 19:03:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.03 on epoch=44
03/23/2022 19:03:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.01 on epoch=49
03/23/2022 19:03:40 - INFO - __main__ - Global step 100 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=49
03/23/2022 19:03:43 - INFO - __main__ - Step 110 Global step 110 Train loss 0.01 on epoch=54
03/23/2022 19:03:46 - INFO - __main__ - Step 120 Global step 120 Train loss 0.03 on epoch=59
03/23/2022 19:03:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.01 on epoch=64
03/23/2022 19:03:53 - INFO - __main__ - Step 140 Global step 140 Train loss 0.01 on epoch=69
03/23/2022 19:03:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.00 on epoch=74
03/23/2022 19:03:56 - INFO - __main__ - Global step 150 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 19:03:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.03 on epoch=79
03/23/2022 19:04:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.00 on epoch=84
03/23/2022 19:04:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.00 on epoch=89
03/23/2022 19:04:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.01 on epoch=94
03/23/2022 19:04:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.00 on epoch=99
03/23/2022 19:04:12 - INFO - __main__ - Global step 200 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=99
03/23/2022 19:04:15 - INFO - __main__ - Step 210 Global step 210 Train loss 0.00 on epoch=104
03/23/2022 19:04:18 - INFO - __main__ - Step 220 Global step 220 Train loss 0.00 on epoch=109
03/23/2022 19:04:21 - INFO - __main__ - Step 230 Global step 230 Train loss 0.00 on epoch=114
03/23/2022 19:04:24 - INFO - __main__ - Step 240 Global step 240 Train loss 0.00 on epoch=119
03/23/2022 19:04:27 - INFO - __main__ - Step 250 Global step 250 Train loss 0.01 on epoch=124
03/23/2022 19:04:28 - INFO - __main__ - Global step 250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 19:04:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 19:04:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 19:04:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 19:04:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 19:04:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.01 on epoch=149
03/23/2022 19:04:43 - INFO - __main__ - Global step 300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 19:04:47 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 19:04:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 19:04:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 19:04:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 19:04:59 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 19:04:59 - INFO - __main__ - Global step 350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=174
03/23/2022 19:05:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 19:05:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 19:05:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.01 on epoch=189
03/23/2022 19:05:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 19:05:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 19:05:15 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9375 on epoch=199
03/23/2022 19:05:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 19:05:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 19:05:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 19:05:27 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 19:05:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 19:05:31 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 19:05:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 19:05:37 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 19:05:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 19:05:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 19:05:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 19:05:47 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=249
03/23/2022 19:05:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 19:05:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 19:05:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 19:05:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 19:06:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 19:06:02 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 19:06:06 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 19:06:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 19:06:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 19:06:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 19:06:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 19:06:18 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 19:06:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 19:06:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 19:06:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 19:06:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 19:06:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 19:06:34 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 19:06:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 19:06:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 19:06:43 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 19:06:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 19:06:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 19:06:50 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 19:06:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 19:06:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 19:06:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 19:07:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 19:07:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 19:07:06 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=374
03/23/2022 19:07:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 19:07:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 19:07:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 19:07:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 19:07:21 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 19:07:21 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=399
03/23/2022 19:07:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 19:07:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 19:07:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 19:07:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 19:07:37 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 19:07:37 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 19:07:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 19:07:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 19:07:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 19:07:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 19:07:52 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 19:07:53 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 19:07:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 19:07:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 19:08:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 19:08:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 19:08:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 19:08:09 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 19:08:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 19:08:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 19:08:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 19:08:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 19:08:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 19:08:25 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=499
03/23/2022 19:08:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 19:08:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 19:08:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 19:08:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 19:08:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 19:08:40 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/23/2022 19:08:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 19:08:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 19:08:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 19:08:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 19:08:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 19:08:56 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/23/2022 19:08:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 19:09:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 19:09:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 19:09:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 19:09:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 19:09:12 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 19:09:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 19:09:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 19:09:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 19:09:24 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 19:09:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 19:09:28 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/23/2022 19:09:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 19:09:34 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 19:09:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 19:09:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 19:09:43 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 19:09:43 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 19:09:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 19:09:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 19:09:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 19:09:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 19:09:58 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 19:09:59 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 19:10:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=654
03/23/2022 19:10:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
03/23/2022 19:10:08 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 19:10:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 19:10:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 19:10:15 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 19:10:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 19:10:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 19:10:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 19:10:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 19:10:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 19:10:31 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 19:10:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 19:10:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 19:10:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 19:10:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 19:10:46 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 19:10:46 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 19:10:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 19:10:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 19:10:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 19:10:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 19:11:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 19:11:02 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/23/2022 19:11:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 19:11:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 19:11:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 19:11:14 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 19:11:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 19:11:18 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/23/2022 19:11:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 19:11:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 19:11:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 19:11:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 19:11:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 19:11:34 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 19:11:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 19:11:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 19:11:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 19:11:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 19:11:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 19:11:49 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/23/2022 19:11:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 19:11:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 19:11:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 19:12:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 19:12:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 19:12:05 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/23/2022 19:12:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 19:12:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 19:12:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 19:12:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 19:12:20 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 19:12:21 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/23/2022 19:12:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 19:12:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 19:12:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 19:12:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 19:12:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 19:12:36 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 19:12:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 19:12:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 19:12:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 19:12:48 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 19:12:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 19:12:52 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/23/2022 19:12:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 19:12:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 19:13:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 19:13:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 19:13:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 19:13:08 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/23/2022 19:13:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 19:13:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 19:13:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 19:13:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 19:13:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 19:13:23 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/23/2022 19:13:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 19:13:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 19:13:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 19:13:35 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 19:13:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 19:13:39 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/23/2022 19:13:39 - INFO - __main__ - save last model!
03/23/2022 19:13:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 19:13:39 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 19:13:39 - INFO - __main__ - Printing 3 examples
03/23/2022 19:13:39 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 19:13:39 - INFO - __main__ - ['negative']
03/23/2022 19:13:39 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 19:13:39 - INFO - __main__ - ['negative']
03/23/2022 19:13:39 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 19:13:39 - INFO - __main__ - ['negative']
03/23/2022 19:13:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:13:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:13:40 - INFO - __main__ - Printing 3 examples
03/23/2022 19:13:40 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/23/2022 19:13:40 - INFO - __main__ - ['negative']
03/23/2022 19:13:40 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/23/2022 19:13:40 - INFO - __main__ - ['negative']
03/23/2022 19:13:40 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/23/2022 19:13:40 - INFO - __main__ - ['negative']
03/23/2022 19:13:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 19:13:40 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:13:40 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:13:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:13:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:13:40 - INFO - __main__ - Printing 3 examples
03/23/2022 19:13:40 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/23/2022 19:13:40 - INFO - __main__ - ['negative']
03/23/2022 19:13:40 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/23/2022 19:13:40 - INFO - __main__ - ['negative']
03/23/2022 19:13:40 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/23/2022 19:13:40 - INFO - __main__ - ['negative']
03/23/2022 19:13:40 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:13:40 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:13:40 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:13:41 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 19:13:57 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:13:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:13:58 - INFO - __main__ - Starting training!
03/23/2022 19:14:04 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_87_0.5_8_predictions.txt
03/23/2022 19:14:04 - INFO - __main__ - Classification-F1 on test data: 0.9370
03/23/2022 19:14:04 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.5, bsz=8, dev_performance=0.9687194525904204, test_performance=0.936998424960624
03/23/2022 19:14:04 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.4, bsz=8 ...
03/23/2022 19:14:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:14:05 - INFO - __main__ - Printing 3 examples
03/23/2022 19:14:05 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/23/2022 19:14:05 - INFO - __main__ - ['negative']
03/23/2022 19:14:05 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/23/2022 19:14:05 - INFO - __main__ - ['negative']
03/23/2022 19:14:05 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/23/2022 19:14:05 - INFO - __main__ - ['negative']
03/23/2022 19:14:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:14:05 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:14:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:14:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:14:05 - INFO - __main__ - Printing 3 examples
03/23/2022 19:14:05 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/23/2022 19:14:05 - INFO - __main__ - ['negative']
03/23/2022 19:14:05 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/23/2022 19:14:05 - INFO - __main__ - ['negative']
03/23/2022 19:14:05 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/23/2022 19:14:05 - INFO - __main__ - ['negative']
03/23/2022 19:14:05 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:14:05 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:14:05 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:14:21 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:14:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:14:22 - INFO - __main__ - Starting training!
03/23/2022 19:14:25 - INFO - __main__ - Step 10 Global step 10 Train loss 0.45 on epoch=4
03/23/2022 19:14:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.29 on epoch=9
03/23/2022 19:14:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.18 on epoch=14
03/23/2022 19:14:34 - INFO - __main__ - Step 40 Global step 40 Train loss 0.12 on epoch=19
03/23/2022 19:14:37 - INFO - __main__ - Step 50 Global step 50 Train loss 0.07 on epoch=24
03/23/2022 19:14:38 - INFO - __main__ - Global step 50 Train loss 0.22 Classification-F1 0.9687194525904204 on epoch=24
03/23/2022 19:14:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9687194525904204 on epoch=24, global_step=50
03/23/2022 19:14:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.05 on epoch=29
03/23/2022 19:14:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.03 on epoch=34
03/23/2022 19:14:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.04 on epoch=39
03/23/2022 19:14:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.07 on epoch=44
03/23/2022 19:14:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.02 on epoch=49
03/23/2022 19:14:54 - INFO - __main__ - Global step 100 Train loss 0.04 Classification-F1 0.9687194525904204 on epoch=49
03/23/2022 19:14:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.02 on epoch=54
03/23/2022 19:15:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.02 on epoch=59
03/23/2022 19:15:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.01 on epoch=64
03/23/2022 19:15:06 - INFO - __main__ - Step 140 Global step 140 Train loss 0.01 on epoch=69
03/23/2022 19:15:09 - INFO - __main__ - Step 150 Global step 150 Train loss 0.01 on epoch=74
03/23/2022 19:15:10 - INFO - __main__ - Global step 150 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 19:15:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.02 on epoch=79
03/23/2022 19:15:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.01 on epoch=84
03/23/2022 19:15:19 - INFO - __main__ - Step 180 Global step 180 Train loss 0.00 on epoch=89
03/23/2022 19:15:22 - INFO - __main__ - Step 190 Global step 190 Train loss 0.00 on epoch=94
03/23/2022 19:15:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.01 on epoch=99
03/23/2022 19:15:26 - INFO - __main__ - Global step 200 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=99
03/23/2022 19:15:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 19:15:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.00 on epoch=109
03/23/2022 19:15:35 - INFO - __main__ - Step 230 Global step 230 Train loss 0.01 on epoch=114
03/23/2022 19:15:38 - INFO - __main__ - Step 240 Global step 240 Train loss 0.02 on epoch=119
03/23/2022 19:15:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.00 on epoch=124
03/23/2022 19:15:41 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 19:15:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.00 on epoch=129
03/23/2022 19:15:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.01 on epoch=134
03/23/2022 19:15:50 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 19:15:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.00 on epoch=144
03/23/2022 19:15:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 19:15:57 - INFO - __main__ - Global step 300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 19:16:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 19:16:03 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 19:16:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 19:16:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 19:16:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.03 on epoch=174
03/23/2022 19:16:13 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=174
03/23/2022 19:16:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 19:16:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.00 on epoch=184
03/23/2022 19:16:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 19:16:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 19:16:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 19:16:29 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=199
03/23/2022 19:16:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 19:16:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 19:16:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.01 on epoch=214
03/23/2022 19:16:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 19:16:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 19:16:45 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 19:16:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 19:16:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 19:16:54 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 19:16:57 - INFO - __main__ - Step 490 Global step 490 Train loss 0.01 on epoch=244
03/23/2022 19:17:00 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 19:17:00 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=249
03/23/2022 19:17:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
03/23/2022 19:17:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 19:17:09 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 19:17:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 19:17:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.01 on epoch=274
03/23/2022 19:17:16 - INFO - __main__ - Global step 550 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 19:17:19 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 19:17:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/23/2022 19:17:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 19:17:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 19:17:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 19:17:32 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 19:17:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 19:17:38 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/23/2022 19:17:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 19:17:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 19:17:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 19:17:48 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 19:17:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 19:17:54 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 19:17:57 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 19:18:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 19:18:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 19:18:03 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 19:18:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 19:18:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 19:18:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 19:18:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 19:18:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 19:18:19 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=374
03/23/2022 19:18:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 19:18:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 19:18:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 19:18:31 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
03/23/2022 19:18:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 19:18:35 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=399
03/23/2022 19:18:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 19:18:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
03/23/2022 19:18:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 19:18:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 19:18:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 19:18:51 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 19:18:54 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 19:18:57 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 19:18:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 19:19:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 19:19:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 19:19:06 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 19:19:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/23/2022 19:19:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 19:19:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 19:19:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 19:19:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 19:19:22 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 19:19:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 19:19:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 19:19:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 19:19:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 19:19:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 19:19:38 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=499
03/23/2022 19:19:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 19:19:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 19:19:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 19:19:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 19:19:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 19:19:54 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/23/2022 19:19:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 19:20:00 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 19:20:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 19:20:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 19:20:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 19:20:09 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/23/2022 19:20:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 19:20:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 19:20:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 19:20:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 19:20:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 19:20:25 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 19:20:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 19:20:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 19:20:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 19:20:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 19:20:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 19:20:41 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/23/2022 19:20:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 19:20:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 19:20:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 19:20:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 19:20:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 19:20:57 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 19:21:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 19:21:03 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 19:21:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 19:21:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 19:21:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 19:21:12 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 19:21:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/23/2022 19:21:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 19:21:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 19:21:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 19:21:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 19:21:28 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 19:21:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 19:21:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 19:21:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 19:21:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 19:21:43 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 19:21:44 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 19:21:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 19:21:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 19:21:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
03/23/2022 19:21:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 19:21:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 19:22:00 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 19:22:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 19:22:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 19:22:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 19:22:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 19:22:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 19:22:16 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/23/2022 19:22:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 19:22:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 19:22:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 19:22:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 19:22:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 19:22:32 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/23/2022 19:22:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 19:22:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 19:22:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 19:22:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 19:22:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 19:22:47 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 19:22:50 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 19:22:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 19:22:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 19:22:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=819
03/23/2022 19:23:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 19:23:03 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=824
03/23/2022 19:23:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 19:23:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 19:23:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/23/2022 19:23:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 19:23:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 19:23:19 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/23/2022 19:23:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
03/23/2022 19:23:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 19:23:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 19:23:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 19:23:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/23/2022 19:23:35 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=874
03/23/2022 19:23:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 19:23:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 19:23:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 19:23:47 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 19:23:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 19:23:51 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 19:23:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 19:23:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 19:24:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 19:24:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=919
03/23/2022 19:24:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 19:24:06 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=924
03/23/2022 19:24:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 19:24:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 19:24:16 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 19:24:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 19:24:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 19:24:22 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/23/2022 19:24:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 19:24:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 19:24:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 19:24:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 19:24:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 19:24:38 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/23/2022 19:24:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 19:24:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 19:24:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 19:24:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 19:24:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 19:24:54 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/23/2022 19:24:54 - INFO - __main__ - save last model!
03/23/2022 19:24:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 19:24:54 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 19:24:54 - INFO - __main__ - Printing 3 examples
03/23/2022 19:24:54 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 19:24:54 - INFO - __main__ - ['negative']
03/23/2022 19:24:54 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 19:24:54 - INFO - __main__ - ['negative']
03/23/2022 19:24:54 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 19:24:54 - INFO - __main__ - ['negative']
03/23/2022 19:24:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:24:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:24:54 - INFO - __main__ - Printing 3 examples
03/23/2022 19:24:54 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/23/2022 19:24:54 - INFO - __main__ - ['negative']
03/23/2022 19:24:54 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/23/2022 19:24:54 - INFO - __main__ - ['negative']
03/23/2022 19:24:54 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/23/2022 19:24:54 - INFO - __main__ - ['negative']
03/23/2022 19:24:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 19:24:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:24:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:24:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:24:54 - INFO - __main__ - Printing 3 examples
03/23/2022 19:24:54 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/23/2022 19:24:54 - INFO - __main__ - ['negative']
03/23/2022 19:24:54 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/23/2022 19:24:54 - INFO - __main__ - ['negative']
03/23/2022 19:24:54 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/23/2022 19:24:54 - INFO - __main__ - ['negative']
03/23/2022 19:24:54 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:24:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:24:54 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:24:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:24:55 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 19:25:10 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:25:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:25:10 - INFO - __main__ - Starting training!
03/23/2022 19:25:19 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_87_0.4_8_predictions.txt
03/23/2022 19:25:19 - INFO - __main__ - Classification-F1 on test data: 0.9390
03/23/2022 19:25:19 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.4, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9389618511569731
03/23/2022 19:25:19 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.3, bsz=8 ...
03/23/2022 19:25:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:25:20 - INFO - __main__ - Printing 3 examples
03/23/2022 19:25:20 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/23/2022 19:25:20 - INFO - __main__ - ['negative']
03/23/2022 19:25:20 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/23/2022 19:25:20 - INFO - __main__ - ['negative']
03/23/2022 19:25:20 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/23/2022 19:25:20 - INFO - __main__ - ['negative']
03/23/2022 19:25:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:25:20 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:25:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:25:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:25:20 - INFO - __main__ - Printing 3 examples
03/23/2022 19:25:20 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/23/2022 19:25:20 - INFO - __main__ - ['negative']
03/23/2022 19:25:20 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/23/2022 19:25:20 - INFO - __main__ - ['negative']
03/23/2022 19:25:20 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/23/2022 19:25:20 - INFO - __main__ - ['negative']
03/23/2022 19:25:20 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:25:20 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:25:20 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:25:39 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:25:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:25:39 - INFO - __main__ - Starting training!
03/23/2022 19:25:43 - INFO - __main__ - Step 10 Global step 10 Train loss 0.48 on epoch=4
03/23/2022 19:25:46 - INFO - __main__ - Step 20 Global step 20 Train loss 0.32 on epoch=9
03/23/2022 19:25:49 - INFO - __main__ - Step 30 Global step 30 Train loss 0.25 on epoch=14
03/23/2022 19:25:52 - INFO - __main__ - Step 40 Global step 40 Train loss 0.17 on epoch=19
03/23/2022 19:25:55 - INFO - __main__ - Step 50 Global step 50 Train loss 0.14 on epoch=24
03/23/2022 19:25:56 - INFO - __main__ - Global step 50 Train loss 0.27 Classification-F1 0.9687194525904204 on epoch=24
03/23/2022 19:25:56 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9687194525904204 on epoch=24, global_step=50
03/23/2022 19:25:59 - INFO - __main__ - Step 60 Global step 60 Train loss 0.07 on epoch=29
03/23/2022 19:26:02 - INFO - __main__ - Step 70 Global step 70 Train loss 0.07 on epoch=34
03/23/2022 19:26:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.07 on epoch=39
03/23/2022 19:26:08 - INFO - __main__ - Step 90 Global step 90 Train loss 0.04 on epoch=44
03/23/2022 19:26:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.05 on epoch=49
03/23/2022 19:26:12 - INFO - __main__ - Global step 100 Train loss 0.06 Classification-F1 0.9687194525904204 on epoch=49
03/23/2022 19:26:15 - INFO - __main__ - Step 110 Global step 110 Train loss 0.03 on epoch=54
03/23/2022 19:26:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.02 on epoch=59
03/23/2022 19:26:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.01 on epoch=64
03/23/2022 19:26:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.01 on epoch=69
03/23/2022 19:26:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.01 on epoch=74
03/23/2022 19:26:27 - INFO - __main__ - Global step 150 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 19:26:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.01 on epoch=79
03/23/2022 19:26:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.03 on epoch=84
03/23/2022 19:26:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.02 on epoch=89
03/23/2022 19:26:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.00 on epoch=94
03/23/2022 19:26:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.04 on epoch=99
03/23/2022 19:26:43 - INFO - __main__ - Global step 200 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=99
03/23/2022 19:26:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 19:26:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.01 on epoch=109
03/23/2022 19:26:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.00 on epoch=114
03/23/2022 19:26:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.01 on epoch=119
03/23/2022 19:26:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.00 on epoch=124
03/23/2022 19:26:59 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 19:27:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.01 on epoch=129
03/23/2022 19:27:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.00 on epoch=134
03/23/2022 19:27:08 - INFO - __main__ - Step 280 Global step 280 Train loss 0.00 on epoch=139
03/23/2022 19:27:11 - INFO - __main__ - Step 290 Global step 290 Train loss 0.04 on epoch=144
03/23/2022 19:27:14 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 19:27:15 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 19:27:18 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 19:27:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 19:27:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.00 on epoch=164
03/23/2022 19:27:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.00 on epoch=169
03/23/2022 19:27:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 19:27:30 - INFO - __main__ - Global step 350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=174
03/23/2022 19:27:33 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/23/2022 19:27:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.01 on epoch=184
03/23/2022 19:27:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/23/2022 19:27:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 19:27:45 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 19:27:46 - INFO - __main__ - Global step 400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=199
03/23/2022 19:27:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.00 on epoch=204
03/23/2022 19:27:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.01 on epoch=209
03/23/2022 19:27:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.01 on epoch=214
03/23/2022 19:27:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 19:28:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 19:28:01 - INFO - __main__ - Global step 450 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 19:28:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 19:28:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 19:28:10 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 19:28:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 19:28:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/23/2022 19:28:17 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=249
03/23/2022 19:28:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 19:28:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 19:28:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 19:28:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 19:28:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 19:28:33 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 19:28:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 19:28:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 19:28:41 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 19:28:44 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 19:28:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 19:28:48 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 19:28:51 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 19:28:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 19:28:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 19:29:00 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 19:29:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 19:29:04 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 19:29:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 19:29:10 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 19:29:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 19:29:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 19:29:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 19:29:19 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 19:29:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 19:29:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 19:29:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 19:29:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 19:29:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 19:29:35 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=374
03/23/2022 19:29:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.16 on epoch=379
03/23/2022 19:29:41 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 19:29:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 19:29:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 19:29:50 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 19:29:50 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=399
03/23/2022 19:29:53 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 19:29:56 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/23/2022 19:29:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 19:30:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 19:30:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 19:30:06 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 19:30:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 19:30:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 19:30:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 19:30:18 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 19:30:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 19:30:21 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 19:30:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 19:30:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 19:30:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 19:30:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 19:30:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 19:30:37 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 19:30:40 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 19:30:43 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 19:30:46 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 19:30:49 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 19:30:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 19:30:53 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=499
03/23/2022 19:30:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 19:30:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 19:31:02 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 19:31:05 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 19:31:08 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 19:31:08 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/23/2022 19:31:11 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 19:31:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 19:31:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 19:31:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 19:31:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 19:31:24 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/23/2022 19:31:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 19:31:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 19:31:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 19:31:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 19:31:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 19:31:40 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 19:31:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 19:31:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 19:31:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 19:31:52 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 19:31:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 19:31:55 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/23/2022 19:31:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 19:32:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 19:32:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 19:32:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 19:32:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/23/2022 19:32:11 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 19:32:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 19:32:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 19:32:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 19:32:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 19:32:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 19:32:27 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 19:32:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 19:32:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 19:32:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 19:32:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 19:32:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 19:32:42 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 19:32:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 19:32:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 19:32:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 19:32:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 19:32:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 19:32:58 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 19:33:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 19:33:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 19:33:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 19:33:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 19:33:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/23/2022 19:33:13 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 19:33:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 19:33:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 19:33:22 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 19:33:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 19:33:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 19:33:29 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/23/2022 19:33:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 19:33:35 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 19:33:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 19:33:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 19:33:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 19:33:44 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/23/2022 19:33:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 19:33:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 19:33:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 19:33:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 19:33:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 19:34:00 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 19:34:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 19:34:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 19:34:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 19:34:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 19:34:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 19:34:16 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/23/2022 19:34:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 19:34:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 19:34:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 19:34:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 19:34:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 19:34:31 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/23/2022 19:34:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 19:34:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 19:34:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 19:34:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 19:34:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 19:34:47 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/23/2022 19:34:50 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 19:34:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 19:34:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 19:34:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 19:35:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 19:35:02 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 19:35:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 19:35:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 19:35:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 19:35:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 19:35:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
03/23/2022 19:35:18 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9375 on epoch=924
03/23/2022 19:35:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.09 on epoch=929
03/23/2022 19:35:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 19:35:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 19:35:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 19:35:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 19:35:34 - INFO - __main__ - Global step 1900 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=949
03/23/2022 19:35:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 19:35:39 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 19:35:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 19:35:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 19:35:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 19:35:49 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/23/2022 19:35:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 19:35:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 19:35:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 19:36:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 19:36:04 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 19:36:05 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/23/2022 19:36:05 - INFO - __main__ - save last model!
03/23/2022 19:36:05 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 19:36:05 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 19:36:05 - INFO - __main__ - Printing 3 examples
03/23/2022 19:36:05 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 19:36:05 - INFO - __main__ - ['negative']
03/23/2022 19:36:05 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 19:36:05 - INFO - __main__ - ['negative']
03/23/2022 19:36:05 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 19:36:05 - INFO - __main__ - ['negative']
03/23/2022 19:36:05 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:36:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:36:05 - INFO - __main__ - Printing 3 examples
03/23/2022 19:36:05 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/23/2022 19:36:05 - INFO - __main__ - ['negative']
03/23/2022 19:36:05 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/23/2022 19:36:05 - INFO - __main__ - ['negative']
03/23/2022 19:36:05 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/23/2022 19:36:05 - INFO - __main__ - ['negative']
03/23/2022 19:36:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 19:36:05 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:36:05 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:36:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:36:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:36:05 - INFO - __main__ - Printing 3 examples
03/23/2022 19:36:05 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/23/2022 19:36:05 - INFO - __main__ - ['negative']
03/23/2022 19:36:05 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/23/2022 19:36:05 - INFO - __main__ - ['negative']
03/23/2022 19:36:05 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/23/2022 19:36:05 - INFO - __main__ - ['negative']
03/23/2022 19:36:05 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:36:05 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:36:05 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:36:06 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 19:36:20 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:36:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:36:21 - INFO - __main__ - Starting training!
03/23/2022 19:36:31 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_87_0.3_8_predictions.txt
03/23/2022 19:36:31 - INFO - __main__ - Classification-F1 on test data: 0.9380
03/23/2022 19:36:31 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.3, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9379878456177411
03/23/2022 19:36:31 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.2, bsz=8 ...
03/23/2022 19:36:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:36:32 - INFO - __main__ - Printing 3 examples
03/23/2022 19:36:32 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/23/2022 19:36:32 - INFO - __main__ - ['negative']
03/23/2022 19:36:32 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/23/2022 19:36:32 - INFO - __main__ - ['negative']
03/23/2022 19:36:32 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/23/2022 19:36:32 - INFO - __main__ - ['negative']
03/23/2022 19:36:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:36:32 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:36:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:36:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:36:32 - INFO - __main__ - Printing 3 examples
03/23/2022 19:36:32 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/23/2022 19:36:32 - INFO - __main__ - ['negative']
03/23/2022 19:36:32 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/23/2022 19:36:32 - INFO - __main__ - ['negative']
03/23/2022 19:36:32 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/23/2022 19:36:32 - INFO - __main__ - ['negative']
03/23/2022 19:36:32 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:36:32 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:36:32 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:36:51 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:36:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:36:51 - INFO - __main__ - Starting training!
03/23/2022 19:36:55 - INFO - __main__ - Step 10 Global step 10 Train loss 0.50 on epoch=4
03/23/2022 19:36:58 - INFO - __main__ - Step 20 Global step 20 Train loss 0.32 on epoch=9
03/23/2022 19:37:01 - INFO - __main__ - Step 30 Global step 30 Train loss 0.26 on epoch=14
03/23/2022 19:37:04 - INFO - __main__ - Step 40 Global step 40 Train loss 0.18 on epoch=19
03/23/2022 19:37:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.16 on epoch=24
03/23/2022 19:37:08 - INFO - __main__ - Global step 50 Train loss 0.28 Classification-F1 0.9687194525904204 on epoch=24
03/23/2022 19:37:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.9687194525904204 on epoch=24, global_step=50
03/23/2022 19:37:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.16 on epoch=29
03/23/2022 19:37:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.10 on epoch=34
03/23/2022 19:37:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.09 on epoch=39
03/23/2022 19:37:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.11 on epoch=44
03/23/2022 19:37:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.08 on epoch=49
03/23/2022 19:37:24 - INFO - __main__ - Global step 100 Train loss 0.11 Classification-F1 0.9687194525904204 on epoch=49
03/23/2022 19:37:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.04 on epoch=54
03/23/2022 19:37:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.04 on epoch=59
03/23/2022 19:37:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.04 on epoch=64
03/23/2022 19:37:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.03 on epoch=69
03/23/2022 19:37:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.02 on epoch=74
03/23/2022 19:37:40 - INFO - __main__ - Global step 150 Train loss 0.04 Classification-F1 0.9687194525904204 on epoch=74
03/23/2022 19:37:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.02 on epoch=79
03/23/2022 19:37:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.01 on epoch=84
03/23/2022 19:37:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.01 on epoch=89
03/23/2022 19:37:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.01 on epoch=94
03/23/2022 19:37:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.01 on epoch=99
03/23/2022 19:37:56 - INFO - __main__ - Global step 200 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=99
03/23/2022 19:37:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.01 on epoch=104
03/23/2022 19:38:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.01 on epoch=109
03/23/2022 19:38:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.01 on epoch=114
03/23/2022 19:38:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.01 on epoch=119
03/23/2022 19:38:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.01 on epoch=124
03/23/2022 19:38:12 - INFO - __main__ - Global step 250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=124
03/23/2022 19:38:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.03 on epoch=129
03/23/2022 19:38:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.04 on epoch=134
03/23/2022 19:38:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.01 on epoch=139
03/23/2022 19:38:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.01 on epoch=144
03/23/2022 19:38:27 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/23/2022 19:38:27 - INFO - __main__ - Global step 300 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=149
03/23/2022 19:38:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.00 on epoch=154
03/23/2022 19:38:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.00 on epoch=159
03/23/2022 19:38:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.02 on epoch=164
03/23/2022 19:38:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.02 on epoch=169
03/23/2022 19:38:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/23/2022 19:38:43 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=174
03/23/2022 19:38:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.02 on epoch=179
03/23/2022 19:38:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.01 on epoch=184
03/23/2022 19:38:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.01 on epoch=189
03/23/2022 19:38:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.00 on epoch=194
03/23/2022 19:38:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.00 on epoch=199
03/23/2022 19:38:59 - INFO - __main__ - Global step 400 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=199
03/23/2022 19:39:02 - INFO - __main__ - Step 410 Global step 410 Train loss 0.01 on epoch=204
03/23/2022 19:39:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/23/2022 19:39:08 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/23/2022 19:39:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 19:39:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/23/2022 19:39:15 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=224
03/23/2022 19:39:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/23/2022 19:39:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/23/2022 19:39:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 19:39:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/23/2022 19:39:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/23/2022 19:39:31 - INFO - __main__ - Global step 500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=249
03/23/2022 19:39:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 19:39:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/23/2022 19:39:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 19:39:43 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 19:39:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 19:39:47 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=274
03/23/2022 19:39:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 19:39:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 19:39:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 19:39:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/23/2022 19:40:02 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
03/23/2022 19:40:02 - INFO - __main__ - Global step 600 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=299
03/23/2022 19:40:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/23/2022 19:40:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 19:40:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 19:40:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 19:40:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 19:40:18 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=324
03/23/2022 19:40:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 19:40:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 19:40:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 19:40:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 19:40:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/23/2022 19:40:34 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=349
03/23/2022 19:40:37 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/23/2022 19:40:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 19:40:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 19:40:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 19:40:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 19:40:50 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=374
03/23/2022 19:40:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/23/2022 19:40:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 19:40:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 19:41:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 19:41:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 19:41:06 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=399
03/23/2022 19:41:09 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 19:41:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 19:41:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 19:41:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 19:41:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 19:41:22 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/23/2022 19:41:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 19:41:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 19:41:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 19:41:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 19:41:37 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 19:41:38 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/23/2022 19:41:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
03/23/2022 19:41:44 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 19:41:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 19:41:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 19:41:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 19:41:54 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=474
03/23/2022 19:41:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 19:42:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 19:42:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 19:42:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 19:42:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 19:42:10 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=499
03/23/2022 19:42:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 19:42:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 19:42:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 19:42:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 19:42:25 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 19:42:25 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/23/2022 19:42:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 19:42:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 19:42:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 19:42:38 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 19:42:41 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 19:42:41 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/23/2022 19:42:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 19:42:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 19:42:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 19:42:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 19:42:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 19:42:57 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/23/2022 19:43:00 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
03/23/2022 19:43:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 19:43:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 19:43:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 19:43:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 19:43:13 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=599
03/23/2022 19:43:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/23/2022 19:43:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 19:43:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 19:43:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 19:43:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 19:43:29 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/23/2022 19:43:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 19:43:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 19:43:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/23/2022 19:43:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 19:43:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 19:43:45 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/23/2022 19:43:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 19:43:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 19:43:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 19:43:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 19:44:00 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 19:44:01 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/23/2022 19:44:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 19:44:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 19:44:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 19:44:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 19:44:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 19:44:17 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/23/2022 19:44:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 19:44:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 19:44:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 19:44:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 19:44:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 19:44:32 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/23/2022 19:44:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/23/2022 19:44:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 19:44:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 19:44:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 19:44:48 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 19:44:48 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/23/2022 19:44:51 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 19:44:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 19:44:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 19:45:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 19:45:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 19:45:04 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/23/2022 19:45:07 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 19:45:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 19:45:13 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 19:45:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 19:45:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 19:45:20 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/23/2022 19:45:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 19:45:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 19:45:29 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 19:45:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 19:45:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 19:45:36 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/23/2022 19:45:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 19:45:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 19:45:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 19:45:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 19:45:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 19:45:52 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/23/2022 19:45:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 19:45:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 19:46:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/23/2022 19:46:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 19:46:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 19:46:08 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/23/2022 19:46:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 19:46:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 19:46:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 19:46:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 19:46:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 19:46:24 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/23/2022 19:46:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 19:46:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 19:46:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 19:46:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 19:46:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 19:46:40 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/23/2022 19:46:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 19:46:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 19:46:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 19:46:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 19:46:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 19:46:56 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/23/2022 19:46:59 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 19:47:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 19:47:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 19:47:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
03/23/2022 19:47:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 19:47:11 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=974
03/23/2022 19:47:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 19:47:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 19:47:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 19:47:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 19:47:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 19:47:27 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/23/2022 19:47:27 - INFO - __main__ - save last model!
03/23/2022 19:47:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 19:47:27 - INFO - __main__ - Start tokenizing ... 1000 instances
03/23/2022 19:47:27 - INFO - __main__ - Printing 3 examples
03/23/2022 19:47:27 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/23/2022 19:47:27 - INFO - __main__ - ['negative']
03/23/2022 19:47:27 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/23/2022 19:47:27 - INFO - __main__ - ['negative']
03/23/2022 19:47:27 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/23/2022 19:47:27 - INFO - __main__ - ['negative']
03/23/2022 19:47:27 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:47:28 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:47:29 - INFO - __main__ - Loaded 1000 examples from test data
03/23/2022 19:47:52 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_87_0.2_8_predictions.txt
03/23/2022 19:47:52 - INFO - __main__ - Classification-F1 on test data: 0.9390
03/23/2022 19:47:52 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.2, bsz=8, dev_performance=0.9687194525904204, test_performance=0.938973087131425
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (26351): No such process
Task: tab_fact, Checkpoint: models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/23/2022 19:47:57 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/23/2022 19:47:57 - INFO - __main__ - models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
03/23/2022 19:47:57 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/23/2022 19:47:57 - INFO - __main__ - models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
03/23/2022 19:47:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/23/2022 19:47:59 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/23/2022 19:47:59 - INFO - __main__ - args.device: cuda:0
03/23/2022 19:47:59 - INFO - __main__ - Using 2 gpus
03/23/2022 19:47:59 - INFO - __main__ - args.device: cuda:1
03/23/2022 19:47:59 - INFO - __main__ - Using 2 gpus
03/23/2022 19:47:59 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
03/23/2022 19:47:59 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
03/23/2022 19:48:04 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.5, bsz=8 ...
03/23/2022 19:48:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:48:05 - INFO - __main__ - Printing 3 examples
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:48:05 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:48:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:48:05 - INFO - __main__ - Printing 3 examples
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 19:48:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:48:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:48:05 - INFO - __main__ - Printing 3 examples
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:48:05 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:48:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 19:48:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 19:48:05 - INFO - __main__ - Printing 3 examples
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/23/2022 19:48:05 - INFO - __main__ - ['refuted']
03/23/2022 19:48:05 - INFO - __main__ - Tokenizing Input ...
03/23/2022 19:48:05 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:48:05 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:48:05 - INFO - __main__ - Tokenizing Output ...
03/23/2022 19:48:05 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 19:48:22 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:48:23 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 19:48:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:48:23 - INFO - __main__ - Starting training!
03/23/2022 19:48:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 19:48:28 - INFO - __main__ - Starting training!
03/23/2022 19:48:33 - INFO - __main__ - Step 10 Global step 10 Train loss 3.63 on epoch=4
03/23/2022 19:48:38 - INFO - __main__ - Step 20 Global step 20 Train loss 1.06 on epoch=9
03/23/2022 19:48:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=14
03/23/2022 19:48:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.35 on epoch=19
03/23/2022 19:48:51 - INFO - __main__ - Step 50 Global step 50 Train loss 0.28 on epoch=24
03/23/2022 19:48:52 - INFO - __main__ - Global step 50 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 19:48:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 19:48:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=29
03/23/2022 19:49:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.25 on epoch=34
03/23/2022 19:49:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=39
03/23/2022 19:49:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.27 on epoch=44
03/23/2022 19:49:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
03/23/2022 19:49:16 - INFO - __main__ - Global step 100 Train loss 0.26 Classification-F1 0.39756367663344405 on epoch=49
03/23/2022 19:49:16 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39756367663344405 on epoch=49, global_step=100
03/23/2022 19:49:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.23 on epoch=54
03/23/2022 19:49:24 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=59
03/23/2022 19:49:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.19 on epoch=64
03/23/2022 19:49:33 - INFO - __main__ - Step 140 Global step 140 Train loss 0.22 on epoch=69
03/23/2022 19:49:37 - INFO - __main__ - Step 150 Global step 150 Train loss 0.20 on epoch=74
03/23/2022 19:49:39 - INFO - __main__ - Global step 150 Train loss 0.21 Classification-F1 0.39756367663344405 on epoch=74
03/23/2022 19:49:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.20 on epoch=79
03/23/2022 19:49:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.18 on epoch=84
03/23/2022 19:49:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.19 on epoch=89
03/23/2022 19:49:56 - INFO - __main__ - Step 190 Global step 190 Train loss 0.19 on epoch=94
03/23/2022 19:50:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.20 on epoch=99
03/23/2022 19:50:02 - INFO - __main__ - Global step 200 Train loss 0.19 Classification-F1 0.39756367663344405 on epoch=99
03/23/2022 19:50:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.12 on epoch=104
03/23/2022 19:50:11 - INFO - __main__ - Step 220 Global step 220 Train loss 0.15 on epoch=109
03/23/2022 19:50:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.13 on epoch=114
03/23/2022 19:50:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.17 on epoch=119
03/23/2022 19:50:24 - INFO - __main__ - Step 250 Global step 250 Train loss 0.13 on epoch=124
03/23/2022 19:50:26 - INFO - __main__ - Global step 250 Train loss 0.14 Classification-F1 0.39756367663344405 on epoch=124
03/23/2022 19:50:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.11 on epoch=129
03/23/2022 19:50:34 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
03/23/2022 19:50:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.10 on epoch=139
03/23/2022 19:50:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.09 on epoch=144
03/23/2022 19:50:47 - INFO - __main__ - Step 300 Global step 300 Train loss 0.05 on epoch=149
03/23/2022 19:50:49 - INFO - __main__ - Global step 300 Train loss 0.10 Classification-F1 0.5835835835835835 on epoch=149
03/23/2022 19:50:49 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.5835835835835835 on epoch=149, global_step=300
03/23/2022 19:50:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.11 on epoch=154
03/23/2022 19:50:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.07 on epoch=159
03/23/2022 19:51:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.10 on epoch=164
03/23/2022 19:51:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.04 on epoch=169
03/23/2022 19:51:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.02 on epoch=174
03/23/2022 19:51:12 - INFO - __main__ - Global step 350 Train loss 0.07 Classification-F1 0.5270935960591133 on epoch=174
03/23/2022 19:51:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.03 on epoch=179
03/23/2022 19:51:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.05 on epoch=184
03/23/2022 19:51:25 - INFO - __main__ - Step 380 Global step 380 Train loss 0.04 on epoch=189
03/23/2022 19:51:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.03 on epoch=194
03/23/2022 19:51:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.02 on epoch=199
03/23/2022 19:51:35 - INFO - __main__ - Global step 400 Train loss 0.03 Classification-F1 0.3333333333333333 on epoch=199
03/23/2022 19:51:40 - INFO - __main__ - Step 410 Global step 410 Train loss 0.05 on epoch=204
03/23/2022 19:51:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.01 on epoch=209
03/23/2022 19:51:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.04 on epoch=214
03/23/2022 19:51:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.01 on epoch=219
03/23/2022 19:51:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.01 on epoch=224
03/23/2022 19:51:59 - INFO - __main__ - Global step 450 Train loss 0.03 Classification-F1 0.5195195195195195 on epoch=224
03/23/2022 19:52:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.02 on epoch=229
03/23/2022 19:52:07 - INFO - __main__ - Step 470 Global step 470 Train loss 0.02 on epoch=234
03/23/2022 19:52:12 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 19:52:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.01 on epoch=244
03/23/2022 19:52:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/23/2022 19:52:22 - INFO - __main__ - Global step 500 Train loss 0.01 Classification-F1 0.3373901284651792 on epoch=249
03/23/2022 19:52:26 - INFO - __main__ - Step 510 Global step 510 Train loss 0.01 on epoch=254
03/23/2022 19:52:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.01 on epoch=259
03/23/2022 19:52:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
03/23/2022 19:52:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
03/23/2022 19:52:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 19:52:45 - INFO - __main__ - Global step 550 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=274
03/23/2022 19:52:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 19:52:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/23/2022 19:52:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 19:53:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/23/2022 19:53:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 19:53:09 - INFO - __main__ - Global step 600 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=299
03/23/2022 19:53:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 19:53:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/23/2022 19:53:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/23/2022 19:53:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 19:53:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 19:53:32 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 0.35714285714285715 on epoch=324
03/23/2022 19:53:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 19:53:40 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 19:53:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 19:53:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 19:53:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 19:53:55 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=349
03/23/2022 19:53:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/23/2022 19:54:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/23/2022 19:54:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 19:54:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
03/23/2022 19:54:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 19:54:19 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=374
03/23/2022 19:54:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/23/2022 19:54:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/23/2022 19:54:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 19:54:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 19:54:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 19:54:42 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=399
03/23/2022 19:54:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/23/2022 19:54:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 19:54:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 19:54:59 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/23/2022 19:55:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/23/2022 19:55:05 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=424
03/23/2022 19:55:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 19:55:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 19:55:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 19:55:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 19:55:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 19:55:28 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=449
03/23/2022 19:55:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 19:55:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 19:55:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 19:55:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 19:55:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 19:55:52 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=474
03/23/2022 19:55:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 19:56:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 19:56:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 19:56:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 19:56:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 19:56:15 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=499
03/23/2022 19:56:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 19:56:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 19:56:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 19:56:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 19:56:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
03/23/2022 19:56:38 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=524
03/23/2022 19:56:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 19:56:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 19:56:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 19:56:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 19:57:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 19:57:02 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=549
03/23/2022 19:57:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 19:57:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 19:57:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 19:57:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 19:57:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 19:57:25 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.3333333333333333 on epoch=574
03/23/2022 19:57:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 19:57:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 19:57:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 19:57:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
03/23/2022 19:57:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 19:57:48 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=599
03/23/2022 19:57:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 19:57:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 19:58:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 19:58:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/23/2022 19:58:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 19:58:12 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.3333333333333333 on epoch=624
03/23/2022 19:58:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 19:58:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 19:58:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 19:58:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 19:58:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 19:58:35 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=649
03/23/2022 19:58:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 19:58:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 19:58:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 19:58:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 19:58:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 19:58:58 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=674
03/23/2022 19:59:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 19:59:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/23/2022 19:59:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 19:59:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 19:59:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 19:59:21 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=699
03/23/2022 19:59:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 19:59:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 19:59:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 19:59:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 19:59:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 19:59:44 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=724
03/23/2022 19:59:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 19:59:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 19:59:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/23/2022 20:00:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 20:00:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 20:00:08 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=749
03/23/2022 20:00:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 20:00:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 20:00:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 20:00:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 20:00:30 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 20:00:31 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=774
03/23/2022 20:00:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 20:00:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 20:00:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 20:00:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 20:00:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 20:00:54 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=799
03/23/2022 20:00:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 20:01:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 20:01:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 20:01:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 20:01:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 20:01:18 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=824
03/23/2022 20:01:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 20:01:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 20:01:31 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 20:01:35 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 20:01:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 20:01:41 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5 on epoch=849
03/23/2022 20:01:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 20:01:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 20:01:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 20:01:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
03/23/2022 20:02:03 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 20:02:05 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=874
03/23/2022 20:02:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 20:02:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 20:02:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 20:02:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 20:02:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 20:02:28 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=899
03/23/2022 20:02:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 20:02:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 20:02:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 20:02:46 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 20:02:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 20:02:51 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=924
03/23/2022 20:02:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 20:03:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 20:03:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 20:03:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 20:03:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 20:03:15 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=949
03/23/2022 20:03:19 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 20:03:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 20:03:28 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 20:03:32 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 20:03:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 20:03:38 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=974
03/23/2022 20:03:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 20:03:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 20:03:51 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 20:03:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 20:04:00 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 20:04:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:04:01 - INFO - __main__ - Printing 3 examples
03/23/2022 20:04:01 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/23/2022 20:04:01 - INFO - __main__ - ['refuted']
03/23/2022 20:04:01 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/23/2022 20:04:01 - INFO - __main__ - ['refuted']
03/23/2022 20:04:01 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/23/2022 20:04:01 - INFO - __main__ - ['refuted']
03/23/2022 20:04:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 20:04:01 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=999
03/23/2022 20:04:01 - INFO - __main__ - save last model!
03/23/2022 20:04:01 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:04:01 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 20:04:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 20:04:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:04:01 - INFO - __main__ - Printing 3 examples
03/23/2022 20:04:01 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/23/2022 20:04:01 - INFO - __main__ - ['refuted']
03/23/2022 20:04:01 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/23/2022 20:04:01 - INFO - __main__ - ['refuted']
03/23/2022 20:04:01 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/23/2022 20:04:01 - INFO - __main__ - ['refuted']
03/23/2022 20:04:01 - INFO - __main__ - Tokenizing Input ...
03/23/2022 20:04:01 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:04:01 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 20:04:02 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 20:04:02 - INFO - __main__ - Printing 3 examples
03/23/2022 20:04:02 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 20:04:02 - INFO - __main__ - ['entailed']
03/23/2022 20:04:02 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 20:04:02 - INFO - __main__ - ['entailed']
03/23/2022 20:04:02 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 20:04:02 - INFO - __main__ - ['entailed']
03/23/2022 20:04:02 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 20:04:20 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 20:04:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 20:04:21 - INFO - __main__ - Starting training!
03/23/2022 20:04:26 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:04:38 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 20:13:01 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.5_8_predictions.txt
03/23/2022 20:13:01 - INFO - __main__ - Classification-F1 on test data: 0.1064
03/23/2022 20:13:01 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.5, bsz=8, dev_performance=0.5835835835835835, test_performance=0.10638976152459481
03/23/2022 20:13:01 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.4, bsz=8 ...
03/23/2022 20:13:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:13:02 - INFO - __main__ - Printing 3 examples
03/23/2022 20:13:02 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/23/2022 20:13:02 - INFO - __main__ - ['refuted']
03/23/2022 20:13:02 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/23/2022 20:13:02 - INFO - __main__ - ['refuted']
03/23/2022 20:13:02 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/23/2022 20:13:02 - INFO - __main__ - ['refuted']
03/23/2022 20:13:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 20:13:02 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:13:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 20:13:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:13:02 - INFO - __main__ - Printing 3 examples
03/23/2022 20:13:02 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/23/2022 20:13:02 - INFO - __main__ - ['refuted']
03/23/2022 20:13:02 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/23/2022 20:13:02 - INFO - __main__ - ['refuted']
03/23/2022 20:13:02 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/23/2022 20:13:02 - INFO - __main__ - ['refuted']
03/23/2022 20:13:02 - INFO - __main__ - Tokenizing Input ...
03/23/2022 20:13:02 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:13:02 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 20:13:17 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 20:13:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 20:13:18 - INFO - __main__ - Starting training!
03/23/2022 20:13:25 - INFO - __main__ - Step 10 Global step 10 Train loss 3.60 on epoch=4
03/23/2022 20:13:29 - INFO - __main__ - Step 20 Global step 20 Train loss 1.34 on epoch=9
03/23/2022 20:13:34 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=14
03/23/2022 20:13:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
03/23/2022 20:13:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=24
03/23/2022 20:13:44 - INFO - __main__ - Global step 50 Train loss 1.23 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 20:13:44 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 20:13:48 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=29
03/23/2022 20:13:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=34
03/23/2022 20:13:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
03/23/2022 20:14:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.21 on epoch=44
03/23/2022 20:14:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.20 on epoch=49
03/23/2022 20:14:07 - INFO - __main__ - Global step 100 Train loss 0.25 Classification-F1 0.3191489361702127 on epoch=49
03/23/2022 20:14:12 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
03/23/2022 20:14:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.24 on epoch=59
03/23/2022 20:14:20 - INFO - __main__ - Step 130 Global step 130 Train loss 0.22 on epoch=64
03/23/2022 20:14:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.19 on epoch=69
03/23/2022 20:14:29 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
03/23/2022 20:14:31 - INFO - __main__ - Global step 150 Train loss 0.23 Classification-F1 0.39756367663344405 on epoch=74
03/23/2022 20:14:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39756367663344405 on epoch=74, global_step=150
03/23/2022 20:14:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.20 on epoch=79
03/23/2022 20:14:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.21 on epoch=84
03/23/2022 20:14:44 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
03/23/2022 20:14:48 - INFO - __main__ - Step 190 Global step 190 Train loss 0.18 on epoch=94
03/23/2022 20:14:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.19 on epoch=99
03/23/2022 20:14:54 - INFO - __main__ - Global step 200 Train loss 0.20 Classification-F1 0.39756367663344405 on epoch=99
03/23/2022 20:14:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.18 on epoch=104
03/23/2022 20:15:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.18 on epoch=109
03/23/2022 20:15:07 - INFO - __main__ - Step 230 Global step 230 Train loss 0.19 on epoch=114
03/23/2022 20:15:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.18 on epoch=119
03/23/2022 20:15:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.20 on epoch=124
03/23/2022 20:15:18 - INFO - __main__ - Global step 250 Train loss 0.18 Classification-F1 0.37662337662337664 on epoch=124
03/23/2022 20:15:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
03/23/2022 20:15:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
03/23/2022 20:15:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.14 on epoch=139
03/23/2022 20:15:36 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
03/23/2022 20:15:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.17 on epoch=149
03/23/2022 20:15:41 - INFO - __main__ - Global step 300 Train loss 0.15 Classification-F1 0.43529411764705883 on epoch=149
03/23/2022 20:15:42 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.43529411764705883 on epoch=149, global_step=300
03/23/2022 20:15:46 - INFO - __main__ - Step 310 Global step 310 Train loss 0.11 on epoch=154
03/23/2022 20:15:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.10 on epoch=159
03/23/2022 20:15:55 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
03/23/2022 20:15:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.12 on epoch=169
03/23/2022 20:16:04 - INFO - __main__ - Step 350 Global step 350 Train loss 0.07 on epoch=174
03/23/2022 20:16:05 - INFO - __main__ - Global step 350 Train loss 0.10 Classification-F1 0.4980392156862745 on epoch=174
03/23/2022 20:16:05 - INFO - __main__ - Saving model with best Classification-F1: 0.43529411764705883 -> 0.4980392156862745 on epoch=174, global_step=350
03/23/2022 20:16:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.10 on epoch=179
03/23/2022 20:16:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.10 on epoch=184
03/23/2022 20:16:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.05 on epoch=189
03/23/2022 20:16:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.05 on epoch=194
03/23/2022 20:16:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.04 on epoch=199
03/23/2022 20:16:29 - INFO - __main__ - Global step 400 Train loss 0.07 Classification-F1 0.4817813765182186 on epoch=199
03/23/2022 20:16:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
03/23/2022 20:16:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.10 on epoch=209
03/23/2022 20:16:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.04 on epoch=214
03/23/2022 20:16:46 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
03/23/2022 20:16:51 - INFO - __main__ - Step 450 Global step 450 Train loss 0.01 on epoch=224
03/23/2022 20:16:52 - INFO - __main__ - Global step 450 Train loss 0.05 Classification-F1 0.464039408866995 on epoch=224
03/23/2022 20:16:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
03/23/2022 20:17:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
03/23/2022 20:17:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.01 on epoch=239
03/23/2022 20:17:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.02 on epoch=244
03/23/2022 20:17:14 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
03/23/2022 20:17:16 - INFO - __main__ - Global step 500 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=249
03/23/2022 20:17:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4980392156862745 -> 0.5270935960591133 on epoch=249, global_step=500
03/23/2022 20:17:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
03/23/2022 20:17:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
03/23/2022 20:17:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
03/23/2022 20:17:33 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
03/23/2022 20:17:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
03/23/2022 20:17:39 - INFO - __main__ - Global step 550 Train loss 0.02 Classification-F1 0.2991452991452992 on epoch=274
03/23/2022 20:17:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/23/2022 20:17:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/23/2022 20:17:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/23/2022 20:17:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/23/2022 20:18:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
03/23/2022 20:18:02 - INFO - __main__ - Global step 600 Train loss 0.01 Classification-F1 0.3520443520443521 on epoch=299
03/23/2022 20:18:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
03/23/2022 20:18:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 20:18:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/23/2022 20:18:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/23/2022 20:18:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 20:18:26 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 0.3373901284651792 on epoch=324
03/23/2022 20:18:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 20:18:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/23/2022 20:18:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 20:18:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/23/2022 20:18:48 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 20:18:49 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.5465587044534412 on epoch=349
03/23/2022 20:18:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5270935960591133 -> 0.5465587044534412 on epoch=349, global_step=700
03/23/2022 20:18:54 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
03/23/2022 20:18:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/23/2022 20:19:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 20:19:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 20:19:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
03/23/2022 20:19:13 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.5835835835835835 on epoch=374
03/23/2022 20:19:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5465587044534412 -> 0.5835835835835835 on epoch=374, global_step=750
03/23/2022 20:19:17 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
03/23/2022 20:19:22 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 20:19:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/23/2022 20:19:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 20:19:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 20:19:36 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=399
03/23/2022 20:19:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 20:19:45 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 20:19:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 20:19:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 20:19:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/23/2022 20:20:00 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.35714285714285715 on epoch=424
03/23/2022 20:20:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/23/2022 20:20:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 20:20:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
03/23/2022 20:20:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 20:20:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 20:20:23 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=449
03/23/2022 20:20:28 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 20:20:32 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 20:20:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
03/23/2022 20:20:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 20:20:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 20:20:47 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=474
03/23/2022 20:20:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 20:20:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 20:21:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 20:21:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/23/2022 20:21:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 20:21:10 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=499
03/23/2022 20:21:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 20:21:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 20:21:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 20:21:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 20:21:32 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 20:21:34 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=524
03/23/2022 20:21:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 20:21:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 20:21:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 20:21:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 20:21:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
03/23/2022 20:21:57 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.5076923076923077 on epoch=549
03/23/2022 20:22:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 20:22:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 20:22:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 20:22:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 20:22:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/23/2022 20:22:21 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.5465587044534412 on epoch=574
03/23/2022 20:22:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/23/2022 20:22:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 20:22:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 20:22:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 20:22:43 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 20:22:44 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=599
03/23/2022 20:22:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 20:22:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 20:22:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 20:23:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 20:23:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 20:23:07 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=624
03/23/2022 20:23:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 20:23:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 20:23:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 20:23:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
03/23/2022 20:23:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 20:23:31 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=649
03/23/2022 20:23:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 20:23:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 20:23:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 20:23:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 20:23:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 20:23:54 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.35185185185185186 on epoch=674
03/23/2022 20:23:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 20:24:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 20:24:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 20:24:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 20:24:16 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 20:24:17 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=699
03/23/2022 20:24:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 20:24:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 20:24:31 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 20:24:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 20:24:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 20:24:41 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=724
03/23/2022 20:24:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 20:24:49 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 20:24:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 20:24:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
03/23/2022 20:25:03 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 20:25:04 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.3373901284651792 on epoch=749
03/23/2022 20:25:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 20:25:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 20:25:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 20:25:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 20:25:26 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 20:25:27 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.25396825396825395 on epoch=774
03/23/2022 20:25:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 20:25:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 20:25:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 20:25:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 20:25:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 20:25:51 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.3373901284651792 on epoch=799
03/23/2022 20:25:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 20:26:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 20:26:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 20:26:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 20:26:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 20:26:14 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=824
03/23/2022 20:26:19 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 20:26:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 20:26:27 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 20:26:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 20:26:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 20:26:38 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5465587044534412 on epoch=849
03/23/2022 20:26:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 20:26:46 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 20:26:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 20:26:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 20:27:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 20:27:01 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.27248677248677244 on epoch=874
03/23/2022 20:27:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 20:27:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 20:27:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 20:27:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 20:27:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 20:27:24 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.27248677248677244 on epoch=899
03/23/2022 20:27:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/23/2022 20:27:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 20:27:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 20:27:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 20:27:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 20:27:48 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=924
03/23/2022 20:27:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 20:27:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 20:28:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 20:28:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 20:28:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 20:28:11 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=949
03/23/2022 20:28:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 20:28:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 20:28:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 20:28:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 20:28:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 20:28:35 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=974
03/23/2022 20:28:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 20:28:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 20:28:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 20:28:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 20:28:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 20:28:58 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.3373901284651792 on epoch=999
03/23/2022 20:28:58 - INFO - __main__ - save last model!
03/23/2022 20:28:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:28:58 - INFO - __main__ - Printing 3 examples
03/23/2022 20:28:58 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/23/2022 20:28:58 - INFO - __main__ - ['refuted']
03/23/2022 20:28:58 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/23/2022 20:28:58 - INFO - __main__ - ['refuted']
03/23/2022 20:28:58 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/23/2022 20:28:58 - INFO - __main__ - ['refuted']
03/23/2022 20:28:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 20:28:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 20:28:58 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:28:58 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 20:28:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 20:28:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:28:58 - INFO - __main__ - Printing 3 examples
03/23/2022 20:28:58 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/23/2022 20:28:58 - INFO - __main__ - ['refuted']
03/23/2022 20:28:58 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/23/2022 20:28:58 - INFO - __main__ - ['refuted']
03/23/2022 20:28:58 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/23/2022 20:28:58 - INFO - __main__ - ['refuted']
03/23/2022 20:28:58 - INFO - __main__ - Tokenizing Input ...
03/23/2022 20:28:58 - INFO - __main__ - Printing 3 examples
03/23/2022 20:28:58 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 20:28:58 - INFO - __main__ - ['entailed']
03/23/2022 20:28:58 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 20:28:58 - INFO - __main__ - ['entailed']
03/23/2022 20:28:58 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 20:28:58 - INFO - __main__ - ['entailed']
03/23/2022 20:28:58 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 20:28:58 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:28:58 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 20:29:13 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 20:29:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 20:29:14 - INFO - __main__ - Starting training!
03/23/2022 20:29:22 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:29:34 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 20:38:05 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.4_8_predictions.txt
03/23/2022 20:38:05 - INFO - __main__ - Classification-F1 on test data: 0.0963
03/23/2022 20:38:06 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.4, bsz=8, dev_performance=0.5835835835835835, test_performance=0.0962930869501041
03/23/2022 20:38:06 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.3, bsz=8 ...
03/23/2022 20:38:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:38:07 - INFO - __main__ - Printing 3 examples
03/23/2022 20:38:07 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/23/2022 20:38:07 - INFO - __main__ - ['refuted']
03/23/2022 20:38:07 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/23/2022 20:38:07 - INFO - __main__ - ['refuted']
03/23/2022 20:38:07 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/23/2022 20:38:07 - INFO - __main__ - ['refuted']
03/23/2022 20:38:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 20:38:07 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:38:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 20:38:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:38:07 - INFO - __main__ - Printing 3 examples
03/23/2022 20:38:07 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/23/2022 20:38:07 - INFO - __main__ - ['refuted']
03/23/2022 20:38:07 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/23/2022 20:38:07 - INFO - __main__ - ['refuted']
03/23/2022 20:38:07 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/23/2022 20:38:07 - INFO - __main__ - ['refuted']
03/23/2022 20:38:07 - INFO - __main__ - Tokenizing Input ...
03/23/2022 20:38:07 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:38:07 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 20:38:22 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 20:38:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 20:38:22 - INFO - __main__ - Starting training!
03/23/2022 20:38:28 - INFO - __main__ - Step 10 Global step 10 Train loss 4.09 on epoch=4
03/23/2022 20:38:32 - INFO - __main__ - Step 20 Global step 20 Train loss 1.78 on epoch=9
03/23/2022 20:38:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.80 on epoch=14
03/23/2022 20:38:41 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=19
03/23/2022 20:38:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.32 on epoch=24
03/23/2022 20:38:47 - INFO - __main__ - Global step 50 Train loss 1.48 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 20:38:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 20:38:51 - INFO - __main__ - Step 60 Global step 60 Train loss 0.35 on epoch=29
03/23/2022 20:38:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.35 on epoch=34
03/23/2022 20:39:00 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=39
03/23/2022 20:39:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=44
03/23/2022 20:39:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=49
03/23/2022 20:39:10 - INFO - __main__ - Global step 100 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=49
03/23/2022 20:39:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=54
03/23/2022 20:39:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=59
03/23/2022 20:39:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.22 on epoch=64
03/23/2022 20:39:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.23 on epoch=69
03/23/2022 20:39:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
03/23/2022 20:39:34 - INFO - __main__ - Global step 150 Train loss 0.24 Classification-F1 0.3191489361702127 on epoch=74
03/23/2022 20:39:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=79
03/23/2022 20:39:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.21 on epoch=84
03/23/2022 20:39:47 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=89
03/23/2022 20:39:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.20 on epoch=94
03/23/2022 20:39:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.20 on epoch=99
03/23/2022 20:39:57 - INFO - __main__ - Global step 200 Train loss 0.21 Classification-F1 0.39756367663344405 on epoch=99
03/23/2022 20:39:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39756367663344405 on epoch=99, global_step=200
03/23/2022 20:40:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=104
03/23/2022 20:40:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.18 on epoch=109
03/23/2022 20:40:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.16 on epoch=114
03/23/2022 20:40:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.19 on epoch=119
03/23/2022 20:40:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.15 on epoch=124
03/23/2022 20:40:21 - INFO - __main__ - Global step 250 Train loss 0.17 Classification-F1 0.5076923076923077 on epoch=124
03/23/2022 20:40:21 - INFO - __main__ - Saving model with best Classification-F1: 0.39756367663344405 -> 0.5076923076923077 on epoch=124, global_step=250
03/23/2022 20:40:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.17 on epoch=129
03/23/2022 20:40:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.15 on epoch=134
03/23/2022 20:40:34 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
03/23/2022 20:40:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
03/23/2022 20:40:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
03/23/2022 20:40:44 - INFO - __main__ - Global step 300 Train loss 0.15 Classification-F1 0.4420512820512821 on epoch=149
03/23/2022 20:40:49 - INFO - __main__ - Step 310 Global step 310 Train loss 0.11 on epoch=154
03/23/2022 20:40:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
03/23/2022 20:40:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.10 on epoch=164
03/23/2022 20:41:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.10 on epoch=169
03/23/2022 20:41:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.09 on epoch=174
03/23/2022 20:41:08 - INFO - __main__ - Global step 350 Train loss 0.11 Classification-F1 0.4554554554554554 on epoch=174
03/23/2022 20:41:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.11 on epoch=179
03/23/2022 20:41:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.10 on epoch=184
03/23/2022 20:41:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.09 on epoch=189
03/23/2022 20:41:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.07 on epoch=194
03/23/2022 20:41:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
03/23/2022 20:41:31 - INFO - __main__ - Global step 400 Train loss 0.10 Classification-F1 0.14447174447174443 on epoch=199
03/23/2022 20:41:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.04 on epoch=204
03/23/2022 20:41:40 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
03/23/2022 20:41:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
03/23/2022 20:41:49 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
03/23/2022 20:41:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
03/23/2022 20:41:54 - INFO - __main__ - Global step 450 Train loss 0.07 Classification-F1 0.13951734539969834 on epoch=224
03/23/2022 20:41:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.02 on epoch=229
03/23/2022 20:42:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
03/23/2022 20:42:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
03/23/2022 20:42:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.06 on epoch=244
03/23/2022 20:42:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
03/23/2022 20:42:18 - INFO - __main__ - Global step 500 Train loss 0.04 Classification-F1 0.21581196581196582 on epoch=249
03/23/2022 20:42:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.08 on epoch=254
03/23/2022 20:42:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
03/23/2022 20:42:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
03/23/2022 20:42:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
03/23/2022 20:42:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
03/23/2022 20:42:41 - INFO - __main__ - Global step 550 Train loss 0.04 Classification-F1 0.4920634920634921 on epoch=274
03/23/2022 20:42:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/23/2022 20:42:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
03/23/2022 20:42:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
03/23/2022 20:42:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/23/2022 20:43:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
03/23/2022 20:43:04 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.19235294117647062 on epoch=299
03/23/2022 20:43:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/23/2022 20:43:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/23/2022 20:43:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/23/2022 20:43:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/23/2022 20:43:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
03/23/2022 20:43:28 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.34313725490196073 on epoch=324
03/23/2022 20:43:32 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
03/23/2022 20:43:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.04 on epoch=334
03/23/2022 20:43:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/23/2022 20:43:45 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/23/2022 20:43:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
03/23/2022 20:43:51 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.2434640522875817 on epoch=349
03/23/2022 20:43:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/23/2022 20:44:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
03/23/2022 20:44:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/23/2022 20:44:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/23/2022 20:44:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
03/23/2022 20:44:14 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.14809523809523809 on epoch=374
03/23/2022 20:44:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/23/2022 20:44:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/23/2022 20:44:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 20:44:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
03/23/2022 20:44:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/23/2022 20:44:38 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.20905172413793105 on epoch=399
03/23/2022 20:44:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/23/2022 20:44:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 20:44:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/23/2022 20:44:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 20:45:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/23/2022 20:45:01 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.35959595959595964 on epoch=424
03/23/2022 20:45:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/23/2022 20:45:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 20:45:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
03/23/2022 20:45:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/23/2022 20:45:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=449
03/23/2022 20:45:25 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.37654320987654327 on epoch=449
03/23/2022 20:45:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 20:45:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 20:45:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/23/2022 20:45:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 20:45:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 20:45:48 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.35714285714285715 on epoch=474
03/23/2022 20:45:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 20:45:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
03/23/2022 20:46:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
03/23/2022 20:46:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.03 on epoch=494
03/23/2022 20:46:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
03/23/2022 20:46:11 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=499
03/23/2022 20:46:11 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5270935960591133 on epoch=499, global_step=1000
03/23/2022 20:46:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 20:46:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
03/23/2022 20:46:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 20:46:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 20:46:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
03/23/2022 20:46:35 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=524
03/23/2022 20:46:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5270935960591133 -> 0.5555555555555556 on epoch=524, global_step=1050
03/23/2022 20:46:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 20:46:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/23/2022 20:46:48 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/23/2022 20:46:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 20:46:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 20:46:58 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.3333333333333333 on epoch=549
03/23/2022 20:47:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/23/2022 20:47:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 20:47:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 20:47:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 20:47:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 20:47:22 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.37654320987654327 on epoch=574
03/23/2022 20:47:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 20:47:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/23/2022 20:47:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 20:47:39 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/23/2022 20:47:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 20:47:45 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.3162393162393162 on epoch=599
03/23/2022 20:47:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 20:47:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 20:47:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 20:48:03 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 20:48:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 20:48:08 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=624
03/23/2022 20:48:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 20:48:17 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 20:48:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 20:48:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 20:48:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 20:48:32 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=649
03/23/2022 20:48:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 20:48:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 20:48:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 20:48:49 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 20:48:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 20:48:55 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=674
03/23/2022 20:49:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 20:49:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 20:49:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 20:49:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 20:49:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 20:49:19 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=699
03/23/2022 20:49:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 20:49:27 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 20:49:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 20:49:36 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 20:49:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 20:49:42 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=724
03/23/2022 20:49:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 20:49:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 20:49:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 20:50:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
03/23/2022 20:50:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 20:50:06 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=749
03/23/2022 20:50:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 20:50:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 20:50:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 20:50:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 20:50:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 20:50:29 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.37654320987654327 on epoch=774
03/23/2022 20:50:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 20:50:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 20:50:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 20:50:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 20:50:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 20:50:52 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=799
03/23/2022 20:50:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 20:51:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 20:51:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/23/2022 20:51:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 20:51:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/23/2022 20:51:16 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.35714285714285715 on epoch=824
03/23/2022 20:51:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 20:51:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 20:51:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 20:51:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 20:51:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 20:51:39 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=849
03/23/2022 20:51:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 20:51:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 20:51:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 20:51:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 20:52:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 20:52:03 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=874
03/23/2022 20:52:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 20:52:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/23/2022 20:52:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 20:52:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 20:52:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 20:52:26 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=899
03/23/2022 20:52:31 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 20:52:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 20:52:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 20:52:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 20:52:48 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 20:52:49 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=924
03/23/2022 20:52:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 20:52:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 20:53:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 20:53:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 20:53:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 20:53:13 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=949
03/23/2022 20:53:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 20:53:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 20:53:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 20:53:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 20:53:35 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 20:53:36 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=974
03/23/2022 20:53:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 20:53:45 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 20:53:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 20:53:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 20:53:58 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 20:54:00 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=999
03/23/2022 20:54:00 - INFO - __main__ - save last model!
03/23/2022 20:54:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 20:54:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:54:00 - INFO - __main__ - Printing 3 examples
03/23/2022 20:54:00 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/23/2022 20:54:00 - INFO - __main__ - ['refuted']
03/23/2022 20:54:00 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/23/2022 20:54:00 - INFO - __main__ - ['refuted']
03/23/2022 20:54:00 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/23/2022 20:54:00 - INFO - __main__ - ['refuted']
03/23/2022 20:54:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 20:54:00 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 20:54:00 - INFO - __main__ - Printing 3 examples
03/23/2022 20:54:00 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 20:54:00 - INFO - __main__ - ['entailed']
03/23/2022 20:54:00 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 20:54:00 - INFO - __main__ - ['entailed']
03/23/2022 20:54:00 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 20:54:00 - INFO - __main__ - ['entailed']
03/23/2022 20:54:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 20:54:00 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:54:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 20:54:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 20:54:00 - INFO - __main__ - Printing 3 examples
03/23/2022 20:54:00 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/23/2022 20:54:00 - INFO - __main__ - ['refuted']
03/23/2022 20:54:00 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/23/2022 20:54:00 - INFO - __main__ - ['refuted']
03/23/2022 20:54:00 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/23/2022 20:54:00 - INFO - __main__ - ['refuted']
03/23/2022 20:54:00 - INFO - __main__ - Tokenizing Input ...
03/23/2022 20:54:00 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:54:00 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 20:54:19 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 20:54:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 20:54:19 - INFO - __main__ - Starting training!
03/23/2022 20:54:23 - INFO - __main__ - Tokenizing Output ...
03/23/2022 20:54:36 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 21:02:47 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.3_8_predictions.txt
03/23/2022 21:02:47 - INFO - __main__ - Classification-F1 on test data: 0.0554
03/23/2022 21:02:48 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.3, bsz=8, dev_performance=0.5555555555555556, test_performance=0.05535754546347395
03/23/2022 21:02:48 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.2, bsz=8 ...
03/23/2022 21:02:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:02:49 - INFO - __main__ - Printing 3 examples
03/23/2022 21:02:49 - INFO - __main__ -  [tab_fact] statement: tampa bay play no game at home during the month of november [SEP] table_caption: 2007 - 08 tampa bay lightning season [SEP] table_text: date#visitor#score#home#decision#attendance#record [n] november 1#tampa bay#0 - 4#ny islanders#denis#11008#5 - 6 - 1 [n] november 3#atlanta#6 - 4#tampa bay#holmqvist#19155#5 - 7 - 1 [n] november 5#tampa bay#3 - 4#florida#holmqvist#10149#5 - 8 - 1 [n] november 7#florida#1 - 3#tampa bay#holmqvist#16526#6 - 8 - 1 [n] november 8#tampa bay#5 - 1#carolina#holmqvist#14017#7 - 8 - 1 [n] november 10#tampa bay#5 - 2#washington#holmqvist#14617#8 - 8 - 1 [n] november 14#carolina#1 - 6#tampa bay#holmqvist#17444#9 - 8 - 1 [n] november 16#washington#2 - 5#tampa bay#holmqvist#19526#10 - 8 - 1 [n] november 19#tampa bay#3 - 4#atlanta#holmqvist#13419#10 - 8 - 2 [n] november 21#ny rangers#2 - 1#tampa bay#holmqvist#20110#10 - 9 - 2 [n] november 23#tampa bay#3 - 4#carolina#holmqvist#18033#10 - 10 - 2 [n] november 24#new jersey#3 - 2#tampa bay#holmqvist#19077#10 - 11 - 2 [n] november 28#tampa bay#1 - 5#chicago#holmqvist#11122#10 - 12 - 2 [n] november 29#tampa bay#2 - 4#detroit#denis#17001#10 - 13 - 2 [n] 
03/23/2022 21:02:49 - INFO - __main__ - ['refuted']
03/23/2022 21:02:49 - INFO - __main__ -  [tab_fact] statement: there be more than 9 silver medalist [SEP] table_caption: archery at the asian games [SEP] table_text: year#location#gold#silver#bronze [n] 1978#bangkok#kim jin - ho#yuriko goto#kim hyang - mi [n] 1982#new delhi#o gwang - sun#kim jin - ho#kim mi - young [n] 1986#seoul#park jung - ah#kim jin - ho#kim mi - ja [n] 1990#beijing#lee jang - mi#lee eun - kyung#kim soo - nyung [n] 1994#hiroshima#lee eun - kyung#lim jung - ah#han hee - jeong [n] 1998#bangkok#kim jo - sun#lee eun - kyung#lin sang [n] 2002#busan#yuan shu - chi#kim mun - jeong#yun mi - jin [n] 2006#doha#park sung - hyun#yun ok - hee#zhao ling [n] 2010#guangzhou#yun ok - hee#cheng ming#kwon un - sil [n] 
03/23/2022 21:02:49 - INFO - __main__ - ['refuted']
03/23/2022 21:02:49 - INFO - __main__ -  [tab_fact] statement: the average point score in achieve second place in the speedway world pair championship be 18 [SEP] table_caption: speedway world pairs championship [SEP] table_text: year#venue#winners#runner - up#3rd place [n] 1968#kempten#sweden (24 pts)#(21 pts)#(16 pts) [n] 1969#stockholm#new zealand (28 pts)#sweden (27 pts)#england (21 pts) [n] year#venue#winners#runner - up#3rd place [n] 1970#malm#new zealand (28 pts)#sweden (25 pts)#england (19 pts) [n] 1971#rybnik#(30 pts)#new zealand (25 pts)#sweden (22 pts) [n] 1972#bors#england (24 + 3 pts)#new zealand (24 + 2 pts)#sweden b (22 + 3 pts) [n] 1973#bors#sweden (24 pts)#(21 + 3 pts)#(21 + 2 pts) [n] 1974#manchester#sweden (28 pts)#australia (23 pts)#new zealand (21 pts) [n] 1975#wrocaw#sweden (24 pts)#(23 pts)#(20 + 3 pts) [n] 1976#eskilstuna#england (27 pts)#(24 pts)#sweden (22 pts) [n] 1977#manchester#england (28 pts)#sweden (18 pts)#west germany (18 pts) [n] 1978#chorzw#england (24 + 3 pts)#new zealand (24 + 2 pts)#(21 pts) [n] 1979#vojens#(25 pts)#england (24 pts)#(20 pts) [n] 1980#krko#england (29 pts)#(22 pts)#(21 pts) [n] 1981#chorzw#united states (23 pts)#new zealand (22 pts)#(21 pts) [n] 1982#liverpool#united states (30 pts)#england (22 pts)#(21 pts) [n] 1983#gothenburg#england (25 pts)#australia (24 pts)#(19 pts) [n] 1984#lonigo#england (27 pts)#(25 + 3 pts)#new zealand (25 + 2 pts) [n] 1985#rybnik#(29 pts)#england (27 pts)#united states (22 pts) [n] 1986#pocking#(46 + 5 pts)#united states (46 + 4 pts)#czechoslovakia (32 pts) [n] 1987#pardubice#(52 pts)#england (44 pts)#united states (36 pts) [n] 1988#bradford#(45 pts)#england (41 pts)#united states (39 pts) [n] 1989#leszno#(48 pts)#sweden (44 pts)#england (37 pts) [n] 1990#landshut#(43 pts)#australia (41 pts)#(33 pts) [n] 1991#pozna#(28 pts)#sweden (24 pts)#(19 pts) [n] 1992#lonigo#united states (23 + 3 pts)#england (23 + 2 pts)#sweden (22 pts) [n] 1993#vojens#sweden (26 pts)#united states (23 pts)#(21 pts) [n] 
03/23/2022 21:02:49 - INFO - __main__ - ['refuted']
03/23/2022 21:02:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 21:02:49 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:02:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 21:02:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:02:49 - INFO - __main__ - Printing 3 examples
03/23/2022 21:02:49 - INFO - __main__ -  [tab_fact] statement: new england win a single overtime game during the 2002 season [SEP] table_caption: 2002 new england patriots season [SEP] table_text: week#kickoff#date#opponent#result#record#game site#attendance [n] 1#9:00 pm edt#september 9 , 2002#pittsburgh steelers#w 30 - 14#1 - 0#gillette stadium#68436 [n] 2#1:00 pm edt#september 15 , 2002#new york jets#w 44 - 7#2 - 0#giants stadium#78726 [n] 3#1:00 pm edt#september 22 , 2002#kansas city chiefs#w 41 - 38 (ot)#3 - 0#gillette stadium#68436 [n] 4#4:15 pm edt#september 29 , 2002#san diego chargers#l 14 - 21#3 - 1#qualcomm stadium#66463 [n] 5#1:00 pm edt#october 6 , 2002#miami dolphins#l 13 - 26#3 - 2#pro player stadium#73369 [n] 6#1:00 pm edt#october 13 , 2002#green bay packers#l 10 - 28#3 - 3#gillette stadium#68436 [n] 7#-#-#-#-#-#-# [n] 8#4:15 pm est#october 27 , 2002#denver broncos#l 16 - 24#3 - 4#gillette stadium#68436 [n] 9#1:00 pm est#november 3 , 2002#buffalo bills#w 38 - 7#4 - 4#ralph wilson stadium#73448 [n] 10#4:15 pm est#november 10 , 2002#chicago bears#w 33 - 30#5 - 4#memorial stadium#63105 [n] 11#8:30 pm est#november 17 , 2002#oakland raiders#l 20 - 27#5 - 5#network associates coliseum#62552 [n] 12#1:00 pm est#november 24 , 2002#minnesota vikings#w 24 - 17#6 - 5#gillette stadium#68436 [n] 13#12:30 pm est#november 28 , 2002#detroit lions#w 20 - 12#7 - 5#ford field#62109 [n] 14#1:00 pm est#december 8 , 2002#buffalo bills#w 27 - 17#8 - 5#gillette stadium#68436 [n] 15#9:00 pm est#december 16 , 2002#tennessee titans#l 7 - 24#8 - 6#the coliseum#68809 [n] 16#8:30 pm est#december 22 , 2002#new york jets#l 17 - 30#8 - 7#gillette stadium#68436 [n] 17#1:00 pm est#december 29 , 2002#miami dolphins#w 27 - 24 (ot)#9 - 7#gillette stadium#68436 [n] 
03/23/2022 21:02:49 - INFO - __main__ - ['refuted']
03/23/2022 21:02:49 - INFO - __main__ -  [tab_fact] statement: when colorado and new mexico be bush then utah be bush in 2000 [SEP] table_caption: southwestern united states [SEP] table_text: year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] year#arizona#california#colorado#nevada#new mexico#oklahoma#texas#utah [n] 1952#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1956# isenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower#eisenhower [n] 1960#nixon#nixon#nixon#kennedy#kennedy#nixon#kennedy#nixon [n] 1964#goldwater#johnson#johnson#johnson#johnson#johnson#johnson#johnson [n] 1968#nixon#nixon#nixon#nixon#nixon#nixon#humphrey#nixon [n] 1972#nixon#nixon#nixon#nixon#nixon#nixon#nixon#nixon [n] 1976#ford#ford#ford#ford#ford#ford#carter#ford [n] 1980#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1984#reagan#reagan#reagan#reagan#reagan#reagan#reagan#reagan [n] 1988#bush#bush#bush#bush#bush#bush#bush#bush [n] 1992#bush#clinton#clinton#clinton#clinton#bush#bush#bush [n] 1996#clinton#clinton#dole#clinton#clinton#dole#dole#dole [n] 2000#bush#gore#bush#bush#gore#bush#bush#bush [n] 2004#bush#kerry#bush#bush#bush#bush#bush#bush [n] 2008#mccain#obama#obama#obama#obama#mccain#mccain#mccain [n] 2012#romney#obama#obama#obama#obama#romney#romney#romney [n] 
03/23/2022 21:02:49 - INFO - __main__ - ['refuted']
03/23/2022 21:02:49 - INFO - __main__ -  [tab_fact] statement: the average year of the film from france and hong kong be before 2001 [SEP] table_caption: new york film critics circle award for best foreign language film [SEP] table_text: year#english title#original title#country#director (s) [n] 2000#yi yi : a one and a two#yi yi#japan / taiwan#edward yang [n] 2001#in the mood for love#fa yeung nin wa#france / hong kong#wong kar - wai [n] 2002#and your mother too#y tu mam tambin#mexico#alfonso cuarn [n] 2003#city of god#cidade de deus#brazil#fernando meirelles [n] 2004#bad education#la mala educacin#spain#pedro almodvar [n] 2005#2046#2046#china / hong kong#wong kar - wai [n] 2006#army of shadows#l'arme des ombres#france / italy#jean - pierre melville [n] 2007#the lives of others#das leben der anderen#germany#florian henckel von donnersmarck [n] 2008#4 months , 3 weeks and 2 days#4 luni , 3 sptmni i 2 zile#romania#cristian mungiu [n] 2009#summer hours#l'heure de t#france#olivier assayas [n] 
03/23/2022 21:02:49 - INFO - __main__ - ['refuted']
03/23/2022 21:02:49 - INFO - __main__ - Tokenizing Input ...
03/23/2022 21:02:49 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:02:49 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 21:03:04 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 21:03:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 21:03:05 - INFO - __main__ - Starting training!
03/23/2022 21:03:10 - INFO - __main__ - Step 10 Global step 10 Train loss 4.40 on epoch=4
03/23/2022 21:03:14 - INFO - __main__ - Step 20 Global step 20 Train loss 2.43 on epoch=9
03/23/2022 21:03:19 - INFO - __main__ - Step 30 Global step 30 Train loss 1.64 on epoch=14
03/23/2022 21:03:23 - INFO - __main__ - Step 40 Global step 40 Train loss 0.99 on epoch=19
03/23/2022 21:03:27 - INFO - __main__ - Step 50 Global step 50 Train loss 0.55 on epoch=24
03/23/2022 21:03:29 - INFO - __main__ - Global step 50 Train loss 2.00 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 21:03:29 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 21:03:33 - INFO - __main__ - Step 60 Global step 60 Train loss 0.38 on epoch=29
03/23/2022 21:03:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.38 on epoch=34
03/23/2022 21:03:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
03/23/2022 21:03:46 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=44
03/23/2022 21:03:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=49
03/23/2022 21:03:52 - INFO - __main__ - Global step 100 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=49
03/23/2022 21:03:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
03/23/2022 21:04:01 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
03/23/2022 21:04:05 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
03/23/2022 21:04:09 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
03/23/2022 21:04:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
03/23/2022 21:04:15 - INFO - __main__ - Global step 150 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=74
03/23/2022 21:04:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=79
03/23/2022 21:04:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
03/23/2022 21:04:28 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/23/2022 21:04:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.19 on epoch=94
03/23/2022 21:04:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
03/23/2022 21:04:39 - INFO - __main__ - Global step 200 Train loss 0.23 Classification-F1 0.3043478260869565 on epoch=99
03/23/2022 21:04:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=104
03/23/2022 21:04:47 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=109
03/23/2022 21:04:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.22 on epoch=114
03/23/2022 21:04:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=119
03/23/2022 21:05:00 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=124
03/23/2022 21:05:02 - INFO - __main__ - Global step 250 Train loss 0.23 Classification-F1 0.4181818181818182 on epoch=124
03/23/2022 21:05:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4181818181818182 on epoch=124, global_step=250
03/23/2022 21:05:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=129
03/23/2022 21:05:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.20 on epoch=134
03/23/2022 21:05:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.18 on epoch=139
03/23/2022 21:05:20 - INFO - __main__ - Step 290 Global step 290 Train loss 0.19 on epoch=144
03/23/2022 21:05:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.20 on epoch=149
03/23/2022 21:05:25 - INFO - __main__ - Global step 300 Train loss 0.20 Classification-F1 0.39756367663344405 on epoch=149
03/23/2022 21:05:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
03/23/2022 21:05:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.20 on epoch=159
03/23/2022 21:05:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.20 on epoch=164
03/23/2022 21:05:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.18 on epoch=169
03/23/2022 21:05:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.19 on epoch=174
03/23/2022 21:05:49 - INFO - __main__ - Global step 350 Train loss 0.19 Classification-F1 0.37662337662337664 on epoch=174
03/23/2022 21:05:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.16 on epoch=179
03/23/2022 21:05:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
03/23/2022 21:06:02 - INFO - __main__ - Step 380 Global step 380 Train loss 0.14 on epoch=189
03/23/2022 21:06:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.17 on epoch=194
03/23/2022 21:06:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.17 on epoch=199
03/23/2022 21:06:12 - INFO - __main__ - Global step 400 Train loss 0.16 Classification-F1 0.5307917888563051 on epoch=199
03/23/2022 21:06:12 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.5307917888563051 on epoch=199, global_step=400
03/23/2022 21:06:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.12 on epoch=204
03/23/2022 21:06:21 - INFO - __main__ - Step 420 Global step 420 Train loss 0.15 on epoch=209
03/23/2022 21:06:25 - INFO - __main__ - Step 430 Global step 430 Train loss 0.11 on epoch=214
03/23/2022 21:06:29 - INFO - __main__ - Step 440 Global step 440 Train loss 0.15 on epoch=219
03/23/2022 21:06:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.11 on epoch=224
03/23/2022 21:06:35 - INFO - __main__ - Global step 450 Train loss 0.13 Classification-F1 0.4920634920634921 on epoch=224
03/23/2022 21:06:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
03/23/2022 21:06:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
03/23/2022 21:06:48 - INFO - __main__ - Step 480 Global step 480 Train loss 0.13 on epoch=239
03/23/2022 21:06:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
03/23/2022 21:06:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
03/23/2022 21:06:58 - INFO - __main__ - Global step 500 Train loss 0.11 Classification-F1 0.4920634920634921 on epoch=249
03/23/2022 21:07:03 - INFO - __main__ - Step 510 Global step 510 Train loss 0.09 on epoch=254
03/23/2022 21:07:07 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
03/23/2022 21:07:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.08 on epoch=264
03/23/2022 21:07:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.07 on epoch=269
03/23/2022 21:07:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
03/23/2022 21:07:21 - INFO - __main__ - Global step 550 Train loss 0.07 Classification-F1 0.4920634920634921 on epoch=274
03/23/2022 21:07:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.06 on epoch=279
03/23/2022 21:07:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
03/23/2022 21:07:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
03/23/2022 21:07:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.04 on epoch=294
03/23/2022 21:07:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
03/23/2022 21:07:44 - INFO - __main__ - Global step 600 Train loss 0.06 Classification-F1 0.35714285714285715 on epoch=299
03/23/2022 21:07:49 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
03/23/2022 21:07:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.08 on epoch=309
03/23/2022 21:07:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/23/2022 21:08:02 - INFO - __main__ - Step 640 Global step 640 Train loss 0.07 on epoch=319
03/23/2022 21:08:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
03/23/2022 21:08:08 - INFO - __main__ - Global step 650 Train loss 0.06 Classification-F1 0.5195195195195195 on epoch=324
03/23/2022 21:08:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
03/23/2022 21:08:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.05 on epoch=334
03/23/2022 21:08:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.03 on epoch=339
03/23/2022 21:08:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/23/2022 21:08:30 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
03/23/2022 21:08:31 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.4920634920634921 on epoch=349
03/23/2022 21:08:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/23/2022 21:08:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
03/23/2022 21:08:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
03/23/2022 21:08:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
03/23/2022 21:08:53 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/23/2022 21:08:54 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.23968253968253966 on epoch=374
03/23/2022 21:08:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/23/2022 21:09:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/23/2022 21:09:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
03/23/2022 21:09:12 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
03/23/2022 21:09:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/23/2022 21:09:17 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.2904761904761905 on epoch=399
03/23/2022 21:09:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/23/2022 21:09:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/23/2022 21:09:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/23/2022 21:09:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/23/2022 21:09:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
03/23/2022 21:09:41 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=424
03/23/2022 21:09:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/23/2022 21:09:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/23/2022 21:09:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
03/23/2022 21:09:58 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 21:10:03 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/23/2022 21:10:04 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.3633156966490299 on epoch=449
03/23/2022 21:10:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
03/23/2022 21:10:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
03/23/2022 21:10:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
03/23/2022 21:10:22 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 21:10:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/23/2022 21:10:27 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.31339031339031337 on epoch=474
03/23/2022 21:10:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.03 on epoch=479
03/23/2022 21:10:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/23/2022 21:10:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/23/2022 21:10:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/23/2022 21:10:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.02 on epoch=499
03/23/2022 21:10:50 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.25735294117647056 on epoch=499
03/23/2022 21:10:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 21:10:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.04 on epoch=509
03/23/2022 21:11:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/23/2022 21:11:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 21:11:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/23/2022 21:11:13 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.37052631578947376 on epoch=524
03/23/2022 21:11:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 21:11:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/23/2022 21:11:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/23/2022 21:11:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
03/23/2022 21:11:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 21:11:37 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.26816239316239315 on epoch=549
03/23/2022 21:11:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 21:11:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/23/2022 21:11:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 21:11:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
03/23/2022 21:11:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 21:12:00 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.19428571428571426 on epoch=574
03/23/2022 21:12:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 21:12:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
03/23/2022 21:12:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 21:12:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 21:12:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 21:12:23 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.2448512585812357 on epoch=599
03/23/2022 21:12:27 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 21:12:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
03/23/2022 21:12:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 21:12:40 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/23/2022 21:12:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 21:12:46 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.37654320987654327 on epoch=624
03/23/2022 21:12:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 21:12:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/23/2022 21:13:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 21:13:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 21:13:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 21:13:10 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.2790346907993967 on epoch=649
03/23/2022 21:13:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
03/23/2022 21:13:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 21:13:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
03/23/2022 21:13:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 21:13:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 21:13:33 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.35714285714285715 on epoch=674
03/23/2022 21:13:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 21:13:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/23/2022 21:13:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 21:13:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 21:13:55 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 21:13:57 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.22494553376906318 on epoch=699
03/23/2022 21:14:01 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 21:14:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/23/2022 21:14:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 21:14:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/23/2022 21:14:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 21:14:20 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.32631578947368417 on epoch=724
03/23/2022 21:14:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 21:14:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 21:14:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 21:14:38 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 21:14:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 21:14:44 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.3386243386243386 on epoch=749
03/23/2022 21:14:48 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 21:14:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 21:14:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 21:15:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 21:15:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 21:15:07 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.31868131868131866 on epoch=774
03/23/2022 21:15:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 21:15:16 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 21:15:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/23/2022 21:15:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 21:15:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/23/2022 21:15:31 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=799
03/23/2022 21:15:31 - INFO - __main__ - Saving model with best Classification-F1: 0.5307917888563051 -> 0.5555555555555556 on epoch=799, global_step=1600
03/23/2022 21:15:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 21:15:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 21:15:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 21:15:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
03/23/2022 21:15:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 21:15:54 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.5465587044534412 on epoch=824
03/23/2022 21:15:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 21:16:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 21:16:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 21:16:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 21:16:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 21:16:17 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.33153153153153153 on epoch=849
03/23/2022 21:16:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
03/23/2022 21:16:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/23/2022 21:16:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 21:16:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 21:16:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 21:16:41 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.34401709401709396 on epoch=874
03/23/2022 21:16:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 21:16:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 21:16:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 21:16:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/23/2022 21:17:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 21:17:04 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.3333333333333333 on epoch=899
03/23/2022 21:17:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 21:17:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 21:17:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
03/23/2022 21:17:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/23/2022 21:17:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 21:17:28 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=924
03/23/2022 21:17:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 21:17:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/23/2022 21:17:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 21:17:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 21:17:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 21:17:52 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.30409356725146197 on epoch=949
03/23/2022 21:17:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/23/2022 21:18:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 21:18:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
03/23/2022 21:18:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 21:18:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 21:18:15 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.19004524886877827 on epoch=974
03/23/2022 21:18:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 21:18:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 21:18:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 21:18:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 21:18:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
03/23/2022 21:18:39 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.3333333333333333 on epoch=999
03/23/2022 21:18:39 - INFO - __main__ - save last model!
03/23/2022 21:18:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 21:18:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:18:39 - INFO - __main__ - Printing 3 examples
03/23/2022 21:18:39 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/23/2022 21:18:39 - INFO - __main__ - ['refuted']
03/23/2022 21:18:39 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/23/2022 21:18:39 - INFO - __main__ - ['refuted']
03/23/2022 21:18:39 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/23/2022 21:18:39 - INFO - __main__ - ['refuted']
03/23/2022 21:18:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 21:18:39 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:18:39 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 21:18:39 - INFO - __main__ - Printing 3 examples
03/23/2022 21:18:39 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 21:18:39 - INFO - __main__ - ['entailed']
03/23/2022 21:18:39 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 21:18:39 - INFO - __main__ - ['entailed']
03/23/2022 21:18:39 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 21:18:39 - INFO - __main__ - ['entailed']
03/23/2022 21:18:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 21:18:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 21:18:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:18:39 - INFO - __main__ - Printing 3 examples
03/23/2022 21:18:39 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/23/2022 21:18:39 - INFO - __main__ - ['refuted']
03/23/2022 21:18:39 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/23/2022 21:18:39 - INFO - __main__ - ['refuted']
03/23/2022 21:18:39 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/23/2022 21:18:39 - INFO - __main__ - ['refuted']
03/23/2022 21:18:39 - INFO - __main__ - Tokenizing Input ...
03/23/2022 21:18:39 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:18:39 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 21:18:57 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 21:18:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 21:18:58 - INFO - __main__ - Starting training!
03/23/2022 21:19:03 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:19:15 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 21:27:23 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_100_0.2_8_predictions.txt
03/23/2022 21:27:23 - INFO - __main__ - Classification-F1 on test data: 0.0771
03/23/2022 21:27:23 - INFO - __main__ - prefix=tab_fact_16_100, lr=0.2, bsz=8, dev_performance=0.5555555555555556, test_performance=0.0770549774306067
03/23/2022 21:27:23 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.5, bsz=8 ...
03/23/2022 21:27:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:27:24 - INFO - __main__ - Printing 3 examples
03/23/2022 21:27:24 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/23/2022 21:27:24 - INFO - __main__ - ['refuted']
03/23/2022 21:27:24 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/23/2022 21:27:24 - INFO - __main__ - ['refuted']
03/23/2022 21:27:24 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/23/2022 21:27:24 - INFO - __main__ - ['refuted']
03/23/2022 21:27:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 21:27:24 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:27:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 21:27:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:27:24 - INFO - __main__ - Printing 3 examples
03/23/2022 21:27:24 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/23/2022 21:27:24 - INFO - __main__ - ['refuted']
03/23/2022 21:27:24 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/23/2022 21:27:24 - INFO - __main__ - ['refuted']
03/23/2022 21:27:24 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/23/2022 21:27:24 - INFO - __main__ - ['refuted']
03/23/2022 21:27:24 - INFO - __main__ - Tokenizing Input ...
03/23/2022 21:27:25 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:27:25 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 21:27:43 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 21:27:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 21:27:44 - INFO - __main__ - Starting training!
03/23/2022 21:27:49 - INFO - __main__ - Step 10 Global step 10 Train loss 3.52 on epoch=4
03/23/2022 21:27:53 - INFO - __main__ - Step 20 Global step 20 Train loss 1.14 on epoch=9
03/23/2022 21:27:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=14
03/23/2022 21:28:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.31 on epoch=19
03/23/2022 21:28:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=24
03/23/2022 21:28:08 - INFO - __main__ - Global step 50 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 21:28:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 21:28:13 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=29
03/23/2022 21:28:17 - INFO - __main__ - Step 70 Global step 70 Train loss 0.26 on epoch=34
03/23/2022 21:28:21 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
03/23/2022 21:28:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.20 on epoch=44
03/23/2022 21:28:30 - INFO - __main__ - Step 100 Global step 100 Train loss 0.21 on epoch=49
03/23/2022 21:28:32 - INFO - __main__ - Global step 100 Train loss 0.24 Classification-F1 0.3191489361702127 on epoch=49
03/23/2022 21:28:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.21 on epoch=54
03/23/2022 21:28:41 - INFO - __main__ - Step 120 Global step 120 Train loss 0.17 on epoch=59
03/23/2022 21:28:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.20 on epoch=64
03/23/2022 21:28:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.19 on epoch=69
03/23/2022 21:28:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.19 on epoch=74
03/23/2022 21:28:55 - INFO - __main__ - Global step 150 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=74
03/23/2022 21:29:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.18 on epoch=79
03/23/2022 21:29:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.16 on epoch=84
03/23/2022 21:29:09 - INFO - __main__ - Step 180 Global step 180 Train loss 0.16 on epoch=89
03/23/2022 21:29:13 - INFO - __main__ - Step 190 Global step 190 Train loss 0.16 on epoch=94
03/23/2022 21:29:18 - INFO - __main__ - Step 200 Global step 200 Train loss 0.11 on epoch=99
03/23/2022 21:29:19 - INFO - __main__ - Global step 200 Train loss 0.15 Classification-F1 0.21739130434782608 on epoch=99
03/23/2022 21:29:23 - INFO - __main__ - Step 210 Global step 210 Train loss 0.10 on epoch=104
03/23/2022 21:29:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.10 on epoch=109
03/23/2022 21:29:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.11 on epoch=114
03/23/2022 21:29:37 - INFO - __main__ - Step 240 Global step 240 Train loss 0.10 on epoch=119
03/23/2022 21:29:41 - INFO - __main__ - Step 250 Global step 250 Train loss 0.07 on epoch=124
03/23/2022 21:29:43 - INFO - __main__ - Global step 250 Train loss 0.10 Classification-F1 0.3191489361702127 on epoch=124
03/23/2022 21:29:47 - INFO - __main__ - Step 260 Global step 260 Train loss 0.05 on epoch=129
03/23/2022 21:29:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.16 on epoch=134
03/23/2022 21:29:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.05 on epoch=139
03/23/2022 21:30:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.05 on epoch=144
03/23/2022 21:30:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.08 on epoch=149
03/23/2022 21:30:06 - INFO - __main__ - Global step 300 Train loss 0.08 Classification-F1 0.4666666666666667 on epoch=149
03/23/2022 21:30:06 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4666666666666667 on epoch=149, global_step=300
03/23/2022 21:30:11 - INFO - __main__ - Step 310 Global step 310 Train loss 0.04 on epoch=154
03/23/2022 21:30:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.04 on epoch=159
03/23/2022 21:30:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.03 on epoch=164
03/23/2022 21:30:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.03 on epoch=169
03/23/2022 21:30:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.02 on epoch=174
03/23/2022 21:30:30 - INFO - __main__ - Global step 350 Train loss 0.03 Classification-F1 0.4817813765182186 on epoch=174
03/23/2022 21:30:30 - INFO - __main__ - Saving model with best Classification-F1: 0.4666666666666667 -> 0.4817813765182186 on epoch=174, global_step=350
03/23/2022 21:30:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.03 on epoch=179
03/23/2022 21:30:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.02 on epoch=184
03/23/2022 21:30:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.02 on epoch=189
03/23/2022 21:30:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.03 on epoch=194
03/23/2022 21:30:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.01 on epoch=199
03/23/2022 21:30:53 - INFO - __main__ - Global step 400 Train loss 0.02 Classification-F1 0.2920847268673355 on epoch=199
03/23/2022 21:30:58 - INFO - __main__ - Step 410 Global step 410 Train loss 0.01 on epoch=204
03/23/2022 21:31:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.02 on epoch=209
03/23/2022 21:31:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.02 on epoch=214
03/23/2022 21:31:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/23/2022 21:31:16 - INFO - __main__ - Step 450 Global step 450 Train loss 0.01 on epoch=224
03/23/2022 21:31:17 - INFO - __main__ - Global step 450 Train loss 0.01 Classification-F1 0.2396825396825397 on epoch=224
03/23/2022 21:31:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.01 on epoch=229
03/23/2022 21:31:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.02 on epoch=234
03/23/2022 21:31:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.00 on epoch=239
03/23/2022 21:31:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.01 on epoch=244
03/23/2022 21:31:39 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/23/2022 21:31:40 - INFO - __main__ - Global step 500 Train loss 0.01 Classification-F1 0.20995423340961097 on epoch=249
03/23/2022 21:31:45 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/23/2022 21:31:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
03/23/2022 21:31:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/23/2022 21:31:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/23/2022 21:32:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/23/2022 21:32:04 - INFO - __main__ - Global step 550 Train loss 0.01 Classification-F1 0.24666666666666667 on epoch=274
03/23/2022 21:32:08 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/23/2022 21:32:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/23/2022 21:32:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/23/2022 21:32:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/23/2022 21:32:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/23/2022 21:32:27 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.2869565217391304 on epoch=299
03/23/2022 21:32:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/23/2022 21:32:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/23/2022 21:32:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/23/2022 21:32:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/23/2022 21:32:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/23/2022 21:32:51 - INFO - __main__ - Global step 650 Train loss 0.00 Classification-F1 0.11713554987212275 on epoch=324
03/23/2022 21:32:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/23/2022 21:33:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/23/2022 21:33:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/23/2022 21:33:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/23/2022 21:33:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/23/2022 21:33:14 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.17978021978021977 on epoch=349
03/23/2022 21:33:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 21:33:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
03/23/2022 21:33:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 21:33:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 21:33:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 21:33:38 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.1590909090909091 on epoch=374
03/23/2022 21:33:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
03/23/2022 21:33:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 21:33:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 21:33:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 21:34:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
03/23/2022 21:34:01 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.15565610859728507 on epoch=399
03/23/2022 21:34:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 21:34:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 21:34:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 21:34:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/23/2022 21:34:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/23/2022 21:34:25 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.20162162162162162 on epoch=424
03/23/2022 21:34:29 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/23/2022 21:34:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 21:34:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 21:34:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 21:34:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 21:34:48 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.1986607142857143 on epoch=449
03/23/2022 21:34:53 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 21:34:57 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 21:35:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 21:35:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 21:35:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 21:35:12 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.11244239631336406 on epoch=474
03/23/2022 21:35:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 21:35:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 21:35:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 21:35:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 21:35:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 21:35:35 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.13243546576879908 on epoch=499
03/23/2022 21:35:40 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 21:35:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 21:35:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 21:35:53 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 21:35:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 21:35:59 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.121989121989122 on epoch=524
03/23/2022 21:36:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 21:36:08 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 21:36:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 21:36:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 21:36:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 21:36:22 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.11218944099378882 on epoch=549
03/23/2022 21:36:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 21:36:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 21:36:36 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 21:36:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 21:36:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 21:36:46 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.10862745098039217 on epoch=574
03/23/2022 21:36:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 21:36:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 21:36:59 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 21:37:04 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 21:37:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 21:37:09 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.08658008658008658 on epoch=599
03/23/2022 21:37:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 21:37:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 21:37:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 21:37:27 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 21:37:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 21:37:33 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.1059011164274322 on epoch=624
03/23/2022 21:37:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 21:37:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 21:37:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 21:37:50 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 21:37:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 21:37:56 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.16022727272727272 on epoch=649
03/23/2022 21:38:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 21:38:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 21:38:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 21:38:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 21:38:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 21:38:20 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.15150000000000002 on epoch=674
03/23/2022 21:38:24 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 21:38:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 21:38:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 21:38:37 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 21:38:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 21:38:43 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.14772727272727273 on epoch=699
03/23/2022 21:38:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 21:38:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 21:38:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 21:39:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 21:39:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 21:39:07 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.1801470588235294 on epoch=724
03/23/2022 21:39:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 21:39:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 21:39:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 21:39:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 21:39:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.06 on epoch=749
03/23/2022 21:39:30 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.08717241379310345 on epoch=749
03/23/2022 21:39:35 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
03/23/2022 21:39:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 21:39:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 21:39:48 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 21:39:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 21:39:54 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.15126050420168066 on epoch=774
03/23/2022 21:39:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 21:40:03 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 21:40:07 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 21:40:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
03/23/2022 21:40:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 21:40:17 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.17857142857142855 on epoch=799
03/23/2022 21:40:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 21:40:26 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 21:40:31 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
03/23/2022 21:40:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 21:40:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 21:40:41 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.1767741935483871 on epoch=824
03/23/2022 21:40:45 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 21:40:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 21:40:54 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 21:40:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 21:41:03 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 21:41:04 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.14141935483870968 on epoch=849
03/23/2022 21:41:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 21:41:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 21:41:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
03/23/2022 21:41:22 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 21:41:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 21:41:28 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.10526315789473684 on epoch=874
03/23/2022 21:41:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 21:41:37 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 21:41:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 21:41:45 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 21:41:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 21:41:51 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.12228260869565218 on epoch=899
03/23/2022 21:41:56 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 21:42:00 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 21:42:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 21:42:09 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 21:42:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/23/2022 21:42:15 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.2692901234567901 on epoch=924
03/23/2022 21:42:19 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 21:42:23 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 21:42:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 21:42:32 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 21:42:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 21:42:38 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.23659673659673658 on epoch=949
03/23/2022 21:42:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 21:42:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 21:42:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 21:42:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 21:43:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 21:43:01 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.15743707093821507 on epoch=974
03/23/2022 21:43:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 21:43:10 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 21:43:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 21:43:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
03/23/2022 21:43:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 21:43:25 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.18013468013468015 on epoch=999
03/23/2022 21:43:25 - INFO - __main__ - save last model!
03/23/2022 21:43:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:43:25 - INFO - __main__ - Printing 3 examples
03/23/2022 21:43:25 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/23/2022 21:43:25 - INFO - __main__ - ['refuted']
03/23/2022 21:43:25 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/23/2022 21:43:25 - INFO - __main__ - ['refuted']
03/23/2022 21:43:25 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/23/2022 21:43:25 - INFO - __main__ - ['refuted']
03/23/2022 21:43:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 21:43:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 21:43:25 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:43:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 21:43:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:43:25 - INFO - __main__ - Printing 3 examples
03/23/2022 21:43:25 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/23/2022 21:43:25 - INFO - __main__ - ['refuted']
03/23/2022 21:43:25 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/23/2022 21:43:25 - INFO - __main__ - ['refuted']
03/23/2022 21:43:25 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/23/2022 21:43:25 - INFO - __main__ - ['refuted']
03/23/2022 21:43:25 - INFO - __main__ - Tokenizing Input ...
03/23/2022 21:43:25 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:43:25 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 21:43:25 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 21:43:25 - INFO - __main__ - Printing 3 examples
03/23/2022 21:43:25 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 21:43:25 - INFO - __main__ - ['entailed']
03/23/2022 21:43:25 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 21:43:25 - INFO - __main__ - ['entailed']
03/23/2022 21:43:25 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 21:43:25 - INFO - __main__ - ['entailed']
03/23/2022 21:43:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 21:43:40 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 21:43:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 21:43:41 - INFO - __main__ - Starting training!
03/23/2022 21:43:49 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:44:02 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 21:52:13 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.5_8_predictions.txt
03/23/2022 21:52:13 - INFO - __main__ - Classification-F1 on test data: 0.0456
03/23/2022 21:52:13 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.5, bsz=8, dev_performance=0.4817813765182186, test_performance=0.04562554036710901
03/23/2022 21:52:13 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.4, bsz=8 ...
03/23/2022 21:52:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:52:14 - INFO - __main__ - Printing 3 examples
03/23/2022 21:52:14 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/23/2022 21:52:14 - INFO - __main__ - ['refuted']
03/23/2022 21:52:14 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/23/2022 21:52:14 - INFO - __main__ - ['refuted']
03/23/2022 21:52:14 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/23/2022 21:52:14 - INFO - __main__ - ['refuted']
03/23/2022 21:52:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 21:52:14 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:52:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 21:52:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 21:52:14 - INFO - __main__ - Printing 3 examples
03/23/2022 21:52:14 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/23/2022 21:52:14 - INFO - __main__ - ['refuted']
03/23/2022 21:52:14 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/23/2022 21:52:14 - INFO - __main__ - ['refuted']
03/23/2022 21:52:14 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/23/2022 21:52:14 - INFO - __main__ - ['refuted']
03/23/2022 21:52:14 - INFO - __main__ - Tokenizing Input ...
03/23/2022 21:52:14 - INFO - __main__ - Tokenizing Output ...
03/23/2022 21:52:14 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 21:52:33 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 21:52:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 21:52:34 - INFO - __main__ - Starting training!
03/23/2022 21:52:39 - INFO - __main__ - Step 10 Global step 10 Train loss 3.42 on epoch=4
03/23/2022 21:52:43 - INFO - __main__ - Step 20 Global step 20 Train loss 1.46 on epoch=9
03/23/2022 21:52:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=14
03/23/2022 21:52:52 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
03/23/2022 21:52:57 - INFO - __main__ - Step 50 Global step 50 Train loss 0.32 on epoch=24
03/23/2022 21:52:58 - INFO - __main__ - Global step 50 Train loss 1.22 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 21:52:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 21:53:03 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=29
03/23/2022 21:53:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.27 on epoch=34
03/23/2022 21:53:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.24 on epoch=39
03/23/2022 21:53:16 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=44
03/23/2022 21:53:20 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
03/23/2022 21:53:22 - INFO - __main__ - Global step 100 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=49
03/23/2022 21:53:26 - INFO - __main__ - Step 110 Global step 110 Train loss 0.22 on epoch=54
03/23/2022 21:53:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=59
03/23/2022 21:53:35 - INFO - __main__ - Step 130 Global step 130 Train loss 0.21 on epoch=64
03/23/2022 21:53:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.21 on epoch=69
03/23/2022 21:53:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.20 on epoch=74
03/23/2022 21:53:45 - INFO - __main__ - Global step 150 Train loss 0.21 Classification-F1 0.4385964912280702 on epoch=74
03/23/2022 21:53:45 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4385964912280702 on epoch=74, global_step=150
03/23/2022 21:53:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.20 on epoch=79
03/23/2022 21:53:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
03/23/2022 21:53:59 - INFO - __main__ - Step 180 Global step 180 Train loss 0.18 on epoch=89
03/23/2022 21:54:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.16 on epoch=94
03/23/2022 21:54:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.17 on epoch=99
03/23/2022 21:54:09 - INFO - __main__ - Global step 200 Train loss 0.17 Classification-F1 0.4589371980676329 on epoch=99
03/23/2022 21:54:09 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.4589371980676329 on epoch=99, global_step=200
03/23/2022 21:54:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
03/23/2022 21:54:17 - INFO - __main__ - Step 220 Global step 220 Train loss 0.14 on epoch=109
03/23/2022 21:54:22 - INFO - __main__ - Step 230 Global step 230 Train loss 0.15 on epoch=114
03/23/2022 21:54:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.15 on epoch=119
03/23/2022 21:54:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.12 on epoch=124
03/23/2022 21:54:32 - INFO - __main__ - Global step 250 Train loss 0.14 Classification-F1 0.4589371980676329 on epoch=124
03/23/2022 21:54:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.09 on epoch=129
03/23/2022 21:54:41 - INFO - __main__ - Step 270 Global step 270 Train loss 0.13 on epoch=134
03/23/2022 21:54:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.12 on epoch=139
03/23/2022 21:54:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.07 on epoch=144
03/23/2022 21:54:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.08 on epoch=149
03/23/2022 21:54:55 - INFO - __main__ - Global step 300 Train loss 0.10 Classification-F1 0.49090909090909085 on epoch=149
03/23/2022 21:54:55 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.49090909090909085 on epoch=149, global_step=300
03/23/2022 21:55:00 - INFO - __main__ - Step 310 Global step 310 Train loss 0.08 on epoch=154
03/23/2022 21:55:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.06 on epoch=159
03/23/2022 21:55:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.08 on epoch=164
03/23/2022 21:55:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.08 on epoch=169
03/23/2022 21:55:17 - INFO - __main__ - Step 350 Global step 350 Train loss 0.06 on epoch=174
03/23/2022 21:55:19 - INFO - __main__ - Global step 350 Train loss 0.07 Classification-F1 0.4009852216748768 on epoch=174
03/23/2022 21:55:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.05 on epoch=179
03/23/2022 21:55:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.06 on epoch=184
03/23/2022 21:55:32 - INFO - __main__ - Step 380 Global step 380 Train loss 0.06 on epoch=189
03/23/2022 21:55:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.05 on epoch=194
03/23/2022 21:55:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
03/23/2022 21:55:42 - INFO - __main__ - Global step 400 Train loss 0.05 Classification-F1 0.2151351351351351 on epoch=199
03/23/2022 21:55:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.04 on epoch=204
03/23/2022 21:55:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.03 on epoch=209
03/23/2022 21:55:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.02 on epoch=214
03/23/2022 21:56:00 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
03/23/2022 21:56:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.04 on epoch=224
03/23/2022 21:56:06 - INFO - __main__ - Global step 450 Train loss 0.03 Classification-F1 0.2149122807017544 on epoch=224
03/23/2022 21:56:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.02 on epoch=229
03/23/2022 21:56:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.04 on epoch=234
03/23/2022 21:56:19 - INFO - __main__ - Step 480 Global step 480 Train loss 0.05 on epoch=239
03/23/2022 21:56:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.02 on epoch=244
03/23/2022 21:56:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.02 on epoch=249
03/23/2022 21:56:29 - INFO - __main__ - Global step 500 Train loss 0.03 Classification-F1 0.15467980295566502 on epoch=249
03/23/2022 21:56:34 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
03/23/2022 21:56:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.03 on epoch=259
03/23/2022 21:56:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
03/23/2022 21:56:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
03/23/2022 21:56:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
03/23/2022 21:56:53 - INFO - __main__ - Global step 550 Train loss 0.02 Classification-F1 0.19407894736842105 on epoch=274
03/23/2022 21:56:57 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/23/2022 21:57:02 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
03/23/2022 21:57:06 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
03/23/2022 21:57:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/23/2022 21:57:15 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/23/2022 21:57:16 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.14448051948051946 on epoch=299
03/23/2022 21:57:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/23/2022 21:57:25 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/23/2022 21:57:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/23/2022 21:57:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/23/2022 21:57:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/23/2022 21:57:40 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.164 on epoch=324
03/23/2022 21:57:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/23/2022 21:57:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/23/2022 21:57:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/23/2022 21:57:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.04 on epoch=344
03/23/2022 21:58:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/23/2022 21:58:03 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.182983682983683 on epoch=349
03/23/2022 21:58:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/23/2022 21:58:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
03/23/2022 21:58:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/23/2022 21:58:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/23/2022 21:58:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
03/23/2022 21:58:27 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.18010752688172044 on epoch=374
03/23/2022 21:58:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/23/2022 21:58:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/23/2022 21:58:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 21:58:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/23/2022 21:58:49 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/23/2022 21:58:50 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.19264069264069267 on epoch=399
03/23/2022 21:58:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/23/2022 21:58:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/23/2022 21:59:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/23/2022 21:59:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/23/2022 21:59:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/23/2022 21:59:14 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.13019079685746351 on epoch=424
03/23/2022 21:59:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/23/2022 21:59:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/23/2022 21:59:27 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 21:59:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 21:59:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/23/2022 21:59:37 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.164 on epoch=449
03/23/2022 21:59:42 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
03/23/2022 21:59:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 21:59:51 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/23/2022 21:59:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 21:59:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/23/2022 22:00:01 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.14423433444568037 on epoch=474
03/23/2022 22:00:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/23/2022 22:00:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 22:00:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/23/2022 22:00:18 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/23/2022 22:00:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/23/2022 22:00:24 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.20192307692307693 on epoch=499
03/23/2022 22:00:29 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 22:00:33 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 22:00:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/23/2022 22:00:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 22:00:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 22:00:48 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.13758241758241757 on epoch=524
03/23/2022 22:00:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 22:00:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 22:01:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 22:01:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 22:01:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 22:01:11 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.15296703296703296 on epoch=549
03/23/2022 22:01:16 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 22:01:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 22:01:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 22:01:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 22:01:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 22:01:35 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.16954022988505746 on epoch=574
03/23/2022 22:01:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/23/2022 22:01:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/23/2022 22:01:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 22:01:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 22:01:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 22:01:58 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.2569444444444444 on epoch=599
03/23/2022 22:02:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 22:02:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 22:02:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 22:02:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 22:02:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/23/2022 22:02:22 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.27934517589690006 on epoch=624
03/23/2022 22:02:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/23/2022 22:02:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 22:02:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 22:02:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 22:02:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 22:02:45 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.27474747474747474 on epoch=649
03/23/2022 22:02:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/23/2022 22:02:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/23/2022 22:02:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 22:03:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 22:03:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 22:03:09 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.13017241379310346 on epoch=674
03/23/2022 22:03:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 22:03:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 22:03:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 22:03:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 22:03:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 22:03:32 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.3142857142857143 on epoch=699
03/23/2022 22:03:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 22:03:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 22:03:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
03/23/2022 22:03:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 22:03:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 22:03:56 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=724
03/23/2022 22:03:56 - INFO - __main__ - Saving model with best Classification-F1: 0.49090909090909085 -> 0.4920634920634921 on epoch=724, global_step=1450
03/23/2022 22:04:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 22:04:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 22:04:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 22:04:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 22:04:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 22:04:20 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.2963709677419355 on epoch=749
03/23/2022 22:04:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 22:04:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 22:04:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 22:04:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 22:04:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 22:04:43 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=774
03/23/2022 22:04:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 22:04:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 22:04:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 22:05:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 22:05:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 22:05:07 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=799
03/23/2022 22:05:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 22:05:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 22:05:20 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 22:05:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 22:05:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 22:05:30 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=824
03/23/2022 22:05:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 22:05:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 22:05:43 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 22:05:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 22:05:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 22:05:54 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=849
03/23/2022 22:05:54 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5270935960591133 on epoch=849, global_step=1700
03/23/2022 22:05:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 22:06:02 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 22:06:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 22:06:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 22:06:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 22:06:17 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=874
03/23/2022 22:06:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 22:06:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 22:06:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 22:06:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/23/2022 22:06:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 22:06:41 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=899
03/23/2022 22:06:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 22:06:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 22:06:54 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 22:06:58 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 22:07:03 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 22:07:04 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=924
03/23/2022 22:07:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 22:07:13 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 22:07:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 22:07:22 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 22:07:26 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 22:07:28 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=949
03/23/2022 22:07:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 22:07:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/23/2022 22:07:41 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 22:07:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 22:07:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 22:07:51 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=974
03/23/2022 22:07:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 22:08:00 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 22:08:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 22:08:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
03/23/2022 22:08:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 22:08:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:08:15 - INFO - __main__ - Printing 3 examples
03/23/2022 22:08:15 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/23/2022 22:08:15 - INFO - __main__ - ['refuted']
03/23/2022 22:08:15 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/23/2022 22:08:15 - INFO - __main__ - ['refuted']
03/23/2022 22:08:15 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/23/2022 22:08:15 - INFO - __main__ - ['refuted']
03/23/2022 22:08:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 22:08:15 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:08:15 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=999
03/23/2022 22:08:15 - INFO - __main__ - save last model!
03/23/2022 22:08:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 22:08:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:08:15 - INFO - __main__ - Printing 3 examples
03/23/2022 22:08:15 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/23/2022 22:08:15 - INFO - __main__ - ['refuted']
03/23/2022 22:08:15 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/23/2022 22:08:15 - INFO - __main__ - ['refuted']
03/23/2022 22:08:15 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/23/2022 22:08:15 - INFO - __main__ - ['refuted']
03/23/2022 22:08:15 - INFO - __main__ - Tokenizing Input ...
03/23/2022 22:08:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 22:08:15 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:08:15 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 22:08:15 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 22:08:15 - INFO - __main__ - Printing 3 examples
03/23/2022 22:08:15 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 22:08:15 - INFO - __main__ - ['entailed']
03/23/2022 22:08:15 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 22:08:15 - INFO - __main__ - ['entailed']
03/23/2022 22:08:15 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 22:08:15 - INFO - __main__ - ['entailed']
03/23/2022 22:08:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 22:08:33 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 22:08:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 22:08:34 - INFO - __main__ - Starting training!
03/23/2022 22:08:38 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:08:51 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 22:17:08 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.4_8_predictions.txt
03/23/2022 22:17:08 - INFO - __main__ - Classification-F1 on test data: 0.0097
03/23/2022 22:17:08 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.4, bsz=8, dev_performance=0.5270935960591133, test_performance=0.009655990232092728
03/23/2022 22:17:08 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.3, bsz=8 ...
03/23/2022 22:17:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:17:09 - INFO - __main__ - Printing 3 examples
03/23/2022 22:17:09 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/23/2022 22:17:09 - INFO - __main__ - ['refuted']
03/23/2022 22:17:09 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/23/2022 22:17:09 - INFO - __main__ - ['refuted']
03/23/2022 22:17:09 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/23/2022 22:17:09 - INFO - __main__ - ['refuted']
03/23/2022 22:17:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 22:17:09 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:17:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 22:17:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:17:09 - INFO - __main__ - Printing 3 examples
03/23/2022 22:17:09 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/23/2022 22:17:09 - INFO - __main__ - ['refuted']
03/23/2022 22:17:09 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/23/2022 22:17:09 - INFO - __main__ - ['refuted']
03/23/2022 22:17:09 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/23/2022 22:17:09 - INFO - __main__ - ['refuted']
03/23/2022 22:17:09 - INFO - __main__ - Tokenizing Input ...
03/23/2022 22:17:09 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:17:09 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 22:17:27 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 22:17:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 22:17:28 - INFO - __main__ - Starting training!
03/23/2022 22:17:33 - INFO - __main__ - Step 10 Global step 10 Train loss 3.55 on epoch=4
03/23/2022 22:17:37 - INFO - __main__ - Step 20 Global step 20 Train loss 1.70 on epoch=9
03/23/2022 22:17:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.72 on epoch=14
03/23/2022 22:17:46 - INFO - __main__ - Step 40 Global step 40 Train loss 0.42 on epoch=19
03/23/2022 22:17:51 - INFO - __main__ - Step 50 Global step 50 Train loss 0.35 on epoch=24
03/23/2022 22:17:52 - INFO - __main__ - Global step 50 Train loss 1.35 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 22:17:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 22:17:57 - INFO - __main__ - Step 60 Global step 60 Train loss 0.26 on epoch=29
03/23/2022 22:18:01 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=34
03/23/2022 22:18:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.29 on epoch=39
03/23/2022 22:18:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.25 on epoch=44
03/23/2022 22:18:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.22 on epoch=49
03/23/2022 22:18:16 - INFO - __main__ - Global step 100 Train loss 0.26 Classification-F1 0.3111111111111111 on epoch=49
03/23/2022 22:18:20 - INFO - __main__ - Step 110 Global step 110 Train loss 0.22 on epoch=54
03/23/2022 22:18:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
03/23/2022 22:18:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.24 on epoch=64
03/23/2022 22:18:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.22 on epoch=69
03/23/2022 22:18:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.21 on epoch=74
03/23/2022 22:18:39 - INFO - __main__ - Global step 150 Train loss 0.23 Classification-F1 0.4589371980676329 on epoch=74
03/23/2022 22:18:39 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=74, global_step=150
03/23/2022 22:18:44 - INFO - __main__ - Step 160 Global step 160 Train loss 0.20 on epoch=79
03/23/2022 22:18:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.17 on epoch=84
03/23/2022 22:18:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.17 on epoch=89
03/23/2022 22:18:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.17 on epoch=94
03/23/2022 22:19:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.17 on epoch=99
03/23/2022 22:19:03 - INFO - __main__ - Global step 200 Train loss 0.18 Classification-F1 0.4589371980676329 on epoch=99
03/23/2022 22:19:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.14 on epoch=104
03/23/2022 22:19:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.15 on epoch=109
03/23/2022 22:19:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.18 on epoch=114
03/23/2022 22:19:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.18 on epoch=119
03/23/2022 22:19:25 - INFO - __main__ - Step 250 Global step 250 Train loss 0.13 on epoch=124
03/23/2022 22:19:26 - INFO - __main__ - Global step 250 Train loss 0.16 Classification-F1 0.4589371980676329 on epoch=124
03/23/2022 22:19:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
03/23/2022 22:19:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.13 on epoch=134
03/23/2022 22:19:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.11 on epoch=139
03/23/2022 22:19:44 - INFO - __main__ - Step 290 Global step 290 Train loss 0.11 on epoch=144
03/23/2022 22:19:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.12 on epoch=149
03/23/2022 22:19:50 - INFO - __main__ - Global step 300 Train loss 0.13 Classification-F1 0.4589371980676329 on epoch=149
03/23/2022 22:19:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.12 on epoch=154
03/23/2022 22:19:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.09 on epoch=159
03/23/2022 22:20:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.09 on epoch=164
03/23/2022 22:20:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.08 on epoch=169
03/23/2022 22:20:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.07 on epoch=174
03/23/2022 22:20:13 - INFO - __main__ - Global step 350 Train loss 0.09 Classification-F1 0.4385964912280702 on epoch=174
03/23/2022 22:20:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.08 on epoch=179
03/23/2022 22:20:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.08 on epoch=184
03/23/2022 22:20:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.06 on epoch=189
03/23/2022 22:20:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.07 on epoch=194
03/23/2022 22:20:36 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
03/23/2022 22:20:37 - INFO - __main__ - Global step 400 Train loss 0.07 Classification-F1 0.4589371980676329 on epoch=199
03/23/2022 22:20:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.04 on epoch=204
03/23/2022 22:20:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.06 on epoch=209
03/23/2022 22:20:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.07 on epoch=214
03/23/2022 22:20:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
03/23/2022 22:20:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.08 on epoch=224
03/23/2022 22:21:01 - INFO - __main__ - Global step 450 Train loss 0.06 Classification-F1 0.5195195195195195 on epoch=224
03/23/2022 22:21:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5195195195195195 on epoch=224, global_step=450
03/23/2022 22:21:05 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
03/23/2022 22:21:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=234
03/23/2022 22:21:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.04 on epoch=239
03/23/2022 22:21:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
03/23/2022 22:21:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.05 on epoch=249
03/23/2022 22:21:24 - INFO - __main__ - Global step 500 Train loss 0.04 Classification-F1 0.29744816586921846 on epoch=249
03/23/2022 22:21:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
03/23/2022 22:21:33 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
03/23/2022 22:21:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
03/23/2022 22:21:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
03/23/2022 22:21:46 - INFO - __main__ - Step 550 Global step 550 Train loss 0.04 on epoch=274
03/23/2022 22:21:48 - INFO - __main__ - Global step 550 Train loss 0.03 Classification-F1 0.30229120473022913 on epoch=274
03/23/2022 22:21:52 - INFO - __main__ - Step 560 Global step 560 Train loss 0.03 on epoch=279
03/23/2022 22:21:57 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
03/23/2022 22:22:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/23/2022 22:22:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/23/2022 22:22:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
03/23/2022 22:22:11 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.3139329805996472 on epoch=299
03/23/2022 22:22:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
03/23/2022 22:22:20 - INFO - __main__ - Step 620 Global step 620 Train loss 0.03 on epoch=309
03/23/2022 22:22:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/23/2022 22:22:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/23/2022 22:22:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
03/23/2022 22:22:35 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.4420512820512821 on epoch=324
03/23/2022 22:22:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/23/2022 22:22:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
03/23/2022 22:22:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/23/2022 22:22:53 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/23/2022 22:22:57 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/23/2022 22:22:59 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.26060606060606056 on epoch=349
03/23/2022 22:23:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
03/23/2022 22:23:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
03/23/2022 22:23:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
03/23/2022 22:23:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/23/2022 22:23:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/23/2022 22:23:22 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.2869565217391304 on epoch=374
03/23/2022 22:23:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/23/2022 22:23:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/23/2022 22:23:36 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/23/2022 22:23:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/23/2022 22:23:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/23/2022 22:23:46 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.25572801182557275 on epoch=399
03/23/2022 22:23:51 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/23/2022 22:23:55 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/23/2022 22:23:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/23/2022 22:24:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/23/2022 22:24:08 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/23/2022 22:24:10 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.2698412698412698 on epoch=424
03/23/2022 22:24:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/23/2022 22:24:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 22:24:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 22:24:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 22:24:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/23/2022 22:24:33 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.2869565217391304 on epoch=449
03/23/2022 22:24:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 22:24:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 22:24:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 22:24:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 22:24:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 22:24:57 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.3162393162393162 on epoch=474
03/23/2022 22:25:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/23/2022 22:25:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
03/23/2022 22:25:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/23/2022 22:25:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/23/2022 22:25:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/23/2022 22:25:20 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.30229120473022913 on epoch=499
03/23/2022 22:25:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 22:25:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
03/23/2022 22:25:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/23/2022 22:25:38 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 22:25:42 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 22:25:44 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.4458874458874459 on epoch=524
03/23/2022 22:25:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/23/2022 22:25:53 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 22:25:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/23/2022 22:26:01 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/23/2022 22:26:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/23/2022 22:26:07 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.3046218487394958 on epoch=549
03/23/2022 22:26:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 22:26:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 22:26:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
03/23/2022 22:26:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 22:26:30 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 22:26:31 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.21818181818181817 on epoch=574
03/23/2022 22:26:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 22:26:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 22:26:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 22:26:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 22:26:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/23/2022 22:26:55 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.2837209302325581 on epoch=599
03/23/2022 22:26:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 22:27:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 22:27:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/23/2022 22:27:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 22:27:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 22:27:18 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.29964912280701755 on epoch=624
03/23/2022 22:27:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 22:27:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 22:27:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/23/2022 22:27:36 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 22:27:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 22:27:42 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=649
03/23/2022 22:27:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 22:27:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 22:27:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 22:28:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 22:28:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/23/2022 22:28:05 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.3810483870967742 on epoch=674
03/23/2022 22:28:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 22:28:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
03/23/2022 22:28:19 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 22:28:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/23/2022 22:28:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 22:28:29 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.3810483870967742 on epoch=699
03/23/2022 22:28:33 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 22:28:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
03/23/2022 22:28:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 22:28:47 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 22:28:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 22:28:53 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3950617283950617 on epoch=724
03/23/2022 22:28:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/23/2022 22:29:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/23/2022 22:29:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 22:29:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 22:29:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 22:29:16 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4385964912280702 on epoch=749
03/23/2022 22:29:21 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 22:29:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 22:29:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 22:29:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 22:29:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=774
03/23/2022 22:29:40 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=774
03/23/2022 22:29:40 - INFO - __main__ - Saving model with best Classification-F1: 0.5195195195195195 -> 0.5901477832512315 on epoch=774, global_step=1550
03/23/2022 22:29:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 22:29:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 22:29:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 22:29:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 22:30:02 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 22:30:03 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.33260869565217394 on epoch=799
03/23/2022 22:30:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 22:30:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 22:30:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 22:30:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 22:30:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 22:30:27 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.33260869565217394 on epoch=824
03/23/2022 22:30:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 22:30:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 22:30:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 22:30:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 22:30:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/23/2022 22:30:51 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4909862142099682 on epoch=849
03/23/2022 22:30:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 22:31:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 22:31:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 22:31:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 22:31:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
03/23/2022 22:31:14 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5625 on epoch=874
03/23/2022 22:31:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 22:31:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 22:31:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 22:31:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/23/2022 22:31:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 22:31:38 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=899
03/23/2022 22:31:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 22:31:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 22:31:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 22:31:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 22:32:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 22:32:02 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=924
03/23/2022 22:32:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 22:32:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 22:32:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 22:32:20 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 22:32:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 22:32:25 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=949
03/23/2022 22:32:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 22:32:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 22:32:39 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 22:32:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 22:32:48 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 22:32:49 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=974
03/23/2022 22:32:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 22:32:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 22:33:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 22:33:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 22:33:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 22:33:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:33:13 - INFO - __main__ - Printing 3 examples
03/23/2022 22:33:13 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/23/2022 22:33:13 - INFO - __main__ - ['refuted']
03/23/2022 22:33:13 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/23/2022 22:33:13 - INFO - __main__ - ['refuted']
03/23/2022 22:33:13 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/23/2022 22:33:13 - INFO - __main__ - ['refuted']
03/23/2022 22:33:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 22:33:13 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:33:13 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=999
03/23/2022 22:33:13 - INFO - __main__ - save last model!
03/23/2022 22:33:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 22:33:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:33:13 - INFO - __main__ - Printing 3 examples
03/23/2022 22:33:13 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/23/2022 22:33:13 - INFO - __main__ - ['refuted']
03/23/2022 22:33:13 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/23/2022 22:33:13 - INFO - __main__ - ['refuted']
03/23/2022 22:33:13 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/23/2022 22:33:13 - INFO - __main__ - ['refuted']
03/23/2022 22:33:13 - INFO - __main__ - Tokenizing Input ...
03/23/2022 22:33:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 22:33:13 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:33:13 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 22:33:13 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 22:33:13 - INFO - __main__ - Printing 3 examples
03/23/2022 22:33:13 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 22:33:13 - INFO - __main__ - ['entailed']
03/23/2022 22:33:13 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 22:33:13 - INFO - __main__ - ['entailed']
03/23/2022 22:33:13 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 22:33:13 - INFO - __main__ - ['entailed']
03/23/2022 22:33:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 22:33:28 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 22:33:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 22:33:29 - INFO - __main__ - Starting training!
03/23/2022 22:33:36 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:33:49 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 22:42:16 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.3_8_predictions.txt
03/23/2022 22:42:16 - INFO - __main__ - Classification-F1 on test data: 0.3306
03/23/2022 22:42:17 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.3, bsz=8, dev_performance=0.5901477832512315, test_performance=0.3306269071425702
03/23/2022 22:42:17 - INFO - __main__ - Running ... prefix=tab_fact_16_13, lr=0.2, bsz=8 ...
03/23/2022 22:42:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:42:18 - INFO - __main__ - Printing 3 examples
03/23/2022 22:42:18 - INFO - __main__ -  [tab_fact] statement: 5000 f be equivalent to a power - to - weight ratio of 8035 w / kg c / 5 [SEP] table_caption: power - to - weight ratio [SEP] table_text: capacity#volts#temp#energy - to - weight ratio#power - to - weight ratio [n] 2000 f#4.0v#25degree#54 kj / kg to 2.0v#44.4 w / kg 5a [n] 2000 f#4.0v#25degree#31 kj / kg to 2.0v#850 w / kg 10a [n] 5000 f#2.7v#25degree#19.58 kj / kg to 1.35v#5.44 w / kg c / 1 (1.875a) [n] 5000 f#2.7v#25degree#5.2 kj / kg to 1.35v#5200 w / kg 2547a [n] 30.693 f#3500v#85degree#1471.98 kj / kg#80.35 w / kg c / 5 [n] 30.693 f#3500v#85degree#1471.98 kj / kg#8035 wkg 20c [n] 20.5 mf#3300v#degree#2.3 kj / kg#6.8 mw / kg 100ka [n] 
03/23/2022 22:42:18 - INFO - __main__ - ['refuted']
03/23/2022 22:42:18 - INFO - __main__ -  [tab_fact] statement: score of 2 - 2 have less than 26.0 point [SEP] table_caption: 1992 - 93 toronto maple leafs season [SEP] table_text: game#date#visitor#score#home#record#points [n] 24#december 1#toronto#3 - 8#new jersey#11 - 10 - 3#25 [n] 25#december 3#toronto#3 - 4#chicago#11 - 11 - 3#25 [n] 26#december 5#chicago#2 - 2#toronto#11 - 11 - 4#26 [n] 27#december 6#toronto#0 - 6#ny rangers#11 - 12 - 4#26 [n] 28#december 9#detroit#5 - 3#toronto#12 - 12 - 4#28 [n] 29#december 11#calgary#3 - 6#toronto#12 - 13 - 4#28 [n] 30#december 15#toronto#5 - 6#minnesota#12 - 14 - 4#28 [n] 31#december 19#ottawa#5 - 1#toronto#13 - 14 - 4#30 [n] 32#december 20#toronto#4 - 5#buffalo#13 - 15 - 4#30 [n] 33#december 22#toronto#4 - 4#detroit#13 - 15 - 5#31 [n] 34#december 26#detroit#1 - 5#toronto#13 - 16 - 5#31 [n] 35#december 27#toronto#6 - 3#st louis#14 - 16 - 5#33 [n] 36#december 29#toronto#3 - 2#ny islanders#15 - 16 - 5#35 [n] 37#december 31#toronto#3 - 3#pittsburgh#15 - 16 - 6#36 [n] 
03/23/2022 22:42:18 - INFO - __main__ - ['refuted']
03/23/2022 22:42:18 - INFO - __main__ -  [tab_fact] statement: western prince park be the venue for round 6 event between home team footscray and away team fitzroy [SEP] table_caption: 1955 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] north melbourne#10.14 (74)#richmond#7.10 (52)#arden street oval#13000#21 may 1955 [n] collingwood#15.11 (101)#essendon#6.11 (47)#victoria park#35000#21 may 1955 [n] carlton#11.9 (75)#south melbourne#12.11 (83)#princes park#23000#21 may 1955 [n] melbourne#11.5 (71)#hawthorn#6.8 (44)#mcg#28338#21 may 1955 [n] st kilda#4.5 (29)#geelong#6.12 (48)#junction oval#11000#21 may 1955 [n] footscray#8.10 (58)#fitzroy#10.6 (66)#western oval#24517#21 may 1955 [n] 
03/23/2022 22:42:18 - INFO - __main__ - ['refuted']
03/23/2022 22:42:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 22:42:18 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:42:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 22:42:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:42:18 - INFO - __main__ - Printing 3 examples
03/23/2022 22:42:18 - INFO - __main__ -  [tab_fact] statement: the score of the final in which melanie south play with partner ksenia lykina during antalya tournament be 2 - 6 , 1 - 6 [SEP] table_caption: melanie south [SEP] table_text: outcome#tournament#surface#partner#opponent in the final#score [n] winner#tipton#hard#rebecca llewellyn#klaudia jans alicja rosolska#2 - 6 6 - 1 6 - 4 [n] runner - up#tipton#hard#katie o'brien#surina de beer rebecca llewellyn#4 - 6 2 - 6 [n] runner - up#hull#hard#katie o'brien#irena bulykina vasilisa davydova#6 - 4 3 - 6 [n] winner#bath#hard#surina de beer#ekaterina kozhokina trudi musgrave#6 - 2 7 - 5 [n] winner#bournemouth#clay#claire peterzan#anna hawkins holly richards#5 - 7 6 - 4 6 - 3 [n] winner#edinburgh#clay#rebecca llewellyn#leonie mekel bibiane schoofs#6 - 0 3 - 6 6 - 3 [n] runner - up#jersey#hard#katie o'brien#andrea hlavkov matea mezak#3 - 6 1 - 6 [n] winner#nottingham#hard#karen paterson#katie o'brien margit rtel#6 - 2 2 - 6 7 - 6 (7 - 1) [n] winner#nantes#hard#rebecca llewellyn#sabine lisicki irena pavlovic#6 - 2 6 - 0 [n] runner - up#stockholm#hard#sorana crstea#danica krstaji olga panova#2 - 6 6 - 0 2 - 6 [n] runner - up#gran canaria#hard#claire curran#sorana crstea mdlina gojnea#6 - 4 6 - 7 (5 - 7) 4 - 6 [n] runner - up#la palma#hard#arantxa parra santonja#petra cetkovsk andrea hlavkov#3 - 6 2 - 6 [n] winner#surbiton#grass#karen paterson#elena baltacha naomi cavaday#6 - 1 6 - 4 [n] winner#felixstowe#grass#karen paterson#jade curtis rebecca llewellyn#6 - 3 6 - 3 [n] winner#la corua#hard#marina erakovic#andrea hlavkov justine ozga#6 - 1 4 - 6 [n] runner - up#nantes#hard#caroline maes#sofia arvidsson johanna larsson#6 - 4 5 - 7 [n] winner#sorrento#hard#monique adamczak#chang kai - chen hwang i - hsuan#6 - 2 6 - 4 [n] runner - up#gifu#carpet#nicole thijssen#kimiko date - krumm kurumi nara#1 - 6 7 - 6 (10 - 8) [n] winner#fukuoka#carpet#nicole thijssen#maya kato julia moriarty#4 - 6 6 - 3 [n] runner - up#monterrey#hard#monique adamczak#jelena pandi magdalna rybrikov#6 - 4 4 - 6 [n] winner#toyota#carpet#emma laine#kimiko date - krumm han xinyun#6 - 1 7 - 5 [n] winner#helsinki#hard#emma laine#anna smith johanna larsson#6 - 3 6 - 3 [n] winner#glasgow#hard#emma laine#evelyn mayr julia mayr#6 - 3 6 - 2 [n] runner - up#jersey#hard#jarmila gajdoov#maret ani anna smith#7 - 5 6 - 4 [n] runner - up#gifu#clay#ksenia lykina#erika sema tomoko yonemura#3 - 6 , 6 - 2 , 2 - 6 [n] winner#tallinn#hard#emma laine#lu jingjing sun shengnan#6 - 3 6 - 4 [n] runner - up#port pirie#clay#remi tezuka#bojana bobusic alenka hubacek#3 - 6 , 2 - 6 [n] winner#traralgon#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#bendigo#hard#tmea babos#jarmila gajdoov jade hopper#6 - 3 6 - 2 [n] winner#sutton#hard#emma laine#marta domachowska darija jurak#6 - 3 , 5 - 7 , [n] runner - up#hammond , louisiana#hard#mervana jugi - salki#christina fusano julie ditty#3 - 6 , 3 - 6 [n] runner - up#woking#hard#emma laine#julie coin eva hrdinov#1 - 6 , 6 - 3 , 4 - 6 [n] runner - up#wrexham#hard#lenka wienerova#anna fitzpatrick jade windley#2 - 6 , 6 - 4 , 4 - 6 [n] winner#burnie#hard#arina rodionova#stephanie bengson tyra calderwood#6 - 2 , 6 - 2 [n] winner#sydney#hard#arina rodionova#duan yingying han xinyun#3 - 6 , 6 - 3 , [n] runner - up#bath#hard (i)#julie coin#tatjana maria stephanie vogt#3 - 6 , 6 - 3 , 3 - 10 [n] runner - up#kurume#grass#ksenia lykina#han xinyun sun shengnan#1 - 6 , 0 - 6 [n] winner#glasgow#hard (i)#tara moore#anna smith francesca stephenson#7 - 6 (7 - 5) , 6 - 3 [n] runner - up#preston#hard (i)#tara moore#samantha murray jade windley#3 - 6 , 6 - 3 , [n] winner#rancho mirage#hard#tara moore#jan abaza louisa chirico#4 - 6 , 6 - 2 , [n] runner - up#phuket#hard (i)#tara moore#nicha lertpitaksinchai peangtarn plipuech#3 - 6 7 - 5 [n] runner - up#wrexham#hard#anna smith#kanae hisami mari tanaka#3 - 6 , 6 - 7 [n] winner#nottingham#hard#anna smith#daneika borthwick anna fitzpatrick#6 - 4 , 6 - 2 [n] runner - up#antalya#hard#emma laine#andrea bentez carla forte#6 - 4 , 3 - 6 , [n] winner#antalya#hard#emma laine#patcharin cheapchandej tanaporn thongsing#6 - 4 , 6 - 3 [n] 
03/23/2022 22:42:18 - INFO - __main__ - ['refuted']
03/23/2022 22:42:18 - INFO - __main__ -  [tab_fact] statement: the raider only lose 6 game during the season [SEP] table_caption: 1971 oakland raiders season [SEP] table_text: week#date#opponent#result#attendance [n] 1#september 19 , 1971#new england patriots#l 20 - 6#55405 [n] 2#september 26 , 1971#san diego chargers#w 34 - 0#54084 [n] 3#october 4 , 1971#cleveland browns#w 34 - 20#84285 [n] 4#october 10 , 1971#denver broncos#w 27 - 16#51200 [n] 5#october 17 , 1971#philadelphia eagles#w 34 - 10#54615 [n] 6#october 24 , 1971#cincinnati bengals#w 31 - 27#54699 [n] 7#october 31 , 1971#kansas city chiefs#t 20 - 20#54715 [n] 8#november 7 , 1971#new orleans saints#t 21 - 21#83102 [n] 9#november 14 , 1971#houston oilers#w 41 - 21#54705 [n] 10#november 21 , 1971#san diego chargers#w 34 - 33#54681 [n] 11#november 28 , 1971#baltimore colts#l 37 - 14#54689 [n] 12#december 5 , 1971#atlanta falcons#l 24 - 13#58850 [n] 13#december 12 , 1971#kansas city chiefs#l 16 - 14#51215 [n] 14#december 19 , 1971#denver broncos#w 21 - 13#54651 [n] 
03/23/2022 22:42:18 - INFO - __main__ - ['refuted']
03/23/2022 22:42:18 - INFO - __main__ -  [tab_fact] statement: brunswick street oval be 1 of the 3 venue that be put to use on 11 june 1949 [SEP] table_caption: 1949 vfl season [SEP] table_text: home team#home team score#away team#away team score#venue#crowd#date [n] collingwood#17.14 (116)#geelong#12.7 (79)#victoria park#27500#11 june 1949 [n] hawthorn#10.13 (73)#footscray#8.15 (63)#glenferrie oval#10000#11 june 1949 [n] south melbourne#15.16 (106)#essendon#12.9 (81)#lake oval#19500#11 june 1949 [n] north melbourne#11.12 (78)#st kilda#7.7 (49)#arden street oval#10000#13 june 1949 [n] fitzroy#7.10 (52)#melbourne#10.14 (74)#brunswick street oval#16000#13 june 1949 [n] richmond#12.12 (84)#carlton#14.15 (99)#punt road oval#46000#13 june 1949 [n] 
03/23/2022 22:42:18 - INFO - __main__ - ['refuted']
03/23/2022 22:42:18 - INFO - __main__ - Tokenizing Input ...
03/23/2022 22:42:18 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:42:18 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 22:42:36 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 22:42:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 22:42:37 - INFO - __main__ - Starting training!
03/23/2022 22:42:42 - INFO - __main__ - Step 10 Global step 10 Train loss 4.30 on epoch=4
03/23/2022 22:42:47 - INFO - __main__ - Step 20 Global step 20 Train loss 2.40 on epoch=9
03/23/2022 22:42:51 - INFO - __main__ - Step 30 Global step 30 Train loss 1.63 on epoch=14
03/23/2022 22:42:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.95 on epoch=19
03/23/2022 22:43:00 - INFO - __main__ - Step 50 Global step 50 Train loss 0.58 on epoch=24
03/23/2022 22:43:01 - INFO - __main__ - Global step 50 Train loss 1.97 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 22:43:01 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 22:43:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.36 on epoch=29
03/23/2022 22:43:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.34 on epoch=34
03/23/2022 22:43:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.29 on epoch=39
03/23/2022 22:43:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=44
03/23/2022 22:43:24 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=49
03/23/2022 22:43:25 - INFO - __main__ - Global step 100 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=49
03/23/2022 22:43:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.23 on epoch=54
03/23/2022 22:43:34 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=59
03/23/2022 22:43:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
03/23/2022 22:43:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
03/23/2022 22:43:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.22 on epoch=74
03/23/2022 22:43:48 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.3013468013468013 on epoch=74
03/23/2022 22:43:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.22 on epoch=79
03/23/2022 22:43:57 - INFO - __main__ - Step 170 Global step 170 Train loss 0.22 on epoch=84
03/23/2022 22:44:02 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/23/2022 22:44:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=94
03/23/2022 22:44:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=99
03/23/2022 22:44:12 - INFO - __main__ - Global step 200 Train loss 0.23 Classification-F1 0.18604651162790697 on epoch=99
03/23/2022 22:44:16 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=104
03/23/2022 22:44:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=109
03/23/2022 22:44:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.19 on epoch=114
03/23/2022 22:44:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.16 on epoch=119
03/23/2022 22:44:34 - INFO - __main__ - Step 250 Global step 250 Train loss 0.17 on epoch=124
03/23/2022 22:44:35 - INFO - __main__ - Global step 250 Train loss 0.19 Classification-F1 0.29629629629629634 on epoch=124
03/23/2022 22:44:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.15 on epoch=129
03/23/2022 22:44:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
03/23/2022 22:44:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
03/23/2022 22:44:53 - INFO - __main__ - Step 290 Global step 290 Train loss 0.16 on epoch=144
03/23/2022 22:44:57 - INFO - __main__ - Step 300 Global step 300 Train loss 0.14 on epoch=149
03/23/2022 22:44:59 - INFO - __main__ - Global step 300 Train loss 0.15 Classification-F1 0.29629629629629634 on epoch=149
03/23/2022 22:45:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.18 on epoch=154
03/23/2022 22:45:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.17 on epoch=159
03/23/2022 22:45:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
03/23/2022 22:45:16 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
03/23/2022 22:45:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.13 on epoch=174
03/23/2022 22:45:22 - INFO - __main__ - Global step 350 Train loss 0.15 Classification-F1 0.4385964912280702 on epoch=174
03/23/2022 22:45:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4385964912280702 on epoch=174, global_step=350
03/23/2022 22:45:27 - INFO - __main__ - Step 360 Global step 360 Train loss 0.16 on epoch=179
03/23/2022 22:45:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.10 on epoch=184
03/23/2022 22:45:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=189
03/23/2022 22:45:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.12 on epoch=194
03/23/2022 22:45:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
03/23/2022 22:45:46 - INFO - __main__ - Global step 400 Train loss 0.13 Classification-F1 0.5151515151515151 on epoch=199
03/23/2022 22:45:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.5151515151515151 on epoch=199, global_step=400
03/23/2022 22:45:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.10 on epoch=204
03/23/2022 22:45:55 - INFO - __main__ - Step 420 Global step 420 Train loss 0.12 on epoch=209
03/23/2022 22:45:59 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=214
03/23/2022 22:46:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.10 on epoch=219
03/23/2022 22:46:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
03/23/2022 22:46:09 - INFO - __main__ - Global step 450 Train loss 0.11 Classification-F1 0.46843853820598 on epoch=224
03/23/2022 22:46:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.07 on epoch=229
03/23/2022 22:46:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=234
03/23/2022 22:46:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.09 on epoch=239
03/23/2022 22:46:27 - INFO - __main__ - Step 490 Global step 490 Train loss 0.11 on epoch=244
03/23/2022 22:46:31 - INFO - __main__ - Step 500 Global step 500 Train loss 0.07 on epoch=249
03/23/2022 22:46:32 - INFO - __main__ - Global step 500 Train loss 0.09 Classification-F1 0.5195195195195195 on epoch=249
03/23/2022 22:46:32 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5195195195195195 on epoch=249, global_step=500
03/23/2022 22:46:37 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
03/23/2022 22:46:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.09 on epoch=259
03/23/2022 22:46:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
03/23/2022 22:46:50 - INFO - __main__ - Step 540 Global step 540 Train loss 0.08 on epoch=269
03/23/2022 22:46:54 - INFO - __main__ - Step 550 Global step 550 Train loss 0.07 on epoch=274
03/23/2022 22:46:56 - INFO - __main__ - Global step 550 Train loss 0.07 Classification-F1 0.4817813765182186 on epoch=274
03/23/2022 22:47:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.08 on epoch=279
03/23/2022 22:47:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
03/23/2022 22:47:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
03/23/2022 22:47:13 - INFO - __main__ - Step 590 Global step 590 Train loss 0.06 on epoch=294
03/23/2022 22:47:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
03/23/2022 22:47:19 - INFO - __main__ - Global step 600 Train loss 0.06 Classification-F1 0.5076923076923077 on epoch=299
03/23/2022 22:47:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
03/23/2022 22:47:28 - INFO - __main__ - Step 620 Global step 620 Train loss 0.09 on epoch=309
03/23/2022 22:47:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
03/23/2022 22:47:37 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
03/23/2022 22:47:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.04 on epoch=324
03/23/2022 22:47:42 - INFO - __main__ - Global step 650 Train loss 0.05 Classification-F1 0.4817813765182186 on epoch=324
03/23/2022 22:47:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.07 on epoch=329
03/23/2022 22:47:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.03 on epoch=334
03/23/2022 22:47:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
03/23/2022 22:48:00 - INFO - __main__ - Step 690 Global step 690 Train loss 0.03 on epoch=344
03/23/2022 22:48:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
03/23/2022 22:48:06 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.5270935960591133 on epoch=349
03/23/2022 22:48:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5195195195195195 -> 0.5270935960591133 on epoch=349, global_step=700
03/23/2022 22:48:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
03/23/2022 22:48:15 - INFO - __main__ - Step 720 Global step 720 Train loss 0.03 on epoch=359
03/23/2022 22:48:19 - INFO - __main__ - Step 730 Global step 730 Train loss 0.03 on epoch=364
03/23/2022 22:48:23 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
03/23/2022 22:48:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
03/23/2022 22:48:29 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.4817813765182186 on epoch=374
03/23/2022 22:48:33 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
03/23/2022 22:48:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/23/2022 22:48:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
03/23/2022 22:48:47 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
03/23/2022 22:48:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
03/23/2022 22:48:52 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 0.4420512820512821 on epoch=399
03/23/2022 22:48:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
03/23/2022 22:49:01 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
03/23/2022 22:49:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
03/23/2022 22:49:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/23/2022 22:49:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/23/2022 22:49:16 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.2739583333333333 on epoch=424
03/23/2022 22:49:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/23/2022 22:49:24 - INFO - __main__ - Step 870 Global step 870 Train loss 0.04 on epoch=434
03/23/2022 22:49:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
03/23/2022 22:49:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.03 on epoch=444
03/23/2022 22:49:38 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/23/2022 22:49:39 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.41700404858299595 on epoch=449
03/23/2022 22:49:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/23/2022 22:49:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.02 on epoch=459
03/23/2022 22:49:52 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/23/2022 22:49:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/23/2022 22:50:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/23/2022 22:50:02 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.3810483870967742 on epoch=474
03/23/2022 22:50:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
03/23/2022 22:50:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
03/23/2022 22:50:15 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/23/2022 22:50:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/23/2022 22:50:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 22:50:26 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.3810483870967742 on epoch=499
03/23/2022 22:50:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/23/2022 22:50:34 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/23/2022 22:50:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/23/2022 22:50:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/23/2022 22:50:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/23/2022 22:50:49 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.3162393162393162 on epoch=524
03/23/2022 22:50:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/23/2022 22:50:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 22:51:02 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/23/2022 22:51:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/23/2022 22:51:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 22:51:12 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.3373737373737374 on epoch=549
03/23/2022 22:51:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/23/2022 22:51:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 22:51:25 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/23/2022 22:51:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 22:51:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 22:51:35 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.1225806451612903 on epoch=574
03/23/2022 22:51:40 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/23/2022 22:51:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/23/2022 22:51:49 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/23/2022 22:51:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/23/2022 22:51:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 22:51:59 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.13271604938271606 on epoch=599
03/23/2022 22:52:03 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/23/2022 22:52:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
03/23/2022 22:52:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 22:52:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/23/2022 22:52:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
03/23/2022 22:52:22 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.22564935064935066 on epoch=624
03/23/2022 22:52:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/23/2022 22:52:31 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 22:52:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 22:52:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/23/2022 22:52:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 22:52:45 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.18598442714126812 on epoch=649
03/23/2022 22:52:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 22:52:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 22:52:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
03/23/2022 22:53:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
03/23/2022 22:53:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 22:53:08 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.2354497354497354 on epoch=674
03/23/2022 22:53:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/23/2022 22:53:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 22:53:22 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 22:53:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/23/2022 22:53:30 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
03/23/2022 22:53:32 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.2623655913978495 on epoch=699
03/23/2022 22:53:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 22:53:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 22:53:45 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 22:53:49 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 22:53:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 22:53:55 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.2623655913978495 on epoch=724
03/23/2022 22:53:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 22:54:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
03/23/2022 22:54:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 22:54:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/23/2022 22:54:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
03/23/2022 22:54:18 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.16009557945041816 on epoch=749
03/23/2022 22:54:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
03/23/2022 22:54:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 22:54:31 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
03/23/2022 22:54:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 22:54:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/23/2022 22:54:42 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.17142857142857143 on epoch=774
03/23/2022 22:54:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 22:54:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 22:54:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 22:54:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 22:55:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 22:55:05 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.1575757575757576 on epoch=799
03/23/2022 22:55:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 22:55:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
03/23/2022 22:55:18 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 22:55:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 22:55:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 22:55:28 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.15702338282983444 on epoch=824
03/23/2022 22:55:32 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 22:55:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 22:55:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/23/2022 22:55:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 22:55:50 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 22:55:51 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.1730909090909091 on epoch=849
03/23/2022 22:55:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 22:56:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/23/2022 22:56:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 22:56:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 22:56:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 22:56:14 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.18601398601398605 on epoch=874
03/23/2022 22:56:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 22:56:23 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 22:56:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
03/23/2022 22:56:32 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 22:56:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 22:56:38 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.16009557945041816 on epoch=899
03/23/2022 22:56:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 22:56:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 22:56:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 22:56:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 22:57:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 22:57:01 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.21620370370370373 on epoch=924
03/23/2022 22:57:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 22:57:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/23/2022 22:57:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 22:57:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 22:57:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 22:57:25 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.23214285714285715 on epoch=949
03/23/2022 22:57:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 22:57:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 22:57:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 22:57:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 22:57:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 22:57:48 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.1679633867276888 on epoch=974
03/23/2022 22:57:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 22:57:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 22:58:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 22:58:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 22:58:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 22:58:12 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.12974789915966387 on epoch=999
03/23/2022 22:58:12 - INFO - __main__ - save last model!
03/23/2022 22:58:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:58:12 - INFO - __main__ - Printing 3 examples
03/23/2022 22:58:12 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/23/2022 22:58:12 - INFO - __main__ - ['entailed']
03/23/2022 22:58:12 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/23/2022 22:58:12 - INFO - __main__ - ['entailed']
03/23/2022 22:58:12 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/23/2022 22:58:12 - INFO - __main__ - ['entailed']
03/23/2022 22:58:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 22:58:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 22:58:12 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:58:12 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 22:58:12 - INFO - __main__ - Printing 3 examples
03/23/2022 22:58:12 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 22:58:12 - INFO - __main__ - ['entailed']
03/23/2022 22:58:12 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 22:58:12 - INFO - __main__ - ['entailed']
03/23/2022 22:58:12 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 22:58:12 - INFO - __main__ - ['entailed']
03/23/2022 22:58:12 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 22:58:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 22:58:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 22:58:12 - INFO - __main__ - Printing 3 examples
03/23/2022 22:58:12 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/23/2022 22:58:12 - INFO - __main__ - ['entailed']
03/23/2022 22:58:12 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/23/2022 22:58:12 - INFO - __main__ - ['entailed']
03/23/2022 22:58:12 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/23/2022 22:58:12 - INFO - __main__ - ['entailed']
03/23/2022 22:58:12 - INFO - __main__ - Tokenizing Input ...
03/23/2022 22:58:12 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:58:12 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 22:58:31 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 22:58:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 22:58:32 - INFO - __main__ - Starting training!
03/23/2022 22:58:35 - INFO - __main__ - Tokenizing Output ...
03/23/2022 22:58:48 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 23:06:52 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_13_0.2_8_predictions.txt
03/23/2022 23:06:53 - INFO - __main__ - Classification-F1 on test data: 0.0420
03/23/2022 23:06:53 - INFO - __main__ - prefix=tab_fact_16_13, lr=0.2, bsz=8, dev_performance=0.5270935960591133, test_performance=0.04196908455314377
03/23/2022 23:06:53 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.5, bsz=8 ...
03/23/2022 23:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:06:54 - INFO - __main__ - Printing 3 examples
03/23/2022 23:06:54 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/23/2022 23:06:54 - INFO - __main__ - ['entailed']
03/23/2022 23:06:54 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/23/2022 23:06:54 - INFO - __main__ - ['entailed']
03/23/2022 23:06:54 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/23/2022 23:06:54 - INFO - __main__ - ['entailed']
03/23/2022 23:06:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 23:06:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:06:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 23:06:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:06:54 - INFO - __main__ - Printing 3 examples
03/23/2022 23:06:54 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/23/2022 23:06:54 - INFO - __main__ - ['entailed']
03/23/2022 23:06:54 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/23/2022 23:06:54 - INFO - __main__ - ['entailed']
03/23/2022 23:06:54 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/23/2022 23:06:54 - INFO - __main__ - ['entailed']
03/23/2022 23:06:54 - INFO - __main__ - Tokenizing Input ...
03/23/2022 23:06:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:06:54 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 23:07:12 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 23:07:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 23:07:13 - INFO - __main__ - Starting training!
03/23/2022 23:07:18 - INFO - __main__ - Step 10 Global step 10 Train loss 2.98 on epoch=4
03/23/2022 23:07:23 - INFO - __main__ - Step 20 Global step 20 Train loss 0.89 on epoch=9
03/23/2022 23:07:27 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=14
03/23/2022 23:07:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.34 on epoch=19
03/23/2022 23:07:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.29 on epoch=24
03/23/2022 23:07:37 - INFO - __main__ - Global step 50 Train loss 0.98 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 23:07:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 23:07:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.30 on epoch=29
03/23/2022 23:07:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.28 on epoch=34
03/23/2022 23:07:51 - INFO - __main__ - Step 80 Global step 80 Train loss 0.29 on epoch=39
03/23/2022 23:07:55 - INFO - __main__ - Step 90 Global step 90 Train loss 0.24 on epoch=44
03/23/2022 23:08:00 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=49
03/23/2022 23:08:01 - INFO - __main__ - Global step 100 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=49
03/23/2022 23:08:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=54
03/23/2022 23:08:10 - INFO - __main__ - Step 120 Global step 120 Train loss 0.21 on epoch=59
03/23/2022 23:08:14 - INFO - __main__ - Step 130 Global step 130 Train loss 0.23 on epoch=64
03/23/2022 23:08:19 - INFO - __main__ - Step 140 Global step 140 Train loss 0.23 on epoch=69
03/23/2022 23:08:23 - INFO - __main__ - Step 150 Global step 150 Train loss 0.21 on epoch=74
03/23/2022 23:08:25 - INFO - __main__ - Global step 150 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
03/23/2022 23:08:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.24 on epoch=79
03/23/2022 23:08:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.22 on epoch=84
03/23/2022 23:08:38 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
03/23/2022 23:08:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
03/23/2022 23:08:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
03/23/2022 23:08:48 - INFO - __main__ - Global step 200 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
03/23/2022 23:08:52 - INFO - __main__ - Step 210 Global step 210 Train loss 0.20 on epoch=104
03/23/2022 23:08:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=109
03/23/2022 23:09:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.22 on epoch=114
03/23/2022 23:09:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.21 on epoch=119
03/23/2022 23:09:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.20 on epoch=124
03/23/2022 23:09:11 - INFO - __main__ - Global step 250 Train loss 0.21 Classification-F1 0.28888888888888886 on epoch=124
03/23/2022 23:09:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.19 on epoch=129
03/23/2022 23:09:20 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=134
03/23/2022 23:09:24 - INFO - __main__ - Step 280 Global step 280 Train loss 0.20 on epoch=139
03/23/2022 23:09:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.18 on epoch=144
03/23/2022 23:09:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.19 on epoch=149
03/23/2022 23:09:34 - INFO - __main__ - Global step 300 Train loss 0.19 Classification-F1 0.3333333333333333 on epoch=149
03/23/2022 23:09:38 - INFO - __main__ - Step 310 Global step 310 Train loss 0.18 on epoch=154
03/23/2022 23:09:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=159
03/23/2022 23:09:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.18 on epoch=164
03/23/2022 23:09:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
03/23/2022 23:09:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.17 on epoch=174
03/23/2022 23:09:57 - INFO - __main__ - Global step 350 Train loss 0.17 Classification-F1 0.39139139139139134 on epoch=174
03/23/2022 23:09:57 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.39139139139139134 on epoch=174, global_step=350
03/23/2022 23:10:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
03/23/2022 23:10:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=184
03/23/2022 23:10:10 - INFO - __main__ - Step 380 Global step 380 Train loss 0.13 on epoch=189
03/23/2022 23:10:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=194
03/23/2022 23:10:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.12 on epoch=199
03/23/2022 23:10:20 - INFO - __main__ - Global step 400 Train loss 0.15 Classification-F1 0.3333333333333333 on epoch=199
03/23/2022 23:10:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.14 on epoch=204
03/23/2022 23:10:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.11 on epoch=209
03/23/2022 23:10:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.13 on epoch=214
03/23/2022 23:10:38 - INFO - __main__ - Step 440 Global step 440 Train loss 0.09 on epoch=219
03/23/2022 23:10:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.10 on epoch=224
03/23/2022 23:10:44 - INFO - __main__ - Global step 450 Train loss 0.12 Classification-F1 0.3650793650793651 on epoch=224
03/23/2022 23:10:48 - INFO - __main__ - Step 460 Global step 460 Train loss 0.11 on epoch=229
03/23/2022 23:10:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.08 on epoch=234
03/23/2022 23:10:57 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
03/23/2022 23:11:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
03/23/2022 23:11:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
03/23/2022 23:11:07 - INFO - __main__ - Global step 500 Train loss 0.08 Classification-F1 0.4554554554554554 on epoch=249
03/23/2022 23:11:08 - INFO - __main__ - Saving model with best Classification-F1: 0.39139139139139134 -> 0.4554554554554554 on epoch=249, global_step=500
03/23/2022 23:11:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=254
03/23/2022 23:11:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
03/23/2022 23:11:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
03/23/2022 23:11:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
03/23/2022 23:11:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
03/23/2022 23:11:31 - INFO - __main__ - Global step 550 Train loss 0.05 Classification-F1 0.4980392156862745 on epoch=274
03/23/2022 23:11:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.4980392156862745 on epoch=274, global_step=550
03/23/2022 23:11:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/23/2022 23:11:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/23/2022 23:11:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/23/2022 23:11:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/23/2022 23:11:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
03/23/2022 23:11:55 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.3273273273273273 on epoch=299
03/23/2022 23:11:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
03/23/2022 23:12:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.07 on epoch=309
03/23/2022 23:12:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/23/2022 23:12:13 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/23/2022 23:12:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
03/23/2022 23:12:18 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.4666666666666667 on epoch=324
03/23/2022 23:12:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/23/2022 23:12:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/23/2022 23:12:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/23/2022 23:12:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/23/2022 23:12:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
03/23/2022 23:12:42 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.39139139139139134 on epoch=349
03/23/2022 23:12:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/23/2022 23:12:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/23/2022 23:12:55 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/23/2022 23:13:00 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/23/2022 23:13:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/23/2022 23:13:06 - INFO - __main__ - Global step 750 Train loss 0.00 Classification-F1 0.35699797160243407 on epoch=374
03/23/2022 23:13:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/23/2022 23:13:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/23/2022 23:13:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/23/2022 23:13:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/23/2022 23:13:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
03/23/2022 23:13:29 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.3333333333333333 on epoch=399
03/23/2022 23:13:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
03/23/2022 23:13:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/23/2022 23:13:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/23/2022 23:13:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/23/2022 23:13:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/23/2022 23:13:53 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.2821052631578947 on epoch=424
03/23/2022 23:13:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
03/23/2022 23:14:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/23/2022 23:14:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 23:14:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/23/2022 23:14:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/23/2022 23:14:17 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=449
03/23/2022 23:14:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 23:14:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 23:14:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/23/2022 23:14:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
03/23/2022 23:14:39 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/23/2022 23:14:40 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.30838530838530837 on epoch=474
03/23/2022 23:14:45 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/23/2022 23:14:49 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/23/2022 23:14:53 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/23/2022 23:14:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 23:15:02 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/23/2022 23:15:04 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.41700404858299595 on epoch=499
03/23/2022 23:15:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/23/2022 23:15:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 23:15:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 23:15:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 23:15:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/23/2022 23:15:27 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.3373901284651792 on epoch=524
03/23/2022 23:15:32 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/23/2022 23:15:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/23/2022 23:15:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/23/2022 23:15:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 23:15:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 23:15:51 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.39139139139139134 on epoch=549
03/23/2022 23:15:55 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 23:16:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 23:16:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/23/2022 23:16:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.02 on epoch=569
03/23/2022 23:16:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 23:16:15 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=574
03/23/2022 23:16:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 23:16:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/23/2022 23:16:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 23:16:32 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 23:16:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/23/2022 23:16:38 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=599
03/23/2022 23:16:38 - INFO - __main__ - Saving model with best Classification-F1: 0.4980392156862745 -> 0.5307917888563051 on epoch=599, global_step=1200
03/23/2022 23:16:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 23:16:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
03/23/2022 23:16:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 23:16:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
03/23/2022 23:17:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/23/2022 23:17:02 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=624
03/23/2022 23:17:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 23:17:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/23/2022 23:17:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/23/2022 23:17:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 23:17:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/23/2022 23:17:25 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.41700404858299595 on epoch=649
03/23/2022 23:17:30 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 23:17:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 23:17:39 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/23/2022 23:17:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/23/2022 23:17:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 23:17:49 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=674
03/23/2022 23:17:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 23:17:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 23:18:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 23:18:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 23:18:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/23/2022 23:18:13 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=699
03/23/2022 23:18:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 23:18:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 23:18:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 23:18:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/23/2022 23:18:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/23/2022 23:18:36 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.41700404858299595 on epoch=724
03/23/2022 23:18:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 23:18:45 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 23:18:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 23:18:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 23:18:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 23:18:59 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.41700404858299595 on epoch=749
03/23/2022 23:19:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 23:19:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/23/2022 23:19:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/23/2022 23:19:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 23:19:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/23/2022 23:19:23 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=774
03/23/2022 23:19:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 23:19:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 23:19:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 23:19:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 23:19:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 23:19:46 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.3373901284651792 on epoch=799
03/23/2022 23:19:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 23:19:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 23:20:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 23:20:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 23:20:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 23:20:10 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=824
03/23/2022 23:20:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 23:20:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 23:20:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/23/2022 23:20:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 23:20:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 23:20:33 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.3142857142857143 on epoch=849
03/23/2022 23:20:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 23:20:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 23:20:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 23:20:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 23:20:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 23:20:57 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=874
03/23/2022 23:21:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 23:21:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/23/2022 23:21:10 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 23:21:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 23:21:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
03/23/2022 23:21:20 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=899
03/23/2022 23:21:20 - INFO - __main__ - Saving model with best Classification-F1: 0.5307917888563051 -> 0.5607843137254902 on epoch=899, global_step=1800
03/23/2022 23:21:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 23:21:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 23:21:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
03/23/2022 23:21:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 23:21:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 23:21:43 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=924
03/23/2022 23:21:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 23:21:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 23:21:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 23:22:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 23:22:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 23:22:07 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=949
03/23/2022 23:22:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 23:22:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 23:22:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 23:22:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 23:22:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/23/2022 23:22:30 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=974
03/23/2022 23:22:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 23:22:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 23:22:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/23/2022 23:22:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 23:22:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 23:22:53 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5270935960591133 on epoch=999
03/23/2022 23:22:53 - INFO - __main__ - save last model!
03/23/2022 23:22:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:22:53 - INFO - __main__ - Printing 3 examples
03/23/2022 23:22:53 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/23/2022 23:22:53 - INFO - __main__ - ['entailed']
03/23/2022 23:22:53 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/23/2022 23:22:53 - INFO - __main__ - ['entailed']
03/23/2022 23:22:53 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/23/2022 23:22:53 - INFO - __main__ - ['entailed']
03/23/2022 23:22:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 23:22:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 23:22:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:22:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 23:22:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:22:54 - INFO - __main__ - Printing 3 examples
03/23/2022 23:22:54 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/23/2022 23:22:54 - INFO - __main__ - ['entailed']
03/23/2022 23:22:54 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/23/2022 23:22:54 - INFO - __main__ - ['entailed']
03/23/2022 23:22:54 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/23/2022 23:22:54 - INFO - __main__ - ['entailed']
03/23/2022 23:22:54 - INFO - __main__ - Tokenizing Input ...
03/23/2022 23:22:54 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:22:54 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 23:22:54 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 23:22:54 - INFO - __main__ - Printing 3 examples
03/23/2022 23:22:54 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 23:22:54 - INFO - __main__ - ['entailed']
03/23/2022 23:22:54 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 23:22:54 - INFO - __main__ - ['entailed']
03/23/2022 23:22:54 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 23:22:54 - INFO - __main__ - ['entailed']
03/23/2022 23:22:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 23:23:09 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 23:23:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 23:23:10 - INFO - __main__ - Starting training!
03/23/2022 23:23:18 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:23:31 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 23:31:37 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.5_8_predictions.txt
03/23/2022 23:31:37 - INFO - __main__ - Classification-F1 on test data: 0.1252
03/23/2022 23:31:37 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.5, bsz=8, dev_performance=0.5607843137254902, test_performance=0.12519305580878987
03/23/2022 23:31:37 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.4, bsz=8 ...
03/23/2022 23:31:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:31:38 - INFO - __main__ - Printing 3 examples
03/23/2022 23:31:38 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/23/2022 23:31:38 - INFO - __main__ - ['entailed']
03/23/2022 23:31:38 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/23/2022 23:31:38 - INFO - __main__ - ['entailed']
03/23/2022 23:31:38 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/23/2022 23:31:38 - INFO - __main__ - ['entailed']
03/23/2022 23:31:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 23:31:38 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:31:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 23:31:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:31:38 - INFO - __main__ - Printing 3 examples
03/23/2022 23:31:38 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/23/2022 23:31:38 - INFO - __main__ - ['entailed']
03/23/2022 23:31:38 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/23/2022 23:31:38 - INFO - __main__ - ['entailed']
03/23/2022 23:31:38 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/23/2022 23:31:38 - INFO - __main__ - ['entailed']
03/23/2022 23:31:38 - INFO - __main__ - Tokenizing Input ...
03/23/2022 23:31:38 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:31:39 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 23:31:57 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 23:31:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 23:31:58 - INFO - __main__ - Starting training!
03/23/2022 23:32:03 - INFO - __main__ - Step 10 Global step 10 Train loss 3.33 on epoch=4
03/23/2022 23:32:07 - INFO - __main__ - Step 20 Global step 20 Train loss 1.22 on epoch=9
03/23/2022 23:32:12 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=14
03/23/2022 23:32:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.37 on epoch=19
03/23/2022 23:32:21 - INFO - __main__ - Step 50 Global step 50 Train loss 0.30 on epoch=24
03/23/2022 23:32:22 - INFO - __main__ - Global step 50 Train loss 1.14 Classification-F1 0.3333333333333333 on epoch=24
03/23/2022 23:32:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/23/2022 23:32:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.27 on epoch=29
03/23/2022 23:32:31 - INFO - __main__ - Step 70 Global step 70 Train loss 0.28 on epoch=34
03/23/2022 23:32:35 - INFO - __main__ - Step 80 Global step 80 Train loss 0.22 on epoch=39
03/23/2022 23:32:40 - INFO - __main__ - Step 90 Global step 90 Train loss 0.24 on epoch=44
03/23/2022 23:32:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.23 on epoch=49
03/23/2022 23:32:46 - INFO - __main__ - Global step 100 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=49
03/23/2022 23:32:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
03/23/2022 23:32:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=59
03/23/2022 23:32:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=64
03/23/2022 23:33:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=69
03/23/2022 23:33:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
03/23/2022 23:33:09 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=74
03/23/2022 23:33:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.20 on epoch=79
03/23/2022 23:33:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
03/23/2022 23:33:23 - INFO - __main__ - Step 180 Global step 180 Train loss 0.20 on epoch=89
03/23/2022 23:33:27 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
03/23/2022 23:33:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=99
03/23/2022 23:33:33 - INFO - __main__ - Global step 200 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=99
03/23/2022 23:33:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=104
03/23/2022 23:33:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
03/23/2022 23:33:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.21 on epoch=114
03/23/2022 23:33:51 - INFO - __main__ - Step 240 Global step 240 Train loss 0.20 on epoch=119
03/23/2022 23:33:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=124
03/23/2022 23:33:57 - INFO - __main__ - Global step 250 Train loss 0.21 Classification-F1 0.3333333333333333 on epoch=124
03/23/2022 23:34:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.20 on epoch=129
03/23/2022 23:34:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.17 on epoch=134
03/23/2022 23:34:10 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
03/23/2022 23:34:14 - INFO - __main__ - Step 290 Global step 290 Train loss 0.17 on epoch=144
03/23/2022 23:34:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.19 on epoch=149
03/23/2022 23:34:20 - INFO - __main__ - Global step 300 Train loss 0.19 Classification-F1 0.25581395348837205 on epoch=149
03/23/2022 23:34:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=154
03/23/2022 23:34:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
03/23/2022 23:34:34 - INFO - __main__ - Step 330 Global step 330 Train loss 0.15 on epoch=164
03/23/2022 23:34:38 - INFO - __main__ - Step 340 Global step 340 Train loss 0.13 on epoch=169
03/23/2022 23:34:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=174
03/23/2022 23:34:44 - INFO - __main__ - Global step 350 Train loss 0.16 Classification-F1 0.28744939271255066 on epoch=174
03/23/2022 23:34:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.13 on epoch=179
03/23/2022 23:34:53 - INFO - __main__ - Step 370 Global step 370 Train loss 0.15 on epoch=184
03/23/2022 23:34:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.19 on epoch=189
03/23/2022 23:35:02 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=194
03/23/2022 23:35:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.11 on epoch=199
03/23/2022 23:35:07 - INFO - __main__ - Global step 400 Train loss 0.14 Classification-F1 0.21951219512195122 on epoch=199
03/23/2022 23:35:12 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
03/23/2022 23:35:16 - INFO - __main__ - Step 420 Global step 420 Train loss 0.13 on epoch=209
03/23/2022 23:35:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.12 on epoch=214
03/23/2022 23:35:25 - INFO - __main__ - Step 440 Global step 440 Train loss 0.08 on epoch=219
03/23/2022 23:35:30 - INFO - __main__ - Step 450 Global step 450 Train loss 0.09 on epoch=224
03/23/2022 23:35:31 - INFO - __main__ - Global step 450 Train loss 0.10 Classification-F1 0.18907563025210083 on epoch=224
03/23/2022 23:35:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.08 on epoch=229
03/23/2022 23:35:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.09 on epoch=234
03/23/2022 23:35:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
03/23/2022 23:35:49 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
03/23/2022 23:35:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.08 on epoch=249
03/23/2022 23:35:54 - INFO - __main__ - Global step 500 Train loss 0.08 Classification-F1 0.3107692307692308 on epoch=249
03/23/2022 23:35:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
03/23/2022 23:36:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.11 on epoch=259
03/23/2022 23:36:08 - INFO - __main__ - Step 530 Global step 530 Train loss 0.05 on epoch=264
03/23/2022 23:36:12 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
03/23/2022 23:36:17 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=274
03/23/2022 23:36:18 - INFO - __main__ - Global step 550 Train loss 0.08 Classification-F1 0.3107692307692308 on epoch=274
03/23/2022 23:36:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
03/23/2022 23:36:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
03/23/2022 23:36:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.06 on epoch=289
03/23/2022 23:36:36 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
03/23/2022 23:36:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
03/23/2022 23:36:41 - INFO - __main__ - Global step 600 Train loss 0.05 Classification-F1 0.24705882352941178 on epoch=299
03/23/2022 23:36:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
03/23/2022 23:36:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/23/2022 23:36:55 - INFO - __main__ - Step 630 Global step 630 Train loss 0.05 on epoch=314
03/23/2022 23:36:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
03/23/2022 23:37:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
03/23/2022 23:37:05 - INFO - __main__ - Global step 650 Train loss 0.04 Classification-F1 0.2805474095796677 on epoch=324
03/23/2022 23:37:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.03 on epoch=329
03/23/2022 23:37:14 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
03/23/2022 23:37:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/23/2022 23:37:23 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/23/2022 23:37:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/23/2022 23:37:28 - INFO - __main__ - Global step 700 Train loss 0.03 Classification-F1 0.3552492046659597 on epoch=349
03/23/2022 23:37:29 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3552492046659597 on epoch=349, global_step=700
03/23/2022 23:37:33 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/23/2022 23:37:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
03/23/2022 23:37:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
03/23/2022 23:37:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
03/23/2022 23:37:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/23/2022 23:37:52 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.14126984126984127 on epoch=374
03/23/2022 23:37:57 - INFO - __main__ - Step 760 Global step 760 Train loss 0.02 on epoch=379
03/23/2022 23:38:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/23/2022 23:38:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/23/2022 23:38:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/23/2022 23:38:14 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/23/2022 23:38:16 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.18571428571428572 on epoch=399
03/23/2022 23:38:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.02 on epoch=404
03/23/2022 23:38:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/23/2022 23:38:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/23/2022 23:38:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/23/2022 23:38:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/23/2022 23:38:39 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.18571428571428572 on epoch=424
03/23/2022 23:38:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/23/2022 23:38:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/23/2022 23:38:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/23/2022 23:38:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/23/2022 23:39:01 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
03/23/2022 23:39:03 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.2805474095796677 on epoch=449
03/23/2022 23:39:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/23/2022 23:39:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/23/2022 23:39:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/23/2022 23:39:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/23/2022 23:39:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
03/23/2022 23:39:26 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.23809523809523808 on epoch=474
03/23/2022 23:39:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
03/23/2022 23:39:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
03/23/2022 23:39:39 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
03/23/2022 23:39:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/23/2022 23:39:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/23/2022 23:39:49 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.2632632632632632 on epoch=499
03/23/2022 23:39:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
03/23/2022 23:39:58 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/23/2022 23:40:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/23/2022 23:40:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/23/2022 23:40:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/23/2022 23:40:13 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.16452991452991453 on epoch=524
03/23/2022 23:40:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/23/2022 23:40:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/23/2022 23:40:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/23/2022 23:40:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/23/2022 23:40:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/23/2022 23:40:37 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.18571428571428572 on epoch=549
03/23/2022 23:40:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/23/2022 23:40:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/23/2022 23:40:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/23/2022 23:40:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/23/2022 23:40:59 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/23/2022 23:41:00 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.30980392156862746 on epoch=574
03/23/2022 23:41:05 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/23/2022 23:41:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
03/23/2022 23:41:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/23/2022 23:41:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/23/2022 23:41:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/23/2022 23:41:24 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.2748768472906404 on epoch=599
03/23/2022 23:41:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/23/2022 23:41:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/23/2022 23:41:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/23/2022 23:41:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
03/23/2022 23:41:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
03/23/2022 23:41:47 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.30980392156862746 on epoch=624
03/23/2022 23:41:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/23/2022 23:41:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/23/2022 23:42:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/23/2022 23:42:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/23/2022 23:42:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/23/2022 23:42:11 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.3273273273273273 on epoch=649
03/23/2022 23:42:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/23/2022 23:42:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/23/2022 23:42:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
03/23/2022 23:42:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/23/2022 23:42:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/23/2022 23:42:35 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.39999999999999997 on epoch=674
03/23/2022 23:42:35 - INFO - __main__ - Saving model with best Classification-F1: 0.3552492046659597 -> 0.39999999999999997 on epoch=674, global_step=1350
03/23/2022 23:42:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/23/2022 23:42:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/23/2022 23:42:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/23/2022 23:42:52 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/23/2022 23:42:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/23/2022 23:42:58 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.33793103448275863 on epoch=699
03/23/2022 23:43:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/23/2022 23:43:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/23/2022 23:43:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/23/2022 23:43:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
03/23/2022 23:43:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/23/2022 23:43:21 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.37254901960784315 on epoch=724
03/23/2022 23:43:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/23/2022 23:43:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/23/2022 23:43:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/23/2022 23:43:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/23/2022 23:43:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/23/2022 23:43:45 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.30158730158730157 on epoch=749
03/23/2022 23:43:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/23/2022 23:43:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/23/2022 23:43:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/23/2022 23:44:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/23/2022 23:44:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/23/2022 23:44:08 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.375 on epoch=774
03/23/2022 23:44:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/23/2022 23:44:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/23/2022 23:44:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/23/2022 23:44:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/23/2022 23:44:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/23/2022 23:44:31 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.3107692307692308 on epoch=799
03/23/2022 23:44:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/23/2022 23:44:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/23/2022 23:44:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/23/2022 23:44:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/23/2022 23:44:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/23/2022 23:44:55 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.3333333333333333 on epoch=824
03/23/2022 23:44:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/23/2022 23:45:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/23/2022 23:45:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/23/2022 23:45:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/23/2022 23:45:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/23/2022 23:45:18 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.3107692307692308 on epoch=849
03/23/2022 23:45:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/23/2022 23:45:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/23/2022 23:45:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/23/2022 23:45:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/23/2022 23:45:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/23/2022 23:45:41 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.3273273273273273 on epoch=874
03/23/2022 23:45:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/23/2022 23:45:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
03/23/2022 23:45:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/23/2022 23:45:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/23/2022 23:46:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/23/2022 23:46:05 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.37662337662337664 on epoch=899
03/23/2022 23:46:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/23/2022 23:46:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/23/2022 23:46:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/23/2022 23:46:22 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/23/2022 23:46:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/23/2022 23:46:28 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.33793103448275863 on epoch=924
03/23/2022 23:46:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/23/2022 23:46:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/23/2022 23:46:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/23/2022 23:46:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/23/2022 23:46:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/23/2022 23:46:52 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.33793103448275863 on epoch=949
03/23/2022 23:46:56 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/23/2022 23:47:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/23/2022 23:47:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/23/2022 23:47:09 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/23/2022 23:47:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/23/2022 23:47:15 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=974
03/23/2022 23:47:15 - INFO - __main__ - Saving model with best Classification-F1: 0.39999999999999997 -> 0.4009852216748768 on epoch=974, global_step=1950
03/23/2022 23:47:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/23/2022 23:47:24 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/23/2022 23:47:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/23/2022 23:47:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/23/2022 23:47:37 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/23/2022 23:47:38 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:47:38 - INFO - __main__ - Printing 3 examples
03/23/2022 23:47:38 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/23/2022 23:47:38 - INFO - __main__ - ['entailed']
03/23/2022 23:47:38 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/23/2022 23:47:38 - INFO - __main__ - ['entailed']
03/23/2022 23:47:38 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/23/2022 23:47:38 - INFO - __main__ - ['entailed']
03/23/2022 23:47:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/23/2022 23:47:38 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.3107692307692308 on epoch=999
03/23/2022 23:47:38 - INFO - __main__ - save last model!
03/23/2022 23:47:38 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:47:39 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/23/2022 23:47:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 23:47:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:47:39 - INFO - __main__ - Printing 3 examples
03/23/2022 23:47:39 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/23/2022 23:47:39 - INFO - __main__ - ['entailed']
03/23/2022 23:47:39 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/23/2022 23:47:39 - INFO - __main__ - ['entailed']
03/23/2022 23:47:39 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/23/2022 23:47:39 - INFO - __main__ - ['entailed']
03/23/2022 23:47:39 - INFO - __main__ - Tokenizing Input ...
03/23/2022 23:47:39 - INFO - __main__ - Start tokenizing ... 12792 instances
03/23/2022 23:47:39 - INFO - __main__ - Printing 3 examples
03/23/2022 23:47:39 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 23:47:39 - INFO - __main__ - ['entailed']
03/23/2022 23:47:39 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 23:47:39 - INFO - __main__ - ['entailed']
03/23/2022 23:47:39 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/23/2022 23:47:39 - INFO - __main__ - ['entailed']
03/23/2022 23:47:39 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 23:47:39 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:47:39 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 23:47:54 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 23:47:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 23:47:54 - INFO - __main__ - Starting training!
03/23/2022 23:48:02 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:48:15 - INFO - __main__ - Loaded 12792 examples from test data
03/23/2022 23:56:32 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.4_8_predictions.txt
03/23/2022 23:56:32 - INFO - __main__ - Classification-F1 on test data: 0.1223
03/23/2022 23:56:32 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.4, bsz=8, dev_performance=0.4009852216748768, test_performance=0.12234518965346827
03/23/2022 23:56:32 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.3, bsz=8 ...
03/23/2022 23:56:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:56:33 - INFO - __main__ - Printing 3 examples
03/23/2022 23:56:33 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/23/2022 23:56:33 - INFO - __main__ - ['entailed']
03/23/2022 23:56:33 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/23/2022 23:56:33 - INFO - __main__ - ['entailed']
03/23/2022 23:56:33 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/23/2022 23:56:33 - INFO - __main__ - ['entailed']
03/23/2022 23:56:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/23/2022 23:56:33 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:56:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/23/2022 23:56:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/23/2022 23:56:33 - INFO - __main__ - Printing 3 examples
03/23/2022 23:56:33 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/23/2022 23:56:33 - INFO - __main__ - ['entailed']
03/23/2022 23:56:33 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/23/2022 23:56:33 - INFO - __main__ - ['entailed']
03/23/2022 23:56:33 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/23/2022 23:56:33 - INFO - __main__ - ['entailed']
03/23/2022 23:56:33 - INFO - __main__ - Tokenizing Input ...
03/23/2022 23:56:33 - INFO - __main__ - Tokenizing Output ...
03/23/2022 23:56:33 - INFO - __main__ - Loaded 32 examples from dev data
03/23/2022 23:56:52 - INFO - __main__ - load prompt embedding from ckpt
03/23/2022 23:56:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/23/2022 23:56:53 - INFO - __main__ - Starting training!
03/23/2022 23:56:58 - INFO - __main__ - Step 10 Global step 10 Train loss 3.42 on epoch=4
03/23/2022 23:57:02 - INFO - __main__ - Step 20 Global step 20 Train loss 1.69 on epoch=9
03/23/2022 23:57:06 - INFO - __main__ - Step 30 Global step 30 Train loss 0.78 on epoch=14
03/23/2022 23:57:11 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=19
03/23/2022 23:57:15 - INFO - __main__ - Step 50 Global step 50 Train loss 0.37 on epoch=24
03/23/2022 23:57:17 - INFO - __main__ - Global step 50 Train loss 1.37 Classification-F1 0.3191489361702127 on epoch=24
03/23/2022 23:57:17 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3191489361702127 on epoch=24, global_step=50
03/23/2022 23:57:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.37 on epoch=29
03/23/2022 23:57:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.34 on epoch=34
03/23/2022 23:57:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.29 on epoch=39
03/23/2022 23:57:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.27 on epoch=44
03/23/2022 23:57:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.31 on epoch=49
03/23/2022 23:57:40 - INFO - __main__ - Global step 100 Train loss 0.32 Classification-F1 0.21276595744680848 on epoch=49
03/23/2022 23:57:44 - INFO - __main__ - Step 110 Global step 110 Train loss 0.22 on epoch=54
03/23/2022 23:57:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=59
03/23/2022 23:57:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=64
03/23/2022 23:57:58 - INFO - __main__ - Step 140 Global step 140 Train loss 0.21 on epoch=69
03/23/2022 23:58:02 - INFO - __main__ - Step 150 Global step 150 Train loss 0.21 on epoch=74
03/23/2022 23:58:04 - INFO - __main__ - Global step 150 Train loss 0.23 Classification-F1 0.20655270655270652 on epoch=74
03/23/2022 23:58:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=79
03/23/2022 23:58:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.22 on epoch=84
03/23/2022 23:58:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.22 on epoch=89
03/23/2022 23:58:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=94
03/23/2022 23:58:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.20 on epoch=99
03/23/2022 23:58:27 - INFO - __main__ - Global step 200 Train loss 0.22 Classification-F1 0.3043478260869565 on epoch=99
03/23/2022 23:58:32 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=104
03/23/2022 23:58:36 - INFO - __main__ - Step 220 Global step 220 Train loss 0.18 on epoch=109
03/23/2022 23:58:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.20 on epoch=114
03/23/2022 23:58:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.20 on epoch=119
03/23/2022 23:58:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
03/23/2022 23:58:51 - INFO - __main__ - Global step 250 Train loss 0.21 Classification-F1 0.28744939271255066 on epoch=124
03/23/2022 23:58:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.22 on epoch=129
03/23/2022 23:59:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.21 on epoch=134
03/23/2022 23:59:04 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
03/23/2022 23:59:08 - INFO - __main__ - Step 290 Global step 290 Train loss 0.19 on epoch=144
03/23/2022 23:59:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.18 on epoch=149
03/23/2022 23:59:14 - INFO - __main__ - Global step 300 Train loss 0.20 Classification-F1 0.3043478260869565 on epoch=149
03/23/2022 23:59:19 - INFO - __main__ - Step 310 Global step 310 Train loss 0.18 on epoch=154
03/23/2022 23:59:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.17 on epoch=159
03/23/2022 23:59:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.19 on epoch=164
03/23/2022 23:59:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.17 on epoch=169
03/23/2022 23:59:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.16 on epoch=174
03/23/2022 23:59:38 - INFO - __main__ - Global step 350 Train loss 0.17 Classification-F1 0.3073593073593074 on epoch=174
03/23/2022 23:59:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.18 on epoch=179
03/23/2022 23:59:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=184
03/23/2022 23:59:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.17 on epoch=189
03/23/2022 23:59:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.14 on epoch=194
03/24/2022 00:00:00 - INFO - __main__ - Step 400 Global step 400 Train loss 0.14 on epoch=199
03/24/2022 00:00:02 - INFO - __main__ - Global step 400 Train loss 0.16 Classification-F1 0.2805474095796677 on epoch=199
03/24/2022 00:00:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.13 on epoch=204
03/24/2022 00:00:10 - INFO - __main__ - Step 420 Global step 420 Train loss 0.14 on epoch=209
03/24/2022 00:00:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.16 on epoch=214
03/24/2022 00:00:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.11 on epoch=219
03/24/2022 00:00:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.14 on epoch=224
03/24/2022 00:00:25 - INFO - __main__ - Global step 450 Train loss 0.13 Classification-F1 0.25 on epoch=224
03/24/2022 00:00:29 - INFO - __main__ - Step 460 Global step 460 Train loss 0.10 on epoch=229
03/24/2022 00:00:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=234
03/24/2022 00:00:38 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
03/24/2022 00:00:43 - INFO - __main__ - Step 490 Global step 490 Train loss 0.10 on epoch=244
03/24/2022 00:00:47 - INFO - __main__ - Step 500 Global step 500 Train loss 0.10 on epoch=249
03/24/2022 00:00:49 - INFO - __main__ - Global step 500 Train loss 0.10 Classification-F1 0.26666666666666666 on epoch=249
03/24/2022 00:00:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
03/24/2022 00:00:58 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
03/24/2022 00:01:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
03/24/2022 00:01:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.10 on epoch=269
03/24/2022 00:01:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
03/24/2022 00:01:12 - INFO - __main__ - Global step 550 Train loss 0.07 Classification-F1 0.2805474095796677 on epoch=274
03/24/2022 00:01:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.05 on epoch=279
03/24/2022 00:01:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.06 on epoch=284
03/24/2022 00:01:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.12 on epoch=289
03/24/2022 00:01:30 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
03/24/2022 00:01:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.04 on epoch=299
03/24/2022 00:01:36 - INFO - __main__ - Global step 600 Train loss 0.07 Classification-F1 0.3273273273273273 on epoch=299
03/24/2022 00:01:36 - INFO - __main__ - Saving model with best Classification-F1: 0.3191489361702127 -> 0.3273273273273273 on epoch=299, global_step=600
03/24/2022 00:01:40 - INFO - __main__ - Step 610 Global step 610 Train loss 0.06 on epoch=304
03/24/2022 00:01:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
03/24/2022 00:01:49 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=314
03/24/2022 00:01:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
03/24/2022 00:01:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
03/24/2022 00:01:59 - INFO - __main__ - Global step 650 Train loss 0.05 Classification-F1 0.1619047619047619 on epoch=324
03/24/2022 00:02:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.04 on epoch=329
03/24/2022 00:02:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/24/2022 00:02:12 - INFO - __main__ - Step 680 Global step 680 Train loss 0.05 on epoch=339
03/24/2022 00:02:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.05 on epoch=344
03/24/2022 00:02:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.05 on epoch=349
03/24/2022 00:02:23 - INFO - __main__ - Global step 700 Train loss 0.04 Classification-F1 0.24705882352941178 on epoch=349
03/24/2022 00:02:27 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
03/24/2022 00:02:31 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
03/24/2022 00:02:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
03/24/2022 00:02:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.04 on epoch=369
03/24/2022 00:02:45 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/24/2022 00:02:46 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.23809523809523808 on epoch=374
03/24/2022 00:02:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/24/2022 00:02:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/24/2022 00:02:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
03/24/2022 00:03:04 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
03/24/2022 00:03:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
03/24/2022 00:03:09 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 0.2748768472906404 on epoch=399
03/24/2022 00:03:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/24/2022 00:03:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/24/2022 00:03:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/24/2022 00:03:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
03/24/2022 00:03:31 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/24/2022 00:03:33 - INFO - __main__ - Global step 850 Train loss 0.02 Classification-F1 0.2246376811594203 on epoch=424
03/24/2022 00:03:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
03/24/2022 00:03:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
03/24/2022 00:03:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/24/2022 00:03:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/24/2022 00:03:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/24/2022 00:03:56 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.30158730158730157 on epoch=449
03/24/2022 00:04:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
03/24/2022 00:04:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/24/2022 00:04:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
03/24/2022 00:04:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/24/2022 00:04:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/24/2022 00:04:19 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.2805474095796677 on epoch=474
03/24/2022 00:04:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/24/2022 00:04:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/24/2022 00:04:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/24/2022 00:04:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/24/2022 00:04:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/24/2022 00:04:42 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.3522267206477733 on epoch=499
03/24/2022 00:04:42 - INFO - __main__ - Saving model with best Classification-F1: 0.3273273273273273 -> 0.3522267206477733 on epoch=499, global_step=1000
03/24/2022 00:04:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/24/2022 00:04:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/24/2022 00:04:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.03 on epoch=514
03/24/2022 00:05:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/24/2022 00:05:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/24/2022 00:05:06 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.1549145299145299 on epoch=524
03/24/2022 00:05:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.04 on epoch=529
03/24/2022 00:05:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/24/2022 00:05:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
03/24/2022 00:05:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/24/2022 00:05:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/24/2022 00:05:29 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.22857142857142856 on epoch=549
03/24/2022 00:05:33 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/24/2022 00:05:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/24/2022 00:05:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/24/2022 00:05:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/24/2022 00:05:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/24/2022 00:05:52 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.18571428571428572 on epoch=574
03/24/2022 00:05:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/24/2022 00:06:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
03/24/2022 00:06:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/24/2022 00:06:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/24/2022 00:06:14 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/24/2022 00:06:15 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.1681081081081081 on epoch=599
03/24/2022 00:06:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/24/2022 00:06:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/24/2022 00:06:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/24/2022 00:06:33 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/24/2022 00:06:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/24/2022 00:06:39 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.24122807017543857 on epoch=624
03/24/2022 00:06:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/24/2022 00:06:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
03/24/2022 00:06:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/24/2022 00:06:56 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/24/2022 00:07:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/24/2022 00:07:02 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.3522267206477733 on epoch=649
03/24/2022 00:07:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/24/2022 00:07:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
03/24/2022 00:07:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/24/2022 00:07:19 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/24/2022 00:07:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/24/2022 00:07:25 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.22857142857142856 on epoch=674
03/24/2022 00:07:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/24/2022 00:07:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/24/2022 00:07:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/24/2022 00:07:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/24/2022 00:07:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/24/2022 00:07:48 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.3522267206477733 on epoch=699
03/24/2022 00:07:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/24/2022 00:07:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/24/2022 00:08:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/24/2022 00:08:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/24/2022 00:08:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/24/2022 00:08:12 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.22414414414414416 on epoch=724
03/24/2022 00:08:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/24/2022 00:08:20 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
03/24/2022 00:08:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
03/24/2022 00:08:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/24/2022 00:08:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/24/2022 00:08:35 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.13942857142857143 on epoch=749
03/24/2022 00:08:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/24/2022 00:08:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
03/24/2022 00:08:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/24/2022 00:08:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/24/2022 00:08:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/24/2022 00:08:58 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.24122807017543857 on epoch=774
03/24/2022 00:09:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/24/2022 00:09:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/24/2022 00:09:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
03/24/2022 00:09:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
03/24/2022 00:09:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/24/2022 00:09:22 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.23589743589743586 on epoch=799
03/24/2022 00:09:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/24/2022 00:09:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/24/2022 00:09:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/24/2022 00:09:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/24/2022 00:09:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/24/2022 00:09:45 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.18363844393592677 on epoch=824
03/24/2022 00:09:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/24/2022 00:09:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/24/2022 00:09:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
03/24/2022 00:10:03 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
03/24/2022 00:10:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/24/2022 00:10:09 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.12577777777777777 on epoch=849
03/24/2022 00:10:13 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/24/2022 00:10:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/24/2022 00:10:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/24/2022 00:10:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/24/2022 00:10:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/24/2022 00:10:32 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.12577777777777777 on epoch=874
03/24/2022 00:10:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/24/2022 00:10:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/24/2022 00:10:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/24/2022 00:10:50 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/24/2022 00:10:55 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/24/2022 00:10:56 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.1681081081081081 on epoch=899
03/24/2022 00:11:00 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/24/2022 00:11:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/24/2022 00:11:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/24/2022 00:11:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/24/2022 00:11:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/24/2022 00:11:19 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.17060810810810811 on epoch=924
03/24/2022 00:11:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/24/2022 00:11:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/24/2022 00:11:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/24/2022 00:11:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/24/2022 00:11:42 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/24/2022 00:11:43 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.2134502923976608 on epoch=949
03/24/2022 00:11:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/24/2022 00:11:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/24/2022 00:11:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/24/2022 00:12:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/24/2022 00:12:05 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/24/2022 00:12:07 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.24691358024691357 on epoch=974
03/24/2022 00:12:11 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/24/2022 00:12:15 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/24/2022 00:12:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
03/24/2022 00:12:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/24/2022 00:12:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/24/2022 00:12:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 00:12:30 - INFO - __main__ - Printing 3 examples
03/24/2022 00:12:30 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/24/2022 00:12:30 - INFO - __main__ - ['entailed']
03/24/2022 00:12:30 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/24/2022 00:12:30 - INFO - __main__ - ['entailed']
03/24/2022 00:12:30 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/24/2022 00:12:30 - INFO - __main__ - ['entailed']
03/24/2022 00:12:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/24/2022 00:12:30 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:12:30 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.22857142857142856 on epoch=999
03/24/2022 00:12:30 - INFO - __main__ - save last model!
03/24/2022 00:12:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 00:12:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 00:12:30 - INFO - __main__ - Printing 3 examples
03/24/2022 00:12:30 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/24/2022 00:12:30 - INFO - __main__ - ['entailed']
03/24/2022 00:12:30 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/24/2022 00:12:30 - INFO - __main__ - ['entailed']
03/24/2022 00:12:30 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/24/2022 00:12:30 - INFO - __main__ - ['entailed']
03/24/2022 00:12:30 - INFO - __main__ - Tokenizing Input ...
03/24/2022 00:12:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/24/2022 00:12:30 - INFO - __main__ - Start tokenizing ... 12792 instances
03/24/2022 00:12:30 - INFO - __main__ - Printing 3 examples
03/24/2022 00:12:30 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 00:12:30 - INFO - __main__ - ['entailed']
03/24/2022 00:12:30 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 00:12:30 - INFO - __main__ - ['entailed']
03/24/2022 00:12:30 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 00:12:30 - INFO - __main__ - ['entailed']
03/24/2022 00:12:30 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 00:12:30 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:12:30 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 00:12:47 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 00:12:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 00:12:47 - INFO - __main__ - Starting training!
03/24/2022 00:12:54 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:13:06 - INFO - __main__ - Loaded 12792 examples from test data
03/24/2022 00:21:29 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.3_8_predictions.txt
03/24/2022 00:21:29 - INFO - __main__ - Classification-F1 on test data: 0.0829
03/24/2022 00:21:29 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.3, bsz=8, dev_performance=0.3522267206477733, test_performance=0.08285334482566707
03/24/2022 00:21:29 - INFO - __main__ - Running ... prefix=tab_fact_16_21, lr=0.2, bsz=8 ...
03/24/2022 00:21:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 00:21:30 - INFO - __main__ - Printing 3 examples
03/24/2022 00:21:30 - INFO - __main__ -  [tab_fact] statement: the hellman award and the sydney theater award both nominated glinda from wicked [SEP] table_caption: lucy durack [SEP] table_text: year#award ceremony#role#production#result [n] 2008#green room awards#glinda#wicked#nominated [n] 2009#helpmann awards#glinda#wicked#nominated [n] 2009#sydney theatre awards#glinda#wicked#nominated [n] 2012#sydney theatre awards#elle woods#legally blonde#won [n] 2013#helpmann awards#elle woods#legally blonde#won [n] 
03/24/2022 00:21:30 - INFO - __main__ - ['entailed']
03/24/2022 00:21:30 - INFO - __main__ -  [tab_fact] statement: each of the team play an equal number of game [SEP] table_caption: wru division five south east [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus [n] porth harlequins rfc#20#0#3#642#173#100#19#12 [n] st joseph 's rfc#20#0#3#503#179#69#17#9 [n] pontyclun rfc#20#1#5#468#218#66#24#7 [n] deri rfc#20#0#6#476#285#65#33#7 [n] st albans rfc#20#0#9#402#423#58#61#7 [n] cowbridge rfc#20#0#12#329#379#37#54#3 [n] old penarthians rfc#20#0#11#231#369#29#53#2 [n] penygraig rfc#20#1#13#260#436#30#63#2 [n] ogmore vale rfc#20#0#14#208#475#27#71#2 [n] canton rfc#20#0#16#248#499#34#67#3 [n] dinas powys rfc#20#0#17#161#492#20#73#1 [n] 
03/24/2022 00:21:30 - INFO - __main__ - ['entailed']
03/24/2022 00:21:30 - INFO - __main__ -  [tab_fact] statement: there be a total of 3 driver from the jordan ford entrant [SEP] table_caption: 2003 formula one season [SEP] table_text: entrant#constructor#chassis#engine#tyre#driver#rounds#free practice driver (s) [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#michael schumacher#all#n / a [n] scuderia ferrari marlboro#ferrari#f2002 f2003 - ga#ferrari 051 ferrari 052#b#rubens barrichello#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#juan pablo montoya#all#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#ralf schumacher#1 - 13 , 15 - 16#n / a [n] bmw williamsf1 team#williams - bmw#fw25#bmw p83#m#marc gen#14#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#david coulthard#all#n / a [n] west mclaren mercedes#mclaren - mercedes#mp4 - 17d#mercedes fo110 m mercedes fo110p#m#kimi rikknen#all#n / a [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#jarno trulli#all#allan mcnish franck montagny [n] mild seven renault f1 team#renault#r23 r23b#renault rs23#m#fernando alonso#all#allan mcnish franck montagny [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#nick heidfeld#all#n / a [n] sauber petronas#sauber - petronas#c22#petronas 03a#b#heinz - harald frentzen#all#n / a [n] jordan ford#jordan - ford#ej13#ford rs1#b#giancarlo fisichella#all#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#ralph firman#1 - 12 , 15 - 16#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jordan ford#jordan - ford#ej13#ford rs1#b#zsolt baumgartner#13 - 14#zsolt baumgartner bjrn wirdheim satoshi motoyama [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#mark webber#all#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#antnio pizzonia#1 - 11#n / a [n] jaguar racing#jaguar - cosworth#r4#cosworth cr - 5#m#justin wilson#12 - 16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jacques villeneuve#1 - 15#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#takuma sato#16#n / a [n] lucky strike bar honda#bar - honda#005#honda ra003e#b#jenson button#all#n / a [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#justin wilson#1 - 11#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#nicolas kiesa#12 - 16#matteo bobbi gianmaria bruni [n] european minardi cosworth#minardi - cosworth#ps03#cosworth cr - 3#b#jos verstappen#all#matteo bobbi gianmaria bruni [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#olivier panis#all#n / a [n] panasonic toyota racing#toyota#tf103#toyota rvx - 03#m#cristiano da matta#all#n / a [n] 
03/24/2022 00:21:30 - INFO - __main__ - ['entailed']
03/24/2022 00:21:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 00:21:30 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:21:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 00:21:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 00:21:30 - INFO - __main__ - Printing 3 examples
03/24/2022 00:21:30 - INFO - __main__ -  [tab_fact] statement: the total receipt for hillary clinton , barack obama , and john edward , be over 200000000 [SEP] table_caption: fundraising for the 2008 united states presidential election [SEP] table_text: candidate#money raised , 3q#loans received , 3q#money spent , 3q#total receipts#cash on hand#after debt [n] hillary clinton#27859861#-#22623680#90935788#50463013#48115527 [n] barack obama#21343291#-#21519789#80256426#36087190#34677451 [n] john edwards#7157232#-#8271937#30329151#12397048#12397048 [n] bill richardson#5358585#-#6666681#18699936#5821587#5746365 [n] christopher dodd#1522061#-#4025458#13598152#3874874#3874874 [n] joe biden#1757394#-#2635896#8215739#1886340#1758130 [n] dennis kucinich#1011696#-#888773#2130200#327094#327094 [n] mike gravel#130598#-#144225#379794#17527#- 68326 [n] 
03/24/2022 00:21:30 - INFO - __main__ - ['entailed']
03/24/2022 00:21:30 - INFO - __main__ -  [tab_fact] statement: of mike phillips , dean sears , donnie speer , and bill duffy bill duffy be the player pick first [SEP] table_caption: 1982 - 83 denver nuggets season [SEP] table_text: round#pick#player#nationality#school / club team [n] 1#19#rob williams#united states#houston [n] 3#62#roylin bond#united states#pepperdine [n] 4#84#alford turner#united states#southwest louisiana [n] 5#109#bill duffy#united states#santa clara [n] 6#131#chris brust#united states#north carolina [n] 7#153#jeb barlow#united states#north carolina [n] 8#178#donnie speer#united states#alabama - birmingham [n] 9#200#dean sears#united states#ucla [n] 10#220#mike phillips#united states#niagara [n] 
03/24/2022 00:21:30 - INFO - __main__ - ['entailed']
03/24/2022 00:21:30 - INFO - __main__ -  [tab_fact] statement: the outcome be winner with irving wright as a partner [SEP] table_caption: molla mallory [SEP] table_text: outcome#year#championship#surface#partner#opponents#score [n] runner - up#1915#us championships#grass#irving wright#harry johnson hazel hotchkiss wightman#0 - 6 , 1 - 6 [n] winner#1917#us championships#grass#irving wright#bill tilden florence ballin#10 - 12 , 6 - 1 , 6 - 3 [n] runner - up#1918#us championships#grass#fred alexander#irving wright hazel hotchkiss wightman#2 - 6 , 3 - 6 [n] runner - up#1920#us championships#grass#craig biddle#wallace johnson hazel hotchkiss wightman#4 - 6 , 3 - 6 [n] runner - up#1921#us championships#grass#bill tilden#bill johnston mary browne#6 - 3 , 4 - 6 , 3 - 6 [n] winner#1922#us championships (2)#grass#bill tilden#howard kinsey helen wills moody#6 - 4 , 6 - 3 [n] winner#1923#us championships (3)#grass#bill tilden#john hawkes kitty mckane godfree#6 - 3 , 2 - 6 , 10 - 8 [n] 
03/24/2022 00:21:30 - INFO - __main__ - ['entailed']
03/24/2022 00:21:30 - INFO - __main__ - Tokenizing Input ...
03/24/2022 00:21:30 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:21:30 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 00:21:49 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 00:21:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 00:21:50 - INFO - __main__ - Starting training!
03/24/2022 00:21:55 - INFO - __main__ - Step 10 Global step 10 Train loss 3.77 on epoch=4
03/24/2022 00:22:00 - INFO - __main__ - Step 20 Global step 20 Train loss 2.39 on epoch=9
03/24/2022 00:22:04 - INFO - __main__ - Step 30 Global step 30 Train loss 1.51 on epoch=14
03/24/2022 00:22:08 - INFO - __main__ - Step 40 Global step 40 Train loss 0.85 on epoch=19
03/24/2022 00:22:13 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=24
03/24/2022 00:22:14 - INFO - __main__ - Global step 50 Train loss 1.80 Classification-F1 0.3333333333333333 on epoch=24
03/24/2022 00:22:14 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/24/2022 00:22:19 - INFO - __main__ - Step 60 Global step 60 Train loss 0.36 on epoch=29
03/24/2022 00:22:23 - INFO - __main__ - Step 70 Global step 70 Train loss 0.38 on epoch=34
03/24/2022 00:22:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
03/24/2022 00:22:32 - INFO - __main__ - Step 90 Global step 90 Train loss 0.35 on epoch=44
03/24/2022 00:22:36 - INFO - __main__ - Step 100 Global step 100 Train loss 0.31 on epoch=49
03/24/2022 00:22:38 - INFO - __main__ - Global step 100 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=49
03/24/2022 00:22:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.30 on epoch=54
03/24/2022 00:22:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.31 on epoch=59
03/24/2022 00:22:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=64
03/24/2022 00:22:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=69
03/24/2022 00:23:00 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
03/24/2022 00:23:01 - INFO - __main__ - Global step 150 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=74
03/24/2022 00:23:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
03/24/2022 00:23:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=84
03/24/2022 00:23:14 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/24/2022 00:23:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
03/24/2022 00:23:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/24/2022 00:23:25 - INFO - __main__ - Global step 200 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=99
03/24/2022 00:23:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=104
03/24/2022 00:23:34 - INFO - __main__ - Step 220 Global step 220 Train loss 0.20 on epoch=109
03/24/2022 00:23:38 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=114
03/24/2022 00:23:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
03/24/2022 00:23:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=124
03/24/2022 00:23:48 - INFO - __main__ - Global step 250 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=124
03/24/2022 00:23:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=129
03/24/2022 00:23:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=134
03/24/2022 00:24:02 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/24/2022 00:24:06 - INFO - __main__ - Step 290 Global step 290 Train loss 0.22 on epoch=144
03/24/2022 00:24:10 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=149
03/24/2022 00:24:12 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=149
03/24/2022 00:24:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=154
03/24/2022 00:24:21 - INFO - __main__ - Step 320 Global step 320 Train loss 0.17 on epoch=159
03/24/2022 00:24:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=164
03/24/2022 00:24:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
03/24/2022 00:24:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=174
03/24/2022 00:24:36 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=174
03/24/2022 00:24:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/24/2022 00:24:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.17 on epoch=184
03/24/2022 00:24:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=189
03/24/2022 00:24:53 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
03/24/2022 00:24:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.19 on epoch=199
03/24/2022 00:24:59 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.3266888150609081 on epoch=199
03/24/2022 00:25:03 - INFO - __main__ - Step 410 Global step 410 Train loss 0.17 on epoch=204
03/24/2022 00:25:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
03/24/2022 00:25:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
03/24/2022 00:25:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=219
03/24/2022 00:25:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.18 on epoch=224
03/24/2022 00:25:23 - INFO - __main__ - Global step 450 Train loss 0.19 Classification-F1 0.37662337662337664 on epoch=224
03/24/2022 00:25:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.37662337662337664 on epoch=224, global_step=450
03/24/2022 00:25:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.19 on epoch=229
03/24/2022 00:25:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=234
03/24/2022 00:25:36 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=239
03/24/2022 00:25:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.13 on epoch=244
03/24/2022 00:25:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=249
03/24/2022 00:25:46 - INFO - __main__ - Global step 500 Train loss 0.19 Classification-F1 0.2873806998939555 on epoch=249
03/24/2022 00:25:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=254
03/24/2022 00:25:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
03/24/2022 00:25:59 - INFO - __main__ - Step 530 Global step 530 Train loss 0.17 on epoch=264
03/24/2022 00:26:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=269
03/24/2022 00:26:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.13 on epoch=274
03/24/2022 00:26:10 - INFO - __main__ - Global step 550 Train loss 0.17 Classification-F1 0.3650793650793651 on epoch=274
03/24/2022 00:26:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=279
03/24/2022 00:26:19 - INFO - __main__ - Step 570 Global step 570 Train loss 0.13 on epoch=284
03/24/2022 00:26:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=289
03/24/2022 00:26:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.13 on epoch=294
03/24/2022 00:26:32 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=299
03/24/2022 00:26:33 - INFO - __main__ - Global step 600 Train loss 0.14 Classification-F1 0.3764102564102564 on epoch=299
03/24/2022 00:26:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
03/24/2022 00:26:42 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=309
03/24/2022 00:26:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.12 on epoch=314
03/24/2022 00:26:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=319
03/24/2022 00:26:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
03/24/2022 00:26:57 - INFO - __main__ - Global step 650 Train loss 0.13 Classification-F1 0.37254901960784315 on epoch=324
03/24/2022 00:27:02 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=329
03/24/2022 00:27:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
03/24/2022 00:27:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=339
03/24/2022 00:27:15 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=344
03/24/2022 00:27:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
03/24/2022 00:27:21 - INFO - __main__ - Global step 700 Train loss 0.11 Classification-F1 0.22920892494929004 on epoch=349
03/24/2022 00:27:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.08 on epoch=354
03/24/2022 00:27:30 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
03/24/2022 00:27:34 - INFO - __main__ - Step 730 Global step 730 Train loss 0.09 on epoch=364
03/24/2022 00:27:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=369
03/24/2022 00:27:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=374
03/24/2022 00:27:44 - INFO - __main__ - Global step 750 Train loss 0.09 Classification-F1 0.1125 on epoch=374
03/24/2022 00:27:49 - INFO - __main__ - Step 760 Global step 760 Train loss 0.07 on epoch=379
03/24/2022 00:27:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=384
03/24/2022 00:27:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
03/24/2022 00:28:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.08 on epoch=394
03/24/2022 00:28:07 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=399
03/24/2022 00:28:08 - INFO - __main__ - Global step 800 Train loss 0.08 Classification-F1 0.2748768472906404 on epoch=399
03/24/2022 00:28:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.07 on epoch=404
03/24/2022 00:28:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
03/24/2022 00:28:22 - INFO - __main__ - Step 830 Global step 830 Train loss 0.05 on epoch=414
03/24/2022 00:28:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.08 on epoch=419
03/24/2022 00:28:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
03/24/2022 00:28:32 - INFO - __main__ - Global step 850 Train loss 0.07 Classification-F1 0.2748768472906404 on epoch=424
03/24/2022 00:28:36 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
03/24/2022 00:28:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
03/24/2022 00:28:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
03/24/2022 00:28:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
03/24/2022 00:28:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=449
03/24/2022 00:28:55 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.2748768472906404 on epoch=449
03/24/2022 00:29:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
03/24/2022 00:29:04 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
03/24/2022 00:29:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=464
03/24/2022 00:29:13 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
03/24/2022 00:29:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
03/24/2022 00:29:19 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.2748768472906404 on epoch=474
03/24/2022 00:29:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=479
03/24/2022 00:29:28 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
03/24/2022 00:29:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.03 on epoch=489
03/24/2022 00:29:37 - INFO - __main__ - Step 990 Global step 990 Train loss 0.05 on epoch=494
03/24/2022 00:29:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
03/24/2022 00:29:42 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.2748768472906404 on epoch=499
03/24/2022 00:29:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
03/24/2022 00:29:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
03/24/2022 00:29:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
03/24/2022 00:30:00 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
03/24/2022 00:30:05 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/24/2022 00:30:06 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.40566959921798634 on epoch=524
03/24/2022 00:30:06 - INFO - __main__ - Saving model with best Classification-F1: 0.37662337662337664 -> 0.40566959921798634 on epoch=524, global_step=1050
03/24/2022 00:30:10 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/24/2022 00:30:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.03 on epoch=534
03/24/2022 00:30:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/24/2022 00:30:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
03/24/2022 00:30:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.02 on epoch=549
03/24/2022 00:30:30 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.464039408866995 on epoch=549
03/24/2022 00:30:30 - INFO - __main__ - Saving model with best Classification-F1: 0.40566959921798634 -> 0.464039408866995 on epoch=549, global_step=1100
03/24/2022 00:30:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.02 on epoch=554
03/24/2022 00:30:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=559
03/24/2022 00:30:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/24/2022 00:30:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
03/24/2022 00:30:52 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
03/24/2022 00:30:53 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.375 on epoch=574
03/24/2022 00:30:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
03/24/2022 00:31:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
03/24/2022 00:31:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
03/24/2022 00:31:11 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
03/24/2022 00:31:15 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
03/24/2022 00:31:16 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.33793103448275863 on epoch=599
03/24/2022 00:31:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
03/24/2022 00:31:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
03/24/2022 00:31:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
03/24/2022 00:31:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/24/2022 00:31:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/24/2022 00:31:39 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.33793103448275863 on epoch=624
03/24/2022 00:31:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/24/2022 00:31:48 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/24/2022 00:31:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
03/24/2022 00:31:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/24/2022 00:32:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/24/2022 00:32:03 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.30158730158730157 on epoch=649
03/24/2022 00:32:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/24/2022 00:32:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/24/2022 00:32:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=664
03/24/2022 00:32:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/24/2022 00:32:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/24/2022 00:32:26 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.30158730158730157 on epoch=674
03/24/2022 00:32:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/24/2022 00:32:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/24/2022 00:32:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
03/24/2022 00:32:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/24/2022 00:32:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/24/2022 00:32:49 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.3273273273273273 on epoch=699
03/24/2022 00:32:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/24/2022 00:32:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/24/2022 00:33:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/24/2022 00:33:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
03/24/2022 00:33:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/24/2022 00:33:12 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=724
03/24/2022 00:33:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
03/24/2022 00:33:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.03 on epoch=734
03/24/2022 00:33:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/24/2022 00:33:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/24/2022 00:33:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
03/24/2022 00:33:35 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.39999999999999997 on epoch=749
03/24/2022 00:33:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/24/2022 00:33:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/24/2022 00:33:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/24/2022 00:33:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/24/2022 00:33:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/24/2022 00:33:58 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=774
03/24/2022 00:34:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/24/2022 00:34:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
03/24/2022 00:34:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
03/24/2022 00:34:16 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/24/2022 00:34:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/24/2022 00:34:21 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.3522267206477733 on epoch=799
03/24/2022 00:34:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/24/2022 00:34:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/24/2022 00:34:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
03/24/2022 00:34:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
03/24/2022 00:34:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/24/2022 00:34:44 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.3764102564102564 on epoch=824
03/24/2022 00:34:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/24/2022 00:34:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
03/24/2022 00:34:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
03/24/2022 00:35:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/24/2022 00:35:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/24/2022 00:35:08 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.30158730158730157 on epoch=849
03/24/2022 00:35:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/24/2022 00:35:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/24/2022 00:35:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/24/2022 00:35:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/24/2022 00:35:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/24/2022 00:35:31 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.2540322580645162 on epoch=874
03/24/2022 00:35:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/24/2022 00:35:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/24/2022 00:35:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
03/24/2022 00:35:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/24/2022 00:35:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/24/2022 00:35:54 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.33793103448275863 on epoch=899
03/24/2022 00:35:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/24/2022 00:36:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/24/2022 00:36:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/24/2022 00:36:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/24/2022 00:36:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/24/2022 00:36:17 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.3764102564102564 on epoch=924
03/24/2022 00:36:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/24/2022 00:36:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/24/2022 00:36:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
03/24/2022 00:36:34 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/24/2022 00:36:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/24/2022 00:36:40 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.4231177094379639 on epoch=949
03/24/2022 00:36:44 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/24/2022 00:36:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/24/2022 00:36:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
03/24/2022 00:36:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/24/2022 00:37:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/24/2022 00:37:03 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=974
03/24/2022 00:37:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/24/2022 00:37:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/24/2022 00:37:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/24/2022 00:37:20 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/24/2022 00:37:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/24/2022 00:37:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 00:37:26 - INFO - __main__ - Printing 3 examples
03/24/2022 00:37:26 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/24/2022 00:37:26 - INFO - __main__ - ['refuted']
03/24/2022 00:37:26 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/24/2022 00:37:26 - INFO - __main__ - ['refuted']
03/24/2022 00:37:26 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/24/2022 00:37:26 - INFO - __main__ - ['refuted']
03/24/2022 00:37:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/24/2022 00:37:26 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:37:26 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.39999999999999997 on epoch=999
03/24/2022 00:37:26 - INFO - __main__ - save last model!
03/24/2022 00:37:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 00:37:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 00:37:26 - INFO - __main__ - Printing 3 examples
03/24/2022 00:37:26 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/24/2022 00:37:26 - INFO - __main__ - ['refuted']
03/24/2022 00:37:26 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/24/2022 00:37:26 - INFO - __main__ - ['refuted']
03/24/2022 00:37:26 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/24/2022 00:37:26 - INFO - __main__ - ['refuted']
03/24/2022 00:37:26 - INFO - __main__ - Tokenizing Input ...
03/24/2022 00:37:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/24/2022 00:37:26 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:37:26 - INFO - __main__ - Start tokenizing ... 12792 instances
03/24/2022 00:37:26 - INFO - __main__ - Printing 3 examples
03/24/2022 00:37:26 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 00:37:26 - INFO - __main__ - ['entailed']
03/24/2022 00:37:26 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 00:37:26 - INFO - __main__ - ['entailed']
03/24/2022 00:37:26 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 00:37:26 - INFO - __main__ - ['entailed']
03/24/2022 00:37:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 00:37:26 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 00:37:45 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 00:37:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 00:37:46 - INFO - __main__ - Starting training!
03/24/2022 00:37:50 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:38:02 - INFO - __main__ - Loaded 12792 examples from test data
03/24/2022 00:46:20 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_21_0.2_8_predictions.txt
03/24/2022 00:46:20 - INFO - __main__ - Classification-F1 on test data: 0.1228
03/24/2022 00:46:20 - INFO - __main__ - prefix=tab_fact_16_21, lr=0.2, bsz=8, dev_performance=0.464039408866995, test_performance=0.12277362808761633
03/24/2022 00:46:20 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.5, bsz=8 ...
03/24/2022 00:46:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 00:46:21 - INFO - __main__ - Printing 3 examples
03/24/2022 00:46:21 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/24/2022 00:46:21 - INFO - __main__ - ['refuted']
03/24/2022 00:46:21 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/24/2022 00:46:21 - INFO - __main__ - ['refuted']
03/24/2022 00:46:21 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/24/2022 00:46:21 - INFO - __main__ - ['refuted']
03/24/2022 00:46:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 00:46:21 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:46:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 00:46:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 00:46:21 - INFO - __main__ - Printing 3 examples
03/24/2022 00:46:21 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/24/2022 00:46:21 - INFO - __main__ - ['refuted']
03/24/2022 00:46:21 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/24/2022 00:46:21 - INFO - __main__ - ['refuted']
03/24/2022 00:46:21 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/24/2022 00:46:21 - INFO - __main__ - ['refuted']
03/24/2022 00:46:21 - INFO - __main__ - Tokenizing Input ...
03/24/2022 00:46:21 - INFO - __main__ - Tokenizing Output ...
03/24/2022 00:46:21 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 00:46:40 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 00:46:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 00:46:41 - INFO - __main__ - Starting training!
03/24/2022 00:46:46 - INFO - __main__ - Step 10 Global step 10 Train loss 2.93 on epoch=4
03/24/2022 00:46:50 - INFO - __main__ - Step 20 Global step 20 Train loss 0.85 on epoch=9
03/24/2022 00:46:55 - INFO - __main__ - Step 30 Global step 30 Train loss 0.38 on epoch=14
03/24/2022 00:46:59 - INFO - __main__ - Step 40 Global step 40 Train loss 0.32 on epoch=19
03/24/2022 00:47:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.30 on epoch=24
03/24/2022 00:47:05 - INFO - __main__ - Global step 50 Train loss 0.96 Classification-F1 0.3333333333333333 on epoch=24
03/24/2022 00:47:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/24/2022 00:47:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=29
03/24/2022 00:47:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.23 on epoch=34
03/24/2022 00:47:18 - INFO - __main__ - Step 80 Global step 80 Train loss 0.23 on epoch=39
03/24/2022 00:47:23 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=44
03/24/2022 00:47:27 - INFO - __main__ - Step 100 Global step 100 Train loss 0.21 on epoch=49
03/24/2022 00:47:29 - INFO - __main__ - Global step 100 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=49
03/24/2022 00:47:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=54
03/24/2022 00:47:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.21 on epoch=59
03/24/2022 00:47:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.22 on epoch=64
03/24/2022 00:47:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.22 on epoch=69
03/24/2022 00:47:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.21 on epoch=74
03/24/2022 00:47:53 - INFO - __main__ - Global step 150 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=74
03/24/2022 00:47:57 - INFO - __main__ - Step 160 Global step 160 Train loss 0.23 on epoch=79
03/24/2022 00:48:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.20 on epoch=84
03/24/2022 00:48:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.19 on epoch=89
03/24/2022 00:48:10 - INFO - __main__ - Step 190 Global step 190 Train loss 0.22 on epoch=94
03/24/2022 00:48:15 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
03/24/2022 00:48:17 - INFO - __main__ - Global step 200 Train loss 0.21 Classification-F1 0.36374269005847953 on epoch=99
03/24/2022 00:48:17 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.36374269005847953 on epoch=99, global_step=200
03/24/2022 00:48:21 - INFO - __main__ - Step 210 Global step 210 Train loss 0.16 on epoch=104
03/24/2022 00:48:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.19 on epoch=109
03/24/2022 00:48:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.12 on epoch=114
03/24/2022 00:48:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.14 on epoch=119
03/24/2022 00:48:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.16 on epoch=124
03/24/2022 00:48:40 - INFO - __main__ - Global step 250 Train loss 0.16 Classification-F1 0.4231177094379639 on epoch=124
03/24/2022 00:48:40 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.4231177094379639 on epoch=124, global_step=250
03/24/2022 00:48:44 - INFO - __main__ - Step 260 Global step 260 Train loss 0.11 on epoch=129
03/24/2022 00:48:49 - INFO - __main__ - Step 270 Global step 270 Train loss 0.14 on epoch=134
03/24/2022 00:48:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.12 on epoch=139
03/24/2022 00:48:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.10 on epoch=144
03/24/2022 00:49:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.18 on epoch=149
03/24/2022 00:49:03 - INFO - __main__ - Global step 300 Train loss 0.13 Classification-F1 0.20634920634920637 on epoch=149
03/24/2022 00:49:08 - INFO - __main__ - Step 310 Global step 310 Train loss 0.11 on epoch=154
03/24/2022 00:49:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.07 on epoch=159
03/24/2022 00:49:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.11 on epoch=164
03/24/2022 00:49:21 - INFO - __main__ - Step 340 Global step 340 Train loss 0.03 on epoch=169
03/24/2022 00:49:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.06 on epoch=174
03/24/2022 00:49:27 - INFO - __main__ - Global step 350 Train loss 0.08 Classification-F1 0.40566959921798634 on epoch=174
03/24/2022 00:49:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.07 on epoch=179
03/24/2022 00:49:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.05 on epoch=184
03/24/2022 00:49:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.03 on epoch=189
03/24/2022 00:49:44 - INFO - __main__ - Step 390 Global step 390 Train loss 0.03 on epoch=194
03/24/2022 00:49:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
03/24/2022 00:49:50 - INFO - __main__ - Global step 400 Train loss 0.04 Classification-F1 0.12280701754385964 on epoch=199
03/24/2022 00:49:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
03/24/2022 00:49:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.04 on epoch=209
03/24/2022 00:50:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.01 on epoch=214
03/24/2022 00:50:08 - INFO - __main__ - Step 440 Global step 440 Train loss 0.02 on epoch=219
03/24/2022 00:50:12 - INFO - __main__ - Step 450 Global step 450 Train loss 0.03 on epoch=224
03/24/2022 00:50:13 - INFO - __main__ - Global step 450 Train loss 0.02 Classification-F1 0.2991452991452992 on epoch=224
03/24/2022 00:50:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.02 on epoch=229
03/24/2022 00:50:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.03 on epoch=234
03/24/2022 00:50:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.02 on epoch=239
03/24/2022 00:50:31 - INFO - __main__ - Step 490 Global step 490 Train loss 0.01 on epoch=244
03/24/2022 00:50:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/24/2022 00:50:37 - INFO - __main__ - Global step 500 Train loss 0.02 Classification-F1 0.16189931350114417 on epoch=249
03/24/2022 00:50:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.01 on epoch=254
03/24/2022 00:50:46 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/24/2022 00:50:50 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/24/2022 00:50:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
03/24/2022 00:50:59 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/24/2022 00:51:00 - INFO - __main__ - Global step 550 Train loss 0.01 Classification-F1 0.10694444444444444 on epoch=274
03/24/2022 00:51:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
03/24/2022 00:51:09 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
03/24/2022 00:51:13 - INFO - __main__ - Step 580 Global step 580 Train loss 0.02 on epoch=289
03/24/2022 00:51:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/24/2022 00:51:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/24/2022 00:51:23 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.27474747474747474 on epoch=299
03/24/2022 00:51:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.00 on epoch=304
03/24/2022 00:51:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/24/2022 00:51:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
03/24/2022 00:51:41 - INFO - __main__ - Step 640 Global step 640 Train loss 0.02 on epoch=319
03/24/2022 00:51:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/24/2022 00:51:47 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.21122807017543857 on epoch=324
03/24/2022 00:51:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/24/2022 00:51:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
03/24/2022 00:52:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/24/2022 00:52:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/24/2022 00:52:09 - INFO - __main__ - Step 700 Global step 700 Train loss 0.01 on epoch=349
03/24/2022 00:52:10 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.2152391546162403 on epoch=349
03/24/2022 00:52:14 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/24/2022 00:52:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.02 on epoch=359
03/24/2022 00:52:23 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/24/2022 00:52:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/24/2022 00:52:32 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/24/2022 00:52:34 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.16129032258064516 on epoch=374
03/24/2022 00:52:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/24/2022 00:52:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/24/2022 00:52:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/24/2022 00:52:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/24/2022 00:52:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/24/2022 00:52:57 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.141991341991342 on epoch=399
03/24/2022 00:53:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/24/2022 00:53:06 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/24/2022 00:53:10 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/24/2022 00:53:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/24/2022 00:53:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/24/2022 00:53:20 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.2963709677419355 on epoch=424
03/24/2022 00:53:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
03/24/2022 00:53:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/24/2022 00:53:33 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/24/2022 00:53:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/24/2022 00:53:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/24/2022 00:53:43 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.13758241758241757 on epoch=449
03/24/2022 00:53:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/24/2022 00:53:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
03/24/2022 00:53:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/24/2022 00:54:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/24/2022 00:54:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/24/2022 00:54:06 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.141991341991342 on epoch=474
03/24/2022 00:54:11 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/24/2022 00:54:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/24/2022 00:54:20 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/24/2022 00:54:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/24/2022 00:54:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/24/2022 00:54:30 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.19558189655172414 on epoch=499
03/24/2022 00:54:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/24/2022 00:54:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/24/2022 00:54:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/24/2022 00:54:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/24/2022 00:54:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/24/2022 00:54:53 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.40566959921798634 on epoch=524
03/24/2022 00:54:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/24/2022 00:55:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/24/2022 00:55:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/24/2022 00:55:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/24/2022 00:55:15 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/24/2022 00:55:16 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=549
03/24/2022 00:55:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4231177094379639 -> 0.4285714285714286 on epoch=549, global_step=1100
03/24/2022 00:55:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/24/2022 00:55:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/24/2022 00:55:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/24/2022 00:55:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/24/2022 00:55:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/24/2022 00:55:40 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.1869772998805257 on epoch=574
03/24/2022 00:55:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/24/2022 00:55:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/24/2022 00:55:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/24/2022 00:55:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/24/2022 00:56:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/24/2022 00:56:03 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.3142857142857143 on epoch=599
03/24/2022 00:56:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/24/2022 00:56:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/24/2022 00:56:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/24/2022 00:56:21 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/24/2022 00:56:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/24/2022 00:56:26 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.2904761904761905 on epoch=624
03/24/2022 00:56:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/24/2022 00:56:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/24/2022 00:56:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/24/2022 00:56:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/24/2022 00:56:49 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
03/24/2022 00:56:50 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.16433189655172414 on epoch=649
03/24/2022 00:56:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/24/2022 00:56:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/24/2022 00:57:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/24/2022 00:57:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/24/2022 00:57:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/24/2022 00:57:13 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=674
03/24/2022 00:57:13 - INFO - __main__ - Saving model with best Classification-F1: 0.4285714285714286 -> 0.4554554554554554 on epoch=674, global_step=1350
03/24/2022 00:57:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/24/2022 00:57:22 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/24/2022 00:57:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/24/2022 00:57:31 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/24/2022 00:57:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/24/2022 00:57:37 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.15454545454545454 on epoch=699
03/24/2022 00:57:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/24/2022 00:57:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/24/2022 00:57:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/24/2022 00:57:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
03/24/2022 00:57:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
03/24/2022 00:58:00 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.27474747474747474 on epoch=724
03/24/2022 00:58:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/24/2022 00:58:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/24/2022 00:58:14 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/24/2022 00:58:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/24/2022 00:58:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/24/2022 00:58:24 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.20950888192267503 on epoch=749
03/24/2022 00:58:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/24/2022 00:58:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/24/2022 00:58:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/24/2022 00:58:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/24/2022 00:58:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/24/2022 00:58:47 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.17711598746081503 on epoch=774
03/24/2022 00:58:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/24/2022 00:58:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/24/2022 00:59:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/24/2022 00:59:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/24/2022 00:59:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/24/2022 00:59:11 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.18010752688172044 on epoch=799
03/24/2022 00:59:16 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/24/2022 00:59:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/24/2022 00:59:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/24/2022 00:59:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/24/2022 00:59:34 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/24/2022 00:59:35 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.1486013986013986 on epoch=824
03/24/2022 00:59:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/24/2022 00:59:44 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/24/2022 00:59:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/24/2022 00:59:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/24/2022 00:59:57 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/24/2022 00:59:58 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.30476190476190473 on epoch=849
03/24/2022 01:00:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/24/2022 01:00:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/24/2022 01:00:12 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/24/2022 01:00:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/24/2022 01:00:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/24/2022 01:00:22 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.21764705882352942 on epoch=874
03/24/2022 01:00:27 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/24/2022 01:00:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/24/2022 01:00:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/24/2022 01:00:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/24/2022 01:00:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/24/2022 01:00:46 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.29479377958079783 on epoch=899
03/24/2022 01:00:50 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/24/2022 01:00:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/24/2022 01:00:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/24/2022 01:01:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/24/2022 01:01:08 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/24/2022 01:01:09 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.25635667014977365 on epoch=924
03/24/2022 01:01:14 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/24/2022 01:01:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/24/2022 01:01:23 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/24/2022 01:01:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/24/2022 01:01:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/24/2022 01:01:33 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.20967741935483872 on epoch=949
03/24/2022 01:01:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/24/2022 01:01:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/24/2022 01:01:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/24/2022 01:01:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/24/2022 01:01:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/24/2022 01:01:57 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.19534632034632035 on epoch=974
03/24/2022 01:02:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/24/2022 01:02:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/24/2022 01:02:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/24/2022 01:02:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/24/2022 01:02:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/24/2022 01:02:20 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.12404040404040405 on epoch=999
03/24/2022 01:02:20 - INFO - __main__ - save last model!
03/24/2022 01:02:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/24/2022 01:02:21 - INFO - __main__ - Start tokenizing ... 12792 instances
03/24/2022 01:02:21 - INFO - __main__ - Printing 3 examples
03/24/2022 01:02:21 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 01:02:21 - INFO - __main__ - ['entailed']
03/24/2022 01:02:21 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 01:02:21 - INFO - __main__ - ['entailed']
03/24/2022 01:02:21 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 01:02:21 - INFO - __main__ - ['entailed']
03/24/2022 01:02:21 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 01:02:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:02:22 - INFO - __main__ - Printing 3 examples
03/24/2022 01:02:22 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/24/2022 01:02:22 - INFO - __main__ - ['refuted']
03/24/2022 01:02:22 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/24/2022 01:02:22 - INFO - __main__ - ['refuted']
03/24/2022 01:02:22 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/24/2022 01:02:22 - INFO - __main__ - ['refuted']
03/24/2022 01:02:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/24/2022 01:02:22 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:02:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 01:02:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:02:22 - INFO - __main__ - Printing 3 examples
03/24/2022 01:02:22 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/24/2022 01:02:22 - INFO - __main__ - ['refuted']
03/24/2022 01:02:22 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/24/2022 01:02:22 - INFO - __main__ - ['refuted']
03/24/2022 01:02:22 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/24/2022 01:02:22 - INFO - __main__ - ['refuted']
03/24/2022 01:02:22 - INFO - __main__ - Tokenizing Input ...
03/24/2022 01:02:22 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:02:22 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 01:02:41 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 01:02:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 01:02:42 - INFO - __main__ - Starting training!
03/24/2022 01:02:44 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:02:57 - INFO - __main__ - Loaded 12792 examples from test data
03/24/2022 01:11:03 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.5_8_predictions.txt
03/24/2022 01:11:04 - INFO - __main__ - Classification-F1 on test data: 0.0596
03/24/2022 01:11:04 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.5, bsz=8, dev_performance=0.4554554554554554, test_performance=0.059571503378592
03/24/2022 01:11:04 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.4, bsz=8 ...
03/24/2022 01:11:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:11:05 - INFO - __main__ - Printing 3 examples
03/24/2022 01:11:05 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/24/2022 01:11:05 - INFO - __main__ - ['refuted']
03/24/2022 01:11:05 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/24/2022 01:11:05 - INFO - __main__ - ['refuted']
03/24/2022 01:11:05 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/24/2022 01:11:05 - INFO - __main__ - ['refuted']
03/24/2022 01:11:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 01:11:05 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:11:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 01:11:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:11:05 - INFO - __main__ - Printing 3 examples
03/24/2022 01:11:05 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/24/2022 01:11:05 - INFO - __main__ - ['refuted']
03/24/2022 01:11:05 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/24/2022 01:11:05 - INFO - __main__ - ['refuted']
03/24/2022 01:11:05 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/24/2022 01:11:05 - INFO - __main__ - ['refuted']
03/24/2022 01:11:05 - INFO - __main__ - Tokenizing Input ...
03/24/2022 01:11:05 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:11:05 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 01:11:23 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 01:11:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 01:11:24 - INFO - __main__ - Starting training!
03/24/2022 01:11:29 - INFO - __main__ - Step 10 Global step 10 Train loss 3.63 on epoch=4
03/24/2022 01:11:34 - INFO - __main__ - Step 20 Global step 20 Train loss 1.21 on epoch=9
03/24/2022 01:11:38 - INFO - __main__ - Step 30 Global step 30 Train loss 0.43 on epoch=14
03/24/2022 01:11:42 - INFO - __main__ - Step 40 Global step 40 Train loss 0.37 on epoch=19
03/24/2022 01:11:47 - INFO - __main__ - Step 50 Global step 50 Train loss 0.28 on epoch=24
03/24/2022 01:11:48 - INFO - __main__ - Global step 50 Train loss 1.19 Classification-F1 0.3333333333333333 on epoch=24
03/24/2022 01:11:48 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/24/2022 01:11:53 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=29
03/24/2022 01:11:57 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=34
03/24/2022 01:12:02 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
03/24/2022 01:12:06 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=44
03/24/2022 01:12:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.24 on epoch=49
03/24/2022 01:12:12 - INFO - __main__ - Global step 100 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=49
03/24/2022 01:12:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.24 on epoch=54
03/24/2022 01:12:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.22 on epoch=59
03/24/2022 01:12:25 - INFO - __main__ - Step 130 Global step 130 Train loss 0.21 on epoch=64
03/24/2022 01:12:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
03/24/2022 01:12:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=74
03/24/2022 01:12:35 - INFO - __main__ - Global step 150 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=74
03/24/2022 01:12:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.22 on epoch=79
03/24/2022 01:12:44 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=84
03/24/2022 01:12:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=89
03/24/2022 01:12:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
03/24/2022 01:12:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.21 on epoch=99
03/24/2022 01:12:59 - INFO - __main__ - Global step 200 Train loss 0.23 Classification-F1 0.4009852216748768 on epoch=99
03/24/2022 01:12:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4009852216748768 on epoch=99, global_step=200
03/24/2022 01:13:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=104
03/24/2022 01:13:08 - INFO - __main__ - Step 220 Global step 220 Train loss 0.21 on epoch=109
03/24/2022 01:13:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.18 on epoch=114
03/24/2022 01:13:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.18 on epoch=119
03/24/2022 01:13:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
03/24/2022 01:13:23 - INFO - __main__ - Global step 250 Train loss 0.20 Classification-F1 0.4375 on epoch=124
03/24/2022 01:13:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4009852216748768 -> 0.4375 on epoch=124, global_step=250
03/24/2022 01:13:27 - INFO - __main__ - Step 260 Global step 260 Train loss 0.16 on epoch=129
03/24/2022 01:13:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.17 on epoch=134
03/24/2022 01:13:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.15 on epoch=139
03/24/2022 01:13:40 - INFO - __main__ - Step 290 Global step 290 Train loss 0.14 on epoch=144
03/24/2022 01:13:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.16 on epoch=149
03/24/2022 01:13:46 - INFO - __main__ - Global step 300 Train loss 0.16 Classification-F1 0.3073593073593074 on epoch=149
03/24/2022 01:13:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.12 on epoch=154
03/24/2022 01:13:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.15 on epoch=159
03/24/2022 01:14:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.13 on epoch=164
03/24/2022 01:14:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.14 on epoch=169
03/24/2022 01:14:09 - INFO - __main__ - Step 350 Global step 350 Train loss 0.11 on epoch=174
03/24/2022 01:14:10 - INFO - __main__ - Global step 350 Train loss 0.13 Classification-F1 0.2193798449612403 on epoch=174
03/24/2022 01:14:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.11 on epoch=179
03/24/2022 01:14:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.08 on epoch=184
03/24/2022 01:14:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.07 on epoch=189
03/24/2022 01:14:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.05 on epoch=194
03/24/2022 01:14:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.08 on epoch=199
03/24/2022 01:14:34 - INFO - __main__ - Global step 400 Train loss 0.08 Classification-F1 0.39999999999999997 on epoch=199
03/24/2022 01:14:38 - INFO - __main__ - Step 410 Global step 410 Train loss 0.03 on epoch=204
03/24/2022 01:14:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.05 on epoch=209
03/24/2022 01:14:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.03 on epoch=214
03/24/2022 01:14:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.03 on epoch=219
03/24/2022 01:14:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.02 on epoch=224
03/24/2022 01:14:58 - INFO - __main__ - Global step 450 Train loss 0.03 Classification-F1 0.37662337662337664 on epoch=224
03/24/2022 01:15:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
03/24/2022 01:15:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=234
03/24/2022 01:15:11 - INFO - __main__ - Step 480 Global step 480 Train loss 0.01 on epoch=239
03/24/2022 01:15:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.05 on epoch=244
03/24/2022 01:15:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.01 on epoch=249
03/24/2022 01:15:21 - INFO - __main__ - Global step 500 Train loss 0.03 Classification-F1 0.37662337662337664 on epoch=249
03/24/2022 01:15:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.05 on epoch=254
03/24/2022 01:15:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
03/24/2022 01:15:34 - INFO - __main__ - Step 530 Global step 530 Train loss 0.04 on epoch=264
03/24/2022 01:15:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.01 on epoch=269
03/24/2022 01:15:43 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
03/24/2022 01:15:45 - INFO - __main__ - Global step 550 Train loss 0.03 Classification-F1 0.4554554554554554 on epoch=274
03/24/2022 01:15:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4375 -> 0.4554554554554554 on epoch=274, global_step=550
03/24/2022 01:15:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
03/24/2022 01:15:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
03/24/2022 01:15:58 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/24/2022 01:16:03 - INFO - __main__ - Step 590 Global step 590 Train loss 0.02 on epoch=294
03/24/2022 01:16:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
03/24/2022 01:16:08 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.37662337662337664 on epoch=299
03/24/2022 01:16:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
03/24/2022 01:16:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.04 on epoch=309
03/24/2022 01:16:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
03/24/2022 01:16:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
03/24/2022 01:16:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/24/2022 01:16:32 - INFO - __main__ - Global step 650 Train loss 0.03 Classification-F1 0.5 on epoch=324
03/24/2022 01:16:32 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.5 on epoch=324, global_step=650
03/24/2022 01:16:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/24/2022 01:16:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/24/2022 01:16:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/24/2022 01:16:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/24/2022 01:16:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
03/24/2022 01:16:56 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.35699797160243407 on epoch=349
03/24/2022 01:17:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/24/2022 01:17:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/24/2022 01:17:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.02 on epoch=364
03/24/2022 01:17:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/24/2022 01:17:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/24/2022 01:17:20 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.2460052025269417 on epoch=374
03/24/2022 01:17:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/24/2022 01:17:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/24/2022 01:17:33 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/24/2022 01:17:38 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/24/2022 01:17:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/24/2022 01:17:44 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=399
03/24/2022 01:17:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/24/2022 01:17:53 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/24/2022 01:17:57 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/24/2022 01:18:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/24/2022 01:18:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/24/2022 01:18:07 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.3333333333333333 on epoch=424
03/24/2022 01:18:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/24/2022 01:18:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/24/2022 01:18:21 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
03/24/2022 01:18:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/24/2022 01:18:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/24/2022 01:18:31 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=449
03/24/2022 01:18:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/24/2022 01:18:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/24/2022 01:18:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/24/2022 01:18:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/24/2022 01:18:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/24/2022 01:18:55 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=474
03/24/2022 01:18:59 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/24/2022 01:19:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/24/2022 01:19:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/24/2022 01:19:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/24/2022 01:19:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/24/2022 01:19:18 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.3552492046659597 on epoch=499
03/24/2022 01:19:23 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/24/2022 01:19:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/24/2022 01:19:32 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/24/2022 01:19:36 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/24/2022 01:19:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/24/2022 01:19:42 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=524
03/24/2022 01:19:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/24/2022 01:19:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/24/2022 01:19:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/24/2022 01:20:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.03 on epoch=544
03/24/2022 01:20:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/24/2022 01:20:06 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=549
03/24/2022 01:20:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/24/2022 01:20:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/24/2022 01:20:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/24/2022 01:20:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/24/2022 01:20:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/24/2022 01:20:29 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=574
03/24/2022 01:20:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/24/2022 01:20:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.03 on epoch=584
03/24/2022 01:20:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/24/2022 01:20:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/24/2022 01:20:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/24/2022 01:20:53 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.40566959921798634 on epoch=599
03/24/2022 01:20:57 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/24/2022 01:21:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/24/2022 01:21:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/24/2022 01:21:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/24/2022 01:21:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/24/2022 01:21:16 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=624
03/24/2022 01:21:21 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/24/2022 01:21:25 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/24/2022 01:21:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/24/2022 01:21:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/24/2022 01:21:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
03/24/2022 01:21:40 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4666666666666667 on epoch=649
03/24/2022 01:21:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/24/2022 01:21:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/24/2022 01:21:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/24/2022 01:21:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/24/2022 01:22:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/24/2022 01:22:03 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.4666666666666667 on epoch=674
03/24/2022 01:22:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/24/2022 01:22:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/24/2022 01:22:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/24/2022 01:22:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/24/2022 01:22:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/24/2022 01:22:27 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=699
03/24/2022 01:22:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/24/2022 01:22:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/24/2022 01:22:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/24/2022 01:22:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/24/2022 01:22:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/24/2022 01:22:51 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=724
03/24/2022 01:22:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/24/2022 01:23:00 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/24/2022 01:23:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/24/2022 01:23:09 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/24/2022 01:23:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/24/2022 01:23:14 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.39999999999999997 on epoch=749
03/24/2022 01:23:19 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/24/2022 01:23:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/24/2022 01:23:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/24/2022 01:23:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/24/2022 01:23:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/24/2022 01:23:38 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.3764102564102564 on epoch=774
03/24/2022 01:23:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/24/2022 01:23:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/24/2022 01:23:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/24/2022 01:23:56 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/24/2022 01:24:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/24/2022 01:24:01 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=799
03/24/2022 01:24:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/24/2022 01:24:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/24/2022 01:24:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/24/2022 01:24:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/24/2022 01:24:24 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/24/2022 01:24:25 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=824
03/24/2022 01:24:29 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/24/2022 01:24:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/24/2022 01:24:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/24/2022 01:24:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
03/24/2022 01:24:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/24/2022 01:24:48 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=849
03/24/2022 01:24:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/24/2022 01:24:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/24/2022 01:25:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/24/2022 01:25:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/24/2022 01:25:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/24/2022 01:25:12 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=874
03/24/2022 01:25:16 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/24/2022 01:25:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/24/2022 01:25:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/24/2022 01:25:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/24/2022 01:25:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/24/2022 01:25:35 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=899
03/24/2022 01:25:40 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/24/2022 01:25:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/24/2022 01:25:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/24/2022 01:25:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/24/2022 01:25:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/24/2022 01:25:59 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=924
03/24/2022 01:26:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/24/2022 01:26:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/24/2022 01:26:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/24/2022 01:26:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/24/2022 01:26:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/24/2022 01:26:22 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=949
03/24/2022 01:26:27 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/24/2022 01:26:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/24/2022 01:26:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/24/2022 01:26:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/24/2022 01:26:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/24/2022 01:26:46 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=974
03/24/2022 01:26:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/24/2022 01:26:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
03/24/2022 01:26:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/24/2022 01:27:03 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/24/2022 01:27:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/24/2022 01:27:09 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=999
03/24/2022 01:27:09 - INFO - __main__ - Saving model with best Classification-F1: 0.5 -> 0.5307917888563051 on epoch=999, global_step=2000
03/24/2022 01:27:09 - INFO - __main__ - save last model!
03/24/2022 01:27:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:27:09 - INFO - __main__ - Printing 3 examples
03/24/2022 01:27:09 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/24/2022 01:27:09 - INFO - __main__ - ['refuted']
03/24/2022 01:27:09 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/24/2022 01:27:09 - INFO - __main__ - ['refuted']
03/24/2022 01:27:09 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/24/2022 01:27:09 - INFO - __main__ - ['refuted']
03/24/2022 01:27:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/24/2022 01:27:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/24/2022 01:27:09 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:27:09 - INFO - __main__ - Start tokenizing ... 12792 instances
03/24/2022 01:27:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 01:27:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:27:09 - INFO - __main__ - Printing 3 examples
03/24/2022 01:27:09 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/24/2022 01:27:09 - INFO - __main__ - ['refuted']
03/24/2022 01:27:09 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/24/2022 01:27:09 - INFO - __main__ - ['refuted']
03/24/2022 01:27:09 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/24/2022 01:27:09 - INFO - __main__ - ['refuted']
03/24/2022 01:27:09 - INFO - __main__ - Tokenizing Input ...
03/24/2022 01:27:09 - INFO - __main__ - Printing 3 examples
03/24/2022 01:27:09 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 01:27:09 - INFO - __main__ - ['entailed']
03/24/2022 01:27:09 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 01:27:09 - INFO - __main__ - ['entailed']
03/24/2022 01:27:09 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 01:27:09 - INFO - __main__ - ['entailed']
03/24/2022 01:27:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 01:27:09 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:27:09 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 01:27:28 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 01:27:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 01:27:29 - INFO - __main__ - Starting training!
03/24/2022 01:27:33 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:27:45 - INFO - __main__ - Loaded 12792 examples from test data
03/24/2022 01:36:01 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.4_8_predictions.txt
03/24/2022 01:36:01 - INFO - __main__ - Classification-F1 on test data: 0.3333
03/24/2022 01:36:01 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.4, bsz=8, dev_performance=0.5307917888563051, test_performance=0.33325736267298056
03/24/2022 01:36:01 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.3, bsz=8 ...
03/24/2022 01:36:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:36:02 - INFO - __main__ - Printing 3 examples
03/24/2022 01:36:02 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/24/2022 01:36:02 - INFO - __main__ - ['refuted']
03/24/2022 01:36:02 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/24/2022 01:36:02 - INFO - __main__ - ['refuted']
03/24/2022 01:36:02 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/24/2022 01:36:02 - INFO - __main__ - ['refuted']
03/24/2022 01:36:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 01:36:02 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:36:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 01:36:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:36:02 - INFO - __main__ - Printing 3 examples
03/24/2022 01:36:02 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/24/2022 01:36:02 - INFO - __main__ - ['refuted']
03/24/2022 01:36:02 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/24/2022 01:36:02 - INFO - __main__ - ['refuted']
03/24/2022 01:36:02 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/24/2022 01:36:02 - INFO - __main__ - ['refuted']
03/24/2022 01:36:02 - INFO - __main__ - Tokenizing Input ...
03/24/2022 01:36:02 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:36:02 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 01:36:21 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 01:36:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 01:36:21 - INFO - __main__ - Starting training!
03/24/2022 01:36:26 - INFO - __main__ - Step 10 Global step 10 Train loss 4.27 on epoch=4
03/24/2022 01:36:31 - INFO - __main__ - Step 20 Global step 20 Train loss 1.95 on epoch=9
03/24/2022 01:36:35 - INFO - __main__ - Step 30 Global step 30 Train loss 0.81 on epoch=14
03/24/2022 01:36:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.39 on epoch=19
03/24/2022 01:36:44 - INFO - __main__ - Step 50 Global step 50 Train loss 0.35 on epoch=24
03/24/2022 01:36:45 - INFO - __main__ - Global step 50 Train loss 1.56 Classification-F1 0.3333333333333333 on epoch=24
03/24/2022 01:36:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/24/2022 01:36:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=29
03/24/2022 01:36:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.28 on epoch=34
03/24/2022 01:36:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.27 on epoch=39
03/24/2022 01:37:03 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=44
03/24/2022 01:37:08 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=49
03/24/2022 01:37:09 - INFO - __main__ - Global step 100 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=49
03/24/2022 01:37:14 - INFO - __main__ - Step 110 Global step 110 Train loss 0.23 on epoch=54
03/24/2022 01:37:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=59
03/24/2022 01:37:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
03/24/2022 01:37:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
03/24/2022 01:37:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.24 on epoch=74
03/24/2022 01:37:33 - INFO - __main__ - Global step 150 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=74
03/24/2022 01:37:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=79
03/24/2022 01:37:42 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=84
03/24/2022 01:37:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=89
03/24/2022 01:37:51 - INFO - __main__ - Step 190 Global step 190 Train loss 0.20 on epoch=94
03/24/2022 01:37:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/24/2022 01:37:56 - INFO - __main__ - Global step 200 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=99
03/24/2022 01:38:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.19 on epoch=104
03/24/2022 01:38:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=109
03/24/2022 01:38:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.21 on epoch=114
03/24/2022 01:38:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
03/24/2022 01:38:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.19 on epoch=124
03/24/2022 01:38:20 - INFO - __main__ - Global step 250 Train loss 0.21 Classification-F1 0.4375 on epoch=124
03/24/2022 01:38:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4375 on epoch=124, global_step=250
03/24/2022 01:38:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.17 on epoch=129
03/24/2022 01:38:29 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=134
03/24/2022 01:38:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.19 on epoch=139
03/24/2022 01:38:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.13 on epoch=144
03/24/2022 01:38:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.19 on epoch=149
03/24/2022 01:38:43 - INFO - __main__ - Global step 300 Train loss 0.18 Classification-F1 0.4817813765182186 on epoch=149
03/24/2022 01:38:43 - INFO - __main__ - Saving model with best Classification-F1: 0.4375 -> 0.4817813765182186 on epoch=149, global_step=300
03/24/2022 01:38:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.17 on epoch=154
03/24/2022 01:38:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.16 on epoch=159
03/24/2022 01:38:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.14 on epoch=164
03/24/2022 01:39:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.10 on epoch=169
03/24/2022 01:39:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.14 on epoch=174
03/24/2022 01:39:07 - INFO - __main__ - Global step 350 Train loss 0.14 Classification-F1 0.39999999999999997 on epoch=174
03/24/2022 01:39:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.14 on epoch=179
03/24/2022 01:39:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.10 on epoch=184
03/24/2022 01:39:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.13 on epoch=189
03/24/2022 01:39:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.08 on epoch=194
03/24/2022 01:39:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.06 on epoch=199
03/24/2022 01:39:31 - INFO - __main__ - Global step 400 Train loss 0.10 Classification-F1 0.4181818181818182 on epoch=199
03/24/2022 01:39:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.07 on epoch=204
03/24/2022 01:39:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.10 on epoch=209
03/24/2022 01:39:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.07 on epoch=214
03/24/2022 01:39:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.05 on epoch=219
03/24/2022 01:39:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=224
03/24/2022 01:39:54 - INFO - __main__ - Global step 450 Train loss 0.07 Classification-F1 0.464039408866995 on epoch=224
03/24/2022 01:39:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
03/24/2022 01:40:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.03 on epoch=234
03/24/2022 01:40:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
03/24/2022 01:40:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
03/24/2022 01:40:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.04 on epoch=249
03/24/2022 01:40:18 - INFO - __main__ - Global step 500 Train loss 0.04 Classification-F1 0.41700404858299595 on epoch=249
03/24/2022 01:40:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.01 on epoch=254
03/24/2022 01:40:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.03 on epoch=259
03/24/2022 01:40:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.01 on epoch=264
03/24/2022 01:40:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
03/24/2022 01:40:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.02 on epoch=274
03/24/2022 01:40:41 - INFO - __main__ - Global step 550 Train loss 0.02 Classification-F1 0.464039408866995 on epoch=274
03/24/2022 01:40:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.01 on epoch=279
03/24/2022 01:40:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.05 on epoch=284
03/24/2022 01:40:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=289
03/24/2022 01:40:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/24/2022 01:41:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/24/2022 01:41:05 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.22807017543859648 on epoch=299
03/24/2022 01:41:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/24/2022 01:41:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/24/2022 01:41:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.03 on epoch=314
03/24/2022 01:41:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/24/2022 01:41:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/24/2022 01:41:28 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 0.2759103641456582 on epoch=324
03/24/2022 01:41:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/24/2022 01:41:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/24/2022 01:41:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/24/2022 01:41:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.02 on epoch=344
03/24/2022 01:41:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
03/24/2022 01:41:52 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.1360153256704981 on epoch=349
03/24/2022 01:41:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.03 on epoch=354
03/24/2022 01:42:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/24/2022 01:42:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/24/2022 01:42:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
03/24/2022 01:42:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/24/2022 01:42:16 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.1683982683982684 on epoch=374
03/24/2022 01:42:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/24/2022 01:42:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/24/2022 01:42:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/24/2022 01:42:33 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
03/24/2022 01:42:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/24/2022 01:42:39 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.22011494252873565 on epoch=399
03/24/2022 01:42:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/24/2022 01:42:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/24/2022 01:42:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/24/2022 01:42:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/24/2022 01:43:01 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/24/2022 01:43:03 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.2712643678160919 on epoch=424
03/24/2022 01:43:07 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/24/2022 01:43:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/24/2022 01:43:16 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/24/2022 01:43:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/24/2022 01:43:25 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/24/2022 01:43:26 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.22011494252873565 on epoch=449
03/24/2022 01:43:31 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/24/2022 01:43:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/24/2022 01:43:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/24/2022 01:43:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/24/2022 01:43:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
03/24/2022 01:43:50 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.21635150166852057 on epoch=474
03/24/2022 01:43:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/24/2022 01:43:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/24/2022 01:44:03 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/24/2022 01:44:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/24/2022 01:44:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/24/2022 01:44:13 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.18222222222222223 on epoch=499
03/24/2022 01:44:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/24/2022 01:44:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/24/2022 01:44:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/24/2022 01:44:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/24/2022 01:44:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
03/24/2022 01:44:37 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.24166666666666667 on epoch=524
03/24/2022 01:44:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/24/2022 01:44:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/24/2022 01:44:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/24/2022 01:44:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/24/2022 01:44:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/24/2022 01:45:01 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.14303959131545338 on epoch=549
03/24/2022 01:45:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/24/2022 01:45:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/24/2022 01:45:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
03/24/2022 01:45:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/24/2022 01:45:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/24/2022 01:45:24 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.08998144712430427 on epoch=574
03/24/2022 01:45:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/24/2022 01:45:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/24/2022 01:45:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/24/2022 01:45:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/24/2022 01:45:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/24/2022 01:45:48 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.1269365317341329 on epoch=599
03/24/2022 01:45:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/24/2022 01:45:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/24/2022 01:46:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/24/2022 01:46:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/24/2022 01:46:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/24/2022 01:46:11 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.18275862068965515 on epoch=624
03/24/2022 01:46:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/24/2022 01:46:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/24/2022 01:46:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/24/2022 01:46:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/24/2022 01:46:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/24/2022 01:46:35 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.15147783251231528 on epoch=649
03/24/2022 01:46:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/24/2022 01:46:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/24/2022 01:46:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/24/2022 01:46:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/24/2022 01:46:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/24/2022 01:46:58 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.28376436781609193 on epoch=674
03/24/2022 01:47:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/24/2022 01:47:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/24/2022 01:47:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/24/2022 01:47:16 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/24/2022 01:47:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/24/2022 01:47:22 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.24193548387096775 on epoch=699
03/24/2022 01:47:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/24/2022 01:47:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/24/2022 01:47:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/24/2022 01:47:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/24/2022 01:47:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/24/2022 01:47:45 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.2748655913978495 on epoch=724
03/24/2022 01:47:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/24/2022 01:47:54 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/24/2022 01:47:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/24/2022 01:48:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/24/2022 01:48:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/24/2022 01:48:09 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.2791666666666666 on epoch=749
03/24/2022 01:48:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/24/2022 01:48:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/24/2022 01:48:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/24/2022 01:48:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/24/2022 01:48:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/24/2022 01:48:32 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.22011494252873565 on epoch=774
03/24/2022 01:48:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/24/2022 01:48:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/24/2022 01:48:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/24/2022 01:48:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/24/2022 01:48:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/24/2022 01:48:56 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.18598442714126812 on epoch=799
03/24/2022 01:49:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/24/2022 01:49:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/24/2022 01:49:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/24/2022 01:49:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/24/2022 01:49:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/24/2022 01:49:19 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.22844827586206895 on epoch=824
03/24/2022 01:49:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/24/2022 01:49:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/24/2022 01:49:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/24/2022 01:49:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/24/2022 01:49:41 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/24/2022 01:49:43 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.3172043010752688 on epoch=849
03/24/2022 01:49:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/24/2022 01:49:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/24/2022 01:49:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/24/2022 01:50:00 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/24/2022 01:50:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/24/2022 01:50:06 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.15 on epoch=874
03/24/2022 01:50:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/24/2022 01:50:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/24/2022 01:50:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/24/2022 01:50:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/24/2022 01:50:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/24/2022 01:50:30 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.23981191222570536 on epoch=899
03/24/2022 01:50:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/24/2022 01:50:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/24/2022 01:50:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/24/2022 01:50:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/24/2022 01:50:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/24/2022 01:50:53 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.2435064935064935 on epoch=924
03/24/2022 01:50:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/24/2022 01:51:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/24/2022 01:51:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/24/2022 01:51:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
03/24/2022 01:51:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/24/2022 01:51:16 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.210239651416122 on epoch=949
03/24/2022 01:51:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/24/2022 01:51:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/24/2022 01:51:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/24/2022 01:51:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/24/2022 01:51:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/24/2022 01:51:40 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.24137931034482757 on epoch=974
03/24/2022 01:51:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
03/24/2022 01:51:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/24/2022 01:51:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/24/2022 01:51:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/24/2022 01:52:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/24/2022 01:52:03 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.22011494252873565 on epoch=999
03/24/2022 01:52:03 - INFO - __main__ - save last model!
03/24/2022 01:52:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/24/2022 01:52:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:52:03 - INFO - __main__ - Printing 3 examples
03/24/2022 01:52:03 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/24/2022 01:52:03 - INFO - __main__ - ['refuted']
03/24/2022 01:52:03 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/24/2022 01:52:03 - INFO - __main__ - ['refuted']
03/24/2022 01:52:03 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/24/2022 01:52:03 - INFO - __main__ - ['refuted']
03/24/2022 01:52:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/24/2022 01:52:03 - INFO - __main__ - Start tokenizing ... 12792 instances
03/24/2022 01:52:03 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:52:03 - INFO - __main__ - Printing 3 examples
03/24/2022 01:52:03 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 01:52:03 - INFO - __main__ - ['entailed']
03/24/2022 01:52:03 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 01:52:03 - INFO - __main__ - ['entailed']
03/24/2022 01:52:03 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 01:52:03 - INFO - __main__ - ['entailed']
03/24/2022 01:52:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 01:52:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 01:52:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 01:52:03 - INFO - __main__ - Printing 3 examples
03/24/2022 01:52:03 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/24/2022 01:52:03 - INFO - __main__ - ['refuted']
03/24/2022 01:52:03 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/24/2022 01:52:03 - INFO - __main__ - ['refuted']
03/24/2022 01:52:03 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/24/2022 01:52:03 - INFO - __main__ - ['refuted']
03/24/2022 01:52:03 - INFO - __main__ - Tokenizing Input ...
03/24/2022 01:52:03 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:52:03 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 01:52:22 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 01:52:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 01:52:23 - INFO - __main__ - Starting training!
03/24/2022 01:52:27 - INFO - __main__ - Tokenizing Output ...
03/24/2022 01:52:40 - INFO - __main__ - Loaded 12792 examples from test data
03/24/2022 02:00:57 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.3_8_predictions.txt
03/24/2022 02:00:57 - INFO - __main__ - Classification-F1 on test data: 0.0642
03/24/2022 02:00:58 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.3, bsz=8, dev_performance=0.4817813765182186, test_performance=0.06415287730241952
03/24/2022 02:00:58 - INFO - __main__ - Running ... prefix=tab_fact_16_42, lr=0.2, bsz=8 ...
03/24/2022 02:00:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 02:00:58 - INFO - __main__ - Printing 3 examples
03/24/2022 02:00:58 - INFO - __main__ -  [tab_fact] statement: more than 6 player make their debut between august 2 and august 30 2007 [SEP] table_caption: 2007 - 08 newcastle jets season [SEP] table_text: name#position#from (club)#date joined#debut [n] noel spencer#midfield#sydney fc#7 may 2007#round 1 [n] adam griffiths#defender#brentford#17 may 2007#round 1 [n] jorge drovandi#forward#rosario central#2 august 2007#round 1 [n] denni#midfield#santo andr#17 august 2007#round 1 [n] scott tunbridge#forward#hamilton academical#4 july 2007#round 11 [n] mrio jardel#forward#anorthosis#13 august 2007#round 4 [n] ben mcnamara#goalkeeper#lake macquarie city#18 august 2007#uncapped [n] jason hoffman#forward#hamilton olympic#30 august 2007#round 2 [n] stephen laybutt#defender#gent#30 august 2007#round 6 [n] james holland#midfield#ais#14 october 2007#round 8 [n] ben kantarovski#midfield#broadmeadow magic#12 january 2008#uncapped [n] song jin - hyung#midfield#fc seoul#18 january 2008#semi final (2nd leg) [n] 
03/24/2022 02:00:58 - INFO - __main__ - ['refuted']
03/24/2022 02:00:58 - INFO - __main__ -  [tab_fact] statement: the boston celtics' cumulative point throughout the series be more than 2 greater than that of the indiana pacer [SEP] table_caption: 1990 - 91 boston celtics season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#series [n] 1#april 26#indiana pacers#w 127 - 120#r lewis (28)#l bird (12)#l bird (12)#boston garden#1 - 0 [n] 2#april 28#indiana pacers#l 118 - 130#r lewis , b shaw (22)#r parish (12)#l bird (10)#boston garden#1 - 1 [n] 3#may 1#indiana pacers#w 112 - 105#k mchale (22)#l bird (9)#b shaw (7)#market square arena#2 - 1 [n] 4#may 3#indiana pacers#l 113 - 116#k mchale (24)#r parish (12)#l bird (8)#market square arena#2 - 2 [n] 5#may 5#indiana pacers#w 124 - 121#l bird (32)#l bird (9)#b shaw (9)#boston garden#3 - 2 [n] 
03/24/2022 02:00:58 - INFO - __main__ - ['refuted']
03/24/2022 02:00:58 - INFO - __main__ -  [tab_fact] statement: kidwelly rfc have 409 point against them [SEP] table_caption: wru division two west [SEP] table_text: club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] club#played#drawn#lost#points for#points against#tries for#tries against#try bonus#losing bonus#points [n] maesteg rfc#22#2#1#615#271#78#24#12#0#92 [n] waunarlwydd rfc#22#1#7#594#359#73#38#10#5#73 [n] bp llandarcy rfc#22#1#7#376#320#43#36#3#5#66 [n] kidwelly rfc#22#0#9#558#393#68#39#6#6#64 [n] aberavon quins rfc#22#0#9#449#424#56#45#6#3#61 [n] ammanford rfc#22#1#10#409#348#45#33#4#8#58 [n] loughor rfc#22#1#11#427#479#47#60#5#4#51 [n] aberystwyth rfc#22#0#12#390#509#46#71#5#4#49 [n] pontyberem rfc#22#0#12#353#520#35#67#4#3#47 [n] mumbles rfc#22#1#14#372#471#51#55#5#4#39 [n] pencoed rfc#22#0#19#321#505#34#62#0#10#22 [n] dunvant rfc#22#1#17#324#589#33#79#0#2#20 [n] 
03/24/2022 02:00:58 - INFO - __main__ - ['refuted']
03/24/2022 02:00:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 02:00:59 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:00:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 02:00:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 02:00:59 - INFO - __main__ - Printing 3 examples
03/24/2022 02:00:59 - INFO - __main__ -  [tab_fact] statement: automobile workshop destroy neighborhood damage area be damage when downtown riyadh be target [SEP] table_caption: al hussein (missile) [SEP] table_text: no#place & date#target#area damaged#cause of damage#intercepted by patriot [n] 2#january 22 riyadh#coalition air base#civilian neighborhood#warhead#yes [n] 3#january 25 riyadh#coalition headquarters#saudi department of interior#warhead#yes [n] 4#january 28 riyadh#downtown riyadh#experimental farm southeast of the capital#debris#yes [n] 5#february 3 riyadh#downtown riyadh#apartments area#warhead#yes [n] 6#february 8 riyadh#north of the city#parking lot#warhead#yes [n] 7#february 11 riyadh#downtown riyadh#islamic university campus#warhead#yes [n] 8#february 14 hafar al - batin#king khalid military city#automobile workshop destroyed neighborhood damaged#warhead#no [n] 9#february 24 riyadh#coalition headquarters#girls school#debris#yes [n] 
03/24/2022 02:00:59 - INFO - __main__ - ['refuted']
03/24/2022 02:00:59 - INFO - __main__ -  [tab_fact] statement: wayne grady never beatover 9 player from 3 other countriesin the1989 open championship [SEP] table_caption: 1989 open championship [SEP] table_text: place#player#country#score#to par [n] 1#wayne grady#australia#68 + 67 + 69 = 204#- 12 [n] 2#tom watson#united states#69 + 68 + 68 = 205#- 11 [n] 3#payne stewart#united states#72 + 65 + 69 = 206#- 10 [n] t4#mark calcavecchia#united states#71 + 68 + 68 = 207#- 9 [n] t4#fred couples#united states#68 + 71 + 68 = 207#- 9 [n] t4#david feherty#northern ireland#71 + 67 + 69 = 207#- 9 [n] t7#paul azinger#united states#68 + 73 + 67 = 208#- 8 [n] t7#jodie mudd#united states#73 + 67 + 68 = 208#- 8 [n] t9#mark mccumber#united states#71 + 68 + 70 = 209#- 7 [n] t9#jos mara olazbal#spain#68 + 72 + 69 = 209#- 7 [n] t9#steve pate#united states#69 + 70 + 70 = 209#- 7 [n] 
03/24/2022 02:00:59 - INFO - __main__ - ['refuted']
03/24/2022 02:00:59 - INFO - __main__ -  [tab_fact] statement: 13 november 2008 be the 1st date of appointment and the last 1 be on 6 april 2009 [SEP] table_caption: 2008 - 09 belgian first division [SEP] table_text: team#outgoing manager#manner of departure#date of vacancy#replaced by#date of appointment#position in table [n] mons#philippe saint - jean#resigned#21 august 2008#thierry pister (caretaker)#21 august 2008#18th [n] roeselare#dirk geeraerd#sacked#26 october 2008#dennis van wijk#29 october 2008#18th [n] germinal beerschot#harm van veldhoven#resigned#13 november 2008#aim anthuenis#14 november 2008#16th [n] mons#thierry pister (caretaker)#sacked#4 december 2008#christophe dessy (caretaker)#4 december 2008#15th [n] charleroi#thierry siquet#sacked#15 december 2008#john collins#15 december 2008#11th [n] genk#ronny van geneugden#resigned#5 march 2009#pierre denier and hans visser (caretakers)#5 march 2009#4th [n] lokeren#georges leekens#resigned#31 march 2009#aleksandar jankovi#6 april 2009#7th [n] 
03/24/2022 02:00:59 - INFO - __main__ - ['refuted']
03/24/2022 02:00:59 - INFO - __main__ - Tokenizing Input ...
03/24/2022 02:00:59 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:00:59 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 02:01:17 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 02:01:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 02:01:18 - INFO - __main__ - Starting training!
03/24/2022 02:01:25 - INFO - __main__ - Step 10 Global step 10 Train loss 4.33 on epoch=4
03/24/2022 02:01:30 - INFO - __main__ - Step 20 Global step 20 Train loss 2.47 on epoch=9
03/24/2022 02:01:34 - INFO - __main__ - Step 30 Global step 30 Train loss 1.53 on epoch=14
03/24/2022 02:01:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.79 on epoch=19
03/24/2022 02:01:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.49 on epoch=24
03/24/2022 02:01:45 - INFO - __main__ - Global step 50 Train loss 1.92 Classification-F1 0.3333333333333333 on epoch=24
03/24/2022 02:01:45 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/24/2022 02:01:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.36 on epoch=29
03/24/2022 02:01:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.36 on epoch=34
03/24/2022 02:01:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=39
03/24/2022 02:02:02 - INFO - __main__ - Step 90 Global step 90 Train loss 0.34 on epoch=44
03/24/2022 02:02:07 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
03/24/2022 02:02:08 - INFO - __main__ - Global step 100 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=49
03/24/2022 02:02:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=54
03/24/2022 02:02:17 - INFO - __main__ - Step 120 Global step 120 Train loss 0.31 on epoch=59
03/24/2022 02:02:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
03/24/2022 02:02:26 - INFO - __main__ - Step 140 Global step 140 Train loss 0.22 on epoch=69
03/24/2022 02:02:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
03/24/2022 02:02:32 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=74
03/24/2022 02:02:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=79
03/24/2022 02:02:41 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
03/24/2022 02:02:45 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
03/24/2022 02:02:50 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
03/24/2022 02:02:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/24/2022 02:02:55 - INFO - __main__ - Global step 200 Train loss 0.28 Classification-F1 0.3992490613266583 on epoch=99
03/24/2022 02:02:55 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=99, global_step=200
03/24/2022 02:03:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
03/24/2022 02:03:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=109
03/24/2022 02:03:09 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
03/24/2022 02:03:13 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=119
03/24/2022 02:03:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.22 on epoch=124
03/24/2022 02:03:19 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.39139139139139134 on epoch=124
03/24/2022 02:03:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.20 on epoch=129
03/24/2022 02:03:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=134
03/24/2022 02:03:33 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
03/24/2022 02:03:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
03/24/2022 02:03:42 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=149
03/24/2022 02:03:43 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3764102564102564 on epoch=149
03/24/2022 02:03:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
03/24/2022 02:03:52 - INFO - __main__ - Step 320 Global step 320 Train loss 0.18 on epoch=159
03/24/2022 02:03:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=164
03/24/2022 02:04:01 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
03/24/2022 02:04:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
03/24/2022 02:04:07 - INFO - __main__ - Global step 350 Train loss 0.21 Classification-F1 0.3552492046659597 on epoch=174
03/24/2022 02:04:11 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=179
03/24/2022 02:04:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=184
03/24/2022 02:04:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.20 on epoch=189
03/24/2022 02:04:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.17 on epoch=194
03/24/2022 02:04:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=199
03/24/2022 02:04:30 - INFO - __main__ - Global step 400 Train loss 0.20 Classification-F1 0.3266888150609081 on epoch=199
03/24/2022 02:04:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.16 on epoch=204
03/24/2022 02:04:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.17 on epoch=209
03/24/2022 02:04:44 - INFO - __main__ - Step 430 Global step 430 Train loss 0.17 on epoch=214
03/24/2022 02:04:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=219
03/24/2022 02:04:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=224
03/24/2022 02:04:54 - INFO - __main__ - Global step 450 Train loss 0.16 Classification-F1 0.3266888150609081 on epoch=224
03/24/2022 02:04:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.12 on epoch=229
03/24/2022 02:05:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.13 on epoch=234
03/24/2022 02:05:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
03/24/2022 02:05:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.12 on epoch=244
03/24/2022 02:05:16 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=249
03/24/2022 02:05:18 - INFO - __main__ - Global step 500 Train loss 0.12 Classification-F1 0.3816425120772947 on epoch=249
03/24/2022 02:05:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.10 on epoch=254
03/24/2022 02:05:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.12 on epoch=259
03/24/2022 02:05:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.12 on epoch=264
03/24/2022 02:05:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.11 on epoch=269
03/24/2022 02:05:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.08 on epoch=274
03/24/2022 02:05:41 - INFO - __main__ - Global step 550 Train loss 0.11 Classification-F1 0.39999999999999997 on epoch=274
03/24/2022 02:05:41 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.39999999999999997 on epoch=274, global_step=550
03/24/2022 02:05:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.12 on epoch=279
03/24/2022 02:05:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.08 on epoch=284
03/24/2022 02:05:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.09 on epoch=289
03/24/2022 02:05:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.07 on epoch=294
03/24/2022 02:06:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.05 on epoch=299
03/24/2022 02:06:05 - INFO - __main__ - Global step 600 Train loss 0.08 Classification-F1 0.3552492046659597 on epoch=299
03/24/2022 02:06:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.05 on epoch=304
03/24/2022 02:06:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.06 on epoch=309
03/24/2022 02:06:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.08 on epoch=314
03/24/2022 02:06:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.04 on epoch=319
03/24/2022 02:06:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.06 on epoch=324
03/24/2022 02:06:29 - INFO - __main__ - Global step 650 Train loss 0.06 Classification-F1 0.3454545454545454 on epoch=324
03/24/2022 02:06:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.05 on epoch=329
03/24/2022 02:06:38 - INFO - __main__ - Step 670 Global step 670 Train loss 0.06 on epoch=334
03/24/2022 02:06:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
03/24/2022 02:06:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=344
03/24/2022 02:06:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
03/24/2022 02:06:52 - INFO - __main__ - Global step 700 Train loss 0.05 Classification-F1 0.4420512820512821 on epoch=349
03/24/2022 02:06:52 - INFO - __main__ - Saving model with best Classification-F1: 0.39999999999999997 -> 0.4420512820512821 on epoch=349, global_step=700
03/24/2022 02:06:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.02 on epoch=354
03/24/2022 02:07:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.04 on epoch=359
03/24/2022 02:07:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
03/24/2022 02:07:10 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
03/24/2022 02:07:14 - INFO - __main__ - Step 750 Global step 750 Train loss 0.02 on epoch=374
03/24/2022 02:07:16 - INFO - __main__ - Global step 750 Train loss 0.03 Classification-F1 0.4817813765182186 on epoch=374
03/24/2022 02:07:16 - INFO - __main__ - Saving model with best Classification-F1: 0.4420512820512821 -> 0.4817813765182186 on epoch=374, global_step=750
03/24/2022 02:07:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
03/24/2022 02:07:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/24/2022 02:07:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
03/24/2022 02:07:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.02 on epoch=394
03/24/2022 02:07:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
03/24/2022 02:07:40 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.39999999999999997 on epoch=399
03/24/2022 02:07:44 - INFO - __main__ - Step 810 Global step 810 Train loss 0.06 on epoch=404
03/24/2022 02:07:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/24/2022 02:07:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
03/24/2022 02:07:57 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
03/24/2022 02:08:02 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
03/24/2022 02:08:03 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.4817813765182186 on epoch=424
03/24/2022 02:08:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
03/24/2022 02:08:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/24/2022 02:08:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/24/2022 02:08:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/24/2022 02:08:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/24/2022 02:08:27 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.39999999999999997 on epoch=449
03/24/2022 02:08:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/24/2022 02:08:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
03/24/2022 02:08:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
03/24/2022 02:08:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/24/2022 02:08:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.04 on epoch=474
03/24/2022 02:08:51 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.39756367663344405 on epoch=474
03/24/2022 02:08:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/24/2022 02:09:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
03/24/2022 02:09:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/24/2022 02:09:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/24/2022 02:09:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.03 on epoch=499
03/24/2022 02:09:15 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.37662337662337664 on epoch=499
03/24/2022 02:09:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/24/2022 02:09:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/24/2022 02:09:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/24/2022 02:09:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
03/24/2022 02:09:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/24/2022 02:09:38 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.4181818181818182 on epoch=524
03/24/2022 02:09:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/24/2022 02:09:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
03/24/2022 02:09:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/24/2022 02:09:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/24/2022 02:10:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/24/2022 02:10:02 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.39139139139139134 on epoch=549
03/24/2022 02:10:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/24/2022 02:10:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/24/2022 02:10:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
03/24/2022 02:10:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/24/2022 02:10:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/24/2022 02:10:25 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.17505720823798626 on epoch=574
03/24/2022 02:10:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/24/2022 02:10:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/24/2022 02:10:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/24/2022 02:10:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
03/24/2022 02:10:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/24/2022 02:10:49 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4375 on epoch=599
03/24/2022 02:10:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/24/2022 02:10:58 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/24/2022 02:11:02 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
03/24/2022 02:11:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/24/2022 02:11:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/24/2022 02:11:12 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.24691358024691357 on epoch=624
03/24/2022 02:11:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/24/2022 02:11:21 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/24/2022 02:11:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/24/2022 02:11:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
03/24/2022 02:11:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/24/2022 02:11:36 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.25210084033613445 on epoch=649
03/24/2022 02:11:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
03/24/2022 02:11:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/24/2022 02:11:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/24/2022 02:11:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/24/2022 02:11:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/24/2022 02:11:59 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.17701525054466233 on epoch=674
03/24/2022 02:12:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/24/2022 02:12:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/24/2022 02:12:12 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/24/2022 02:12:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/24/2022 02:12:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/24/2022 02:12:22 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.18835978835978834 on epoch=699
03/24/2022 02:12:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
03/24/2022 02:12:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/24/2022 02:12:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/24/2022 02:12:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
03/24/2022 02:12:44 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/24/2022 02:12:46 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.24930306650736758 on epoch=724
03/24/2022 02:12:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/24/2022 02:12:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
03/24/2022 02:12:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/24/2022 02:13:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/24/2022 02:13:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/24/2022 02:13:09 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.25562817719680464 on epoch=749
03/24/2022 02:13:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/24/2022 02:13:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/24/2022 02:13:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/24/2022 02:13:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/24/2022 02:13:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/24/2022 02:13:33 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.20693277310924368 on epoch=774
03/24/2022 02:13:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/24/2022 02:13:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/24/2022 02:13:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/24/2022 02:13:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/24/2022 02:13:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/24/2022 02:13:56 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=799
03/24/2022 02:14:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
03/24/2022 02:14:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/24/2022 02:14:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/24/2022 02:14:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/24/2022 02:14:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/24/2022 02:14:19 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=824
03/24/2022 02:14:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
03/24/2022 02:14:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/24/2022 02:14:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/24/2022 02:14:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/24/2022 02:14:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/24/2022 02:14:43 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4285714285714286 on epoch=849
03/24/2022 02:14:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/24/2022 02:14:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
03/24/2022 02:14:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/24/2022 02:15:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/24/2022 02:15:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/24/2022 02:15:06 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.2647262647262647 on epoch=874
03/24/2022 02:15:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/24/2022 02:15:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/24/2022 02:15:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/24/2022 02:15:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/24/2022 02:15:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/24/2022 02:15:30 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.2554385964912281 on epoch=899
03/24/2022 02:15:34 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/24/2022 02:15:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/24/2022 02:15:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/24/2022 02:15:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/24/2022 02:15:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
03/24/2022 02:15:53 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.2714285714285714 on epoch=924
03/24/2022 02:15:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/24/2022 02:16:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/24/2022 02:16:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/24/2022 02:16:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
03/24/2022 02:16:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/24/2022 02:16:16 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.20693277310924368 on epoch=949
03/24/2022 02:16:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/24/2022 02:16:25 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/24/2022 02:16:30 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/24/2022 02:16:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/24/2022 02:16:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/24/2022 02:16:40 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.24691358024691357 on epoch=974
03/24/2022 02:16:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/24/2022 02:16:49 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/24/2022 02:16:53 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/24/2022 02:16:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/24/2022 02:17:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/24/2022 02:17:03 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.39139139139139134 on epoch=999
03/24/2022 02:17:03 - INFO - __main__ - save last model!
03/24/2022 02:17:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 02:17:03 - INFO - __main__ - Printing 3 examples
03/24/2022 02:17:03 - INFO - __main__ -  [tab_fact] statement: the episode title sin of the father have a share value of 10 [SEP] table_caption: none [SEP] table_text: #episode#air date#timeslot (est)#rating#share#18 - 49 (rating / share)#viewers (m)#weekly rank  [n] 1#a death in the family#october 1 , 2009#thursday 10:00 pm#7.6#13#4.6 / 13#11.58#20 [n] 2#the way we were#october 8 , 2009#thursday 10:00 pm#6.2#11#3.6 / 10#9.50#25 [n] 3#right here , right now#october 15 , 2009#thursday 10:00 pm#6.8#12#3.8 / 11#10.36#21 [n] 4#pushing the limits#october 22 , 2009#thursday 10:00 pm#6.7#11#3.7 / 10#9.928#28 [n] 5#strange bedfellows#october 29 , 2009#thursday 10:00 pm#6.1#10#3.6 / 9#9.155#29 [n] 6#slip slidin away#november 5 , 2009#thursday 10:00 pm#6.0#10#3.4 / 10#9.11#27 [n] 7#the hard part#november 12 , 2009#thursday 10:00 pm#6.7#11#3.9 / 11#10.249#tba [n] 8#sins of the father#november 19 , 2009#thursday 10:00 pm#6.0#10#3.1 / 9#8.926#tba [n] 9#the parent trap#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 10#blowups#december 3 , 2009#thursday 10:00 pm#6.3#10#3.2 / 8#9.211#24 [n] 11#another second chance#january 14 , 2010#thursday 10:00 pm#7.1#12#4.2 / 12#10.963#tba [n] 12#best laid plans#january 21 , 2010#thursday 10:00 pm#6.6#11#3.6 / 10#9.637#tba [n] 13#shotgun#february 4 , 2010#thursday 10:00 pm#6.2#11#3.3 / 10#9.254#tba [n] 14#love bites#february 11 , 2010#thursday 10:00 pm#6.1#10#3.1 / 9#9.036#26 [n] 15#'til death do us part#february 18 , 2010#thursday 10:00 pm#5.1#8#2.8 / 7#7.593#32 [n] 16#fear of flying#march 4 , 2010#thursday 10:00 pm#5.2#9#2.7 / 8#7.572#36 [n] 17#triangles#march 11 , 2010#thursday 10:00 pm#5.3#9#2.8 / 8#7.656#tba [n] 18#pulling the plug#march 25 , 2010#thursday 10:00 pm#5.8#10#2.9 / 8#8.705#tba [n] 19#eyes wide open#april 1 , 2010#thursday 10:00 pm#5.3#9#2.6 / 8#7.822#tba [n] 20#second choices#april 22 , 2010#thursday 9:00 pm#5.1#9#2.3 / 6#7.491#tba [n] 21#war#april 29 , 2010#thursday 10:00 pm#5.4#9#2.9 / 9#7.775#tba [n] 22#in the name of love#may 6 , 2010#thursday 10:00 pm#5.7#10#2.8 / 8#8.152#tba [n] 
03/24/2022 02:17:03 - INFO - __main__ - ['entailed']
03/24/2022 02:17:03 - INFO - __main__ -  [tab_fact] statement: all team draw exactly 1 game out of 5 [SEP] table_caption: 2001 in paraguayan football [SEP] table_text: position#team#played#wins#draws#losses#scored#conceded#bonus points#points [n] 1#12 de octubre#5#3#1#1#10#4#-#10 [n] 2#olimpia#5#3#1#1#8#5#-#10 [n] 3#libertad#5#2#1#2#11#11#-#7 [n] 4#guaran#5#2#1#2#4#5#-#7 [n] 5#sportivo luqueo#5#1#1#3#7#13#-#7 [n] 6#sol de america#5#1#1#3#8#10#-#4 [n] 
03/24/2022 02:17:03 - INFO - __main__ - ['entailed']
03/24/2022 02:17:03 - INFO - __main__ -  [tab_fact] statement: new york be 1 of 5 team to beat the raptor during february 2008 [SEP] table_caption: 2007 - 08 toronto raptors season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 46#february 1#la lakers#l 101 - 121 (ot)#andrea bargnani (28)#chris bosh (15)#juan dixon (6)#air canada centre 19800#25 - 21 [n] 47#february 4#miami#w 114 - 82 (ot)#chris bosh (24)#jamario moon (9)#jos caldern (10)#american airlines arena 19600#26 - 21 [n] 48#february 8#la clippers#l 98 - 102 (ot)#chris bosh (29)#chris bosh (12)#jos caldern (14)#air canada centre 19800#26 - 22 [n] 49#february 10#minnesota#w 105 - 82 (ot)#andrea bargnani (16)#chris bosh , carlos delfino (9)#t j ford (13)#target center 13785#27 - 22 [n] 50#february 11#san antonio#l 88 - 93 (ot)#jos caldern (27)#chris bosh , carlos delfino , jamario moon (8)#jos caldern (6)#air canada centre 19800#27 - 23 [n] 51#february 13#new jersey#w 109 - 91 (ot)#chris bosh (27)#chris bosh , carlos delfino (9)#jos caldern (12)#air canada centre 19800#28 - 23 [n] 52#february 20#orlando#w 127 - 110 (ot)#chris bosh (40)#jamario moon (12)#jos caldern (13)#air canada centre 19800#29 - 23 [n] 53#february 22#new york#l 99 - 103 (ot)#chris bosh (23)#chris bosh , jamario moon (8)#jos caldern (6)#madison square garden 19763#29 - 24 [n] 54#february 24#new york#w 115 - 92 (ot)#andrea bargnani (25)#jamario moon , radoslav nesterovi (8)#jos caldern (7)#air canada centre 19800#30 - 24 [n] 55#february 25#indiana#w 102 - 98 (ot)#chris bosh (24)#anthony parker (11)#t j ford (7)#conseco fieldhouse 10468#31 - 24 [n] 56#february 27#minnesota#w 107 - 85 (ot)#chris bosh (28)#chris bosh , jamario moon (7)#jos caldern (7)#air canada centre 18325#32 - 24 [n] 57#february 29#indiana#l 111 - 122 (ot)#andrea bargnani (27)#andrea bargnani (9)#jos caldern (11)#air canada centre 19800#32 - 25 [n] 
03/24/2022 02:17:03 - INFO - __main__ - ['entailed']
03/24/2022 02:17:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/24/2022 02:17:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/24/2022 02:17:03 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:17:03 - INFO - __main__ - Start tokenizing ... 12792 instances
03/24/2022 02:17:03 - INFO - __main__ - Printing 3 examples
03/24/2022 02:17:03 - INFO - __main__ -  [tab_fact] statement: during the third round of the turkish cup , there be no new entry during that stage [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 02:17:03 - INFO - __main__ - ['entailed']
03/24/2022 02:17:03 - INFO - __main__ -  [tab_fact] statement: the highest number of winner from a previous round in the turkish cup be 54 in round 3 [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 02:17:03 - INFO - __main__ - ['entailed']
03/24/2022 02:17:03 - INFO - __main__ -  [tab_fact] statement: sper lig be the most common league to win a round in the turkish cup [SEP] table_caption: turkish cup [SEP] table_text: round#clubs remaining#clubs involved#winners from previous round#new entries this round#leagues entering at this round [n] first round#156#86#none#86#tff third league & turkish regional amateur league [n] second round#113#108#43#65#sper lig & tff first league & tff second league [n] third round#59#54#54#none#none [n] fourth round#32#32#27#5#sper lig [n] fifth round#16#16#16#none#none [n] group stage#8#8#8#none#none [n] semi - finals#4#4#4#none#none [n] final#2#2#2#none#none [n] 
03/24/2022 02:17:03 - INFO - __main__ - ['entailed']
03/24/2022 02:17:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 02:17:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 02:17:03 - INFO - __main__ - Printing 3 examples
03/24/2022 02:17:03 - INFO - __main__ -  [tab_fact] statement: w 48 - 3 be the result in the bryant - denny stadium tuscaloosa , al [SEP] table_caption: 2010 southeastern conference football season [SEP] table_text: date#time#visiting team#home team#site#broadcast#result#attendance [n] september 2#7:30 pm#southern miss#south carolina#williams - brice stadium columbia , sc#espn#w 41 - 13#70438 [n] september 4#12:00 pm#miami (oh)#4 florida#ben hill griffin stadium gainesville , fl#espn#w 34 - 12#90178 [n] september 4#12:21 pm#louisiana - lafayette#23 georgia#sanford stadium athens , ga#sec network#w 55 - 7#92746 [n] september 4#3:30 pm#kentucky#louisville#papa john 's cardinal stadium louisville , ky#abc#w 23 - 16#55327 [n] september 4#3:30 pm#jacksonville state#mississippi#vaught - hemingway stadium oxford , ms#css#l 48 - 49 2ot#55768 [n] september 4#6:00 pm#tennessee - martin#tennessee#neyland stadium knoxville , tn#ppv#w 50 - 0#99123 [n] september 4#7:00 pm#san jose state#1 alabama#bryant - denny stadium tuscaloosa , al#ppv#w 48 - 3#101821 [n] september 4#7:00 pm#arkansas state#22 auburn#jordan - hare stadium auburn , al#fsn south#w 52 - 26#83441 [n] september 4#7:00 pm#tennessee tech#17 arkansas#razorback stadium fayetteville , ar#ppv#w 44 - 3#69596 [n] september 4#7:00 pm#memphis#mississippi state#davis wade stadium starkville , ms#espnu#w 49 - 7#56032 [n] september 4#7:30 pm#northwestern#vanderbilt#vanderbilt stadium nashville , tn#css#l 21 - 23#37210 [n] 
03/24/2022 02:17:03 - INFO - __main__ - ['entailed']
03/24/2022 02:17:03 - INFO - __main__ -  [tab_fact] statement: the washington wizard have 8 loss in the 2009 - 10 season [SEP] table_caption: 2009 - 10 washington wizards season [SEP] table_text: game#date#team#score#high points#high rebounds#high assists#location attendance#record [n] 4#november 3#cleveland#l 90 - 102 (ot)#gilbert arenas , caron butler (22)#brendan haywood (9)#gilbert arenas (5)#quicken loans arena 20562#2 - 2 [n] 5#november 4#miami#l 89 - 93 (ot)#gilbert arenas (32)#brendan haywood (11)#gilbert arenas , mike miller & fabricio oberto (3)#verizon center 17413#2 - 3 [n] 6#november 6#indiana#l 86 - 102 (ot)#caron butler (24)#brendan haywood (19)#gilbert arenas (5)#conseco fieldhouse 14556#2 - 4 [n] 7#november 8#phoenix#l 90 - 102 (ot)#gilbert arenas & andray blatche (20)#brendan haywood (10)#gilbert arenas (6)#verizon center 14143#2 - 5 [n] 8#november 10#miami#l 76 - 90 (ot)#gilbert arenas (21)#brendan haywood (11)#gilbert arenas (8)#american airlines arena 15054#2 - 6 [n] 9#november 14#detroit#l 103 - 106 (ot)#mike miller , earl boykins (20)#andray blatche (11)#gilbert arenas (10)#verizon center 20173#2 - 7 [n] 10#november 18#cleveland#w 108 - 91 (ot)#antawn jamison (31)#brendan haywood (13)#gilbert arenas (8)#verizon center 20173#3 - 7 [n] 11#november 20#oklahoma city#l 108 - 127 (ot)#caron butler (24)#brendan haywood (16)#gilbert arenas (8)#ford center 18203#3 - 8 [n] 12#november 21#san antonio#l 84 - 106 (ot)#gilbert arenas (18)#brendan haywood (8)#earl boykins (4)#at&t center 16888#3 - 9 [n] 13#november 24#philadelphia#w 108 - 107 (ot)#antawn jamison (32)#antawn jamison (14)#gilbert arenas (8)#verizon center 14485#4 - 9 [n] 14#november 27#miami#w 94 - 84 (ot)#antawn jamison (24)#antawn jamison (13)#earl boykins (9)#american airlines arena 17684#5 - 9 [n] 
03/24/2022 02:17:03 - INFO - __main__ - ['entailed']
03/24/2022 02:17:03 - INFO - __main__ -  [tab_fact] statement: beau boulter represent the republican party [SEP] table_caption: united states house of representatives elections , 1988 [SEP] table_text: district#incumbent#party#first elected#result#candidates [n] texas 1#jim chapman#democratic#1985#re - elected#jim chapman (d) 62.2% horace mcqueen (r) 37.8% [n] texas 3#steve bartlett#republican#1982#re - elected#steve bartlett (r) 81.8% blake cowden (d) 18.2% [n] texas 8#jack fields#republican#1980#re - elected#jack fields (r) unopposed [n] texas 9#jack brooks#democratic#1952#re - elected#jack brooks (d) unopposed [n] texas 10#j j pickle#democratic#1963#re - elected#j j pickle (d) 93.4% vincent j may ( l ) 6.6% [n] texas 12#jim wright#democratic#1954#re - elected#jim wright (d) unopposed [n] texas 13#beau boulter#republican#1984#retired to run for u s senate democratic gain#bill sarpalius (d) 52.5% larry s milner (r) 47.5% [n] texas 16#ronald d coleman#democratic#1982#re - elected#ronald d coleman (d) unopposed [n] texas 17#charles stenholm#democratic#1978#re - elected#charles stenholm (d) unopposed [n] texas 19#larry combest#republican#1984#re - elected#larry combest (r) 67.7% gerald mccathern (d) 32.3% [n] texas 21#lamar s smith#republican#1986#re - elected#lamar s smith (r) 93.2% jim robinson ( l ) 6.8% [n] texas 24#martin frost#democratic#1978#re - elected#martin frost (d) 92.6% leo sadovy (r) 7.4% [n] texas 26#dick armey#republican#1984#re - elected#dick armey (r) 69.3% jo ann reyes (d) 30.7% [n] 
03/24/2022 02:17:03 - INFO - __main__ - ['entailed']
03/24/2022 02:17:03 - INFO - __main__ - Tokenizing Input ...
03/24/2022 02:17:03 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 02:17:03 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:17:03 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 02:17:22 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 02:17:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 02:17:23 - INFO - __main__ - Starting training!
03/24/2022 02:17:27 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:17:40 - INFO - __main__ - Loaded 12792 examples from test data
03/24/2022 02:25:50 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact/tab_fact_16_42_0.2_8_predictions.txt
03/24/2022 02:25:51 - INFO - __main__ - Classification-F1 on test data: 0.1616
03/24/2022 02:25:51 - INFO - __main__ - prefix=tab_fact_16_42, lr=0.2, bsz=8, dev_performance=0.4817813765182186, test_performance=0.16160706711121625
03/24/2022 02:25:51 - INFO - __main__ - Running ... prefix=tab_fact_16_87, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_cls2cls.py', '--local_rank=1', '--task_dir', 'data/tab_fact/', '--task_name', 'tab_fact', '--identifier', 'T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 26356
Killing subprocess 26357
++++++++++++++++++++++++++++++
kill: (26869): No such process
Task: anli, Checkpoint: models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:25:59 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:25:59 - INFO - __main__ - models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-anli
03/24/2022 02:25:59 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:25:59 - INFO - __main__ - models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-anli
03/24/2022 02:26:00 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:26:00 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:26:00 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:26:00 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:00 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:26:00 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:00 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
03/24/2022 02:26:00 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
03/24/2022 02:26:05 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 227, in <module>
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 208, in main
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_cls2cls.py', '--local_rank=1', '--task_dir', 'data/anli/', '--task_name', 'anli', '--identifier', 'T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-anli', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 26874
Killing subprocess 26875
++++++++++++++++++++++++++++++
kill: (26891): No such process
Task: ethos-race, Checkpoint: models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:26:11 - INFO - __main__ - Namespace(task_dir='data/ethos-race/', task_name='ethos-race', identifier='T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:11 - INFO - __main__ - models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-ethos-race
03/24/2022 02:26:11 - INFO - __main__ - Namespace(task_dir='data/ethos-race/', task_name='ethos-race', identifier='T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:11 - INFO - __main__ - models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-ethos-race
03/24/2022 02:26:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:26:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:26:11 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:26:11 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:11 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_16_100', 'ethos-race_16_13', 'ethos-race_16_21', 'ethos-race_16_42', 'ethos-race_16_87']
03/24/2022 02:26:11 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:26:11 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:11 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_16_100', 'ethos-race_16_13', 'ethos-race_16_21', 'ethos-race_16_42', 'ethos-race_16_87']
03/24/2022 02:26:18 - INFO - __main__ - Running ... prefix=ethos-race_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 208, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_cls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_cls2cls.py', '--local_rank=1', '--task_dir', 'data/ethos-race/', '--task_name', 'ethos-race', '--identifier', 'T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-cls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-cls2cls-3e-5-2-5000-5e-1/singletask-ethos-race', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 26896
Killing subprocess 26897
++++++++++++++++++++++++++++++
kill: (26913): No such process
Task: superglue-cb, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
03/24/2022 02:26:23 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:23 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-superglue-cb
Output directory () already exists and is not empty.
03/24/2022 02:26:23 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:23 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-superglue-cb
03/24/2022 02:26:24 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:26:24 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:26:24 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:26:24 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:24 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:26:24 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:24 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
03/24/2022 02:26:24 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
03/24/2022 02:26:29 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/added_tokens.json
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/superglue-cb/', '--task_name', 'superglue-cb', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-superglue-cb', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 26927
Killing subprocess 26928
++++++++++++++++++++++++++++++
kill: (26944): No such process
Task: dbpedia_14, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
03/24/2022 02:26:33 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:33 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14
Output directory () already exists and is not empty.
03/24/2022 02:26:33 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:33 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14
03/24/2022 02:26:34 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:26:34 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:26:34 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:26:34 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:34 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:26:34 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:34 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
03/24/2022 02:26:34 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
03/24/2022 02:26:39 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/special_tokens_map.json
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/dbpedia_14/', '--task_name', 'dbpedia_14', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 26949
Killing subprocess 26950
++++++++++++++++++++++++++++++
kill: (26966): No such process
Task: wiki_qa, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:26:45 - INFO - __main__ - Namespace(task_dir='data/wiki_qa/', task_name='wiki_qa', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:45 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-wiki_qa
03/24/2022 02:26:45 - INFO - __main__ - Namespace(task_dir='data/wiki_qa/', task_name='wiki_qa', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:45 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-wiki_qa
03/24/2022 02:26:45 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:26:45 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:26:45 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:26:45 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:45 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:26:45 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:45 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_16_100', 'wiki_qa_16_13', 'wiki_qa_16_21', 'wiki_qa_16_42', 'wiki_qa_16_87']
03/24/2022 02:26:45 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_16_100', 'wiki_qa_16_13', 'wiki_qa_16_21', 'wiki_qa_16_42', 'wiki_qa_16_87']
03/24/2022 02:26:54 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/wiki_qa/', '--task_name', 'wiki_qa', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-wiki_qa', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 26971
Killing subprocess 26972
++++++++++++++++++++++++++++++
kill: (26988): No such process
Task: emo, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:26:58 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:58 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-emo
03/24/2022 02:26:58 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:26:58 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-emo
03/24/2022 02:26:58 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:26:58 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:26:58 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:26:58 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:58 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:26:58 - INFO - __main__ - Using 2 gpus
03/24/2022 02:26:58 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/24/2022 02:26:58 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/24/2022 02:27:03 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/emo/', '--task_name', 'emo', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-emo', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 26993
Killing subprocess 26994
++++++++++++++++++++++++++++++
kill: (27010): No such process
Task: yelp_polarity, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:27:07 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:27:07 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity
03/24/2022 02:27:07 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:27:07 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity
03/24/2022 02:27:07 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:27:07 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:27:07 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:27:07 - INFO - __main__ - Using 2 gpus
03/24/2022 02:27:07 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:27:07 - INFO - __main__ - Using 2 gpus
03/24/2022 02:27:07 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/24/2022 02:27:07 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/24/2022 02:27:12 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/yelp_polarity/', '--task_name', 'yelp_polarity', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27015
Killing subprocess 27016
++++++++++++++++++++++++++++++
kill: (27032): No such process
Task: ethos-religion, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
03/24/2022 02:27:16 - INFO - __main__ - Namespace(task_dir='data/ethos-religion/', task_name='ethos-religion', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:27:16 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-religion
Output directory () already exists and is not empty.
03/24/2022 02:27:16 - INFO - __main__ - Namespace(task_dir='data/ethos-religion/', task_name='ethos-religion', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:27:16 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-religion
03/24/2022 02:27:18 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:27:18 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:27:18 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:27:18 - INFO - __main__ - Using 2 gpus
03/24/2022 02:27:18 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:27:18 - INFO - __main__ - Using 2 gpus
03/24/2022 02:27:18 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_16_100', 'ethos-religion_16_13', 'ethos-religion_16_21', 'ethos-religion_16_42', 'ethos-religion_16_87']
03/24/2022 02:27:18 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_16_100', 'ethos-religion_16_13', 'ethos-religion_16_21', 'ethos-religion_16_42', 'ethos-religion_16_87']
03/24/2022 02:27:24 - INFO - __main__ - Running ... prefix=ethos-religion_16_100, lr=0.5, bsz=8 ...
03/24/2022 02:27:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 02:27:26 - INFO - __main__ - Printing 3 examples
03/24/2022 02:27:26 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/24/2022 02:27:26 - INFO - __main__ - ['false']
03/24/2022 02:27:26 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/24/2022 02:27:26 - INFO - __main__ - ['false']
03/24/2022 02:27:26 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/24/2022 02:27:26 - INFO - __main__ - ['false']
03/24/2022 02:27:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 02:27:26 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:27:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 02:27:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 02:27:26 - INFO - __main__ - Printing 3 examples
03/24/2022 02:27:26 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/24/2022 02:27:26 - INFO - __main__ - ['false']
03/24/2022 02:27:26 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/24/2022 02:27:26 - INFO - __main__ - ['false']
03/24/2022 02:27:26 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/24/2022 02:27:26 - INFO - __main__ - ['false']
03/24/2022 02:27:26 - INFO - __main__ - Tokenizing Input ...
03/24/2022 02:27:26 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:27:26 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 02:27:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 02:27:27 - INFO - __main__ - Printing 3 examples
03/24/2022 02:27:27 - INFO - __main__ -  [ethos-religion] If I were your dad, I'd Google "Abortions at 30 years old mentally handicapped.
03/24/2022 02:27:27 - INFO - __main__ - ['false']
03/24/2022 02:27:27 - INFO - __main__ -  [ethos-religion] I liked macklemore,  not anymore , gays should fucking die motherfuckers cocksukers
03/24/2022 02:27:27 - INFO - __main__ - ['false']
03/24/2022 02:27:27 - INFO - __main__ -  [ethos-religion] an Asian Moe Howard piece of shit
03/24/2022 02:27:27 - INFO - __main__ - ['false']
03/24/2022 02:27:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 02:27:27 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:27:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/24/2022 02:27:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/24/2022 02:27:27 - INFO - __main__ - Printing 3 examples
03/24/2022 02:27:27 - INFO - __main__ -  [ethos-religion] Women!!! Grow a pair and refuse to compete with men!!! This is asinine!
03/24/2022 02:27:27 - INFO - __main__ - ['false']
03/24/2022 02:27:27 - INFO - __main__ -  [ethos-religion] Maybe it's because you're autistic?? Wish you death
03/24/2022 02:27:27 - INFO - __main__ - ['false']
03/24/2022 02:27:27 - INFO - __main__ -  [ethos-religion] all mental illnesses are awful and must be treated
03/24/2022 02:27:27 - INFO - __main__ - ['false']
03/24/2022 02:27:27 - INFO - __main__ - Tokenizing Input ...
03/24/2022 02:27:27 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:27:28 - INFO - __main__ - Loaded 32 examples from dev data
03/24/2022 02:27:44 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 02:27:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 02:27:45 - INFO - __main__ - Starting training!
03/24/2022 02:27:46 - INFO - __main__ - load prompt embedding from ckpt
03/24/2022 02:27:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/24/2022 02:27:55 - INFO - __main__ - Starting training!
03/24/2022 02:27:59 - INFO - __main__ - Step 10 Global step 10 Train loss 1.74 on epoch=4
03/24/2022 02:28:01 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=9
03/24/2022 02:28:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=14
03/24/2022 02:28:06 - INFO - __main__ - Step 40 Global step 40 Train loss 0.47 on epoch=19
03/24/2022 02:28:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=24
03/24/2022 02:28:09 - INFO - __main__ - Global step 50 Train loss 0.75 Classification-F1 0.3333333333333333 on epoch=24
03/24/2022 02:28:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/24/2022 02:28:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.43 on epoch=29
03/24/2022 02:28:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=34
03/24/2022 02:28:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=39
03/24/2022 02:28:19 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=44
03/24/2022 02:28:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
03/24/2022 02:28:22 - INFO - __main__ - Global step 100 Train loss 0.44 Classification-F1 0.5636363636363637 on epoch=49
03/24/2022 02:28:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5636363636363637 on epoch=49, global_step=100
03/24/2022 02:28:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=54
03/24/2022 02:28:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
03/24/2022 02:28:29 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=64
03/24/2022 02:28:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=69
03/24/2022 02:28:34 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
03/24/2022 02:28:34 - INFO - __main__ - Global step 150 Train loss 0.35 Classification-F1 0.49090909090909085 on epoch=74
03/24/2022 02:28:37 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=79
03/24/2022 02:28:39 - INFO - __main__ - Step 170 Global step 170 Train loss 0.22 on epoch=84
03/24/2022 02:28:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.21 on epoch=89
03/24/2022 02:28:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.19 on epoch=94
03/24/2022 02:28:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.13 on epoch=99
03/24/2022 02:28:47 - INFO - __main__ - Global step 200 Train loss 0.22 Classification-F1 0.7046153846153846 on epoch=99
03/24/2022 02:28:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5636363636363637 -> 0.7046153846153846 on epoch=99, global_step=200
03/24/2022 02:28:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.06 on epoch=104
03/24/2022 02:28:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.05 on epoch=109
03/24/2022 02:28:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.03 on epoch=114
03/24/2022 02:28:57 - INFO - __main__ - Step 240 Global step 240 Train loss 0.02 on epoch=119
03/24/2022 02:28:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.01 on epoch=124
03/24/2022 02:29:00 - INFO - __main__ - Global step 250 Train loss 0.04 Classification-F1 0.9372549019607843 on epoch=124
03/24/2022 02:29:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7046153846153846 -> 0.9372549019607843 on epoch=124, global_step=250
03/24/2022 02:29:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.01 on epoch=129
03/24/2022 02:29:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.03 on epoch=134
03/24/2022 02:29:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.01 on epoch=139
03/24/2022 02:29:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.01 on epoch=144
03/24/2022 02:29:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.00 on epoch=149
03/24/2022 02:29:13 - INFO - __main__ - Global step 300 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=149
03/24/2022 02:29:13 - INFO - __main__ - Saving model with best Classification-F1: 0.9372549019607843 -> 0.9687194525904204 on epoch=149, global_step=300
03/24/2022 02:29:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.01 on epoch=154
03/24/2022 02:29:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.01 on epoch=159
03/24/2022 02:29:20 - INFO - __main__ - Step 330 Global step 330 Train loss 0.01 on epoch=164
03/24/2022 02:29:22 - INFO - __main__ - Step 340 Global step 340 Train loss 0.01 on epoch=169
03/24/2022 02:29:25 - INFO - __main__ - Step 350 Global step 350 Train loss 0.00 on epoch=174
03/24/2022 02:29:25 - INFO - __main__ - Global step 350 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=174
03/24/2022 02:29:28 - INFO - __main__ - Step 360 Global step 360 Train loss 0.00 on epoch=179
03/24/2022 02:29:30 - INFO - __main__ - Step 370 Global step 370 Train loss 0.01 on epoch=184
03/24/2022 02:29:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.00 on epoch=189
03/24/2022 02:29:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.01 on epoch=194
03/24/2022 02:29:37 - INFO - __main__ - Step 400 Global step 400 Train loss 0.05 on epoch=199
03/24/2022 02:29:38 - INFO - __main__ - Global step 400 Train loss 0.01 Classification-F1 0.9375 on epoch=199
03/24/2022 02:29:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.01 on epoch=204
03/24/2022 02:29:43 - INFO - __main__ - Step 420 Global step 420 Train loss 0.00 on epoch=209
03/24/2022 02:29:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.00 on epoch=214
03/24/2022 02:29:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.00 on epoch=219
03/24/2022 02:29:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.00 on epoch=224
03/24/2022 02:29:51 - INFO - __main__ - Global step 450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=224
03/24/2022 02:29:53 - INFO - __main__ - Step 460 Global step 460 Train loss 0.00 on epoch=229
03/24/2022 02:29:56 - INFO - __main__ - Step 470 Global step 470 Train loss 0.00 on epoch=234
03/24/2022 02:29:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.03 on epoch=239
03/24/2022 02:30:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.00 on epoch=244
03/24/2022 02:30:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.00 on epoch=249
03/24/2022 02:30:03 - INFO - __main__ - Global step 500 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=249
03/24/2022 02:30:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.00 on epoch=254
03/24/2022 02:30:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.00 on epoch=259
03/24/2022 02:30:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.00 on epoch=264
03/24/2022 02:30:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.00 on epoch=269
03/24/2022 02:30:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.00 on epoch=274
03/24/2022 02:30:16 - INFO - __main__ - Global step 550 Train loss 0.00 Classification-F1 0.9375 on epoch=274
03/24/2022 02:30:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.00 on epoch=279
03/24/2022 02:30:21 - INFO - __main__ - Step 570 Global step 570 Train loss 0.00 on epoch=284
03/24/2022 02:30:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.00 on epoch=289
03/24/2022 02:30:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.00 on epoch=294
03/24/2022 02:30:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.00 on epoch=299
03/24/2022 02:30:29 - INFO - __main__ - Global step 600 Train loss 0.00 Classification-F1 0.9375 on epoch=299
03/24/2022 02:30:31 - INFO - __main__ - Step 610 Global step 610 Train loss 0.04 on epoch=304
03/24/2022 02:30:33 - INFO - __main__ - Step 620 Global step 620 Train loss 0.00 on epoch=309
03/24/2022 02:30:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.00 on epoch=314
03/24/2022 02:30:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.00 on epoch=319
03/24/2022 02:30:41 - INFO - __main__ - Step 650 Global step 650 Train loss 0.00 on epoch=324
03/24/2022 02:30:41 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=324
03/24/2022 02:30:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/24/2022 02:30:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.00 on epoch=334
03/24/2022 02:30:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.00 on epoch=339
03/24/2022 02:30:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.00 on epoch=344
03/24/2022 02:30:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/24/2022 02:30:54 - INFO - __main__ - Global step 700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=349
03/24/2022 02:30:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/24/2022 02:30:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/24/2022 02:31:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/24/2022 02:31:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/24/2022 02:31:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.03 on epoch=374
03/24/2022 02:31:07 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=374
03/24/2022 02:31:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/24/2022 02:31:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/24/2022 02:31:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/24/2022 02:31:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/24/2022 02:31:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/24/2022 02:31:20 - INFO - __main__ - Global step 800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=399
03/24/2022 02:31:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/24/2022 02:31:25 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/24/2022 02:31:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/24/2022 02:31:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/24/2022 02:31:32 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/24/2022 02:31:33 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/24/2022 02:31:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/24/2022 02:31:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/24/2022 02:31:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
03/24/2022 02:31:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/24/2022 02:31:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/24/2022 02:31:46 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9375 on epoch=449
03/24/2022 02:31:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/24/2022 02:31:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/24/2022 02:31:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/24/2022 02:31:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/24/2022 02:31:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/24/2022 02:31:59 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=474
03/24/2022 02:32:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/24/2022 02:32:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/24/2022 02:32:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/24/2022 02:32:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/24/2022 02:32:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/24/2022 02:32:12 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9375 on epoch=499
03/24/2022 02:32:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/24/2022 02:32:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/24/2022 02:32:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/24/2022 02:32:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/24/2022 02:32:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/24/2022 02:32:25 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9375 on epoch=524
03/24/2022 02:32:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/24/2022 02:32:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.04 on epoch=534
03/24/2022 02:32:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/24/2022 02:32:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/24/2022 02:32:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/24/2022 02:32:38 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.906158357771261 on epoch=549
03/24/2022 02:32:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/24/2022 02:32:43 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.02 on epoch=559
03/24/2022 02:32:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/24/2022 02:32:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/24/2022 02:32:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/24/2022 02:32:51 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9375 on epoch=574
03/24/2022 02:32:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/24/2022 02:32:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/24/2022 02:32:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/24/2022 02:33:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/24/2022 02:33:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/24/2022 02:33:04 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9375 on epoch=599
03/24/2022 02:33:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/24/2022 02:33:09 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/24/2022 02:33:11 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/24/2022 02:33:14 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/24/2022 02:33:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/24/2022 02:33:17 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9375 on epoch=624
03/24/2022 02:33:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/24/2022 02:33:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/24/2022 02:33:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/24/2022 02:33:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/24/2022 02:33:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/24/2022 02:33:30 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9375 on epoch=649
03/24/2022 02:33:33 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/24/2022 02:33:35 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/24/2022 02:33:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/24/2022 02:33:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/24/2022 02:33:43 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/24/2022 02:33:44 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9375 on epoch=674
03/24/2022 02:33:46 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/24/2022 02:33:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/24/2022 02:33:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/24/2022 02:33:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/24/2022 02:33:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/24/2022 02:33:57 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9375 on epoch=699
03/24/2022 02:33:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/24/2022 02:34:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/24/2022 02:34:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/24/2022 02:34:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/24/2022 02:34:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/24/2022 02:34:10 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9375 on epoch=724
03/24/2022 02:34:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/24/2022 02:34:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/24/2022 02:34:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/24/2022 02:34:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/24/2022 02:34:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/24/2022 02:34:23 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9375 on epoch=749
03/24/2022 02:34:25 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/24/2022 02:34:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/24/2022 02:34:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/24/2022 02:34:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/24/2022 02:34:35 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/24/2022 02:34:35 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9375 on epoch=774
03/24/2022 02:34:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/24/2022 02:34:40 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/24/2022 02:34:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/24/2022 02:34:45 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/24/2022 02:34:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/24/2022 02:34:48 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9375 on epoch=799
03/24/2022 02:34:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/24/2022 02:34:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/24/2022 02:34:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/24/2022 02:34:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/24/2022 02:35:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/24/2022 02:35:01 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9375 on epoch=824
03/24/2022 02:35:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/24/2022 02:35:06 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/24/2022 02:35:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/24/2022 02:35:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/24/2022 02:35:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/24/2022 02:35:14 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9375 on epoch=849
03/24/2022 02:35:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/24/2022 02:35:18 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/24/2022 02:35:21 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/24/2022 02:35:23 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/24/2022 02:35:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/24/2022 02:35:26 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9375 on epoch=874
03/24/2022 02:35:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/24/2022 02:35:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/24/2022 02:35:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/24/2022 02:35:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/24/2022 02:35:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/24/2022 02:35:39 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9375 on epoch=899
03/24/2022 02:35:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/24/2022 02:35:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/24/2022 02:35:46 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/24/2022 02:35:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/24/2022 02:35:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/24/2022 02:35:52 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.906158357771261 on epoch=924
03/24/2022 02:35:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/24/2022 02:35:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/24/2022 02:35:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/24/2022 02:36:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/24/2022 02:36:04 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/24/2022 02:36:04 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9375 on epoch=949
03/24/2022 02:36:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/24/2022 02:36:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/24/2022 02:36:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/24/2022 02:36:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/24/2022 02:36:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/24/2022 02:36:17 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9375 on epoch=974
03/24/2022 02:36:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/24/2022 02:36:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/24/2022 02:36:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/24/2022 02:36:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/24/2022 02:36:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
03/24/2022 02:36:29 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9375 on epoch=999
03/24/2022 02:36:29 - INFO - __main__ - save last model!
03/24/2022 02:36:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/24/2022 02:36:29 - INFO - __main__ - Start tokenizing ... 87 instances
03/24/2022 02:36:29 - INFO - __main__ - Printing 3 examples
03/24/2022 02:36:29 - INFO - __main__ -  [ethos-religion] Hopefully the non-white parasites will flee to a new host nation. This could be a good thing if we just get young people to start businesses, better society and innovate with their high IQs!
03/24/2022 02:36:29 - INFO - __main__ - ['false']
03/24/2022 02:36:29 - INFO - __main__ -  [ethos-religion] Looks like the gay poster boy with syphilis
03/24/2022 02:36:29 - INFO - __main__ - ['false']
03/24/2022 02:36:29 - INFO - __main__ -  [ethos-religion] Jewish piece of shits like him should get hanged
03/24/2022 02:36:29 - INFO - __main__ - ['true']
03/24/2022 02:36:29 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/24/2022 02:36:29 - INFO - __main__ - Tokenizing Output ...
03/24/2022 02:36:29 - INFO - __main__ - Loaded 87 examples from test data
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/ethos-religion/', '--task_name', 'ethos-religion', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-religion', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27037
Killing subprocess 27038
++++++++++++++++++++++++++++++
kill: (27469): No such process
Task: amazon_polarity, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:36:34 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:36:34 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity
03/24/2022 02:36:34 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:36:34 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity
03/24/2022 02:36:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:36:35 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:36:35 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:36:35 - INFO - __main__ - Using 2 gpus
03/24/2022 02:36:35 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:36:35 - INFO - __main__ - Using 2 gpus
03/24/2022 02:36:35 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
03/24/2022 02:36:35 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
03/24/2022 02:36:40 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/amazon_polarity/', '--task_name', 'amazon_polarity', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27474
Killing subprocess 27475
++++++++++++++++++++++++++++++
kill: (27491): No such process
Task: tab_fact, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:36:44 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:36:44 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
03/24/2022 02:36:44 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:36:44 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-tab_fact
03/24/2022 02:36:44 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:36:44 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:36:44 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:36:44 - INFO - __main__ - Using 2 gpus
03/24/2022 02:36:44 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:36:44 - INFO - __main__ - Using 2 gpus
03/24/2022 02:36:44 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
03/24/2022 02:36:44 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
03/24/2022 02:36:52 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/tab_fact/', '--task_name', 'tab_fact', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-tab_fact', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27496
Killing subprocess 27497
++++++++++++++++++++++++++++++
kill: (27513): No such process
Task: anli, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
03/24/2022 02:36:57 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:36:57 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-anli
Output directory () already exists and is not empty.
03/24/2022 02:36:57 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:36:57 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-anli
03/24/2022 02:36:58 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:36:58 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:36:58 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:36:58 - INFO - __main__ - Using 2 gpus
03/24/2022 02:36:58 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
03/24/2022 02:36:58 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:36:58 - INFO - __main__ - Using 2 gpus
03/24/2022 02:36:58 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
03/24/2022 02:37:03 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/added_tokens.json
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/added_tokens.json
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/anli/', '--task_name', 'anli', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-anli', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27518
Killing subprocess 27519
++++++++++++++++++++++++++++++
kill: (27535): No such process
Task: ethos-race, Checkpoint: models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1
03/24/2022 02:37:08 - INFO - __main__ - Namespace(task_dir='data/ethos-race/', task_name='ethos-race', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:08 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-race
Output directory () already exists and is not empty.
03/24/2022 02:37:08 - INFO - __main__ - Namespace(task_dir='data/ethos-race/', task_name='ethos-race', identifier='T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:08 - INFO - __main__ - models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-race
03/24/2022 02:37:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:37:09 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:37:09 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:37:09 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:37:09 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:09 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:09 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_16_100', 'ethos-race_16_13', 'ethos-race_16_21', 'ethos-race_16_42', 'ethos-race_16_87']
03/24/2022 02:37:09 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_16_100', 'ethos-race_16_13', 'ethos-race_16_21', 'ethos-race_16_42', 'ethos-race_16_87']
03/24/2022 02:37:14 - INFO - __main__ - Running ... prefix=ethos-race_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_nocls2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
      File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
        r.raise_for_status()output_path = get_from_cache(

  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_nocls2cls.py', '--local_rank=1', '--task_dir', 'data/ethos-race/', '--task_name', 'ethos-race', '--identifier', 'T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-nocls2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-nocls2cls-3e-5-2-5000-5e-1/singletask-ethos-race', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27540
Killing subprocess 27541
++++++++++++++++++++++++++++++
kill: (27557): No such process
Task: superglue-cb, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
03/24/2022 02:37:18 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:18 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-superglue-cb
Output directory () already exists and is not empty.
03/24/2022 02:37:18 - INFO - __main__ - Namespace(task_dir='data/superglue-cb/', task_name='superglue-cb', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-superglue-cb', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:18 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-superglue-cb
03/24/2022 02:37:20 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:37:20 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:37:20 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:37:20 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:20 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:37:20 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:20 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
03/24/2022 02:37:20 - INFO - __main__ - Fine-tuning the following samples: ['superglue-cb_16_100', 'superglue-cb_16_13', 'superglue-cb_16_21', 'superglue-cb_16_42', 'superglue-cb_16_87']
03/24/2022 02:37:25 - INFO - __main__ - Running ... prefix=superglue-cb_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
      File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/superglue-cb/', '--task_name', 'superglue-cb', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-superglue-cb', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27571
Killing subprocess 27572
++++++++++++++++++++++++++++++
kill: (27588): No such process
Task: dbpedia_14, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
03/24/2022 02:37:30 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:30 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14
Output directory () already exists and is not empty.
03/24/2022 02:37:30 - INFO - __main__ - Namespace(task_dir='data/dbpedia_14/', task_name='dbpedia_14', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:30 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14
03/24/2022 02:37:31 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:37:31 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:37:31 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:37:31 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:31 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:37:31 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:31 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
03/24/2022 02:37:31 - INFO - __main__ - Fine-tuning the following samples: ['dbpedia_14_16_100', 'dbpedia_14_16_13', 'dbpedia_14_16_21', 'dbpedia_14_16_42', 'dbpedia_14_16_87']
03/24/2022 02:37:36 - INFO - __main__ - Running ... prefix=dbpedia_14_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/dbpedia_14/', '--task_name', 'dbpedia_14', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-dbpedia_14', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27593
Killing subprocess 27594
++++++++++++++++++++++++++++++
kill: (27610): No such process
Task: wiki_qa, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:37:40 - INFO - __main__ - Namespace(task_dir='data/wiki_qa/', task_name='wiki_qa', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:40 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-wiki_qa
03/24/2022 02:37:40 - INFO - __main__ - Namespace(task_dir='data/wiki_qa/', task_name='wiki_qa', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-wiki_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:40 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-wiki_qa
03/24/2022 02:37:41 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:37:41 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:37:41 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:37:41 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:41 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_16_100', 'wiki_qa_16_13', 'wiki_qa_16_21', 'wiki_qa_16_42', 'wiki_qa_16_87']
03/24/2022 02:37:41 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:37:41 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:41 - INFO - __main__ - Fine-tuning the following samples: ['wiki_qa_16_100', 'wiki_qa_16_13', 'wiki_qa_16_21', 'wiki_qa_16_42', 'wiki_qa_16_87']
03/24/2022 02:37:47 - INFO - __main__ - Running ... prefix=wiki_qa_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/wiki_qa/', '--task_name', 'wiki_qa', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-wiki_qa', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27615
Killing subprocess 27616
++++++++++++++++++++++++++++++
kill: (27632): No such process
Task: emo, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:37:52 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:52 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-emo
03/24/2022 02:37:52 - INFO - __main__ - Namespace(task_dir='data/emo/', task_name='emo', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-emo', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:37:52 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-emo
03/24/2022 02:37:52 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:37:52 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:37:52 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:37:52 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:52 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:37:52 - INFO - __main__ - Using 2 gpus
03/24/2022 02:37:52 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/24/2022 02:37:52 - INFO - __main__ - Fine-tuning the following samples: ['emo_16_100', 'emo_16_13', 'emo_16_21', 'emo_16_42', 'emo_16_87']
03/24/2022 02:37:57 - INFO - __main__ - Running ... prefix=emo_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
      File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model    
resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/emo/', '--task_name', 'emo', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-emo', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27637
Killing subprocess 27638
++++++++++++++++++++++++++++++
kill: (27654): No such process
Task: yelp_polarity, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:38:01 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:01 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity
03/24/2022 02:38:01 - INFO - __main__ - Namespace(task_dir='data/yelp_polarity/', task_name='yelp_polarity', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:01 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity
03/24/2022 02:38:02 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:38:02 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:38:02 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:38:02 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:02 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:38:02 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:02 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/24/2022 02:38:02 - INFO - __main__ - Fine-tuning the following samples: ['yelp_polarity_16_100', 'yelp_polarity_16_13', 'yelp_polarity_16_21', 'yelp_polarity_16_42', 'yelp_polarity_16_87']
03/24/2022 02:38:07 - INFO - __main__ - Running ... prefix=yelp_polarity_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/yelp_polarity/', '--task_name', 'yelp_polarity', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-yelp_polarity', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27659
Killing subprocess 27660
++++++++++++++++++++++++++++++
kill: (27676): No such process
Task: ethos-religion, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:38:11 - INFO - __main__ - Namespace(task_dir='data/ethos-religion/', task_name='ethos-religion', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:11 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-religion
03/24/2022 02:38:11 - INFO - __main__ - Namespace(task_dir='data/ethos-religion/', task_name='ethos-religion', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-religion', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:11 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-religion
03/24/2022 02:38:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:38:11 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:38:11 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:38:11 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:11 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:38:11 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:11 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_16_100', 'ethos-religion_16_13', 'ethos-religion_16_21', 'ethos-religion_16_42', 'ethos-religion_16_87']
03/24/2022 02:38:11 - INFO - __main__ - Fine-tuning the following samples: ['ethos-religion_16_100', 'ethos-religion_16_13', 'ethos-religion_16_21', 'ethos-religion_16_42', 'ethos-religion_16_87']
03/24/2022 02:38:18 - INFO - __main__ - Running ... prefix=ethos-religion_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/added_tokens.json
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/ethos-religion/', '--task_name', 'ethos-religion', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-religion', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27681
Killing subprocess 27682
++++++++++++++++++++++++++++++
kill: (27698): No such process
Task: amazon_polarity, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:38:23 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:23 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity
03/24/2022 02:38:23 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:23 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity
03/24/2022 02:38:24 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:38:24 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:38:24 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:38:24 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:24 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
03/24/2022 02:38:24 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:38:24 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:24 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
03/24/2022 02:38:29 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/amazon_polarity/', '--task_name', 'amazon_polarity', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-amazon_polarity', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27703
Killing subprocess 27704
++++++++++++++++++++++++++++++
kill: (27720): No such process
Task: tab_fact, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:38:33 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:33 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-tab_fact
03/24/2022 02:38:33 - INFO - __main__ - Namespace(task_dir='data/tab_fact/', task_name='tab_fact', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-tab_fact', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:33 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-tab_fact
03/24/2022 02:38:34 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:38:34 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:38:34 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:38:34 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:38:34 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:34 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:34 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
03/24/2022 02:38:34 - INFO - __main__ - Fine-tuning the following samples: ['tab_fact_16_100', 'tab_fact_16_13', 'tab_fact_16_21', 'tab_fact_16_42', 'tab_fact_16_87']
03/24/2022 02:38:39 - INFO - __main__ - Running ... prefix=tab_fact_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    resolved_vocab_files[file_id] = cached_path(    
output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/tab_fact/', '--task_name', 'tab_fact', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-tab_fact', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27725
Killing subprocess 27726
++++++++++++++++++++++++++++++
kill: (27742): No such process
Task: anli, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:38:43 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:43 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-anli
03/24/2022 02:38:43 - INFO - __main__ - Namespace(task_dir='data/anli/', task_name='anli', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-anli', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:43 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-anli
03/24/2022 02:38:43 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:38:43 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:38:43 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:38:43 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:43 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
03/24/2022 02:38:43 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:38:43 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:43 - INFO - __main__ - Fine-tuning the following samples: ['anli_16_100', 'anli_16_13', 'anli_16_21', 'anli_16_42', 'anli_16_87']
03/24/2022 02:38:53 - INFO - __main__ - Running ... prefix=anli_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/anli/', '--task_name', 'anli', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-anli', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27747
Killing subprocess 27748
++++++++++++++++++++++++++++++
kill: (27781): No such process
Task: ethos-race, Checkpoint: models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-both2cls-3e-5-2-5000-5e-1
Output directory () already exists and is not empty.
03/24/2022 02:38:57 - INFO - __main__ - Namespace(task_dir='data/ethos-race/', task_name='ethos-race', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:57 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-race
03/24/2022 02:38:57 - INFO - __main__ - Namespace(task_dir='data/ethos-race/', task_name='ethos-race', identifier='T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-race', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='0,1')
03/24/2022 02:38:57 - INFO - __main__ - models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-race
03/24/2022 02:38:57 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
03/24/2022 02:38:57 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
03/24/2022 02:38:57 - INFO - __main__ - args.device: cuda:0
03/24/2022 02:38:57 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:57 - INFO - __main__ - args.device: cuda:1
03/24/2022 02:38:57 - INFO - __main__ - Using 2 gpus
03/24/2022 02:38:57 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_16_100', 'ethos-race_16_13', 'ethos-race_16_21', 'ethos-race_16_42', 'ethos-race_16_87']
03/24/2022 02:38:57 - INFO - __main__ - Fine-tuning the following samples: ['ethos-race_16_100', 'ethos-race_16_13', 'ethos-race_16_21', 'ethos-race_16_42', 'ethos-race_16_87']
03/24/2022 02:39:02 - INFO - __main__ - Running ... prefix=ethos-race_16_100, lr=0.5, bsz=8 ...
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/spiece.model
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_fomaml_both2cls.py", line 208, in main
    dev_performance, test_performance = run(args, logger)
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/run_singletask_ddp_prompt.py", line 27, in run
    tokenizer = T5Tokenizer.from_pretrained(args.model, cache_dir=args.cache_dir)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1694, in from_pretrained
    raise err
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py", line 1672, in from_pretrained
    resolved_vocab_files[file_id] = cached_path(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1329, in cached_path
    output_path = get_from_cache(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/file_utils.py", line 1500, in get_from_cache
    r.raise_for_status()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/requests/models.py", line 960, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/google/t5-v1_1-large/resolve/main/added_tokens.json
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/envs/meta/bin/python', '-u', 'singletask_from_fomaml_both2cls.py', '--local_rank=1', '--task_dir', 'data/ethos-race/', '--task_name', 'ethos-race', '--identifier', 'T5-large-fomaml-both2cls-3e-5-2-5000-5e-1', '--checkpoint', 'models/upstream-fomaml-both2cls-3e-5-2-5000-5e-1/last-model.pt', '--do_train', '--do_predict', '--learning_rate_list', '5e-1', '4e-1', '3e-1', '2e-1', '--bsz_list', '8', '--predict_batch_size', '16', '--total_steps', '3000', '--eval_period', '50', '--warmup_steps', '50', '--num_train_epochs', '1000.0', '--gradient_accumulation_steps', '1', '--output_dir', 'models/T5-large-fomaml-both2cls-3e-5-2-5000-5e-1/singletask-ethos-race', '--cuda', '0,1', '--lm_adapted_path', '/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', '--model', 'google/t5-v1_1-large', '--prompt_number', '100']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 27789
Killing subprocess 27790
++++++++++++++++++++++++++++++
kill: (27825): No such process
