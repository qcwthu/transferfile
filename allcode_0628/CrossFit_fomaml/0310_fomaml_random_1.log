nohup: ignoring input
Task: quoref, Checkpoint: models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_fomaml_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24568
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_w0puy887/none_n3oiw5ew
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24568
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_w0puy887/none_n3oiw5ew/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_w0puy887/none_n3oiw5ew/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/10/2022 15:18:27 - INFO - __main__ - Namespace(task_dir='data/quoref/', task_name='quoref', identifier='T5-large-fomaml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/10/2022 15:18:27 - INFO - __main__ - models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref
03/10/2022 15:18:27 - INFO - __main__ - Namespace(task_dir='data/quoref/', task_name='quoref', identifier='T5-large-fomaml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/10/2022 15:18:27 - INFO - __main__ - models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref
03/10/2022 15:18:27 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/10/2022 15:18:27 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/10/2022 15:18:27 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/10/2022 15:18:27 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/10/2022 15:18:27 - INFO - __main__ - args.device: cuda:0
03/10/2022 15:18:27 - INFO - __main__ - Using 2 gpus
03/10/2022 15:18:27 - INFO - __main__ - args.device: cuda:1
03/10/2022 15:18:27 - INFO - __main__ - Fine-tuning the following samples: ['quoref_32_100', 'quoref_32_13', 'quoref_32_21', 'quoref_32_42', 'quoref_32_87']
03/10/2022 15:18:27 - INFO - __main__ - Using 2 gpus
03/10/2022 15:18:27 - INFO - __main__ - Fine-tuning the following samples: ['quoref_32_100', 'quoref_32_13', 'quoref_32_21', 'quoref_32_42', 'quoref_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/10/2022 15:18:32 - INFO - __main__ - Running ... prefix=quoref_32_100, lr=0.5, bsz=8 ...
03/10/2022 15:18:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:18:33 - INFO - __main__ - Printing 3 examples
03/10/2022 15:18:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/10/2022 15:18:33 - INFO - __main__ - Printing 3 examples
03/10/2022 15:18:33 - INFO - __main__ - ['Ringo Starr']
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/10/2022 15:18:33 - INFO - __main__ - ['Ringo Starr']
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/10/2022 15:18:33 - INFO - __main__ - ['Jimmy']
03/10/2022 15:18:33 - INFO - __main__ - ['Jimmy']
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/10/2022 15:18:33 - INFO - __main__ - ['The Alan Bush Music Trust']
03/10/2022 15:18:33 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:18:33 - INFO - __main__ - ['The Alan Bush Music Trust']
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 15:18:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 15:18:33 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:18:33 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:18:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:18:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:18:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:18:33 - INFO - __main__ - Printing 3 examples
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/10/2022 15:18:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:18:33 - INFO - __main__ - ['Elk', 'pronghorn']
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/10/2022 15:18:33 - INFO - __main__ - Printing 3 examples
03/10/2022 15:18:33 - INFO - __main__ - ['Jeff']
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/10/2022 15:18:33 - INFO - __main__ - ['November 1991']
03/10/2022 15:18:33 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:18:33 - INFO - __main__ - ['Elk', 'pronghorn']
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/10/2022 15:18:33 - INFO - __main__ - ['Jeff']
03/10/2022 15:18:33 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/10/2022 15:18:33 - INFO - __main__ - ['November 1991']
03/10/2022 15:18:33 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:18:33 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:18:33 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:18:33 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:18:33 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:19:24 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 15:19:24 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 15:19:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 15:19:25 - INFO - __main__ - Starting training!
03/10/2022 15:19:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 15:19:30 - INFO - __main__ - Starting training!
03/10/2022 15:19:36 - INFO - __main__ - Step 10 Global step 10 Train loss 3.11 on epoch=4
03/10/2022 15:19:40 - INFO - __main__ - Step 20 Global step 20 Train loss 2.47 on epoch=9
03/10/2022 15:19:44 - INFO - __main__ - Step 30 Global step 30 Train loss 2.25 on epoch=14
03/10/2022 15:19:48 - INFO - __main__ - Step 40 Global step 40 Train loss 2.78 on epoch=19
03/10/2022 15:19:53 - INFO - __main__ - Step 50 Global step 50 Train loss 2.69 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/10/2022 15:19:54 - INFO - __main__ - Global step 50 Train loss 2.66 QA-F1 0.1875 on epoch=24
03/10/2022 15:19:54 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.1875 on epoch=24, global_step=50
03/10/2022 15:19:59 - INFO - __main__ - Step 60 Global step 60 Train loss 2.57 on epoch=29
03/10/2022 15:20:03 - INFO - __main__ - Step 70 Global step 70 Train loss 2.45 on epoch=34
03/10/2022 15:20:07 - INFO - __main__ - Step 80 Global step 80 Train loss 2.57 on epoch=39
03/10/2022 15:20:12 - INFO - __main__ - Step 90 Global step 90 Train loss 2.37 on epoch=44
03/10/2022 15:20:16 - INFO - __main__ - Step 100 Global step 100 Train loss 2.48 on epoch=49
03/10/2022 15:20:18 - INFO - __main__ - Global step 100 Train loss 2.49 QA-F1 0.1875 on epoch=49
03/10/2022 15:20:22 - INFO - __main__ - Step 110 Global step 110 Train loss 2.42 on epoch=54
03/10/2022 15:20:26 - INFO - __main__ - Step 120 Global step 120 Train loss 2.34 on epoch=59
03/10/2022 15:20:31 - INFO - __main__ - Step 130 Global step 130 Train loss 2.19 on epoch=64
03/10/2022 15:20:35 - INFO - __main__ - Step 140 Global step 140 Train loss 2.16 on epoch=69
03/10/2022 15:20:39 - INFO - __main__ - Step 150 Global step 150 Train loss 2.21 on epoch=74
03/10/2022 15:20:41 - INFO - __main__ - Global step 150 Train loss 2.27 QA-F1 0.15625 on epoch=74
03/10/2022 15:20:45 - INFO - __main__ - Step 160 Global step 160 Train loss 2.29 on epoch=79
03/10/2022 15:20:49 - INFO - __main__ - Step 170 Global step 170 Train loss 2.02 on epoch=84
03/10/2022 15:20:54 - INFO - __main__ - Step 180 Global step 180 Train loss 2.10 on epoch=89
03/10/2022 15:20:58 - INFO - __main__ - Step 190 Global step 190 Train loss 2.15 on epoch=94
03/10/2022 15:21:02 - INFO - __main__ - Step 200 Global step 200 Train loss 1.85 on epoch=99
03/10/2022 15:21:04 - INFO - __main__ - Global step 200 Train loss 2.08 QA-F1 0.28125 on epoch=99
03/10/2022 15:21:04 - INFO - __main__ - Saving model with best QA-F1: 0.1875 -> 0.28125 on epoch=99, global_step=200
03/10/2022 15:21:08 - INFO - __main__ - Step 210 Global step 210 Train loss 1.86 on epoch=104
03/10/2022 15:21:12 - INFO - __main__ - Step 220 Global step 220 Train loss 1.95 on epoch=109
03/10/2022 15:21:17 - INFO - __main__ - Step 230 Global step 230 Train loss 1.89 on epoch=114
03/10/2022 15:21:21 - INFO - __main__ - Step 240 Global step 240 Train loss 1.74 on epoch=119
03/10/2022 15:21:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.86 on epoch=124
03/10/2022 15:21:27 - INFO - __main__ - Global step 250 Train loss 1.86 QA-F1 0.1875 on epoch=124
03/10/2022 15:21:31 - INFO - __main__ - Step 260 Global step 260 Train loss 1.80 on epoch=129
03/10/2022 15:21:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.80 on epoch=134
03/10/2022 15:21:40 - INFO - __main__ - Step 280 Global step 280 Train loss 1.66 on epoch=139
03/10/2022 15:21:44 - INFO - __main__ - Step 290 Global step 290 Train loss 1.71 on epoch=144
03/10/2022 15:21:48 - INFO - __main__ - Step 300 Global step 300 Train loss 1.75 on epoch=149
03/10/2022 15:21:50 - INFO - __main__ - Global step 300 Train loss 1.75 QA-F1 0.25 on epoch=149
03/10/2022 15:21:54 - INFO - __main__ - Step 310 Global step 310 Train loss 1.77 on epoch=154
03/10/2022 15:21:58 - INFO - __main__ - Step 320 Global step 320 Train loss 1.68 on epoch=159
03/10/2022 15:22:03 - INFO - __main__ - Step 330 Global step 330 Train loss 1.70 on epoch=164
03/10/2022 15:22:07 - INFO - __main__ - Step 340 Global step 340 Train loss 1.54 on epoch=169
03/10/2022 15:22:11 - INFO - __main__ - Step 350 Global step 350 Train loss 1.64 on epoch=174
03/10/2022 15:22:13 - INFO - __main__ - Global step 350 Train loss 1.67 QA-F1 0.22916666666666666 on epoch=174
03/10/2022 15:22:17 - INFO - __main__ - Step 360 Global step 360 Train loss 1.63 on epoch=179
03/10/2022 15:22:21 - INFO - __main__ - Step 370 Global step 370 Train loss 1.54 on epoch=184
03/10/2022 15:22:26 - INFO - __main__ - Step 380 Global step 380 Train loss 1.51 on epoch=189
03/10/2022 15:22:30 - INFO - __main__ - Step 390 Global step 390 Train loss 1.51 on epoch=194
03/10/2022 15:22:34 - INFO - __main__ - Step 400 Global step 400 Train loss 1.43 on epoch=199
03/10/2022 15:22:36 - INFO - __main__ - Global step 400 Train loss 1.52 QA-F1 0.29166666666666663 on epoch=199
03/10/2022 15:22:36 - INFO - __main__ - Saving model with best QA-F1: 0.28125 -> 0.29166666666666663 on epoch=199, global_step=400
03/10/2022 15:22:40 - INFO - __main__ - Step 410 Global step 410 Train loss 1.42 on epoch=204
03/10/2022 15:22:45 - INFO - __main__ - Step 420 Global step 420 Train loss 1.53 on epoch=209
03/10/2022 15:22:49 - INFO - __main__ - Step 430 Global step 430 Train loss 1.49 on epoch=214
03/10/2022 15:22:53 - INFO - __main__ - Step 440 Global step 440 Train loss 1.31 on epoch=219
03/10/2022 15:22:58 - INFO - __main__ - Step 450 Global step 450 Train loss 1.41 on epoch=224
03/10/2022 15:22:59 - INFO - __main__ - Global step 450 Train loss 1.43 QA-F1 0.3354166666666667 on epoch=224
03/10/2022 15:22:59 - INFO - __main__ - Saving model with best QA-F1: 0.29166666666666663 -> 0.3354166666666667 on epoch=224, global_step=450
03/10/2022 15:23:03 - INFO - __main__ - Step 460 Global step 460 Train loss 1.38 on epoch=229
03/10/2022 15:23:08 - INFO - __main__ - Step 470 Global step 470 Train loss 1.25 on epoch=234
03/10/2022 15:23:12 - INFO - __main__ - Step 480 Global step 480 Train loss 1.35 on epoch=239
03/10/2022 15:23:16 - INFO - __main__ - Step 490 Global step 490 Train loss 1.33 on epoch=244
03/10/2022 15:23:21 - INFO - __main__ - Step 500 Global step 500 Train loss 1.23 on epoch=249
03/10/2022 15:23:22 - INFO - __main__ - Global step 500 Train loss 1.31 QA-F1 0.35625 on epoch=249
03/10/2022 15:23:22 - INFO - __main__ - Saving model with best QA-F1: 0.3354166666666667 -> 0.35625 on epoch=249, global_step=500
03/10/2022 15:23:27 - INFO - __main__ - Step 510 Global step 510 Train loss 1.35 on epoch=254
03/10/2022 15:23:31 - INFO - __main__ - Step 520 Global step 520 Train loss 1.35 on epoch=259
03/10/2022 15:23:35 - INFO - __main__ - Step 530 Global step 530 Train loss 1.32 on epoch=264
03/10/2022 15:23:40 - INFO - __main__ - Step 540 Global step 540 Train loss 1.22 on epoch=269
03/10/2022 15:23:44 - INFO - __main__ - Step 550 Global step 550 Train loss 1.24 on epoch=274
03/10/2022 15:23:46 - INFO - __main__ - Global step 550 Train loss 1.30 QA-F1 0.3411458333333333 on epoch=274
03/10/2022 15:23:50 - INFO - __main__ - Step 560 Global step 560 Train loss 1.18 on epoch=279
03/10/2022 15:23:54 - INFO - __main__ - Step 570 Global step 570 Train loss 1.14 on epoch=284
03/10/2022 15:23:59 - INFO - __main__ - Step 580 Global step 580 Train loss 1.16 on epoch=289
03/10/2022 15:24:03 - INFO - __main__ - Step 590 Global step 590 Train loss 1.14 on epoch=294
03/10/2022 15:24:07 - INFO - __main__ - Step 600 Global step 600 Train loss 1.11 on epoch=299
03/10/2022 15:24:09 - INFO - __main__ - Global step 600 Train loss 1.15 QA-F1 0.3446180555555555 on epoch=299
03/10/2022 15:24:13 - INFO - __main__ - Step 610 Global step 610 Train loss 1.06 on epoch=304
03/10/2022 15:24:18 - INFO - __main__ - Step 620 Global step 620 Train loss 1.11 on epoch=309
03/10/2022 15:24:22 - INFO - __main__ - Step 630 Global step 630 Train loss 0.95 on epoch=314
03/10/2022 15:24:26 - INFO - __main__ - Step 640 Global step 640 Train loss 1.04 on epoch=319
03/10/2022 15:24:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.88 on epoch=324
03/10/2022 15:24:32 - INFO - __main__ - Global step 650 Train loss 1.01 QA-F1 0.3758680555555555 on epoch=324
03/10/2022 15:24:32 - INFO - __main__ - Saving model with best QA-F1: 0.35625 -> 0.3758680555555555 on epoch=324, global_step=650
03/10/2022 15:24:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.99 on epoch=329
03/10/2022 15:24:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.81 on epoch=334
03/10/2022 15:24:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.78 on epoch=339
03/10/2022 15:24:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.84 on epoch=344
03/10/2022 15:24:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.90 on epoch=349
03/10/2022 15:24:55 - INFO - __main__ - Global step 700 Train loss 0.86 QA-F1 0.3111111111111111 on epoch=349
03/10/2022 15:24:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.84 on epoch=354
03/10/2022 15:25:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.74 on epoch=359
03/10/2022 15:25:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.76 on epoch=364
03/10/2022 15:25:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.74 on epoch=369
03/10/2022 15:25:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.82 on epoch=374
03/10/2022 15:25:18 - INFO - __main__ - Global step 750 Train loss 0.78 QA-F1 0.30156249999999996 on epoch=374
03/10/2022 15:25:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.86 on epoch=379
03/10/2022 15:25:27 - INFO - __main__ - Step 770 Global step 770 Train loss 0.76 on epoch=384
03/10/2022 15:25:31 - INFO - __main__ - Step 780 Global step 780 Train loss 0.77 on epoch=389
03/10/2022 15:25:35 - INFO - __main__ - Step 790 Global step 790 Train loss 0.64 on epoch=394
03/10/2022 15:25:40 - INFO - __main__ - Step 800 Global step 800 Train loss 0.73 on epoch=399
03/10/2022 15:25:41 - INFO - __main__ - Global step 800 Train loss 0.75 QA-F1 0.3480902777777778 on epoch=399
03/10/2022 15:25:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.65 on epoch=404
03/10/2022 15:25:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.77 on epoch=409
03/10/2022 15:25:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.72 on epoch=414
03/10/2022 15:25:58 - INFO - __main__ - Step 840 Global step 840 Train loss 0.67 on epoch=419
03/10/2022 15:26:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.60 on epoch=424
03/10/2022 15:26:04 - INFO - __main__ - Global step 850 Train loss 0.68 QA-F1 0.2911458333333333 on epoch=424
03/10/2022 15:26:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.54 on epoch=429
03/10/2022 15:26:13 - INFO - __main__ - Step 870 Global step 870 Train loss 0.58 on epoch=434
03/10/2022 15:26:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.56 on epoch=439
03/10/2022 15:26:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.55 on epoch=444
03/10/2022 15:26:26 - INFO - __main__ - Step 900 Global step 900 Train loss 0.53 on epoch=449
03/10/2022 15:26:28 - INFO - __main__ - Global step 900 Train loss 0.55 QA-F1 0.2578125 on epoch=449
03/10/2022 15:26:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.64 on epoch=454
03/10/2022 15:26:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.57 on epoch=459
03/10/2022 15:26:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.62 on epoch=464
03/10/2022 15:26:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.56 on epoch=469
03/10/2022 15:26:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.61 on epoch=474
03/10/2022 15:26:51 - INFO - __main__ - Global step 950 Train loss 0.60 QA-F1 0.27031249999999996 on epoch=474
03/10/2022 15:26:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=479
03/10/2022 15:26:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
03/10/2022 15:27:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.54 on epoch=489
03/10/2022 15:27:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.44 on epoch=494
03/10/2022 15:27:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.49 on epoch=499
03/10/2022 15:27:14 - INFO - __main__ - Global step 1000 Train loss 0.47 QA-F1 0.3055555555555556 on epoch=499
03/10/2022 15:27:18 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.51 on epoch=504
03/10/2022 15:27:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.56 on epoch=509
03/10/2022 15:27:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.48 on epoch=514
03/10/2022 15:27:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.49 on epoch=519
03/10/2022 15:27:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.44 on epoch=524
03/10/2022 15:27:37 - INFO - __main__ - Global step 1050 Train loss 0.50 QA-F1 0.3368055555555555 on epoch=524
03/10/2022 15:27:41 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.53 on epoch=529
03/10/2022 15:27:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.38 on epoch=534
03/10/2022 15:27:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=539
03/10/2022 15:27:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.80 on epoch=544
03/10/2022 15:27:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.68 on epoch=549
03/10/2022 15:28:00 - INFO - __main__ - Global step 1100 Train loss 0.57 QA-F1 0.2833333333333333 on epoch=549
03/10/2022 15:28:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.59 on epoch=554
03/10/2022 15:28:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.46 on epoch=559
03/10/2022 15:28:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=564
03/10/2022 15:28:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.41 on epoch=569
03/10/2022 15:28:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=574
03/10/2022 15:28:24 - INFO - __main__ - Global step 1150 Train loss 0.47 QA-F1 0.30416666666666664 on epoch=574
03/10/2022 15:28:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.49 on epoch=579
03/10/2022 15:28:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=584
03/10/2022 15:28:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.41 on epoch=589
03/10/2022 15:28:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=594
03/10/2022 15:28:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=599
03/10/2022 15:28:47 - INFO - __main__ - Global step 1200 Train loss 0.42 QA-F1 0.24739583333333331 on epoch=599
03/10/2022 15:28:51 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=604
03/10/2022 15:28:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=609
03/10/2022 15:29:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=614
03/10/2022 15:29:04 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.44 on epoch=619
03/10/2022 15:29:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=624
03/10/2022 15:29:10 - INFO - __main__ - Global step 1250 Train loss 0.40 QA-F1 0.2534722222222222 on epoch=624
03/10/2022 15:29:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=629
03/10/2022 15:29:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=634
03/10/2022 15:29:23 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.35 on epoch=639
03/10/2022 15:29:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.48 on epoch=644
03/10/2022 15:29:31 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=649
03/10/2022 15:29:33 - INFO - __main__ - Global step 1300 Train loss 0.37 QA-F1 0.3237847222222222 on epoch=649
03/10/2022 15:29:37 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=654
03/10/2022 15:29:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.37 on epoch=659
03/10/2022 15:29:46 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.34 on epoch=664
03/10/2022 15:29:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.46 on epoch=669
03/10/2022 15:29:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.37 on epoch=674
03/10/2022 15:29:56 - INFO - __main__ - Global step 1350 Train loss 0.37 QA-F1 0.32499999999999996 on epoch=674
03/10/2022 15:30:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.31 on epoch=679
03/10/2022 15:30:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.32 on epoch=684
03/10/2022 15:30:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.36 on epoch=689
03/10/2022 15:30:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=694
03/10/2022 15:30:18 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=699
03/10/2022 15:30:19 - INFO - __main__ - Global step 1400 Train loss 0.35 QA-F1 0.26249999999999996 on epoch=699
03/10/2022 15:30:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=704
03/10/2022 15:30:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=709
03/10/2022 15:30:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.42 on epoch=714
03/10/2022 15:30:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
03/10/2022 15:30:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=724
03/10/2022 15:30:43 - INFO - __main__ - Global step 1450 Train loss 0.36 QA-F1 0.28125 on epoch=724
03/10/2022 15:30:47 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.32 on epoch=729
03/10/2022 15:30:51 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=734
03/10/2022 15:30:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
03/10/2022 15:31:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=744
03/10/2022 15:31:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=749
03/10/2022 15:31:06 - INFO - __main__ - Global step 1500 Train loss 0.33 QA-F1 0.26249999999999996 on epoch=749
03/10/2022 15:31:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=754
03/10/2022 15:31:14 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=759
03/10/2022 15:31:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=764
03/10/2022 15:31:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.29 on epoch=769
03/10/2022 15:31:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=774
03/10/2022 15:31:29 - INFO - __main__ - Global step 1550 Train loss 0.32 QA-F1 0.2847222222222222 on epoch=774
03/10/2022 15:31:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=779
03/10/2022 15:31:37 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.30 on epoch=784
03/10/2022 15:31:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.32 on epoch=789
03/10/2022 15:31:46 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.29 on epoch=794
03/10/2022 15:31:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.20 on epoch=799
03/10/2022 15:31:52 - INFO - __main__ - Global step 1600 Train loss 0.27 QA-F1 0.2847222222222222 on epoch=799
03/10/2022 15:31:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=804
03/10/2022 15:32:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
03/10/2022 15:32:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=814
03/10/2022 15:32:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=819
03/10/2022 15:32:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.27 on epoch=824
03/10/2022 15:32:15 - INFO - __main__ - Global step 1650 Train loss 0.30 QA-F1 0.26388888888888884 on epoch=824
03/10/2022 15:32:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=829
03/10/2022 15:32:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.37 on epoch=834
03/10/2022 15:32:28 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.30 on epoch=839
03/10/2022 15:32:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=844
03/10/2022 15:32:37 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
03/10/2022 15:32:39 - INFO - __main__ - Global step 1700 Train loss 0.31 QA-F1 0.29513888888888884 on epoch=849
03/10/2022 15:32:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=854
03/10/2022 15:32:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=859
03/10/2022 15:32:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.31 on epoch=864
03/10/2022 15:32:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=869
03/10/2022 15:33:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.28 on epoch=874
03/10/2022 15:33:02 - INFO - __main__ - Global step 1750 Train loss 0.27 QA-F1 0.2951388888888889 on epoch=874
03/10/2022 15:33:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=879
03/10/2022 15:33:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=884
03/10/2022 15:33:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=889
03/10/2022 15:33:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=894
03/10/2022 15:33:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.29 on epoch=899
03/10/2022 15:33:25 - INFO - __main__ - Global step 1800 Train loss 0.29 QA-F1 0.2638888888888889 on epoch=899
03/10/2022 15:33:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=904
03/10/2022 15:33:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=909
03/10/2022 15:33:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.34 on epoch=914
03/10/2022 15:33:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=919
03/10/2022 15:33:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=924
03/10/2022 15:33:48 - INFO - __main__ - Global step 1850 Train loss 0.27 QA-F1 0.24305555555555555 on epoch=924
03/10/2022 15:33:52 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
03/10/2022 15:33:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=934
03/10/2022 15:34:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=939
03/10/2022 15:34:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.28 on epoch=944
03/10/2022 15:34:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=949
03/10/2022 15:34:11 - INFO - __main__ - Global step 1900 Train loss 0.28 QA-F1 0.29166666666666663 on epoch=949
03/10/2022 15:34:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=954
03/10/2022 15:34:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=959
03/10/2022 15:34:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=964
03/10/2022 15:34:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=969
03/10/2022 15:34:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=974
03/10/2022 15:34:34 - INFO - __main__ - Global step 1950 Train loss 0.24 QA-F1 0.2520833333333333 on epoch=974
03/10/2022 15:34:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=979
03/10/2022 15:34:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=984
03/10/2022 15:34:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=989
03/10/2022 15:34:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=994
03/10/2022 15:34:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=999
03/10/2022 15:34:57 - INFO - __main__ - Global step 2000 Train loss 0.23 QA-F1 0.29166666666666663 on epoch=999
03/10/2022 15:34:57 - INFO - __main__ - save last model!
03/10/2022 15:34:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 15:34:58 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 15:34:58 - INFO - __main__ - Printing 3 examples
03/10/2022 15:34:58 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 15:34:58 - INFO - __main__ - ['Frankie']
03/10/2022 15:34:58 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 15:34:58 - INFO - __main__ - ['Frankie']
03/10/2022 15:34:58 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 15:34:58 - INFO - __main__ - ['Frankie']
03/10/2022 15:34:58 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:34:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:34:58 - INFO - __main__ - Printing 3 examples
03/10/2022 15:34:58 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/10/2022 15:34:58 - INFO - __main__ - ['Ringo Starr']
03/10/2022 15:34:58 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/10/2022 15:34:58 - INFO - __main__ - ['Jimmy']
03/10/2022 15:34:58 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/10/2022 15:34:58 - INFO - __main__ - ['The Alan Bush Music Trust']
03/10/2022 15:34:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 15:34:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:34:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:34:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:34:58 - INFO - __main__ - Printing 3 examples
03/10/2022 15:34:58 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/10/2022 15:34:58 - INFO - __main__ - ['Elk', 'pronghorn']
03/10/2022 15:34:58 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/10/2022 15:34:58 - INFO - __main__ - ['Jeff']
03/10/2022 15:34:58 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/10/2022 15:34:58 - INFO - __main__ - ['November 1991']
03/10/2022 15:34:58 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:34:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:34:58 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:35:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:35:04 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 15:35:11 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 15:35:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 15:35:11 - INFO - __main__ - Starting training!
03/10/2022 15:37:01 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_100_0.5_8_predictions.txt
03/10/2022 15:37:01 - INFO - __main__ - QA-F1 on test data: 0.2672
03/10/2022 15:37:01 - INFO - __main__ - prefix=quoref_32_100, lr=0.5, bsz=8, dev_performance=0.3758680555555555, test_performance=0.26718854622080423
03/10/2022 15:37:01 - INFO - __main__ - Running ... prefix=quoref_32_100, lr=0.4, bsz=8 ...
03/10/2022 15:37:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:37:02 - INFO - __main__ - Printing 3 examples
03/10/2022 15:37:02 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/10/2022 15:37:02 - INFO - __main__ - ['Ringo Starr']
03/10/2022 15:37:02 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/10/2022 15:37:02 - INFO - __main__ - ['Jimmy']
03/10/2022 15:37:02 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/10/2022 15:37:02 - INFO - __main__ - ['The Alan Bush Music Trust']
03/10/2022 15:37:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 15:37:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:37:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:37:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:37:02 - INFO - __main__ - Printing 3 examples
03/10/2022 15:37:02 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/10/2022 15:37:02 - INFO - __main__ - ['Elk', 'pronghorn']
03/10/2022 15:37:02 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/10/2022 15:37:02 - INFO - __main__ - ['Jeff']
03/10/2022 15:37:02 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/10/2022 15:37:02 - INFO - __main__ - ['November 1991']
03/10/2022 15:37:02 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:37:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:37:02 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:37:15 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 15:37:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 15:37:16 - INFO - __main__ - Starting training!
03/10/2022 15:37:23 - INFO - __main__ - Step 10 Global step 10 Train loss 3.27 on epoch=4
03/10/2022 15:37:27 - INFO - __main__ - Step 20 Global step 20 Train loss 2.68 on epoch=9
03/10/2022 15:37:31 - INFO - __main__ - Step 30 Global step 30 Train loss 2.31 on epoch=14
03/10/2022 15:37:36 - INFO - __main__ - Step 40 Global step 40 Train loss 2.06 on epoch=19
03/10/2022 15:37:40 - INFO - __main__ - Step 50 Global step 50 Train loss 1.86 on epoch=24
03/10/2022 15:37:41 - INFO - __main__ - Global step 50 Train loss 2.43 QA-F1 0.3333333333333333 on epoch=24
03/10/2022 15:37:41 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 15:37:46 - INFO - __main__ - Step 60 Global step 60 Train loss 1.78 on epoch=29
03/10/2022 15:37:50 - INFO - __main__ - Step 70 Global step 70 Train loss 1.83 on epoch=34
03/10/2022 15:37:54 - INFO - __main__ - Step 80 Global step 80 Train loss 1.61 on epoch=39
03/10/2022 15:37:59 - INFO - __main__ - Step 90 Global step 90 Train loss 1.44 on epoch=44
03/10/2022 15:38:03 - INFO - __main__ - Step 100 Global step 100 Train loss 1.48 on epoch=49
03/10/2022 15:38:05 - INFO - __main__ - Global step 100 Train loss 1.63 QA-F1 0.3145833333333333 on epoch=49
03/10/2022 15:38:09 - INFO - __main__ - Step 110 Global step 110 Train loss 1.43 on epoch=54
03/10/2022 15:38:13 - INFO - __main__ - Step 120 Global step 120 Train loss 1.34 on epoch=59
03/10/2022 15:38:18 - INFO - __main__ - Step 130 Global step 130 Train loss 1.23 on epoch=64
03/10/2022 15:38:22 - INFO - __main__ - Step 140 Global step 140 Train loss 1.11 on epoch=69
03/10/2022 15:38:26 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=74
03/10/2022 15:38:28 - INFO - __main__ - Global step 150 Train loss 1.23 QA-F1 0.2729166666666667 on epoch=74
03/10/2022 15:38:32 - INFO - __main__ - Step 160 Global step 160 Train loss 1.16 on epoch=79
03/10/2022 15:38:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.11 on epoch=84
03/10/2022 15:38:41 - INFO - __main__ - Step 180 Global step 180 Train loss 1.12 on epoch=89
03/10/2022 15:38:45 - INFO - __main__ - Step 190 Global step 190 Train loss 1.06 on epoch=94
03/10/2022 15:38:49 - INFO - __main__ - Step 200 Global step 200 Train loss 1.03 on epoch=99
03/10/2022 15:38:51 - INFO - __main__ - Global step 200 Train loss 1.10 QA-F1 0.3145833333333333 on epoch=99
03/10/2022 15:38:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.98 on epoch=104
03/10/2022 15:38:59 - INFO - __main__ - Step 220 Global step 220 Train loss 1.01 on epoch=109
03/10/2022 15:39:04 - INFO - __main__ - Step 230 Global step 230 Train loss 1.06 on epoch=114
03/10/2022 15:39:08 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=119
03/10/2022 15:39:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.83 on epoch=124
03/10/2022 15:39:14 - INFO - __main__ - Global step 250 Train loss 0.95 QA-F1 0.3432291666666667 on epoch=124
03/10/2022 15:39:14 - INFO - __main__ - Saving model with best QA-F1: 0.3333333333333333 -> 0.3432291666666667 on epoch=124, global_step=250
03/10/2022 15:39:18 - INFO - __main__ - Step 260 Global step 260 Train loss 1.02 on epoch=129
03/10/2022 15:39:22 - INFO - __main__ - Step 270 Global step 270 Train loss 0.92 on epoch=134
03/10/2022 15:39:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=139
03/10/2022 15:39:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.84 on epoch=144
03/10/2022 15:39:35 - INFO - __main__ - Step 300 Global step 300 Train loss 0.80 on epoch=149
03/10/2022 15:39:37 - INFO - __main__ - Global step 300 Train loss 0.88 QA-F1 0.23124999999999998 on epoch=149
03/10/2022 15:39:41 - INFO - __main__ - Step 310 Global step 310 Train loss 0.76 on epoch=154
03/10/2022 15:39:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.73 on epoch=159
03/10/2022 15:39:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.79 on epoch=164
03/10/2022 15:39:54 - INFO - __main__ - Step 340 Global step 340 Train loss 0.62 on epoch=169
03/10/2022 15:39:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.74 on epoch=174
03/10/2022 15:39:59 - INFO - __main__ - Global step 350 Train loss 0.73 QA-F1 0.23124999999999998 on epoch=174
03/10/2022 15:40:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.66 on epoch=179
03/10/2022 15:40:08 - INFO - __main__ - Step 370 Global step 370 Train loss 0.63 on epoch=184
03/10/2022 15:40:12 - INFO - __main__ - Step 380 Global step 380 Train loss 0.67 on epoch=189
03/10/2022 15:40:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.70 on epoch=194
03/10/2022 15:40:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.62 on epoch=199
03/10/2022 15:40:22 - INFO - __main__ - Global step 400 Train loss 0.66 QA-F1 0.21041666666666664 on epoch=199
03/10/2022 15:40:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.59 on epoch=204
03/10/2022 15:40:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.63 on epoch=209
03/10/2022 15:40:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.58 on epoch=214
03/10/2022 15:40:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=219
03/10/2022 15:40:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.55 on epoch=224
03/10/2022 15:40:45 - INFO - __main__ - Global step 450 Train loss 0.58 QA-F1 0.2517361111111111 on epoch=224
03/10/2022 15:40:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.56 on epoch=229
03/10/2022 15:40:54 - INFO - __main__ - Step 470 Global step 470 Train loss 0.66 on epoch=234
03/10/2022 15:40:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.52 on epoch=239
03/10/2022 15:41:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.52 on epoch=244
03/10/2022 15:41:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.55 on epoch=249
03/10/2022 15:41:08 - INFO - __main__ - Global step 500 Train loss 0.56 QA-F1 0.3510416666666667 on epoch=249
03/10/2022 15:41:08 - INFO - __main__ - Saving model with best QA-F1: 0.3432291666666667 -> 0.3510416666666667 on epoch=249, global_step=500
03/10/2022 15:41:12 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=254
03/10/2022 15:41:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.47 on epoch=259
03/10/2022 15:41:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=264
03/10/2022 15:41:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.54 on epoch=269
03/10/2022 15:41:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.49 on epoch=274
03/10/2022 15:41:31 - INFO - __main__ - Global step 550 Train loss 0.50 QA-F1 0.2881944444444444 on epoch=274
03/10/2022 15:41:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.57 on epoch=279
03/10/2022 15:41:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=284
03/10/2022 15:41:44 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=289
03/10/2022 15:41:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=294
03/10/2022 15:41:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=299
03/10/2022 15:41:54 - INFO - __main__ - Global step 600 Train loss 0.48 QA-F1 0.24652777777777776 on epoch=299
03/10/2022 15:41:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.53 on epoch=304
03/10/2022 15:42:03 - INFO - __main__ - Step 620 Global step 620 Train loss 0.47 on epoch=309
03/10/2022 15:42:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.47 on epoch=314
03/10/2022 15:42:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.46 on epoch=319
03/10/2022 15:42:16 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=324
03/10/2022 15:42:18 - INFO - __main__ - Global step 650 Train loss 0.46 QA-F1 0.21041666666666664 on epoch=324
03/10/2022 15:42:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.46 on epoch=329
03/10/2022 15:42:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.45 on epoch=334
03/10/2022 15:42:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=339
03/10/2022 15:42:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=344
03/10/2022 15:42:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.45 on epoch=349
03/10/2022 15:42:41 - INFO - __main__ - Global step 700 Train loss 0.44 QA-F1 0.265625 on epoch=349
03/10/2022 15:42:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.43 on epoch=354
03/10/2022 15:42:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=359
03/10/2022 15:42:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.28 on epoch=364
03/10/2022 15:42:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=369
03/10/2022 15:43:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=374
03/10/2022 15:43:05 - INFO - __main__ - Global step 750 Train loss 0.37 QA-F1 0.25260416666666663 on epoch=374
03/10/2022 15:43:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.51 on epoch=379
03/10/2022 15:43:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=384
03/10/2022 15:43:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=389
03/10/2022 15:43:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=394
03/10/2022 15:43:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=399
03/10/2022 15:43:28 - INFO - __main__ - Global step 800 Train loss 0.42 QA-F1 0.17708333333333331 on epoch=399
03/10/2022 15:43:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.43 on epoch=404
03/10/2022 15:43:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=409
03/10/2022 15:43:41 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=414
03/10/2022 15:43:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=419
03/10/2022 15:43:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=424
03/10/2022 15:43:51 - INFO - __main__ - Global step 850 Train loss 0.38 QA-F1 0.21875 on epoch=424
03/10/2022 15:43:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=429
03/10/2022 15:44:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=434
03/10/2022 15:44:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=439
03/10/2022 15:44:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=444
03/10/2022 15:44:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=449
03/10/2022 15:44:14 - INFO - __main__ - Global step 900 Train loss 0.36 QA-F1 0.15625 on epoch=449
03/10/2022 15:44:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=454
03/10/2022 15:44:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=459
03/10/2022 15:44:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=464
03/10/2022 15:44:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=469
03/10/2022 15:44:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=474
03/10/2022 15:44:37 - INFO - __main__ - Global step 950 Train loss 0.35 QA-F1 0.140625 on epoch=474
03/10/2022 15:44:42 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=479
03/10/2022 15:44:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
03/10/2022 15:44:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=489
03/10/2022 15:44:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=494
03/10/2022 15:44:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=499
03/10/2022 15:45:01 - INFO - __main__ - Global step 1000 Train loss 0.35 QA-F1 0.1827651515151515 on epoch=499
03/10/2022 15:45:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=504
03/10/2022 15:45:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=509
03/10/2022 15:45:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=514
03/10/2022 15:45:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=519
03/10/2022 15:45:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=524
03/10/2022 15:45:25 - INFO - __main__ - Global step 1050 Train loss 0.27 QA-F1 0.19791666666666663 on epoch=524
03/10/2022 15:45:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.38 on epoch=529
03/10/2022 15:45:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=534
03/10/2022 15:45:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=539
03/10/2022 15:45:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=544
03/10/2022 15:45:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=549
03/10/2022 15:45:48 - INFO - __main__ - Global step 1100 Train loss 0.31 QA-F1 0.24999999999999997 on epoch=549
03/10/2022 15:45:53 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=554
03/10/2022 15:45:57 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=559
03/10/2022 15:46:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.29 on epoch=564
03/10/2022 15:46:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=569
03/10/2022 15:46:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=574
03/10/2022 15:46:12 - INFO - __main__ - Global step 1150 Train loss 0.29 QA-F1 0.19642857142857142 on epoch=574
03/10/2022 15:46:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=579
03/10/2022 15:46:20 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=584
03/10/2022 15:46:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=589
03/10/2022 15:46:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.30 on epoch=594
03/10/2022 15:46:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=599
03/10/2022 15:46:35 - INFO - __main__ - Global step 1200 Train loss 0.25 QA-F1 0.20163690476190477 on epoch=599
03/10/2022 15:46:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.33 on epoch=604
03/10/2022 15:46:44 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=609
03/10/2022 15:46:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=614
03/10/2022 15:46:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=619
03/10/2022 15:46:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.30 on epoch=624
03/10/2022 15:46:59 - INFO - __main__ - Global step 1250 Train loss 0.28 QA-F1 0.17708333333333331 on epoch=624
03/10/2022 15:47:03 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=629
03/10/2022 15:47:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=634
03/10/2022 15:47:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.21 on epoch=639
03/10/2022 15:47:16 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=644
03/10/2022 15:47:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=649
03/10/2022 15:47:23 - INFO - __main__ - Global step 1300 Train loss 0.22 QA-F1 0.2035984848484848 on epoch=649
03/10/2022 15:47:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=654
03/10/2022 15:47:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=659
03/10/2022 15:47:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=664
03/10/2022 15:47:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.30 on epoch=669
03/10/2022 15:47:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=674
03/10/2022 15:47:46 - INFO - __main__ - Global step 1350 Train loss 0.25 QA-F1 0.2589285714285714 on epoch=674
03/10/2022 15:47:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=679
03/10/2022 15:47:55 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=684
03/10/2022 15:47:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=689
03/10/2022 15:48:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=694
03/10/2022 15:48:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=699
03/10/2022 15:48:10 - INFO - __main__ - Global step 1400 Train loss 0.24 QA-F1 0.24526515151515152 on epoch=699
03/10/2022 15:48:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=704
03/10/2022 15:48:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.27 on epoch=709
03/10/2022 15:48:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=714
03/10/2022 15:48:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=719
03/10/2022 15:48:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=724
03/10/2022 15:48:33 - INFO - __main__ - Global step 1450 Train loss 0.22 QA-F1 0.22443181818181818 on epoch=724
03/10/2022 15:48:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=729
03/10/2022 15:48:42 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=734
03/10/2022 15:48:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=739
03/10/2022 15:48:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=744
03/10/2022 15:48:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=749
03/10/2022 15:48:57 - INFO - __main__ - Global step 1500 Train loss 0.21 QA-F1 0.29734848484848486 on epoch=749
03/10/2022 15:49:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=754
03/10/2022 15:49:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.18 on epoch=759
03/10/2022 15:49:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=764
03/10/2022 15:49:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.19 on epoch=769
03/10/2022 15:49:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=774
03/10/2022 15:49:21 - INFO - __main__ - Global step 1550 Train loss 0.19 QA-F1 0.2556818181818182 on epoch=774
03/10/2022 15:49:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.28 on epoch=779
03/10/2022 15:49:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.45 on epoch=784
03/10/2022 15:49:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=789
03/10/2022 15:49:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.37 on epoch=794
03/10/2022 15:49:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=799
03/10/2022 15:49:45 - INFO - __main__ - Global step 1600 Train loss 0.35 QA-F1 0.20833333333333331 on epoch=799
03/10/2022 15:49:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
03/10/2022 15:49:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.28 on epoch=809
03/10/2022 15:49:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=814
03/10/2022 15:50:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=819
03/10/2022 15:50:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=824
03/10/2022 15:50:08 - INFO - __main__ - Global step 1650 Train loss 0.25 QA-F1 0.2556818181818182 on epoch=824
03/10/2022 15:50:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=829
03/10/2022 15:50:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.21 on epoch=834
03/10/2022 15:50:21 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=839
03/10/2022 15:50:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=844
03/10/2022 15:50:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=849
03/10/2022 15:50:32 - INFO - __main__ - Global step 1700 Train loss 0.19 QA-F1 0.2537202380952381 on epoch=849
03/10/2022 15:50:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=854
03/10/2022 15:50:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.17 on epoch=859
03/10/2022 15:50:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=864
03/10/2022 15:50:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.19 on epoch=869
03/10/2022 15:50:54 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=874
03/10/2022 15:50:56 - INFO - __main__ - Global step 1750 Train loss 0.20 QA-F1 0.23484848484848486 on epoch=874
03/10/2022 15:51:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=879
03/10/2022 15:51:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=884
03/10/2022 15:51:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=889
03/10/2022 15:51:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=894
03/10/2022 15:51:17 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=899
03/10/2022 15:51:20 - INFO - __main__ - Global step 1800 Train loss 0.23 QA-F1 0.3077651515151515 on epoch=899
03/10/2022 15:51:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=904
03/10/2022 15:51:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=909
03/10/2022 15:51:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=914
03/10/2022 15:51:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=919
03/10/2022 15:51:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=924
03/10/2022 15:51:43 - INFO - __main__ - Global step 1850 Train loss 0.19 QA-F1 0.2035984848484848 on epoch=924
03/10/2022 15:51:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.26 on epoch=929
03/10/2022 15:51:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=934
03/10/2022 15:51:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=939
03/10/2022 15:52:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=944
03/10/2022 15:52:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=949
03/10/2022 15:52:07 - INFO - __main__ - Global step 1900 Train loss 0.19 QA-F1 0.2765151515151515 on epoch=949
03/10/2022 15:52:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/10/2022 15:52:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=959
03/10/2022 15:52:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.18 on epoch=964
03/10/2022 15:52:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=969
03/10/2022 15:52:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=974
03/10/2022 15:52:30 - INFO - __main__ - Global step 1950 Train loss 0.19 QA-F1 0.2869318181818182 on epoch=974
03/10/2022 15:52:35 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=979
03/10/2022 15:52:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
03/10/2022 15:52:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=989
03/10/2022 15:52:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=994
03/10/2022 15:52:52 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=999
03/10/2022 15:52:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:52:53 - INFO - __main__ - Printing 3 examples
03/10/2022 15:52:53 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/10/2022 15:52:53 - INFO - __main__ - ['Ringo Starr']
03/10/2022 15:52:53 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/10/2022 15:52:53 - INFO - __main__ - ['Jimmy']
03/10/2022 15:52:53 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/10/2022 15:52:53 - INFO - __main__ - ['The Alan Bush Music Trust']
03/10/2022 15:52:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 15:52:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:52:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:52:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:52:53 - INFO - __main__ - Printing 3 examples
03/10/2022 15:52:53 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/10/2022 15:52:53 - INFO - __main__ - ['Elk', 'pronghorn']
03/10/2022 15:52:53 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/10/2022 15:52:53 - INFO - __main__ - ['Jeff']
03/10/2022 15:52:53 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/10/2022 15:52:53 - INFO - __main__ - ['November 1991']
03/10/2022 15:52:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:52:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:52:53 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:52:54 - INFO - __main__ - Global step 2000 Train loss 0.20 QA-F1 0.2452651515151515 on epoch=999
03/10/2022 15:52:54 - INFO - __main__ - save last model!
03/10/2022 15:52:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 15:52:54 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 15:52:54 - INFO - __main__ - Printing 3 examples
03/10/2022 15:52:54 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 15:52:54 - INFO - __main__ - ['Frankie']
03/10/2022 15:52:54 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 15:52:54 - INFO - __main__ - ['Frankie']
03/10/2022 15:52:54 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 15:52:54 - INFO - __main__ - ['Frankie']
03/10/2022 15:52:54 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:52:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:53:01 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 15:53:07 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 15:53:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 15:53:08 - INFO - __main__ - Starting training!
03/10/2022 15:55:00 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_100_0.4_8_predictions.txt
03/10/2022 15:55:00 - INFO - __main__ - QA-F1 on test data: 0.2790
03/10/2022 15:55:00 - INFO - __main__ - prefix=quoref_32_100, lr=0.4, bsz=8, dev_performance=0.3510416666666667, test_performance=0.2790277524148492
03/10/2022 15:55:00 - INFO - __main__ - Running ... prefix=quoref_32_100, lr=0.3, bsz=8 ...
03/10/2022 15:55:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:55:01 - INFO - __main__ - Printing 3 examples
03/10/2022 15:55:01 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/10/2022 15:55:01 - INFO - __main__ - ['Ringo Starr']
03/10/2022 15:55:01 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/10/2022 15:55:01 - INFO - __main__ - ['Jimmy']
03/10/2022 15:55:01 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/10/2022 15:55:01 - INFO - __main__ - ['The Alan Bush Music Trust']
03/10/2022 15:55:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 15:55:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:55:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 15:55:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 15:55:01 - INFO - __main__ - Printing 3 examples
03/10/2022 15:55:01 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/10/2022 15:55:01 - INFO - __main__ - ['Elk', 'pronghorn']
03/10/2022 15:55:01 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/10/2022 15:55:01 - INFO - __main__ - ['Jeff']
03/10/2022 15:55:01 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/10/2022 15:55:01 - INFO - __main__ - ['November 1991']
03/10/2022 15:55:01 - INFO - __main__ - Tokenizing Input ...
03/10/2022 15:55:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 15:55:01 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 15:55:14 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 15:55:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 15:55:15 - INFO - __main__ - Starting training!
03/10/2022 15:55:21 - INFO - __main__ - Step 10 Global step 10 Train loss 3.42 on epoch=4
03/10/2022 15:55:25 - INFO - __main__ - Step 20 Global step 20 Train loss 2.86 on epoch=9
03/10/2022 15:55:29 - INFO - __main__ - Step 30 Global step 30 Train loss 2.58 on epoch=14
03/10/2022 15:55:34 - INFO - __main__ - Step 40 Global step 40 Train loss 2.27 on epoch=19
03/10/2022 15:55:38 - INFO - __main__ - Step 50 Global step 50 Train loss 2.05 on epoch=24
03/10/2022 15:55:40 - INFO - __main__ - Global step 50 Train loss 2.64 QA-F1 0.2552083333333333 on epoch=24
03/10/2022 15:55:40 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.2552083333333333 on epoch=24, global_step=50
03/10/2022 15:55:44 - INFO - __main__ - Step 60 Global step 60 Train loss 1.80 on epoch=29
03/10/2022 15:55:48 - INFO - __main__ - Step 70 Global step 70 Train loss 1.90 on epoch=34
03/10/2022 15:55:53 - INFO - __main__ - Step 80 Global step 80 Train loss 1.77 on epoch=39
03/10/2022 15:55:57 - INFO - __main__ - Step 90 Global step 90 Train loss 1.92 on epoch=44
03/10/2022 15:56:01 - INFO - __main__ - Step 100 Global step 100 Train loss 1.76 on epoch=49
03/10/2022 15:56:03 - INFO - __main__ - Global step 100 Train loss 1.83 QA-F1 0.29687499999999994 on epoch=49
03/10/2022 15:56:03 - INFO - __main__ - Saving model with best QA-F1: 0.2552083333333333 -> 0.29687499999999994 on epoch=49, global_step=100
03/10/2022 15:56:07 - INFO - __main__ - Step 110 Global step 110 Train loss 1.59 on epoch=54
03/10/2022 15:56:11 - INFO - __main__ - Step 120 Global step 120 Train loss 1.67 on epoch=59
03/10/2022 15:56:16 - INFO - __main__ - Step 130 Global step 130 Train loss 1.59 on epoch=64
03/10/2022 15:56:20 - INFO - __main__ - Step 140 Global step 140 Train loss 1.67 on epoch=69
03/10/2022 15:56:24 - INFO - __main__ - Step 150 Global step 150 Train loss 1.55 on epoch=74
03/10/2022 15:56:26 - INFO - __main__ - Global step 150 Train loss 1.61 QA-F1 0.2833333333333333 on epoch=74
03/10/2022 15:56:30 - INFO - __main__ - Step 160 Global step 160 Train loss 1.63 on epoch=79
03/10/2022 15:56:34 - INFO - __main__ - Step 170 Global step 170 Train loss 1.84 on epoch=84
03/10/2022 15:56:39 - INFO - __main__ - Step 180 Global step 180 Train loss 1.68 on epoch=89
03/10/2022 15:56:43 - INFO - __main__ - Step 190 Global step 190 Train loss 1.53 on epoch=94
03/10/2022 15:56:47 - INFO - __main__ - Step 200 Global step 200 Train loss 1.56 on epoch=99
03/10/2022 15:56:49 - INFO - __main__ - Global step 200 Train loss 1.65 QA-F1 0.26249999999999996 on epoch=99
03/10/2022 15:56:53 - INFO - __main__ - Step 210 Global step 210 Train loss 1.59 on epoch=104
03/10/2022 15:56:58 - INFO - __main__ - Step 220 Global step 220 Train loss 1.50 on epoch=109
03/10/2022 15:57:02 - INFO - __main__ - Step 230 Global step 230 Train loss 1.73 on epoch=114
03/10/2022 15:57:06 - INFO - __main__ - Step 240 Global step 240 Train loss 1.52 on epoch=119
03/10/2022 15:57:11 - INFO - __main__ - Step 250 Global step 250 Train loss 1.50 on epoch=124
03/10/2022 15:57:12 - INFO - __main__ - Global step 250 Train loss 1.57 QA-F1 0.26249999999999996 on epoch=124
03/10/2022 15:57:16 - INFO - __main__ - Step 260 Global step 260 Train loss 1.48 on epoch=129
03/10/2022 15:57:21 - INFO - __main__ - Step 270 Global step 270 Train loss 1.44 on epoch=134
03/10/2022 15:57:25 - INFO - __main__ - Step 280 Global step 280 Train loss 1.39 on epoch=139
03/10/2022 15:57:29 - INFO - __main__ - Step 290 Global step 290 Train loss 1.43 on epoch=144
03/10/2022 15:57:34 - INFO - __main__ - Step 300 Global step 300 Train loss 1.40 on epoch=149
03/10/2022 15:57:35 - INFO - __main__ - Global step 300 Train loss 1.43 QA-F1 0.2833333333333333 on epoch=149
03/10/2022 15:57:40 - INFO - __main__ - Step 310 Global step 310 Train loss 1.45 on epoch=154
03/10/2022 15:57:44 - INFO - __main__ - Step 320 Global step 320 Train loss 1.45 on epoch=159
03/10/2022 15:57:48 - INFO - __main__ - Step 330 Global step 330 Train loss 1.33 on epoch=164
03/10/2022 15:57:53 - INFO - __main__ - Step 340 Global step 340 Train loss 1.38 on epoch=169
03/10/2022 15:57:57 - INFO - __main__ - Step 350 Global step 350 Train loss 1.40 on epoch=174
03/10/2022 15:57:59 - INFO - __main__ - Global step 350 Train loss 1.40 QA-F1 0.30416666666666664 on epoch=174
03/10/2022 15:57:59 - INFO - __main__ - Saving model with best QA-F1: 0.29687499999999994 -> 0.30416666666666664 on epoch=174, global_step=350
03/10/2022 15:58:03 - INFO - __main__ - Step 360 Global step 360 Train loss 1.39 on epoch=179
03/10/2022 15:58:07 - INFO - __main__ - Step 370 Global step 370 Train loss 1.43 on epoch=184
03/10/2022 15:58:12 - INFO - __main__ - Step 380 Global step 380 Train loss 1.43 on epoch=189
03/10/2022 15:58:16 - INFO - __main__ - Step 390 Global step 390 Train loss 1.35 on epoch=194
03/10/2022 15:58:20 - INFO - __main__ - Step 400 Global step 400 Train loss 1.31 on epoch=199
03/10/2022 15:58:22 - INFO - __main__ - Global step 400 Train loss 1.38 QA-F1 0.29374999999999996 on epoch=199
03/10/2022 15:58:26 - INFO - __main__ - Step 410 Global step 410 Train loss 1.31 on epoch=204
03/10/2022 15:58:31 - INFO - __main__ - Step 420 Global step 420 Train loss 1.30 on epoch=209
03/10/2022 15:58:35 - INFO - __main__ - Step 430 Global step 430 Train loss 1.30 on epoch=214
03/10/2022 15:58:40 - INFO - __main__ - Step 440 Global step 440 Train loss 1.30 on epoch=219
03/10/2022 15:58:44 - INFO - __main__ - Step 450 Global step 450 Train loss 1.25 on epoch=224
03/10/2022 15:58:45 - INFO - __main__ - Global step 450 Train loss 1.29 QA-F1 0.32499999999999996 on epoch=224
03/10/2022 15:58:45 - INFO - __main__ - Saving model with best QA-F1: 0.30416666666666664 -> 0.32499999999999996 on epoch=224, global_step=450
03/10/2022 15:58:50 - INFO - __main__ - Step 460 Global step 460 Train loss 1.34 on epoch=229
03/10/2022 15:58:54 - INFO - __main__ - Step 470 Global step 470 Train loss 1.56 on epoch=234
03/10/2022 15:58:58 - INFO - __main__ - Step 480 Global step 480 Train loss 1.41 on epoch=239
03/10/2022 15:59:03 - INFO - __main__ - Step 490 Global step 490 Train loss 1.37 on epoch=244
03/10/2022 15:59:07 - INFO - __main__ - Step 500 Global step 500 Train loss 1.26 on epoch=249
03/10/2022 15:59:09 - INFO - __main__ - Global step 500 Train loss 1.39 QA-F1 0.2989583333333333 on epoch=249
03/10/2022 15:59:13 - INFO - __main__ - Step 510 Global step 510 Train loss 1.39 on epoch=254
03/10/2022 15:59:18 - INFO - __main__ - Step 520 Global step 520 Train loss 1.21 on epoch=259
03/10/2022 15:59:22 - INFO - __main__ - Step 530 Global step 530 Train loss 1.34 on epoch=264
03/10/2022 15:59:26 - INFO - __main__ - Step 540 Global step 540 Train loss 1.30 on epoch=269
03/10/2022 15:59:31 - INFO - __main__ - Step 550 Global step 550 Train loss 1.21 on epoch=274
03/10/2022 15:59:32 - INFO - __main__ - Global step 550 Train loss 1.29 QA-F1 0.29374999999999996 on epoch=274
03/10/2022 15:59:37 - INFO - __main__ - Step 560 Global step 560 Train loss 1.17 on epoch=279
03/10/2022 15:59:41 - INFO - __main__ - Step 570 Global step 570 Train loss 1.27 on epoch=284
03/10/2022 15:59:45 - INFO - __main__ - Step 580 Global step 580 Train loss 1.25 on epoch=289
03/10/2022 15:59:50 - INFO - __main__ - Step 590 Global step 590 Train loss 1.16 on epoch=294
03/10/2022 15:59:54 - INFO - __main__ - Step 600 Global step 600 Train loss 1.31 on epoch=299
03/10/2022 15:59:56 - INFO - __main__ - Global step 600 Train loss 1.23 QA-F1 0.3458333333333333 on epoch=299
03/10/2022 15:59:56 - INFO - __main__ - Saving model with best QA-F1: 0.32499999999999996 -> 0.3458333333333333 on epoch=299, global_step=600
03/10/2022 16:00:00 - INFO - __main__ - Step 610 Global step 610 Train loss 1.19 on epoch=304
03/10/2022 16:00:04 - INFO - __main__ - Step 620 Global step 620 Train loss 1.24 on epoch=309
03/10/2022 16:00:09 - INFO - __main__ - Step 630 Global step 630 Train loss 1.23 on epoch=314
03/10/2022 16:00:13 - INFO - __main__ - Step 640 Global step 640 Train loss 1.05 on epoch=319
03/10/2022 16:00:17 - INFO - __main__ - Step 650 Global step 650 Train loss 1.22 on epoch=324
03/10/2022 16:00:19 - INFO - __main__ - Global step 650 Train loss 1.19 QA-F1 0.29374999999999996 on epoch=324
03/10/2022 16:00:23 - INFO - __main__ - Step 660 Global step 660 Train loss 1.11 on epoch=329
03/10/2022 16:00:28 - INFO - __main__ - Step 670 Global step 670 Train loss 1.10 on epoch=334
03/10/2022 16:00:32 - INFO - __main__ - Step 680 Global step 680 Train loss 1.16 on epoch=339
03/10/2022 16:00:36 - INFO - __main__ - Step 690 Global step 690 Train loss 1.17 on epoch=344
03/10/2022 16:00:41 - INFO - __main__ - Step 700 Global step 700 Train loss 1.15 on epoch=349
03/10/2022 16:00:42 - INFO - __main__ - Global step 700 Train loss 1.14 QA-F1 0.2798611111111111 on epoch=349
03/10/2022 16:00:47 - INFO - __main__ - Step 710 Global step 710 Train loss 1.05 on epoch=354
03/10/2022 16:00:51 - INFO - __main__ - Step 720 Global step 720 Train loss 1.13 on epoch=359
03/10/2022 16:00:55 - INFO - __main__ - Step 730 Global step 730 Train loss 1.06 on epoch=364
03/10/2022 16:01:00 - INFO - __main__ - Step 740 Global step 740 Train loss 1.16 on epoch=369
03/10/2022 16:01:04 - INFO - __main__ - Step 750 Global step 750 Train loss 1.01 on epoch=374
03/10/2022 16:01:06 - INFO - __main__ - Global step 750 Train loss 1.08 QA-F1 0.29374999999999996 on epoch=374
03/10/2022 16:01:10 - INFO - __main__ - Step 760 Global step 760 Train loss 1.10 on epoch=379
03/10/2022 16:01:14 - INFO - __main__ - Step 770 Global step 770 Train loss 1.04 on epoch=384
03/10/2022 16:01:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.99 on epoch=389
03/10/2022 16:01:23 - INFO - __main__ - Step 790 Global step 790 Train loss 1.07 on epoch=394
03/10/2022 16:01:27 - INFO - __main__ - Step 800 Global step 800 Train loss 1.02 on epoch=399
03/10/2022 16:01:29 - INFO - __main__ - Global step 800 Train loss 1.04 QA-F1 0.26249999999999996 on epoch=399
03/10/2022 16:01:33 - INFO - __main__ - Step 810 Global step 810 Train loss 1.26 on epoch=404
03/10/2022 16:01:38 - INFO - __main__ - Step 820 Global step 820 Train loss 1.02 on epoch=409
03/10/2022 16:01:42 - INFO - __main__ - Step 830 Global step 830 Train loss 1.05 on epoch=414
03/10/2022 16:01:46 - INFO - __main__ - Step 840 Global step 840 Train loss 1.03 on epoch=419
03/10/2022 16:01:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.99 on epoch=424
03/10/2022 16:01:52 - INFO - __main__ - Global step 850 Train loss 1.07 QA-F1 0.3111111111111111 on epoch=424
03/10/2022 16:01:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.94 on epoch=429
03/10/2022 16:02:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.96 on epoch=434
03/10/2022 16:02:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.94 on epoch=439
03/10/2022 16:02:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.91 on epoch=444
03/10/2022 16:02:14 - INFO - __main__ - Step 900 Global step 900 Train loss 1.07 on epoch=449
03/10/2022 16:02:16 - INFO - __main__ - Global step 900 Train loss 0.96 QA-F1 0.2361111111111111 on epoch=449
03/10/2022 16:02:20 - INFO - __main__ - Step 910 Global step 910 Train loss 1.02 on epoch=454
03/10/2022 16:02:25 - INFO - __main__ - Step 920 Global step 920 Train loss 0.87 on epoch=459
03/10/2022 16:02:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.92 on epoch=464
03/10/2022 16:02:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.97 on epoch=469
03/10/2022 16:02:38 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=474
03/10/2022 16:02:39 - INFO - __main__ - Global step 950 Train loss 0.92 QA-F1 0.24861111111111112 on epoch=474
03/10/2022 16:02:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.87 on epoch=479
03/10/2022 16:02:48 - INFO - __main__ - Step 970 Global step 970 Train loss 0.99 on epoch=484
03/10/2022 16:02:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.97 on epoch=489
03/10/2022 16:02:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.99 on epoch=494
03/10/2022 16:03:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.81 on epoch=499
03/10/2022 16:03:02 - INFO - __main__ - Global step 1000 Train loss 0.93 QA-F1 0.2520833333333333 on epoch=499
03/10/2022 16:03:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.75 on epoch=504
03/10/2022 16:03:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.90 on epoch=509
03/10/2022 16:03:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.84 on epoch=514
03/10/2022 16:03:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.76 on epoch=519
03/10/2022 16:03:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.83 on epoch=524
03/10/2022 16:03:26 - INFO - __main__ - Global step 1050 Train loss 0.82 QA-F1 0.24861111111111112 on epoch=524
03/10/2022 16:03:30 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.95 on epoch=529
03/10/2022 16:03:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.83 on epoch=534
03/10/2022 16:03:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.82 on epoch=539
03/10/2022 16:03:43 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.74 on epoch=544
03/10/2022 16:03:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.86 on epoch=549
03/10/2022 16:03:49 - INFO - __main__ - Global step 1100 Train loss 0.84 QA-F1 0.27291666666666664 on epoch=549
03/10/2022 16:03:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.80 on epoch=554
03/10/2022 16:03:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.81 on epoch=559
03/10/2022 16:04:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.85 on epoch=564
03/10/2022 16:04:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.85 on epoch=569
03/10/2022 16:04:11 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.82 on epoch=574
03/10/2022 16:04:13 - INFO - __main__ - Global step 1150 Train loss 0.83 QA-F1 0.27291666666666664 on epoch=574
03/10/2022 16:04:17 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.78 on epoch=579
03/10/2022 16:04:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.81 on epoch=584
03/10/2022 16:04:26 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.78 on epoch=589
03/10/2022 16:04:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.73 on epoch=594
03/10/2022 16:04:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.80 on epoch=599
03/10/2022 16:04:36 - INFO - __main__ - Global step 1200 Train loss 0.78 QA-F1 0.21041666666666664 on epoch=599
03/10/2022 16:04:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.88 on epoch=604
03/10/2022 16:04:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.72 on epoch=609
03/10/2022 16:04:49 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.70 on epoch=614
03/10/2022 16:04:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.73 on epoch=619
03/10/2022 16:04:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.72 on epoch=624
03/10/2022 16:04:59 - INFO - __main__ - Global step 1250 Train loss 0.75 QA-F1 0.21875 on epoch=624
03/10/2022 16:05:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.72 on epoch=629
03/10/2022 16:05:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.74 on epoch=634
03/10/2022 16:05:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.72 on epoch=639
03/10/2022 16:05:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.78 on epoch=644
03/10/2022 16:05:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.81 on epoch=649
03/10/2022 16:05:22 - INFO - __main__ - Global step 1300 Train loss 0.75 QA-F1 0.23124999999999998 on epoch=649
03/10/2022 16:05:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.73 on epoch=654
03/10/2022 16:05:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.78 on epoch=659
03/10/2022 16:05:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.69 on epoch=664
03/10/2022 16:05:40 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.76 on epoch=669
03/10/2022 16:05:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.67 on epoch=674
03/10/2022 16:05:46 - INFO - __main__ - Global step 1350 Train loss 0.72 QA-F1 0.2578125 on epoch=674
03/10/2022 16:05:50 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.65 on epoch=679
03/10/2022 16:05:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.71 on epoch=684
03/10/2022 16:05:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.65 on epoch=689
03/10/2022 16:06:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.63 on epoch=694
03/10/2022 16:06:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.59 on epoch=699
03/10/2022 16:06:09 - INFO - __main__ - Global step 1400 Train loss 0.65 QA-F1 0.23124999999999998 on epoch=699
03/10/2022 16:06:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.58 on epoch=704
03/10/2022 16:06:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.66 on epoch=709
03/10/2022 16:06:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.62 on epoch=714
03/10/2022 16:06:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.63 on epoch=719
03/10/2022 16:06:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.70 on epoch=724
03/10/2022 16:06:32 - INFO - __main__ - Global step 1450 Train loss 0.64 QA-F1 0.27031249999999996 on epoch=724
03/10/2022 16:06:37 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.70 on epoch=729
03/10/2022 16:06:41 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.61 on epoch=734
03/10/2022 16:06:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.69 on epoch=739
03/10/2022 16:06:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.59 on epoch=744
03/10/2022 16:06:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.66 on epoch=749
03/10/2022 16:06:56 - INFO - __main__ - Global step 1500 Train loss 0.65 QA-F1 0.26249999999999996 on epoch=749
03/10/2022 16:07:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.66 on epoch=754
03/10/2022 16:07:04 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.54 on epoch=759
03/10/2022 16:07:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.51 on epoch=764
03/10/2022 16:07:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.55 on epoch=769
03/10/2022 16:07:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=774
03/10/2022 16:07:19 - INFO - __main__ - Global step 1550 Train loss 0.55 QA-F1 0.25 on epoch=774
03/10/2022 16:07:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.57 on epoch=779
03/10/2022 16:07:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=784
03/10/2022 16:07:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.58 on epoch=789
03/10/2022 16:07:36 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.52 on epoch=794
03/10/2022 16:07:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=799
03/10/2022 16:07:42 - INFO - __main__ - Global step 1600 Train loss 0.53 QA-F1 0.3145833333333333 on epoch=799
03/10/2022 16:07:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.55 on epoch=804
03/10/2022 16:07:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.59 on epoch=809
03/10/2022 16:07:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.58 on epoch=814
03/10/2022 16:08:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.51 on epoch=819
03/10/2022 16:08:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=824
03/10/2022 16:08:06 - INFO - __main__ - Global step 1650 Train loss 0.55 QA-F1 0.2520833333333333 on epoch=824
03/10/2022 16:08:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.55 on epoch=829
03/10/2022 16:08:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.61 on epoch=834
03/10/2022 16:08:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.57 on epoch=839
03/10/2022 16:08:23 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.52 on epoch=844
03/10/2022 16:08:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=849
03/10/2022 16:08:29 - INFO - __main__ - Global step 1700 Train loss 0.53 QA-F1 0.25 on epoch=849
03/10/2022 16:08:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.48 on epoch=854
03/10/2022 16:08:38 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.50 on epoch=859
03/10/2022 16:08:43 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
03/10/2022 16:08:47 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=869
03/10/2022 16:08:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.49 on epoch=874
03/10/2022 16:08:53 - INFO - __main__ - Global step 1750 Train loss 0.48 QA-F1 0.24999999999999997 on epoch=874
03/10/2022 16:08:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=879
03/10/2022 16:09:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.52 on epoch=884
03/10/2022 16:09:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.41 on epoch=889
03/10/2022 16:09:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.45 on epoch=894
03/10/2022 16:09:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.46 on epoch=899
03/10/2022 16:09:16 - INFO - __main__ - Global step 1800 Train loss 0.46 QA-F1 0.28125 on epoch=899
03/10/2022 16:09:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.50 on epoch=904
03/10/2022 16:09:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.52 on epoch=909
03/10/2022 16:09:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
03/10/2022 16:09:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
03/10/2022 16:09:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
03/10/2022 16:09:40 - INFO - __main__ - Global step 1850 Train loss 0.45 QA-F1 0.27291666666666664 on epoch=924
03/10/2022 16:09:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.53 on epoch=929
03/10/2022 16:09:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.51 on epoch=934
03/10/2022 16:09:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.46 on epoch=939
03/10/2022 16:09:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.48 on epoch=944
03/10/2022 16:10:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
03/10/2022 16:10:03 - INFO - __main__ - Global step 1900 Train loss 0.49 QA-F1 0.29513888888888884 on epoch=949
03/10/2022 16:10:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.43 on epoch=954
03/10/2022 16:10:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.42 on epoch=959
03/10/2022 16:10:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.42 on epoch=964
03/10/2022 16:10:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
03/10/2022 16:10:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.39 on epoch=974
03/10/2022 16:10:26 - INFO - __main__ - Global step 1950 Train loss 0.42 QA-F1 0.32499999999999996 on epoch=974
03/10/2022 16:10:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.48 on epoch=979
03/10/2022 16:10:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.38 on epoch=984
03/10/2022 16:10:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=989
03/10/2022 16:10:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.44 on epoch=994
03/10/2022 16:10:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.48 on epoch=999
03/10/2022 16:10:50 - INFO - __main__ - Global step 2000 Train loss 0.45 QA-F1 0.32499999999999996 on epoch=999
03/10/2022 16:10:50 - INFO - __main__ - save last model!
03/10/2022 16:10:50 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 16:10:50 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 16:10:50 - INFO - __main__ - Printing 3 examples
03/10/2022 16:10:50 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 16:10:50 - INFO - __main__ - ['Frankie']
03/10/2022 16:10:50 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 16:10:50 - INFO - __main__ - ['Frankie']
03/10/2022 16:10:50 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 16:10:50 - INFO - __main__ - ['Frankie']
03/10/2022 16:10:50 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:10:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:10:50 - INFO - __main__ - Printing 3 examples
03/10/2022 16:10:50 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/10/2022 16:10:50 - INFO - __main__ - ['Ringo Starr']
03/10/2022 16:10:50 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/10/2022 16:10:50 - INFO - __main__ - ['Jimmy']
03/10/2022 16:10:50 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/10/2022 16:10:50 - INFO - __main__ - ['The Alan Bush Music Trust']
03/10/2022 16:10:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:10:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:10:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:10:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:10:51 - INFO - __main__ - Printing 3 examples
03/10/2022 16:10:51 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/10/2022 16:10:51 - INFO - __main__ - ['Elk', 'pronghorn']
03/10/2022 16:10:51 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/10/2022 16:10:51 - INFO - __main__ - ['Jeff']
03/10/2022 16:10:51 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/10/2022 16:10:51 - INFO - __main__ - ['November 1991']
03/10/2022 16:10:51 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:10:51 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:10:51 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:10:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:10:57 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 16:11:04 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 16:11:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 16:11:05 - INFO - __main__ - Starting training!
03/10/2022 16:13:09 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_100_0.3_8_predictions.txt
03/10/2022 16:13:09 - INFO - __main__ - QA-F1 on test data: 0.2283
03/10/2022 16:13:10 - INFO - __main__ - prefix=quoref_32_100, lr=0.3, bsz=8, dev_performance=0.3458333333333333, test_performance=0.2282635524571008
03/10/2022 16:13:10 - INFO - __main__ - Running ... prefix=quoref_32_100, lr=0.2, bsz=8 ...
03/10/2022 16:13:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:13:10 - INFO - __main__ - Printing 3 examples
03/10/2022 16:13:10 - INFO - __main__ -  [quoref] question: What is the full professional name of the person who occasionally sang lead vocals in the Beatles? context: Sir Richard Starkey  (born 7 July 1940), known professionally as Ringo Starr, is an English musician, singer, songwriter and actor who gained worldwide fame as the drummer for the Beatles. He occasionally sang lead vocals with the group, usually for one song on each album, including "With a Little Help from My Friends", "Yellow Submarine", "Good Night", and their cover of "Act Naturally". He also wrote and sang the Beatles' songs "Don't Pass Me By" and "Octopus's Garden", and is credited as a co-writer of others, including "What Goes On" and "Flying". Starr was afflicted by life-threatening illnesses during childhood, and he fell behind in school as a result of prolonged hospitalisations. He briefly held a position with British Rail before securing an apprenticeship at a Liverpool equipment manufacturer. Soon afterwards, he became interested in the UK skiffle craze and developed a fervent admiration for the genre. In 1957, he co-founded his first band, the Eddie Clayton Skiffle Group, which earned several prestigious local bookings before the fad succumbed to American rock and roll by early 1958. When the Beatles formed in 1960, Starr was a member of another Liverpool group, Rory Storm and the Hurricanes. After achieving moderate success in the UK and Hamburg, he quit the Hurricanes and joined the Beatles in August 1962, replacing Pete Best. Starr played key roles in the Beatles' films and appeared in numerous others. After the band's break-up in 1970, he released several successful singles including the US number-four hit "It Don't Come Easy", and number ones "Photograph" and "You're Sixteen". In 1972, he released his most successful UK single, "Back Off Boogaloo", which peaked at number two. He achieved commercial and critical success with his 1973 album Ringo, which was a top-ten release in both the UK and the US. He has featured in a number of documentaries and hosted television shows.  He also narrated the first two series of the children's television programme Thomas & Friends and portrayed "Mr Conductor" during the first season of the PBS children's television series Shining Time Station. Since 1989, he has toured with thirteen variations of Ringo Starr & His All-Starr Band. Starr's musicianship has received praise from other drummers, including Phil Collins and Journey's Steve Smith. He was inducted into the Modern Drummer Hall of Fame in 1998. In 2011, Rolling Stone readers named Starr the fifth-greatest drummer of all time. Starr, who was previously inducted into the Rock and Roll Hall of Fame as a Beatle in 1988, was inducted for his solo career in 2015, making him one of 21 performers inducted more than once. He is the richest drummer in the world with a net worth of US$350 million. He was appointed a Knight Bachelor in the 2018 New Year Honours for services to music.
03/10/2022 16:13:10 - INFO - __main__ - ['Ringo Starr']
03/10/2022 16:13:10 - INFO - __main__ -  [quoref] question: What is the first name of the person that is arrested for murder? context: Charles Spotswoode's son Jimmy became involved with "the Canary", a conniving showgirl. The Canary, determined to force Jimmy to marry her so she can join the social elite, threatens to reveal that Jimmy was embezzling from his father. She turns down the elder Spotswoode's bribe to leave Jimmy alone. She telephones two men she has been blackmailing, Cleaver and Mannix, and demands one final generous gift from each of them by the next day. She makes the same request of "creepy" admirer Dr. Lindquist. Her ex-husband Tony Sheel eavesdrops and wants half, but she refuses to give him anything, even after he hits her. Cleaver, Mannix and Lindquist are all shown lurking about her apartment building late that night. Spotswoode visits her at her apartment around midnight, but cannot get her to change her mind. After he reaches the lobby of her building, he and another person hear screams from her place. They knock on the door, but she assures them that she is fine. The Canary is found strangled the next day. The coroner places the time of death around midnight. District Attorney Markham investigates, aided by Philo Vance (a close friend of Charles Spotswoode) and Police Sergeant Heath. After all the suspects are brought in for questioning, Vance asks Markham to keep them waiting for a few hours. Markham agrees. Vance subtly maneuvers Cleaver, Mannix, Lindquist and the two Spotswoodes into playing poker to pass the time so he can observe their personality traits. Only one shows the daring, imagination and discipline required for the crime; that man bluffs Vance, betting everything with just a pair of deuces. The suspects are then released. Sheel, who witnessed the murder while hiding in the closet, sends the killer several blackmail letters. He too is strangled. A pen found at the scene has Jimmy's name on it, so Heath arrests him for the murder. Jimmy then confesses to both murders, but Vance knows better.
03/10/2022 16:13:10 - INFO - __main__ - ['Jimmy']
03/10/2022 16:13:10 - INFO - __main__ -  [quoref] question: What was founded in 1997 by family members of the man who had a long career as a teacher? context: Bush's long career as a teacher influenced generations of English composers and performers. Tippett was never a formal pupil, but has acknowledged a deep debt to Bush.  Herbert Murrill, a pupil of Bush's at the RAM in the 1920s,   wrote in 1950 of his tutor: "[T]here is humility in his makeup, and I believe that no man can achieve greatness in the arts without humility ... To Alan Bush I owe much, not least the artistic strength and right to differ from him". Among postwar Bush students who later led distinguished careers are the composers Edward Gregson, Roger Steptoe and Michael Nyman, and the pianists John Bingham and Graham Johnson.  Through his sponsorship of the London String Quartet Bush helped launch the careers of string players such as Norbert Brainin and Emanuel Hurwitz, both of whom later achieved international recognition.Bush's music was under-represented in the concert repertoire in his lifetime, and virtually disappeared after his death. The 2000 centenary of his birth was markedly low key; the Prom season ignored him, although there was a memorial concert at the Wigmore Hall on 1 November, and a BBC broadcast of the Piano Concerto on 19 December.   The centenary, albeit  quietly observed, helped to introduce the name and music of Bush to a new generation of music lovers, and generated an increase in both performance and recordings. The centenary also heralded an awakening of scholarly interest in Bush, whose life and works were the subject of numerous PhD theses in the early 20th century. Scholars such as Paul Harper-Scott and Joanna Bullivant have obtained access to new material, including documents released since the collapse of the Soviet Union, and Bush's MI5 file.  This, says Bullivant, enables a more informed assessment of the interrelationships within  Bush's music and his communism, and of the inherent conflicting priorities.In October 1997 family members and friends founded The Alan Bush Music Trust "to promote the education and appreciation by the public in and of music and, in particular, the works of the British composer Alan Bush". The trust provides a newsletter, features news stories, promotes performances and  recordings of Bush's works, and through its website reproduces wide-ranging critical and biographical material. It continues to monitor  concert performances of Bush's works, and other Bush-related events, at home and abroad.At the time of Bush's centenary, Martin Anderson, writing in the British Music Information Centre's newsletter, summarised Bush's compositional career: Bush was not a natural melodist à la Dvorák, though he could produce an appealing tune when he set his mind to it. But he was a first-rate contrapuntist, and his harmonic world can glow with a rare internal warmth. It would be foolish to claim that everything he wrote was a masterpiece – and equally idiotic to turn our backs on the many outstanding scores still awaiting assiduous attention.
03/10/2022 16:13:10 - INFO - __main__ - ['The Alan Bush Music Trust']
03/10/2022 16:13:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:13:11 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:13:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:13:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:13:11 - INFO - __main__ - Printing 3 examples
03/10/2022 16:13:11 - INFO - __main__ -  [quoref] question: Which animals sometimes wander into Bryce Canyon? context: More than 400 native plant species live in the park. There are three life zones in the park based on elevation:  The lowest areas of the park are dominated by dwarf forests of pinyon pine and juniper with manzanita, serviceberry, and antelope bitterbrush in between. Aspen, cottonwood, water birch, and willow grow along streams. Ponderosa pine forests cover the mid-elevations with blue spruce and Douglas fir in water-rich areas and manzanita and bitterbrush as underbrush. Douglas fir and white fir, along with aspen and Engelmann spruce, make up the forests on the Paunsaugunt Plateau. The harshest areas have limber pine and ancient Great Basin bristlecone pine, some more than 1,600 years old, holding on.The forests and meadows of Bryce Canyon provide the habitat to support diverse animal life including foxes, badgers, porcupines, elk, black bears, bobcats, and woodpeckers. Mule deer are the most common large mammals in the park. Elk and pronghorn, which have been reintroduced nearby, sometimes venture into the park. Bryce Canyon National Park forms part of the habitat of three wildlife species that are listed under the Endangered Species Act: the Utah prairie dog, the California condor, and the southwestern willow flycatcher.  The Utah prairie dog is a threatened species that was reintroduced to the park for conservation, and the largest protected population is found within the park's boundaries.About 170 species of birds visit the park each year, including swifts and swallows. Most species migrate to warmer regions in winter, although jays, ravens, nuthatches, eagles, and owls stay. In winter, the mule deer, cougars, and coyotes migrate to lower elevations. Ground squirrels and marmots pass the winter in hibernation.Eleven species of reptiles and four species of amphibians have been found in the park. Reptiles include the Great Basin rattlesnake, short-horned lizard, side-blotched lizard, striped whipsnake, and the tiger salamander.Also in the park are the black, lumpy, very slow-growing colonies of cryptobiotic soil, which are a mix of lichens, algae, fungi, and cyanobacteria. Together these organisms slow erosion, add nitrogen to soil, and help it to retain moisture.
03/10/2022 16:13:11 - INFO - __main__ - ['Elk', 'pronghorn']
03/10/2022 16:13:11 - INFO - __main__ -  [quoref] question: What is the first name of the person that is intercepted by Ding Bell? context: Tired of killing, war veteran Jefferson Waring rides west, but in Missouri he sees "squatters" mowed down by men working for rich, ruthless Artemus Taylor. He spends the night at Independence newspaperman Peter Sharpe's place, but is jailed when daughter Cathy Sharpe finds this total stranger in her room. The local marshal, John Harding, is just one of many men on Taylor's payroll. Peter's business is threatened by banker Stone unless he takes Taylor's side against "squatters" settling in the region. The blind and wheelchair-bound Taylor and ambitious daughter Norah are secretly aware that railroad surveyors are considering laying tracks nearby, so they want all the land for themselves. Jeff decides to leave. Norah and henchman Ding Bell intercept him; Norah shoots at him but misses. They take him to see Artemus, who tells a vocally reluctant Bell to take Jeff off to a remote canyon and murder him. Under Norah's instructions, Artemus's chief thug Sam Tobin goes after them to murder both; he wounds Jeff and kills Bell, but not before Bell hits him with a fatal shot. A doctor treats Jeff's wounds but Marshall Harding turns up and charges Jeff with the two killings. When the situation escalates and two of Taylor's thugs gun down Peter Sharpe, Jeff breaks out of jail and organizes a group of settlers to resist Taylor's planned big attack. The settlers slaughter Taylor's thugs; Taylor dies of a heart attack; Norah, having shot and she thinks killed banker Justin Stone in order to get some getaway money, is killed by him as she leaves. Jeff stays in town to run the paper with Cathy.
03/10/2022 16:13:11 - INFO - __main__ - ['Jeff']
03/10/2022 16:13:11 - INFO - __main__ -  [quoref] question: What month and year was the band's seventh studio album released? context: Stung by the criticism of Rattle and Hum, the band sought to transform themselves musically. Seeking inspiration from German reunification, they began work on their seventh studio album, Achtung Baby, at Berlin's Hansa Studios in October 1990 with producers Daniel Lanois and Brian Eno. The sessions were fraught with conflict, as the band argued over their musical direction and the quality of their material. While Clayton and Mullen preferred a sound similar to U2's previous work, Bono and the Edge were inspired by European industrial music and electronic dance music and advocated a change. Weeks of tension and slow progress nearly prompted the group to break up until they made a breakthrough with the improvised writing of the song "One". They returned to Dublin in 1991, where morale improved and the majority of the album was completed. Achtung Baby was released in November 1991. The album represented a calculated change in musical and thematic direction for the group; the shift was one of their most dramatic since The Unforgettable Fire. Sonically, the record incorporated influences from alternative rock, dance, and industrial music of the time, and Bono referred to its musical departure as "four men chopping down the Joshua Tree". Thematically, it was a more introspective and personal record; it was darker, yet at times more flippant than the band's previous work. Commercially and critically, it has been one of the band's most successful albums. It produced five hit singles, including "The Fly", "Mysterious Ways", and "One", and it was a crucial part of the band's early 1990s reinvention. In 1993, Achtung Baby won the Grammy Award for Best Rock Performance by a Duo or Group with Vocal. Like The Joshua Tree, many publications have cited the record as one of rock's greatest.
03/10/2022 16:13:11 - INFO - __main__ - ['November 1991']
03/10/2022 16:13:11 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:13:11 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:13:11 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:13:24 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 16:13:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 16:13:25 - INFO - __main__ - Starting training!
03/10/2022 16:13:30 - INFO - __main__ - Step 10 Global step 10 Train loss 3.40 on epoch=4
03/10/2022 16:13:34 - INFO - __main__ - Step 20 Global step 20 Train loss 2.99 on epoch=9
03/10/2022 16:13:38 - INFO - __main__ - Step 30 Global step 30 Train loss 2.56 on epoch=14
03/10/2022 16:13:43 - INFO - __main__ - Step 40 Global step 40 Train loss 2.51 on epoch=19
03/10/2022 16:13:47 - INFO - __main__ - Step 50 Global step 50 Train loss 2.39 on epoch=24
03/10/2022 16:13:49 - INFO - __main__ - Global step 50 Train loss 2.77 QA-F1 0.22916666666666666 on epoch=24
03/10/2022 16:13:49 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.22916666666666666 on epoch=24, global_step=50
03/10/2022 16:13:53 - INFO - __main__ - Step 60 Global step 60 Train loss 2.00 on epoch=29
03/10/2022 16:13:57 - INFO - __main__ - Step 70 Global step 70 Train loss 1.86 on epoch=34
03/10/2022 16:14:02 - INFO - __main__ - Step 80 Global step 80 Train loss 1.76 on epoch=39
03/10/2022 16:14:06 - INFO - __main__ - Step 90 Global step 90 Train loss 1.78 on epoch=44
03/10/2022 16:14:10 - INFO - __main__ - Step 100 Global step 100 Train loss 1.76 on epoch=49
03/10/2022 16:14:12 - INFO - __main__ - Global step 100 Train loss 1.83 QA-F1 0.28125 on epoch=49
03/10/2022 16:14:12 - INFO - __main__ - Saving model with best QA-F1: 0.22916666666666666 -> 0.28125 on epoch=49, global_step=100
03/10/2022 16:14:16 - INFO - __main__ - Step 110 Global step 110 Train loss 1.67 on epoch=54
03/10/2022 16:14:20 - INFO - __main__ - Step 120 Global step 120 Train loss 1.70 on epoch=59
03/10/2022 16:14:25 - INFO - __main__ - Step 130 Global step 130 Train loss 1.71 on epoch=64
03/10/2022 16:14:29 - INFO - __main__ - Step 140 Global step 140 Train loss 1.67 on epoch=69
03/10/2022 16:14:33 - INFO - __main__ - Step 150 Global step 150 Train loss 1.69 on epoch=74
03/10/2022 16:14:35 - INFO - __main__ - Global step 150 Train loss 1.69 QA-F1 0.2520833333333333 on epoch=74
03/10/2022 16:14:39 - INFO - __main__ - Step 160 Global step 160 Train loss 1.58 on epoch=79
03/10/2022 16:14:43 - INFO - __main__ - Step 170 Global step 170 Train loss 1.37 on epoch=84
03/10/2022 16:14:48 - INFO - __main__ - Step 180 Global step 180 Train loss 1.38 on epoch=89
03/10/2022 16:14:52 - INFO - __main__ - Step 190 Global step 190 Train loss 1.43 on epoch=94
03/10/2022 16:14:56 - INFO - __main__ - Step 200 Global step 200 Train loss 1.36 on epoch=99
03/10/2022 16:14:58 - INFO - __main__ - Global step 200 Train loss 1.42 QA-F1 0.2743055555555556 on epoch=99
03/10/2022 16:15:02 - INFO - __main__ - Step 210 Global step 210 Train loss 1.39 on epoch=104
03/10/2022 16:15:06 - INFO - __main__ - Step 220 Global step 220 Train loss 1.27 on epoch=109
03/10/2022 16:15:11 - INFO - __main__ - Step 230 Global step 230 Train loss 1.28 on epoch=114
03/10/2022 16:15:15 - INFO - __main__ - Step 240 Global step 240 Train loss 1.24 on epoch=119
03/10/2022 16:15:19 - INFO - __main__ - Step 250 Global step 250 Train loss 1.16 on epoch=124
03/10/2022 16:15:21 - INFO - __main__ - Global step 250 Train loss 1.27 QA-F1 0.2743055555555556 on epoch=124
03/10/2022 16:15:25 - INFO - __main__ - Step 260 Global step 260 Train loss 1.19 on epoch=129
03/10/2022 16:15:30 - INFO - __main__ - Step 270 Global step 270 Train loss 1.29 on epoch=134
03/10/2022 16:15:34 - INFO - __main__ - Step 280 Global step 280 Train loss 1.10 on epoch=139
03/10/2022 16:15:38 - INFO - __main__ - Step 290 Global step 290 Train loss 1.23 on epoch=144
03/10/2022 16:15:43 - INFO - __main__ - Step 300 Global step 300 Train loss 1.09 on epoch=149
03/10/2022 16:15:44 - INFO - __main__ - Global step 300 Train loss 1.18 QA-F1 0.2743055555555556 on epoch=149
03/10/2022 16:15:48 - INFO - __main__ - Step 310 Global step 310 Train loss 1.07 on epoch=154
03/10/2022 16:15:53 - INFO - __main__ - Step 320 Global step 320 Train loss 0.92 on epoch=159
03/10/2022 16:15:57 - INFO - __main__ - Step 330 Global step 330 Train loss 1.04 on epoch=164
03/10/2022 16:16:01 - INFO - __main__ - Step 340 Global step 340 Train loss 1.10 on epoch=169
03/10/2022 16:16:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.94 on epoch=174
03/10/2022 16:16:07 - INFO - __main__ - Global step 350 Train loss 1.01 QA-F1 0.2951388888888889 on epoch=174
03/10/2022 16:16:07 - INFO - __main__ - Saving model with best QA-F1: 0.28125 -> 0.2951388888888889 on epoch=174, global_step=350
03/10/2022 16:16:11 - INFO - __main__ - Step 360 Global step 360 Train loss 1.05 on epoch=179
03/10/2022 16:16:16 - INFO - __main__ - Step 370 Global step 370 Train loss 1.02 on epoch=184
03/10/2022 16:16:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.89 on epoch=189
03/10/2022 16:16:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.86 on epoch=194
03/10/2022 16:16:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.86 on epoch=199
03/10/2022 16:16:30 - INFO - __main__ - Global step 400 Train loss 0.94 QA-F1 0.2222222222222222 on epoch=199
03/10/2022 16:16:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.97 on epoch=204
03/10/2022 16:16:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.80 on epoch=209
03/10/2022 16:16:43 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=214
03/10/2022 16:16:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=219
03/10/2022 16:16:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.80 on epoch=224
03/10/2022 16:16:53 - INFO - __main__ - Global step 450 Train loss 0.86 QA-F1 0.2951388888888889 on epoch=224
03/10/2022 16:16:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=229
03/10/2022 16:17:02 - INFO - __main__ - Step 470 Global step 470 Train loss 0.82 on epoch=234
03/10/2022 16:17:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.85 on epoch=239
03/10/2022 16:17:11 - INFO - __main__ - Step 490 Global step 490 Train loss 0.95 on epoch=244
03/10/2022 16:17:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.74 on epoch=249
03/10/2022 16:17:17 - INFO - __main__ - Global step 500 Train loss 0.84 QA-F1 0.24305555555555555 on epoch=249
03/10/2022 16:17:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.73 on epoch=254
03/10/2022 16:17:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.74 on epoch=259
03/10/2022 16:17:30 - INFO - __main__ - Step 530 Global step 530 Train loss 0.74 on epoch=264
03/10/2022 16:17:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.72 on epoch=269
03/10/2022 16:17:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.69 on epoch=274
03/10/2022 16:17:40 - INFO - __main__ - Global step 550 Train loss 0.73 QA-F1 0.2265625 on epoch=274
03/10/2022 16:17:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.79 on epoch=279
03/10/2022 16:17:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.67 on epoch=284
03/10/2022 16:17:53 - INFO - __main__ - Step 580 Global step 580 Train loss 0.73 on epoch=289
03/10/2022 16:17:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.67 on epoch=294
03/10/2022 16:18:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.65 on epoch=299
03/10/2022 16:18:03 - INFO - __main__ - Global step 600 Train loss 0.70 QA-F1 0.2326388888888889 on epoch=299
03/10/2022 16:18:07 - INFO - __main__ - Step 610 Global step 610 Train loss 0.68 on epoch=304
03/10/2022 16:18:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.56 on epoch=309
03/10/2022 16:18:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.62 on epoch=314
03/10/2022 16:18:20 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=319
03/10/2022 16:18:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.52 on epoch=324
03/10/2022 16:18:26 - INFO - __main__ - Global step 650 Train loss 0.59 QA-F1 0.21180555555555555 on epoch=324
03/10/2022 16:18:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.55 on epoch=329
03/10/2022 16:18:34 - INFO - __main__ - Step 670 Global step 670 Train loss 0.58 on epoch=334
03/10/2022 16:18:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.53 on epoch=339
03/10/2022 16:18:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.59 on epoch=344
03/10/2022 16:18:47 - INFO - __main__ - Step 700 Global step 700 Train loss 0.58 on epoch=349
03/10/2022 16:18:49 - INFO - __main__ - Global step 700 Train loss 0.57 QA-F1 0.2482638888888889 on epoch=349
03/10/2022 16:18:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=354
03/10/2022 16:18:57 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
03/10/2022 16:19:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.50 on epoch=364
03/10/2022 16:19:06 - INFO - __main__ - Step 740 Global step 740 Train loss 0.56 on epoch=369
03/10/2022 16:19:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=374
03/10/2022 16:19:12 - INFO - __main__ - Global step 750 Train loss 0.52 QA-F1 0.2361111111111111 on epoch=374
03/10/2022 16:19:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.53 on epoch=379
03/10/2022 16:19:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=384
03/10/2022 16:19:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=389
03/10/2022 16:19:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=394
03/10/2022 16:19:33 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=399
03/10/2022 16:19:35 - INFO - __main__ - Global step 800 Train loss 0.48 QA-F1 0.2729166666666667 on epoch=399
03/10/2022 16:19:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=404
03/10/2022 16:19:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=409
03/10/2022 16:19:48 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=414
03/10/2022 16:19:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.52 on epoch=419
03/10/2022 16:19:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.45 on epoch=424
03/10/2022 16:19:58 - INFO - __main__ - Global step 850 Train loss 0.44 QA-F1 0.21180555555555555 on epoch=424
03/10/2022 16:20:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
03/10/2022 16:20:07 - INFO - __main__ - Step 870 Global step 870 Train loss 0.40 on epoch=434
03/10/2022 16:20:11 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=439
03/10/2022 16:20:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=444
03/10/2022 16:20:20 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
03/10/2022 16:20:22 - INFO - __main__ - Global step 900 Train loss 0.41 QA-F1 0.1953125 on epoch=449
03/10/2022 16:20:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=454
03/10/2022 16:20:30 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=459
03/10/2022 16:20:35 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
03/10/2022 16:20:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
03/10/2022 16:20:43 - INFO - __main__ - Step 950 Global step 950 Train loss 0.42 on epoch=474
03/10/2022 16:20:45 - INFO - __main__ - Global step 950 Train loss 0.42 QA-F1 0.21875 on epoch=474
03/10/2022 16:20:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=479
03/10/2022 16:20:53 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=484
03/10/2022 16:20:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=489
03/10/2022 16:21:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.40 on epoch=494
03/10/2022 16:21:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=499
03/10/2022 16:21:08 - INFO - __main__ - Global step 1000 Train loss 0.39 QA-F1 0.23125 on epoch=499
03/10/2022 16:21:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
03/10/2022 16:21:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=509
03/10/2022 16:21:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=514
03/10/2022 16:21:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=519
03/10/2022 16:21:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=524
03/10/2022 16:21:31 - INFO - __main__ - Global step 1050 Train loss 0.40 QA-F1 0.2569444444444444 on epoch=524
03/10/2022 16:21:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=529
03/10/2022 16:21:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.39 on epoch=534
03/10/2022 16:21:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=539
03/10/2022 16:21:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=544
03/10/2022 16:21:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=549
03/10/2022 16:21:54 - INFO - __main__ - Global step 1100 Train loss 0.36 QA-F1 0.1996527777777778 on epoch=549
03/10/2022 16:21:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
03/10/2022 16:22:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=559
03/10/2022 16:22:07 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=564
03/10/2022 16:22:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=569
03/10/2022 16:22:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=574
03/10/2022 16:22:18 - INFO - __main__ - Global step 1150 Train loss 0.32 QA-F1 0.18340773809523808 on epoch=574
03/10/2022 16:22:22 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=579
03/10/2022 16:22:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=584
03/10/2022 16:22:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=589
03/10/2022 16:22:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=594
03/10/2022 16:22:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=599
03/10/2022 16:22:41 - INFO - __main__ - Global step 1200 Train loss 0.31 QA-F1 0.16875 on epoch=599
03/10/2022 16:22:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
03/10/2022 16:22:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=609
03/10/2022 16:22:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=614
03/10/2022 16:22:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=619
03/10/2022 16:23:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.33 on epoch=624
03/10/2022 16:23:04 - INFO - __main__ - Global step 1250 Train loss 0.29 QA-F1 0.18484848484848485 on epoch=624
03/10/2022 16:23:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=629
03/10/2022 16:23:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=634
03/10/2022 16:23:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=639
03/10/2022 16:23:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=644
03/10/2022 16:23:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.35 on epoch=649
03/10/2022 16:23:28 - INFO - __main__ - Global step 1300 Train loss 0.30 QA-F1 0.18489583333333331 on epoch=649
03/10/2022 16:23:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=654
03/10/2022 16:23:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=659
03/10/2022 16:23:41 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.31 on epoch=664
03/10/2022 16:23:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.26 on epoch=669
03/10/2022 16:23:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=674
03/10/2022 16:23:51 - INFO - __main__ - Global step 1350 Train loss 0.29 QA-F1 0.19479166666666664 on epoch=674
03/10/2022 16:23:56 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=679
03/10/2022 16:24:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=684
03/10/2022 16:24:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.32 on epoch=689
03/10/2022 16:24:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=694
03/10/2022 16:24:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=699
03/10/2022 16:24:15 - INFO - __main__ - Global step 1400 Train loss 0.30 QA-F1 0.2265625 on epoch=699
03/10/2022 16:24:19 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=704
03/10/2022 16:24:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=709
03/10/2022 16:24:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
03/10/2022 16:24:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.27 on epoch=719
03/10/2022 16:24:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=724
03/10/2022 16:24:38 - INFO - __main__ - Global step 1450 Train loss 0.30 QA-F1 0.16540404040404041 on epoch=724
03/10/2022 16:24:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=729
03/10/2022 16:24:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=734
03/10/2022 16:24:51 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=739
03/10/2022 16:24:55 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.30 on epoch=744
03/10/2022 16:25:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.30 on epoch=749
03/10/2022 16:25:02 - INFO - __main__ - Global step 1500 Train loss 0.29 QA-F1 0.2 on epoch=749
03/10/2022 16:25:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=754
03/10/2022 16:25:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
03/10/2022 16:25:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.31 on epoch=764
03/10/2022 16:25:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=769
03/10/2022 16:25:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.31 on epoch=774
03/10/2022 16:25:25 - INFO - __main__ - Global step 1550 Train loss 0.28 QA-F1 0.18489583333333331 on epoch=774
03/10/2022 16:25:29 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=779
03/10/2022 16:25:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=784
03/10/2022 16:25:38 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=789
03/10/2022 16:25:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.30 on epoch=794
03/10/2022 16:25:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=799
03/10/2022 16:25:48 - INFO - __main__ - Global step 1600 Train loss 0.26 QA-F1 0.14930555555555555 on epoch=799
03/10/2022 16:25:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=804
03/10/2022 16:25:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=809
03/10/2022 16:26:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=814
03/10/2022 16:26:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.23 on epoch=819
03/10/2022 16:26:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=824
03/10/2022 16:26:12 - INFO - __main__ - Global step 1650 Train loss 0.23 QA-F1 0.21614583333333331 on epoch=824
03/10/2022 16:26:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=829
03/10/2022 16:26:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=834
03/10/2022 16:26:25 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=839
03/10/2022 16:26:29 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=844
03/10/2022 16:26:33 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=849
03/10/2022 16:26:35 - INFO - __main__ - Global step 1700 Train loss 0.21 QA-F1 0.1284722222222222 on epoch=849
03/10/2022 16:26:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=854
03/10/2022 16:26:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=859
03/10/2022 16:26:48 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=864
03/10/2022 16:26:52 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
03/10/2022 16:26:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=874
03/10/2022 16:26:58 - INFO - __main__ - Global step 1750 Train loss 0.23 QA-F1 0.1953125 on epoch=874
03/10/2022 16:27:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.23 on epoch=879
03/10/2022 16:27:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.29 on epoch=884
03/10/2022 16:27:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=889
03/10/2022 16:27:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=894
03/10/2022 16:27:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.30 on epoch=899
03/10/2022 16:27:22 - INFO - __main__ - Global step 1800 Train loss 0.26 QA-F1 0.18958333333333333 on epoch=899
03/10/2022 16:27:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.20 on epoch=904
03/10/2022 16:27:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=909
03/10/2022 16:27:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=914
03/10/2022 16:27:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.23 on epoch=919
03/10/2022 16:27:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=924
03/10/2022 16:27:45 - INFO - __main__ - Global step 1850 Train loss 0.22 QA-F1 0.17916666666666664 on epoch=924
03/10/2022 16:27:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=929
03/10/2022 16:27:54 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=934
03/10/2022 16:27:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=939
03/10/2022 16:28:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=944
03/10/2022 16:28:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.21 on epoch=949
03/10/2022 16:28:09 - INFO - __main__ - Global step 1900 Train loss 0.21 QA-F1 0.125 on epoch=949
03/10/2022 16:28:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=954
03/10/2022 16:28:17 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=959
03/10/2022 16:28:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=964
03/10/2022 16:28:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=969
03/10/2022 16:28:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.20 on epoch=974
03/10/2022 16:28:32 - INFO - __main__ - Global step 1950 Train loss 0.23 QA-F1 0.18958333333333333 on epoch=974
03/10/2022 16:28:36 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=979
03/10/2022 16:28:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=984
03/10/2022 16:28:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.27 on epoch=989
03/10/2022 16:28:49 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=994
03/10/2022 16:28:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.20 on epoch=999
03/10/2022 16:28:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:28:55 - INFO - __main__ - Printing 3 examples
03/10/2022 16:28:55 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/10/2022 16:28:55 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/10/2022 16:28:55 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/10/2022 16:28:55 - INFO - __main__ - ['Up']
03/10/2022 16:28:55 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/10/2022 16:28:55 - INFO - __main__ - ['Norway']
03/10/2022 16:28:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:28:55 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:28:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:28:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:28:55 - INFO - __main__ - Printing 3 examples
03/10/2022 16:28:55 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/10/2022 16:28:55 - INFO - __main__ - ['George Gipp']
03/10/2022 16:28:55 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/10/2022 16:28:55 - INFO - __main__ - ['The Blooming Baroque']
03/10/2022 16:28:55 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/10/2022 16:28:55 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/10/2022 16:28:55 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:28:55 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:28:55 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:28:56 - INFO - __main__ - Global step 2000 Train loss 0.23 QA-F1 0.17916666666666664 on epoch=999
03/10/2022 16:28:56 - INFO - __main__ - save last model!
03/10/2022 16:28:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 16:28:56 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 16:28:56 - INFO - __main__ - Printing 3 examples
03/10/2022 16:28:56 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 16:28:56 - INFO - __main__ - ['Frankie']
03/10/2022 16:28:56 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 16:28:56 - INFO - __main__ - ['Frankie']
03/10/2022 16:28:56 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 16:28:56 - INFO - __main__ - ['Frankie']
03/10/2022 16:28:56 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:29:00 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:29:03 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 16:29:08 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 16:29:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 16:29:09 - INFO - __main__ - Starting training!
03/10/2022 16:31:04 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_100_0.2_8_predictions.txt
03/10/2022 16:31:04 - INFO - __main__ - QA-F1 on test data: 0.2653
03/10/2022 16:31:04 - INFO - __main__ - prefix=quoref_32_100, lr=0.2, bsz=8, dev_performance=0.2951388888888889, test_performance=0.26527846406878663
03/10/2022 16:31:04 - INFO - __main__ - Running ... prefix=quoref_32_13, lr=0.5, bsz=8 ...
03/10/2022 16:31:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:31:05 - INFO - __main__ - Printing 3 examples
03/10/2022 16:31:05 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/10/2022 16:31:05 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/10/2022 16:31:05 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/10/2022 16:31:05 - INFO - __main__ - ['Up']
03/10/2022 16:31:05 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/10/2022 16:31:05 - INFO - __main__ - ['Norway']
03/10/2022 16:31:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:31:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:31:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:31:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:31:05 - INFO - __main__ - Printing 3 examples
03/10/2022 16:31:05 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/10/2022 16:31:05 - INFO - __main__ - ['George Gipp']
03/10/2022 16:31:05 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/10/2022 16:31:05 - INFO - __main__ - ['The Blooming Baroque']
03/10/2022 16:31:05 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/10/2022 16:31:05 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/10/2022 16:31:05 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:31:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:31:05 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:31:18 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 16:31:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 16:31:19 - INFO - __main__ - Starting training!
03/10/2022 16:31:26 - INFO - __main__ - Step 10 Global step 10 Train loss 3.00 on epoch=4
03/10/2022 16:31:30 - INFO - __main__ - Step 20 Global step 20 Train loss 2.46 on epoch=9
03/10/2022 16:31:34 - INFO - __main__ - Step 30 Global step 30 Train loss 1.89 on epoch=14
03/10/2022 16:31:39 - INFO - __main__ - Step 40 Global step 40 Train loss 1.63 on epoch=19
03/10/2022 16:31:43 - INFO - __main__ - Step 50 Global step 50 Train loss 1.64 on epoch=24
03/10/2022 16:31:44 - INFO - __main__ - Global step 50 Train loss 2.12 QA-F1 0.359375 on epoch=24
03/10/2022 16:31:44 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.359375 on epoch=24, global_step=50
03/10/2022 16:31:49 - INFO - __main__ - Step 60 Global step 60 Train loss 1.61 on epoch=29
03/10/2022 16:31:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.52 on epoch=34
03/10/2022 16:31:57 - INFO - __main__ - Step 80 Global step 80 Train loss 1.46 on epoch=39
03/10/2022 16:32:02 - INFO - __main__ - Step 90 Global step 90 Train loss 1.39 on epoch=44
03/10/2022 16:32:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.34 on epoch=49
03/10/2022 16:32:08 - INFO - __main__ - Global step 100 Train loss 1.46 QA-F1 0.3489583333333333 on epoch=49
03/10/2022 16:32:12 - INFO - __main__ - Step 110 Global step 110 Train loss 1.27 on epoch=54
03/10/2022 16:32:16 - INFO - __main__ - Step 120 Global step 120 Train loss 1.27 on epoch=59
03/10/2022 16:32:21 - INFO - __main__ - Step 130 Global step 130 Train loss 1.20 on epoch=64
03/10/2022 16:32:25 - INFO - __main__ - Step 140 Global step 140 Train loss 1.21 on epoch=69
03/10/2022 16:32:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.21 on epoch=74
03/10/2022 16:32:31 - INFO - __main__ - Global step 150 Train loss 1.23 QA-F1 0.34895833333333337 on epoch=74
03/10/2022 16:32:35 - INFO - __main__ - Step 160 Global step 160 Train loss 1.15 on epoch=79
03/10/2022 16:32:40 - INFO - __main__ - Step 170 Global step 170 Train loss 1.04 on epoch=84
03/10/2022 16:32:44 - INFO - __main__ - Step 180 Global step 180 Train loss 1.07 on epoch=89
03/10/2022 16:32:48 - INFO - __main__ - Step 190 Global step 190 Train loss 1.00 on epoch=94
03/10/2022 16:32:53 - INFO - __main__ - Step 200 Global step 200 Train loss 0.96 on epoch=99
03/10/2022 16:32:54 - INFO - __main__ - Global step 200 Train loss 1.04 QA-F1 0.359375 on epoch=99
03/10/2022 16:32:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.95 on epoch=104
03/10/2022 16:33:03 - INFO - __main__ - Step 220 Global step 220 Train loss 1.01 on epoch=109
03/10/2022 16:33:07 - INFO - __main__ - Step 230 Global step 230 Train loss 1.01 on epoch=114
03/10/2022 16:33:11 - INFO - __main__ - Step 240 Global step 240 Train loss 0.99 on epoch=119
03/10/2022 16:33:16 - INFO - __main__ - Step 250 Global step 250 Train loss 0.92 on epoch=124
03/10/2022 16:33:17 - INFO - __main__ - Global step 250 Train loss 0.98 QA-F1 0.33854166666666663 on epoch=124
03/10/2022 16:33:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.86 on epoch=129
03/10/2022 16:33:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.82 on epoch=134
03/10/2022 16:33:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.88 on epoch=139
03/10/2022 16:33:35 - INFO - __main__ - Step 290 Global step 290 Train loss 0.85 on epoch=144
03/10/2022 16:33:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.72 on epoch=149
03/10/2022 16:33:41 - INFO - __main__ - Global step 300 Train loss 0.82 QA-F1 0.328125 on epoch=149
03/10/2022 16:33:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.84 on epoch=154
03/10/2022 16:33:49 - INFO - __main__ - Step 320 Global step 320 Train loss 0.72 on epoch=159
03/10/2022 16:33:54 - INFO - __main__ - Step 330 Global step 330 Train loss 0.72 on epoch=164
03/10/2022 16:33:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.73 on epoch=169
03/10/2022 16:34:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.71 on epoch=174
03/10/2022 16:34:04 - INFO - __main__ - Global step 350 Train loss 0.74 QA-F1 0.18229166666666666 on epoch=174
03/10/2022 16:34:08 - INFO - __main__ - Step 360 Global step 360 Train loss 0.62 on epoch=179
03/10/2022 16:34:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=184
03/10/2022 16:34:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.60 on epoch=189
03/10/2022 16:34:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=194
03/10/2022 16:34:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.66 on epoch=199
03/10/2022 16:34:27 - INFO - __main__ - Global step 400 Train loss 0.60 QA-F1 0.18229166666666666 on epoch=199
03/10/2022 16:34:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.64 on epoch=204
03/10/2022 16:34:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=209
03/10/2022 16:34:40 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=214
03/10/2022 16:34:44 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=219
03/10/2022 16:34:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=224
03/10/2022 16:34:50 - INFO - __main__ - Global step 450 Train loss 0.55 QA-F1 0.18229166666666669 on epoch=224
03/10/2022 16:34:54 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=229
03/10/2022 16:34:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=234
03/10/2022 16:35:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.51 on epoch=239
03/10/2022 16:35:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=244
03/10/2022 16:35:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.41 on epoch=249
03/10/2022 16:35:13 - INFO - __main__ - Global step 500 Train loss 0.46 QA-F1 0.25520833333333337 on epoch=249
03/10/2022 16:35:17 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=254
03/10/2022 16:35:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.51 on epoch=259
03/10/2022 16:35:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=264
03/10/2022 16:35:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=269
03/10/2022 16:35:35 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=274
03/10/2022 16:35:36 - INFO - __main__ - Global step 550 Train loss 0.42 QA-F1 0.2552083333333333 on epoch=274
03/10/2022 16:35:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=279
03/10/2022 16:35:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.42 on epoch=284
03/10/2022 16:35:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=289
03/10/2022 16:35:54 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=294
03/10/2022 16:35:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.34 on epoch=299
03/10/2022 16:36:00 - INFO - __main__ - Global step 600 Train loss 0.38 QA-F1 0.234375 on epoch=299
03/10/2022 16:36:04 - INFO - __main__ - Step 610 Global step 610 Train loss 0.34 on epoch=304
03/10/2022 16:36:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=309
03/10/2022 16:36:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=314
03/10/2022 16:36:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=319
03/10/2022 16:36:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=324
03/10/2022 16:36:23 - INFO - __main__ - Global step 650 Train loss 0.34 QA-F1 0.3177083333333333 on epoch=324
03/10/2022 16:36:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.29 on epoch=329
03/10/2022 16:36:32 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=334
03/10/2022 16:36:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.30 on epoch=339
03/10/2022 16:36:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=344
03/10/2022 16:36:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=349
03/10/2022 16:36:46 - INFO - __main__ - Global step 700 Train loss 0.29 QA-F1 0.296875 on epoch=349
03/10/2022 16:36:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=354
03/10/2022 16:36:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.29 on epoch=359
03/10/2022 16:36:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=364
03/10/2022 16:37:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.28 on epoch=369
03/10/2022 16:37:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=374
03/10/2022 16:37:09 - INFO - __main__ - Global step 750 Train loss 0.29 QA-F1 0.296875 on epoch=374
03/10/2022 16:37:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=379
03/10/2022 16:37:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.35 on epoch=384
03/10/2022 16:37:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
03/10/2022 16:37:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.28 on epoch=394
03/10/2022 16:37:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=399
03/10/2022 16:37:33 - INFO - __main__ - Global step 800 Train loss 0.28 QA-F1 0.22916666666666666 on epoch=399
03/10/2022 16:37:37 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=404
03/10/2022 16:37:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=409
03/10/2022 16:37:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=414
03/10/2022 16:37:50 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=419
03/10/2022 16:37:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
03/10/2022 16:37:56 - INFO - __main__ - Global step 850 Train loss 0.25 QA-F1 0.22395833333333331 on epoch=424
03/10/2022 16:38:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
03/10/2022 16:38:05 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=434
03/10/2022 16:38:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=439
03/10/2022 16:38:13 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=444
03/10/2022 16:38:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
03/10/2022 16:38:19 - INFO - __main__ - Global step 900 Train loss 0.27 QA-F1 0.22916666666666666 on epoch=449
03/10/2022 16:38:23 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=454
03/10/2022 16:38:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
03/10/2022 16:38:32 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=464
03/10/2022 16:38:36 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=469
03/10/2022 16:38:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=474
03/10/2022 16:38:42 - INFO - __main__ - Global step 950 Train loss 0.23 QA-F1 0.25 on epoch=474
03/10/2022 16:38:47 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=479
03/10/2022 16:38:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.28 on epoch=484
03/10/2022 16:38:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=489
03/10/2022 16:39:00 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=494
03/10/2022 16:39:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=499
03/10/2022 16:39:06 - INFO - __main__ - Global step 1000 Train loss 0.25 QA-F1 0.27604166666666663 on epoch=499
03/10/2022 16:39:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=504
03/10/2022 16:39:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=509
03/10/2022 16:39:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=514
03/10/2022 16:39:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=519
03/10/2022 16:39:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=524
03/10/2022 16:39:29 - INFO - __main__ - Global step 1050 Train loss 0.19 QA-F1 0.3020833333333333 on epoch=524
03/10/2022 16:39:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=529
03/10/2022 16:39:38 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=534
03/10/2022 16:39:43 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=539
03/10/2022 16:39:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=544
03/10/2022 16:39:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=549
03/10/2022 16:39:53 - INFO - __main__ - Global step 1100 Train loss 0.19 QA-F1 0.3020833333333333 on epoch=549
03/10/2022 16:39:57 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.15 on epoch=554
03/10/2022 16:40:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=559
03/10/2022 16:40:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=564
03/10/2022 16:40:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=569
03/10/2022 16:40:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.17 on epoch=574
03/10/2022 16:40:16 - INFO - __main__ - Global step 1150 Train loss 0.17 QA-F1 0.26041666666666663 on epoch=574
03/10/2022 16:40:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=579
03/10/2022 16:40:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.20 on epoch=584
03/10/2022 16:40:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=589
03/10/2022 16:40:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.15 on epoch=594
03/10/2022 16:40:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=599
03/10/2022 16:40:40 - INFO - __main__ - Global step 1200 Train loss 0.18 QA-F1 0.26041666666666663 on epoch=599
03/10/2022 16:40:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.16 on epoch=604
03/10/2022 16:40:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=609
03/10/2022 16:40:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=614
03/10/2022 16:40:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=619
03/10/2022 16:41:02 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=624
03/10/2022 16:41:03 - INFO - __main__ - Global step 1250 Train loss 0.14 QA-F1 0.2520833333333333 on epoch=624
03/10/2022 16:41:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=629
03/10/2022 16:41:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=634
03/10/2022 16:41:16 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.18 on epoch=639
03/10/2022 16:41:21 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=644
03/10/2022 16:41:25 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=649
03/10/2022 16:41:27 - INFO - __main__ - Global step 1300 Train loss 0.14 QA-F1 0.29166666666666663 on epoch=649
03/10/2022 16:41:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=654
03/10/2022 16:41:36 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=659
03/10/2022 16:41:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=664
03/10/2022 16:41:44 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=669
03/10/2022 16:41:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=674
03/10/2022 16:41:50 - INFO - __main__ - Global step 1350 Train loss 0.15 QA-F1 0.27604166666666663 on epoch=674
03/10/2022 16:41:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=679
03/10/2022 16:41:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=684
03/10/2022 16:42:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.17 on epoch=689
03/10/2022 16:42:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=694
03/10/2022 16:42:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=699
03/10/2022 16:42:13 - INFO - __main__ - Global step 1400 Train loss 0.16 QA-F1 0.26041666666666663 on epoch=699
03/10/2022 16:42:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=704
03/10/2022 16:42:22 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
03/10/2022 16:42:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=714
03/10/2022 16:42:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=719
03/10/2022 16:42:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=724
03/10/2022 16:42:37 - INFO - __main__ - Global step 1450 Train loss 0.13 QA-F1 0.26041666666666663 on epoch=724
03/10/2022 16:42:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=729
03/10/2022 16:42:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=734
03/10/2022 16:42:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.12 on epoch=739
03/10/2022 16:42:54 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=744
03/10/2022 16:42:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=749
03/10/2022 16:43:00 - INFO - __main__ - Global step 1500 Train loss 0.12 QA-F1 0.2708333333333333 on epoch=749
03/10/2022 16:43:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=754
03/10/2022 16:43:09 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=759
03/10/2022 16:43:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=764
03/10/2022 16:43:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=769
03/10/2022 16:43:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=774
03/10/2022 16:43:23 - INFO - __main__ - Global step 1550 Train loss 0.14 QA-F1 0.296875 on epoch=774
03/10/2022 16:43:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
03/10/2022 16:43:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
03/10/2022 16:43:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=789
03/10/2022 16:43:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=794
03/10/2022 16:43:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.12 on epoch=799
03/10/2022 16:43:46 - INFO - __main__ - Global step 1600 Train loss 0.12 QA-F1 0.30729166666666663 on epoch=799
03/10/2022 16:43:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=804
03/10/2022 16:43:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.17 on epoch=809
03/10/2022 16:43:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=814
03/10/2022 16:44:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=819
03/10/2022 16:44:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=824
03/10/2022 16:44:10 - INFO - __main__ - Global step 1650 Train loss 0.12 QA-F1 0.30729166666666663 on epoch=824
03/10/2022 16:44:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=829
03/10/2022 16:44:19 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=834
03/10/2022 16:44:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=839
03/10/2022 16:44:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=844
03/10/2022 16:44:32 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=849
03/10/2022 16:44:33 - INFO - __main__ - Global step 1700 Train loss 0.12 QA-F1 0.2677083333333333 on epoch=849
03/10/2022 16:44:37 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=854
03/10/2022 16:44:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=859
03/10/2022 16:44:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
03/10/2022 16:44:50 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=869
03/10/2022 16:44:55 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=874
03/10/2022 16:44:56 - INFO - __main__ - Global step 1750 Train loss 0.08 QA-F1 0.278125 on epoch=874
03/10/2022 16:45:01 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=879
03/10/2022 16:45:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.13 on epoch=884
03/10/2022 16:45:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=889
03/10/2022 16:45:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=894
03/10/2022 16:45:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.07 on epoch=899
03/10/2022 16:45:19 - INFO - __main__ - Global step 1800 Train loss 0.10 QA-F1 0.3177083333333333 on epoch=899
03/10/2022 16:45:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=904
03/10/2022 16:45:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=909
03/10/2022 16:45:32 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=914
03/10/2022 16:45:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=919
03/10/2022 16:45:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=924
03/10/2022 16:45:43 - INFO - __main__ - Global step 1850 Train loss 0.09 QA-F1 0.29166666666666663 on epoch=924
03/10/2022 16:45:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=929
03/10/2022 16:45:52 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/10/2022 16:45:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=939
03/10/2022 16:46:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
03/10/2022 16:46:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=949
03/10/2022 16:46:07 - INFO - __main__ - Global step 1900 Train loss 0.10 QA-F1 0.27604166666666663 on epoch=949
03/10/2022 16:46:11 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
03/10/2022 16:46:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=959
03/10/2022 16:46:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=964
03/10/2022 16:46:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=969
03/10/2022 16:46:28 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=974
03/10/2022 16:46:30 - INFO - __main__ - Global step 1950 Train loss 0.09 QA-F1 0.35416666666666663 on epoch=974
03/10/2022 16:46:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
03/10/2022 16:46:38 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=984
03/10/2022 16:46:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=989
03/10/2022 16:46:47 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
03/10/2022 16:46:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=999
03/10/2022 16:46:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:46:53 - INFO - __main__ - Printing 3 examples
03/10/2022 16:46:53 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/10/2022 16:46:53 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/10/2022 16:46:53 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/10/2022 16:46:53 - INFO - __main__ - ['Up']
03/10/2022 16:46:53 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/10/2022 16:46:53 - INFO - __main__ - ['Norway']
03/10/2022 16:46:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 16:46:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:46:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:46:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:46:53 - INFO - __main__ - Printing 3 examples
03/10/2022 16:46:53 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/10/2022 16:46:53 - INFO - __main__ - ['George Gipp']
03/10/2022 16:46:53 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/10/2022 16:46:53 - INFO - __main__ - ['The Blooming Baroque']
03/10/2022 16:46:53 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/10/2022 16:46:53 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/10/2022 16:46:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:46:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:46:53 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:46:53 - INFO - __main__ - Global step 2000 Train loss 0.09 QA-F1 0.34375 on epoch=999
03/10/2022 16:46:53 - INFO - __main__ - save last model!
03/10/2022 16:46:53 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 16:46:53 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 16:46:53 - INFO - __main__ - Printing 3 examples
03/10/2022 16:46:53 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 16:46:53 - INFO - __main__ - ['Frankie']
03/10/2022 16:46:53 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 16:46:53 - INFO - __main__ - ['Frankie']
03/10/2022 16:46:53 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 16:46:53 - INFO - __main__ - ['Frankie']
03/10/2022 16:46:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:46:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:47:00 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 16:47:07 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 16:47:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 16:47:07 - INFO - __main__ - Starting training!
03/10/2022 16:48:50 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_13_0.5_8_predictions.txt
03/10/2022 16:48:50 - INFO - __main__ - QA-F1 on test data: 0.3099
03/10/2022 16:48:51 - INFO - __main__ - prefix=quoref_32_13, lr=0.5, bsz=8, dev_performance=0.359375, test_performance=0.30987435503564537
03/10/2022 16:48:51 - INFO - __main__ - Running ... prefix=quoref_32_13, lr=0.4, bsz=8 ...
03/10/2022 16:48:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:48:52 - INFO - __main__ - Printing 3 examples
03/10/2022 16:48:52 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/10/2022 16:48:52 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/10/2022 16:48:52 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/10/2022 16:48:52 - INFO - __main__ - ['Up']
03/10/2022 16:48:52 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/10/2022 16:48:52 - INFO - __main__ - ['Norway']
03/10/2022 16:48:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 16:48:52 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:48:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 16:48:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 16:48:52 - INFO - __main__ - Printing 3 examples
03/10/2022 16:48:52 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/10/2022 16:48:52 - INFO - __main__ - ['George Gipp']
03/10/2022 16:48:52 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/10/2022 16:48:52 - INFO - __main__ - ['The Blooming Baroque']
03/10/2022 16:48:52 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/10/2022 16:48:52 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/10/2022 16:48:52 - INFO - __main__ - Tokenizing Input ...
03/10/2022 16:48:52 - INFO - __main__ - Tokenizing Output ...
03/10/2022 16:48:52 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 16:49:05 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 16:49:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 16:49:06 - INFO - __main__ - Starting training!
03/10/2022 16:49:11 - INFO - __main__ - Step 10 Global step 10 Train loss 3.16 on epoch=4
03/10/2022 16:49:16 - INFO - __main__ - Step 20 Global step 20 Train loss 2.69 on epoch=9
03/10/2022 16:49:20 - INFO - __main__ - Step 30 Global step 30 Train loss 2.08 on epoch=14
03/10/2022 16:49:24 - INFO - __main__ - Step 40 Global step 40 Train loss 1.79 on epoch=19
03/10/2022 16:49:29 - INFO - __main__ - Step 50 Global step 50 Train loss 1.74 on epoch=24
03/10/2022 16:49:30 - INFO - __main__ - Global step 50 Train loss 2.29 QA-F1 0.421875 on epoch=24
03/10/2022 16:49:30 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.421875 on epoch=24, global_step=50
03/10/2022 16:49:34 - INFO - __main__ - Step 60 Global step 60 Train loss 1.44 on epoch=29
03/10/2022 16:49:39 - INFO - __main__ - Step 70 Global step 70 Train loss 1.50 on epoch=34
03/10/2022 16:49:43 - INFO - __main__ - Step 80 Global step 80 Train loss 1.45 on epoch=39
03/10/2022 16:49:47 - INFO - __main__ - Step 90 Global step 90 Train loss 1.40 on epoch=44
03/10/2022 16:49:52 - INFO - __main__ - Step 100 Global step 100 Train loss 1.32 on epoch=49
03/10/2022 16:49:53 - INFO - __main__ - Global step 100 Train loss 1.42 QA-F1 0.375 on epoch=49
03/10/2022 16:49:57 - INFO - __main__ - Step 110 Global step 110 Train loss 1.33 on epoch=54
03/10/2022 16:50:02 - INFO - __main__ - Step 120 Global step 120 Train loss 1.24 on epoch=59
03/10/2022 16:50:06 - INFO - __main__ - Step 130 Global step 130 Train loss 1.18 on epoch=64
03/10/2022 16:50:10 - INFO - __main__ - Step 140 Global step 140 Train loss 1.14 on epoch=69
03/10/2022 16:50:15 - INFO - __main__ - Step 150 Global step 150 Train loss 1.07 on epoch=74
03/10/2022 16:50:16 - INFO - __main__ - Global step 150 Train loss 1.19 QA-F1 0.3489583333333333 on epoch=74
03/10/2022 16:50:20 - INFO - __main__ - Step 160 Global step 160 Train loss 1.02 on epoch=79
03/10/2022 16:50:25 - INFO - __main__ - Step 170 Global step 170 Train loss 1.02 on epoch=84
03/10/2022 16:50:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.91 on epoch=89
03/10/2022 16:50:33 - INFO - __main__ - Step 190 Global step 190 Train loss 0.89 on epoch=94
03/10/2022 16:50:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.88 on epoch=99
03/10/2022 16:50:39 - INFO - __main__ - Global step 200 Train loss 0.95 QA-F1 0.2552083333333333 on epoch=99
03/10/2022 16:50:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.74 on epoch=104
03/10/2022 16:50:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.75 on epoch=109
03/10/2022 16:50:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.73 on epoch=114
03/10/2022 16:50:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.77 on epoch=119
03/10/2022 16:51:01 - INFO - __main__ - Step 250 Global step 250 Train loss 0.67 on epoch=124
03/10/2022 16:51:02 - INFO - __main__ - Global step 250 Train loss 0.73 QA-F1 0.23958333333333331 on epoch=124
03/10/2022 16:51:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.63 on epoch=129
03/10/2022 16:51:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.69 on epoch=134
03/10/2022 16:51:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.56 on epoch=139
03/10/2022 16:51:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.52 on epoch=144
03/10/2022 16:51:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.54 on epoch=149
03/10/2022 16:51:25 - INFO - __main__ - Global step 300 Train loss 0.59 QA-F1 0.2833333333333333 on epoch=149
03/10/2022 16:51:29 - INFO - __main__ - Step 310 Global step 310 Train loss 0.61 on epoch=154
03/10/2022 16:51:34 - INFO - __main__ - Step 320 Global step 320 Train loss 0.60 on epoch=159
03/10/2022 16:51:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.53 on epoch=164
03/10/2022 16:51:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.50 on epoch=169
03/10/2022 16:51:47 - INFO - __main__ - Step 350 Global step 350 Train loss 0.55 on epoch=174
03/10/2022 16:51:48 - INFO - __main__ - Global step 350 Train loss 0.56 QA-F1 0.30208333333333337 on epoch=174
03/10/2022 16:51:52 - INFO - __main__ - Step 360 Global step 360 Train loss 0.43 on epoch=179
03/10/2022 16:51:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.49 on epoch=184
03/10/2022 16:52:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.46 on epoch=189
03/10/2022 16:52:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.41 on epoch=194
03/10/2022 16:52:10 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=199
03/10/2022 16:52:11 - INFO - __main__ - Global step 400 Train loss 0.42 QA-F1 0.325 on epoch=199
03/10/2022 16:52:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=204
03/10/2022 16:52:20 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=209
03/10/2022 16:52:24 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=214
03/10/2022 16:52:28 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=219
03/10/2022 16:52:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.39 on epoch=224
03/10/2022 16:52:35 - INFO - __main__ - Global step 450 Train loss 0.39 QA-F1 0.3458333333333333 on epoch=224
03/10/2022 16:52:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
03/10/2022 16:52:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
03/10/2022 16:52:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.28 on epoch=239
03/10/2022 16:52:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=244
03/10/2022 16:52:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.45 on epoch=249
03/10/2022 16:52:58 - INFO - __main__ - Global step 500 Train loss 0.35 QA-F1 0.36979166666666663 on epoch=249
03/10/2022 16:53:02 - INFO - __main__ - Step 510 Global step 510 Train loss 0.32 on epoch=254
03/10/2022 16:53:06 - INFO - __main__ - Step 520 Global step 520 Train loss 0.28 on epoch=259
03/10/2022 16:53:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.31 on epoch=264
03/10/2022 16:53:15 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=269
03/10/2022 16:53:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=274
03/10/2022 16:53:21 - INFO - __main__ - Global step 550 Train loss 0.31 QA-F1 0.3145833333333333 on epoch=274
03/10/2022 16:53:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.28 on epoch=279
03/10/2022 16:53:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.29 on epoch=284
03/10/2022 16:53:34 - INFO - __main__ - Step 580 Global step 580 Train loss 0.27 on epoch=289
03/10/2022 16:53:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=294
03/10/2022 16:53:42 - INFO - __main__ - Step 600 Global step 600 Train loss 0.32 on epoch=299
03/10/2022 16:53:44 - INFO - __main__ - Global step 600 Train loss 0.29 QA-F1 0.24166666666666664 on epoch=299
03/10/2022 16:53:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.31 on epoch=304
03/10/2022 16:53:53 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=309
03/10/2022 16:53:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=314
03/10/2022 16:54:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=319
03/10/2022 16:54:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
03/10/2022 16:54:07 - INFO - __main__ - Global step 650 Train loss 0.27 QA-F1 0.3458333333333333 on epoch=324
03/10/2022 16:54:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=329
03/10/2022 16:54:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=334
03/10/2022 16:54:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
03/10/2022 16:54:25 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=344
03/10/2022 16:54:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=349
03/10/2022 16:54:31 - INFO - __main__ - Global step 700 Train loss 0.24 QA-F1 0.35625 on epoch=349
03/10/2022 16:54:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=354
03/10/2022 16:54:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.30 on epoch=359
03/10/2022 16:54:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=364
03/10/2022 16:54:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=369
03/10/2022 16:54:52 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=374
03/10/2022 16:54:54 - INFO - __main__ - Global step 750 Train loss 0.23 QA-F1 0.325 on epoch=374
03/10/2022 16:54:58 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
03/10/2022 16:55:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.18 on epoch=384
03/10/2022 16:55:07 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=389
03/10/2022 16:55:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=394
03/10/2022 16:55:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=399
03/10/2022 16:55:17 - INFO - __main__ - Global step 800 Train loss 0.23 QA-F1 0.325 on epoch=399
03/10/2022 16:55:22 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=404
03/10/2022 16:55:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=409
03/10/2022 16:55:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=414
03/10/2022 16:55:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=419
03/10/2022 16:55:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
03/10/2022 16:55:40 - INFO - __main__ - Global step 850 Train loss 0.20 QA-F1 0.30416666666666664 on epoch=424
03/10/2022 16:55:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
03/10/2022 16:55:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.17 on epoch=434
03/10/2022 16:55:53 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=439
03/10/2022 16:55:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=444
03/10/2022 16:56:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=449
03/10/2022 16:56:04 - INFO - __main__ - Global step 900 Train loss 0.19 QA-F1 0.35625 on epoch=449
03/10/2022 16:56:08 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=454
03/10/2022 16:56:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=459
03/10/2022 16:56:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=464
03/10/2022 16:56:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=469
03/10/2022 16:56:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
03/10/2022 16:56:27 - INFO - __main__ - Global step 950 Train loss 0.18 QA-F1 0.3458333333333333 on epoch=474
03/10/2022 16:56:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=479
03/10/2022 16:56:36 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=484
03/10/2022 16:56:40 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=489
03/10/2022 16:56:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=494
03/10/2022 16:56:49 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=499
03/10/2022 16:56:50 - INFO - __main__ - Global step 1000 Train loss 0.17 QA-F1 0.36666666666666664 on epoch=499
03/10/2022 16:56:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=504
03/10/2022 16:56:59 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=509
03/10/2022 16:57:03 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=514
03/10/2022 16:57:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=519
03/10/2022 16:57:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=524
03/10/2022 16:57:13 - INFO - __main__ - Global step 1050 Train loss 0.16 QA-F1 0.30416666666666664 on epoch=524
03/10/2022 16:57:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=529
03/10/2022 16:57:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=534
03/10/2022 16:57:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=539
03/10/2022 16:57:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=544
03/10/2022 16:57:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=549
03/10/2022 16:57:37 - INFO - __main__ - Global step 1100 Train loss 0.14 QA-F1 0.35625 on epoch=549
03/10/2022 16:57:41 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=554
03/10/2022 16:57:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=559
03/10/2022 16:57:50 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.16 on epoch=564
03/10/2022 16:57:54 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=569
03/10/2022 16:57:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=574
03/10/2022 16:58:00 - INFO - __main__ - Global step 1150 Train loss 0.13 QA-F1 0.3770833333333333 on epoch=574
03/10/2022 16:58:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=579
03/10/2022 16:58:08 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=584
03/10/2022 16:58:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=589
03/10/2022 16:58:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=594
03/10/2022 16:58:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=599
03/10/2022 16:58:23 - INFO - __main__ - Global step 1200 Train loss 0.13 QA-F1 0.3145833333333333 on epoch=599
03/10/2022 16:58:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.14 on epoch=604
03/10/2022 16:58:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=609
03/10/2022 16:58:36 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=614
03/10/2022 16:58:41 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=619
03/10/2022 16:58:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
03/10/2022 16:58:46 - INFO - __main__ - Global step 1250 Train loss 0.11 QA-F1 0.29375 on epoch=624
03/10/2022 16:58:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=629
03/10/2022 16:58:55 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=634
03/10/2022 16:58:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.13 on epoch=639
03/10/2022 16:59:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.10 on epoch=644
03/10/2022 16:59:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=649
03/10/2022 16:59:10 - INFO - __main__ - Global step 1300 Train loss 0.10 QA-F1 0.325 on epoch=649
03/10/2022 16:59:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=654
03/10/2022 16:59:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=659
03/10/2022 16:59:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=664
03/10/2022 16:59:27 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.09 on epoch=669
03/10/2022 16:59:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.11 on epoch=674
03/10/2022 16:59:33 - INFO - __main__ - Global step 1350 Train loss 0.12 QA-F1 0.325 on epoch=674
03/10/2022 16:59:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=679
03/10/2022 16:59:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=684
03/10/2022 16:59:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.11 on epoch=689
03/10/2022 16:59:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
03/10/2022 16:59:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=699
03/10/2022 16:59:55 - INFO - __main__ - Global step 1400 Train loss 0.11 QA-F1 0.29375 on epoch=699
03/10/2022 17:00:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=704
03/10/2022 17:00:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=709
03/10/2022 17:00:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=714
03/10/2022 17:00:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=719
03/10/2022 17:00:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.12 on epoch=724
03/10/2022 17:00:19 - INFO - __main__ - Global step 1450 Train loss 0.11 QA-F1 0.2625 on epoch=724
03/10/2022 17:00:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=729
03/10/2022 17:00:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=734
03/10/2022 17:00:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=739
03/10/2022 17:00:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=744
03/10/2022 17:00:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=749
03/10/2022 17:00:42 - INFO - __main__ - Global step 1500 Train loss 0.15 QA-F1 0.2520833333333333 on epoch=749
03/10/2022 17:00:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=754
03/10/2022 17:00:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=759
03/10/2022 17:00:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=764
03/10/2022 17:00:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.11 on epoch=769
03/10/2022 17:01:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=774
03/10/2022 17:01:05 - INFO - __main__ - Global step 1550 Train loss 0.11 QA-F1 0.2833333333333333 on epoch=774
03/10/2022 17:01:09 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=779
03/10/2022 17:01:14 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.08 on epoch=784
03/10/2022 17:01:18 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.11 on epoch=789
03/10/2022 17:01:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=794
03/10/2022 17:01:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=799
03/10/2022 17:01:28 - INFO - __main__ - Global step 1600 Train loss 0.11 QA-F1 0.2625 on epoch=799
03/10/2022 17:01:33 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=804
03/10/2022 17:01:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.09 on epoch=809
03/10/2022 17:01:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=814
03/10/2022 17:01:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=819
03/10/2022 17:01:50 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=824
03/10/2022 17:01:52 - INFO - __main__ - Global step 1650 Train loss 0.09 QA-F1 0.2833333333333333 on epoch=824
03/10/2022 17:01:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=829
03/10/2022 17:02:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=834
03/10/2022 17:02:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=839
03/10/2022 17:02:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=844
03/10/2022 17:02:13 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=849
03/10/2022 17:02:15 - INFO - __main__ - Global step 1700 Train loss 0.11 QA-F1 0.296875 on epoch=849
03/10/2022 17:02:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=854
03/10/2022 17:02:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=859
03/10/2022 17:02:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=864
03/10/2022 17:02:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=869
03/10/2022 17:02:37 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=874
03/10/2022 17:02:38 - INFO - __main__ - Global step 1750 Train loss 0.08 QA-F1 0.2625 on epoch=874
03/10/2022 17:02:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.10 on epoch=879
03/10/2022 17:02:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=884
03/10/2022 17:02:51 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.11 on epoch=889
03/10/2022 17:02:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.10 on epoch=894
03/10/2022 17:03:00 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=899
03/10/2022 17:03:02 - INFO - __main__ - Global step 1800 Train loss 0.10 QA-F1 0.30416666666666664 on epoch=899
03/10/2022 17:03:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=904
03/10/2022 17:03:11 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=909
03/10/2022 17:03:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=914
03/10/2022 17:03:19 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=919
03/10/2022 17:03:24 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=924
03/10/2022 17:03:26 - INFO - __main__ - Global step 1850 Train loss 0.09 QA-F1 0.3145833333333333 on epoch=924
03/10/2022 17:03:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=929
03/10/2022 17:03:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
03/10/2022 17:03:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=939
03/10/2022 17:03:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=944
03/10/2022 17:03:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=949
03/10/2022 17:03:49 - INFO - __main__ - Global step 1900 Train loss 0.08 QA-F1 0.2833333333333333 on epoch=949
03/10/2022 17:03:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=954
03/10/2022 17:03:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=959
03/10/2022 17:04:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=964
03/10/2022 17:04:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=969
03/10/2022 17:04:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=974
03/10/2022 17:04:13 - INFO - __main__ - Global step 1950 Train loss 0.10 QA-F1 0.2625 on epoch=974
03/10/2022 17:04:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=979
03/10/2022 17:04:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=984
03/10/2022 17:04:26 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=989
03/10/2022 17:04:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
03/10/2022 17:04:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=999
03/10/2022 17:04:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:04:36 - INFO - __main__ - Printing 3 examples
03/10/2022 17:04:36 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/10/2022 17:04:36 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/10/2022 17:04:36 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/10/2022 17:04:36 - INFO - __main__ - ['Up']
03/10/2022 17:04:36 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/10/2022 17:04:36 - INFO - __main__ - ['Norway']
03/10/2022 17:04:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:04:36 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:04:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:04:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:04:36 - INFO - __main__ - Printing 3 examples
03/10/2022 17:04:36 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/10/2022 17:04:36 - INFO - __main__ - ['George Gipp']
03/10/2022 17:04:36 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/10/2022 17:04:36 - INFO - __main__ - ['The Blooming Baroque']
03/10/2022 17:04:36 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/10/2022 17:04:36 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/10/2022 17:04:36 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:04:36 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:04:36 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:04:37 - INFO - __main__ - Global step 2000 Train loss 0.09 QA-F1 0.2625 on epoch=999
03/10/2022 17:04:37 - INFO - __main__ - save last model!
03/10/2022 17:04:37 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 17:04:37 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 17:04:37 - INFO - __main__ - Printing 3 examples
03/10/2022 17:04:37 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:04:37 - INFO - __main__ - ['Frankie']
03/10/2022 17:04:37 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:04:37 - INFO - __main__ - ['Frankie']
03/10/2022 17:04:37 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:04:37 - INFO - __main__ - ['Frankie']
03/10/2022 17:04:37 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:04:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:04:43 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 17:04:50 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 17:04:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 17:04:50 - INFO - __main__ - Starting training!
03/10/2022 17:06:25 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_13_0.4_8_predictions.txt
03/10/2022 17:06:25 - INFO - __main__ - QA-F1 on test data: 0.2816
03/10/2022 17:06:25 - INFO - __main__ - prefix=quoref_32_13, lr=0.4, bsz=8, dev_performance=0.421875, test_performance=0.28155500413564927
03/10/2022 17:06:25 - INFO - __main__ - Running ... prefix=quoref_32_13, lr=0.3, bsz=8 ...
03/10/2022 17:06:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:06:26 - INFO - __main__ - Printing 3 examples
03/10/2022 17:06:26 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/10/2022 17:06:26 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/10/2022 17:06:26 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/10/2022 17:06:26 - INFO - __main__ - ['Up']
03/10/2022 17:06:26 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/10/2022 17:06:26 - INFO - __main__ - ['Norway']
03/10/2022 17:06:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:06:26 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:06:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:06:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:06:26 - INFO - __main__ - Printing 3 examples
03/10/2022 17:06:26 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/10/2022 17:06:26 - INFO - __main__ - ['George Gipp']
03/10/2022 17:06:26 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/10/2022 17:06:26 - INFO - __main__ - ['The Blooming Baroque']
03/10/2022 17:06:26 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/10/2022 17:06:26 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/10/2022 17:06:26 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:06:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:06:27 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:06:40 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 17:06:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 17:06:41 - INFO - __main__ - Starting training!
03/10/2022 17:06:46 - INFO - __main__ - Step 10 Global step 10 Train loss 3.02 on epoch=4
03/10/2022 17:06:50 - INFO - __main__ - Step 20 Global step 20 Train loss 2.51 on epoch=9
03/10/2022 17:06:54 - INFO - __main__ - Step 30 Global step 30 Train loss 2.12 on epoch=14
03/10/2022 17:06:59 - INFO - __main__ - Step 40 Global step 40 Train loss 1.90 on epoch=19
03/10/2022 17:07:03 - INFO - __main__ - Step 50 Global step 50 Train loss 1.79 on epoch=24
03/10/2022 17:07:04 - INFO - __main__ - Global step 50 Train loss 2.27 QA-F1 0.3927083333333333 on epoch=24
03/10/2022 17:07:04 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.3927083333333333 on epoch=24, global_step=50
03/10/2022 17:07:09 - INFO - __main__ - Step 60 Global step 60 Train loss 1.66 on epoch=29
03/10/2022 17:07:13 - INFO - __main__ - Step 70 Global step 70 Train loss 1.67 on epoch=34
03/10/2022 17:07:17 - INFO - __main__ - Step 80 Global step 80 Train loss 1.62 on epoch=39
03/10/2022 17:07:22 - INFO - __main__ - Step 90 Global step 90 Train loss 1.49 on epoch=44
03/10/2022 17:07:26 - INFO - __main__ - Step 100 Global step 100 Train loss 1.42 on epoch=49
03/10/2022 17:07:28 - INFO - __main__ - Global step 100 Train loss 1.57 QA-F1 0.40312499999999996 on epoch=49
03/10/2022 17:07:28 - INFO - __main__ - Saving model with best QA-F1: 0.3927083333333333 -> 0.40312499999999996 on epoch=49, global_step=100
03/10/2022 17:07:32 - INFO - __main__ - Step 110 Global step 110 Train loss 1.41 on epoch=54
03/10/2022 17:07:36 - INFO - __main__ - Step 120 Global step 120 Train loss 1.30 on epoch=59
03/10/2022 17:07:41 - INFO - __main__ - Step 130 Global step 130 Train loss 1.28 on epoch=64
03/10/2022 17:07:45 - INFO - __main__ - Step 140 Global step 140 Train loss 1.30 on epoch=69
03/10/2022 17:07:49 - INFO - __main__ - Step 150 Global step 150 Train loss 1.26 on epoch=74
03/10/2022 17:07:51 - INFO - __main__ - Global step 150 Train loss 1.31 QA-F1 0.36979166666666663 on epoch=74
03/10/2022 17:07:55 - INFO - __main__ - Step 160 Global step 160 Train loss 1.29 on epoch=79
03/10/2022 17:08:00 - INFO - __main__ - Step 170 Global step 170 Train loss 1.14 on epoch=84
03/10/2022 17:08:04 - INFO - __main__ - Step 180 Global step 180 Train loss 1.10 on epoch=89
03/10/2022 17:08:08 - INFO - __main__ - Step 190 Global step 190 Train loss 1.15 on epoch=94
03/10/2022 17:08:13 - INFO - __main__ - Step 200 Global step 200 Train loss 1.05 on epoch=99
03/10/2022 17:08:14 - INFO - __main__ - Global step 200 Train loss 1.15 QA-F1 0.328125 on epoch=99
03/10/2022 17:08:18 - INFO - __main__ - Step 210 Global step 210 Train loss 1.01 on epoch=104
03/10/2022 17:08:23 - INFO - __main__ - Step 220 Global step 220 Train loss 1.48 on epoch=109
03/10/2022 17:08:27 - INFO - __main__ - Step 230 Global step 230 Train loss 1.85 on epoch=114
03/10/2022 17:08:31 - INFO - __main__ - Step 240 Global step 240 Train loss 1.46 on epoch=119
03/10/2022 17:08:36 - INFO - __main__ - Step 250 Global step 250 Train loss 1.09 on epoch=124
03/10/2022 17:08:37 - INFO - __main__ - Global step 250 Train loss 1.38 QA-F1 0.33854166666666663 on epoch=124
03/10/2022 17:08:42 - INFO - __main__ - Step 260 Global step 260 Train loss 1.11 on epoch=129
03/10/2022 17:08:46 - INFO - __main__ - Step 270 Global step 270 Train loss 1.67 on epoch=134
03/10/2022 17:08:50 - INFO - __main__ - Step 280 Global step 280 Train loss 1.69 on epoch=139
03/10/2022 17:08:55 - INFO - __main__ - Step 290 Global step 290 Train loss 3.78 on epoch=144
03/10/2022 17:08:59 - INFO - __main__ - Step 300 Global step 300 Train loss 5.28 on epoch=149
03/10/2022 17:09:01 - INFO - __main__ - Global step 300 Train loss 2.70 QA-F1 0.36979166666666663 on epoch=149
03/10/2022 17:09:05 - INFO - __main__ - Step 310 Global step 310 Train loss 4.18 on epoch=154
03/10/2022 17:09:09 - INFO - __main__ - Step 320 Global step 320 Train loss 3.81 on epoch=159
03/10/2022 17:09:14 - INFO - __main__ - Step 330 Global step 330 Train loss 2.57 on epoch=164
03/10/2022 17:09:18 - INFO - __main__ - Step 340 Global step 340 Train loss 1.81 on epoch=169
03/10/2022 17:09:22 - INFO - __main__ - Step 350 Global step 350 Train loss 1.66 on epoch=174
03/10/2022 17:09:24 - INFO - __main__ - Global step 350 Train loss 2.81 QA-F1 0.33854166666666663 on epoch=174
03/10/2022 17:09:28 - INFO - __main__ - Step 360 Global step 360 Train loss 1.36 on epoch=179
03/10/2022 17:09:33 - INFO - __main__ - Step 370 Global step 370 Train loss 1.18 on epoch=184
03/10/2022 17:09:37 - INFO - __main__ - Step 380 Global step 380 Train loss 1.17 on epoch=189
03/10/2022 17:09:41 - INFO - __main__ - Step 390 Global step 390 Train loss 1.19 on epoch=194
03/10/2022 17:09:46 - INFO - __main__ - Step 400 Global step 400 Train loss 1.73 on epoch=199
03/10/2022 17:09:47 - INFO - __main__ - Global step 400 Train loss 1.33 QA-F1 0.36979166666666663 on epoch=199
03/10/2022 17:09:52 - INFO - __main__ - Step 410 Global step 410 Train loss 1.67 on epoch=204
03/10/2022 17:09:56 - INFO - __main__ - Step 420 Global step 420 Train loss 1.87 on epoch=209
03/10/2022 17:10:00 - INFO - __main__ - Step 430 Global step 430 Train loss 1.92 on epoch=214
03/10/2022 17:10:05 - INFO - __main__ - Step 440 Global step 440 Train loss 1.59 on epoch=219
03/10/2022 17:10:09 - INFO - __main__ - Step 450 Global step 450 Train loss 1.65 on epoch=224
03/10/2022 17:10:10 - INFO - __main__ - Global step 450 Train loss 1.74 QA-F1 0.3489583333333333 on epoch=224
03/10/2022 17:10:15 - INFO - __main__ - Step 460 Global step 460 Train loss 1.38 on epoch=229
03/10/2022 17:10:19 - INFO - __main__ - Step 470 Global step 470 Train loss 1.30 on epoch=234
03/10/2022 17:10:24 - INFO - __main__ - Step 480 Global step 480 Train loss 1.38 on epoch=239
03/10/2022 17:10:28 - INFO - __main__ - Step 490 Global step 490 Train loss 1.30 on epoch=244
03/10/2022 17:10:32 - INFO - __main__ - Step 500 Global step 500 Train loss 1.40 on epoch=249
03/10/2022 17:10:34 - INFO - __main__ - Global step 500 Train loss 1.35 QA-F1 0.33854166666666663 on epoch=249
03/10/2022 17:10:38 - INFO - __main__ - Step 510 Global step 510 Train loss 1.33 on epoch=254
03/10/2022 17:10:43 - INFO - __main__ - Step 520 Global step 520 Train loss 1.29 on epoch=259
03/10/2022 17:10:47 - INFO - __main__ - Step 530 Global step 530 Train loss 1.26 on epoch=264
03/10/2022 17:10:51 - INFO - __main__ - Step 540 Global step 540 Train loss 1.40 on epoch=269
03/10/2022 17:10:56 - INFO - __main__ - Step 550 Global step 550 Train loss 1.28 on epoch=274
03/10/2022 17:10:57 - INFO - __main__ - Global step 550 Train loss 1.31 QA-F1 0.359375 on epoch=274
03/10/2022 17:11:02 - INFO - __main__ - Step 560 Global step 560 Train loss 1.22 on epoch=279
03/10/2022 17:11:06 - INFO - __main__ - Step 570 Global step 570 Train loss 1.25 on epoch=284
03/10/2022 17:11:10 - INFO - __main__ - Step 580 Global step 580 Train loss 1.24 on epoch=289
03/10/2022 17:11:15 - INFO - __main__ - Step 590 Global step 590 Train loss 1.37 on epoch=294
03/10/2022 17:11:19 - INFO - __main__ - Step 600 Global step 600 Train loss 1.26 on epoch=299
03/10/2022 17:11:21 - INFO - __main__ - Global step 600 Train loss 1.27 QA-F1 0.36979166666666663 on epoch=299
03/10/2022 17:11:25 - INFO - __main__ - Step 610 Global step 610 Train loss 1.27 on epoch=304
03/10/2022 17:11:29 - INFO - __main__ - Step 620 Global step 620 Train loss 1.31 on epoch=309
03/10/2022 17:11:34 - INFO - __main__ - Step 630 Global step 630 Train loss 1.24 on epoch=314
03/10/2022 17:11:38 - INFO - __main__ - Step 640 Global step 640 Train loss 1.32 on epoch=319
03/10/2022 17:11:42 - INFO - __main__ - Step 650 Global step 650 Train loss 1.22 on epoch=324
03/10/2022 17:11:44 - INFO - __main__ - Global step 650 Train loss 1.27 QA-F1 0.3645833333333333 on epoch=324
03/10/2022 17:11:49 - INFO - __main__ - Step 660 Global step 660 Train loss 1.16 on epoch=329
03/10/2022 17:11:53 - INFO - __main__ - Step 670 Global step 670 Train loss 1.26 on epoch=334
03/10/2022 17:11:57 - INFO - __main__ - Step 680 Global step 680 Train loss 1.41 on epoch=339
03/10/2022 17:12:02 - INFO - __main__ - Step 690 Global step 690 Train loss 1.23 on epoch=344
03/10/2022 17:12:06 - INFO - __main__ - Step 700 Global step 700 Train loss 1.12 on epoch=349
03/10/2022 17:12:08 - INFO - __main__ - Global step 700 Train loss 1.24 QA-F1 0.3489583333333333 on epoch=349
03/10/2022 17:12:12 - INFO - __main__ - Step 710 Global step 710 Train loss 1.33 on epoch=354
03/10/2022 17:12:17 - INFO - __main__ - Step 720 Global step 720 Train loss 1.12 on epoch=359
03/10/2022 17:12:21 - INFO - __main__ - Step 730 Global step 730 Train loss 1.12 on epoch=364
03/10/2022 17:12:25 - INFO - __main__ - Step 740 Global step 740 Train loss 1.21 on epoch=369
03/10/2022 17:12:30 - INFO - __main__ - Step 750 Global step 750 Train loss 1.27 on epoch=374
03/10/2022 17:12:31 - INFO - __main__ - Global step 750 Train loss 1.21 QA-F1 0.359375 on epoch=374
03/10/2022 17:12:36 - INFO - __main__ - Step 760 Global step 760 Train loss 1.20 on epoch=379
03/10/2022 17:12:40 - INFO - __main__ - Step 770 Global step 770 Train loss 1.26 on epoch=384
03/10/2022 17:12:44 - INFO - __main__ - Step 780 Global step 780 Train loss 1.13 on epoch=389
03/10/2022 17:12:49 - INFO - __main__ - Step 790 Global step 790 Train loss 1.10 on epoch=394
03/10/2022 17:12:53 - INFO - __main__ - Step 800 Global step 800 Train loss 1.14 on epoch=399
03/10/2022 17:12:55 - INFO - __main__ - Global step 800 Train loss 1.17 QA-F1 0.33854166666666663 on epoch=399
03/10/2022 17:12:59 - INFO - __main__ - Step 810 Global step 810 Train loss 1.21 on epoch=404
03/10/2022 17:13:03 - INFO - __main__ - Step 820 Global step 820 Train loss 1.05 on epoch=409
03/10/2022 17:13:08 - INFO - __main__ - Step 830 Global step 830 Train loss 1.12 on epoch=414
03/10/2022 17:13:12 - INFO - __main__ - Step 840 Global step 840 Train loss 1.11 on epoch=419
03/10/2022 17:13:16 - INFO - __main__ - Step 850 Global step 850 Train loss 1.27 on epoch=424
03/10/2022 17:13:18 - INFO - __main__ - Global step 850 Train loss 1.15 QA-F1 0.33854166666666663 on epoch=424
03/10/2022 17:13:22 - INFO - __main__ - Step 860 Global step 860 Train loss 1.11 on epoch=429
03/10/2022 17:13:27 - INFO - __main__ - Step 870 Global step 870 Train loss 1.02 on epoch=434
03/10/2022 17:13:31 - INFO - __main__ - Step 880 Global step 880 Train loss 1.01 on epoch=439
03/10/2022 17:13:35 - INFO - __main__ - Step 890 Global step 890 Train loss 1.09 on epoch=444
03/10/2022 17:13:40 - INFO - __main__ - Step 900 Global step 900 Train loss 1.03 on epoch=449
03/10/2022 17:13:41 - INFO - __main__ - Global step 900 Train loss 1.05 QA-F1 0.33854166666666663 on epoch=449
03/10/2022 17:13:46 - INFO - __main__ - Step 910 Global step 910 Train loss 1.04 on epoch=454
03/10/2022 17:13:50 - INFO - __main__ - Step 920 Global step 920 Train loss 1.02 on epoch=459
03/10/2022 17:13:54 - INFO - __main__ - Step 930 Global step 930 Train loss 1.10 on epoch=464
03/10/2022 17:13:59 - INFO - __main__ - Step 940 Global step 940 Train loss 0.99 on epoch=469
03/10/2022 17:14:03 - INFO - __main__ - Step 950 Global step 950 Train loss 1.09 on epoch=474
03/10/2022 17:14:05 - INFO - __main__ - Global step 950 Train loss 1.05 QA-F1 0.34374999999999994 on epoch=474
03/10/2022 17:14:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.97 on epoch=479
03/10/2022 17:14:13 - INFO - __main__ - Step 970 Global step 970 Train loss 1.07 on epoch=484
03/10/2022 17:14:18 - INFO - __main__ - Step 980 Global step 980 Train loss 1.11 on epoch=489
03/10/2022 17:14:22 - INFO - __main__ - Step 990 Global step 990 Train loss 1.08 on epoch=494
03/10/2022 17:14:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.01 on epoch=499
03/10/2022 17:14:28 - INFO - __main__ - Global step 1000 Train loss 1.05 QA-F1 0.36458333333333326 on epoch=499
03/10/2022 17:14:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.99 on epoch=504
03/10/2022 17:14:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.08 on epoch=509
03/10/2022 17:14:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.03 on epoch=514
03/10/2022 17:14:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.98 on epoch=519
03/10/2022 17:14:50 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.07 on epoch=524
03/10/2022 17:14:52 - INFO - __main__ - Global step 1050 Train loss 1.03 QA-F1 0.359375 on epoch=524
03/10/2022 17:14:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.11 on epoch=529
03/10/2022 17:15:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.04 on epoch=534
03/10/2022 17:15:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.06 on epoch=539
03/10/2022 17:15:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.00 on epoch=544
03/10/2022 17:15:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.04 on epoch=549
03/10/2022 17:15:15 - INFO - __main__ - Global step 1100 Train loss 1.05 QA-F1 0.3489583333333333 on epoch=549
03/10/2022 17:15:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.92 on epoch=554
03/10/2022 17:15:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.95 on epoch=559
03/10/2022 17:15:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.93 on epoch=564
03/10/2022 17:15:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.98 on epoch=569
03/10/2022 17:15:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.00 on epoch=574
03/10/2022 17:15:38 - INFO - __main__ - Global step 1150 Train loss 0.95 QA-F1 0.3354166666666667 on epoch=574
03/10/2022 17:15:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.83 on epoch=579
03/10/2022 17:15:47 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.96 on epoch=584
03/10/2022 17:15:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.03 on epoch=589
03/10/2022 17:15:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.96 on epoch=594
03/10/2022 17:16:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.08 on epoch=599
03/10/2022 17:16:01 - INFO - __main__ - Global step 1200 Train loss 0.97 QA-F1 0.359375 on epoch=599
03/10/2022 17:16:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.92 on epoch=604
03/10/2022 17:16:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.91 on epoch=609
03/10/2022 17:16:14 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.92 on epoch=614
03/10/2022 17:16:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.94 on epoch=619
03/10/2022 17:16:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.96 on epoch=624
03/10/2022 17:16:25 - INFO - __main__ - Global step 1250 Train loss 0.93 QA-F1 0.38229166666666664 on epoch=624
03/10/2022 17:16:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.97 on epoch=629
03/10/2022 17:16:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.01 on epoch=634
03/10/2022 17:16:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.89 on epoch=639
03/10/2022 17:16:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.97 on epoch=644
03/10/2022 17:16:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.88 on epoch=649
03/10/2022 17:16:48 - INFO - __main__ - Global step 1300 Train loss 0.94 QA-F1 0.36979166666666663 on epoch=649
03/10/2022 17:16:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.90 on epoch=654
03/10/2022 17:16:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.93 on epoch=659
03/10/2022 17:17:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.91 on epoch=664
03/10/2022 17:17:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.90 on epoch=669
03/10/2022 17:17:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.88 on epoch=674
03/10/2022 17:17:11 - INFO - __main__ - Global step 1350 Train loss 0.90 QA-F1 0.33854166666666663 on epoch=674
03/10/2022 17:17:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.86 on epoch=679
03/10/2022 17:17:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.80 on epoch=684
03/10/2022 17:17:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.98 on epoch=689
03/10/2022 17:17:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.93 on epoch=694
03/10/2022 17:17:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.87 on epoch=699
03/10/2022 17:17:34 - INFO - __main__ - Global step 1400 Train loss 0.89 QA-F1 0.36979166666666663 on epoch=699
03/10/2022 17:17:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.92 on epoch=704
03/10/2022 17:17:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.95 on epoch=709
03/10/2022 17:17:47 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.84 on epoch=714
03/10/2022 17:17:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.91 on epoch=719
03/10/2022 17:17:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.92 on epoch=724
03/10/2022 17:17:57 - INFO - __main__ - Global step 1450 Train loss 0.91 QA-F1 0.3177083333333333 on epoch=724
03/10/2022 17:18:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.87 on epoch=729
03/10/2022 17:18:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.78 on epoch=734
03/10/2022 17:18:10 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.83 on epoch=739
03/10/2022 17:18:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.77 on epoch=744
03/10/2022 17:18:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.88 on epoch=749
03/10/2022 17:18:20 - INFO - __main__ - Global step 1500 Train loss 0.83 QA-F1 0.29166666666666663 on epoch=749
03/10/2022 17:18:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.80 on epoch=754
03/10/2022 17:18:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.83 on epoch=759
03/10/2022 17:18:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.82 on epoch=764
03/10/2022 17:18:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.71 on epoch=769
03/10/2022 17:18:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.72 on epoch=774
03/10/2022 17:18:43 - INFO - __main__ - Global step 1550 Train loss 0.78 QA-F1 0.3510416666666667 on epoch=774
03/10/2022 17:18:47 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.71 on epoch=779
03/10/2022 17:18:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.95 on epoch=784
03/10/2022 17:18:56 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.77 on epoch=789
03/10/2022 17:19:00 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.80 on epoch=794
03/10/2022 17:19:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.79 on epoch=799
03/10/2022 17:19:06 - INFO - __main__ - Global step 1600 Train loss 0.81 QA-F1 0.3489583333333333 on epoch=799
03/10/2022 17:19:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.77 on epoch=804
03/10/2022 17:19:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.79 on epoch=809
03/10/2022 17:19:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.77 on epoch=814
03/10/2022 17:19:23 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.72 on epoch=819
03/10/2022 17:19:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.86 on epoch=824
03/10/2022 17:19:29 - INFO - __main__ - Global step 1650 Train loss 0.78 QA-F1 0.38229166666666664 on epoch=824
03/10/2022 17:19:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.76 on epoch=829
03/10/2022 17:19:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.70 on epoch=834
03/10/2022 17:19:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.76 on epoch=839
03/10/2022 17:19:47 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.74 on epoch=844
03/10/2022 17:19:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.73 on epoch=849
03/10/2022 17:19:52 - INFO - __main__ - Global step 1700 Train loss 0.74 QA-F1 0.40312499999999996 on epoch=849
03/10/2022 17:19:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.86 on epoch=854
03/10/2022 17:20:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.69 on epoch=859
03/10/2022 17:20:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.69 on epoch=864
03/10/2022 17:20:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.70 on epoch=869
03/10/2022 17:20:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.70 on epoch=874
03/10/2022 17:20:15 - INFO - __main__ - Global step 1750 Train loss 0.73 QA-F1 0.34062499999999996 on epoch=874
03/10/2022 17:20:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.75 on epoch=879
03/10/2022 17:20:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.66 on epoch=884
03/10/2022 17:20:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.80 on epoch=889
03/10/2022 17:20:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.76 on epoch=894
03/10/2022 17:20:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.67 on epoch=899
03/10/2022 17:20:39 - INFO - __main__ - Global step 1800 Train loss 0.73 QA-F1 0.36875 on epoch=899
03/10/2022 17:20:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.73 on epoch=904
03/10/2022 17:20:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.67 on epoch=909
03/10/2022 17:20:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.74 on epoch=914
03/10/2022 17:20:56 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.79 on epoch=919
03/10/2022 17:21:01 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.72 on epoch=924
03/10/2022 17:21:02 - INFO - __main__ - Global step 1850 Train loss 0.73 QA-F1 0.37916666666666665 on epoch=924
03/10/2022 17:21:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.72 on epoch=929
03/10/2022 17:21:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.63 on epoch=934
03/10/2022 17:21:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.76 on epoch=939
03/10/2022 17:21:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.66 on epoch=944
03/10/2022 17:21:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.71 on epoch=949
03/10/2022 17:21:25 - INFO - __main__ - Global step 1900 Train loss 0.70 QA-F1 0.3041666666666667 on epoch=949
03/10/2022 17:21:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.68 on epoch=954
03/10/2022 17:21:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.68 on epoch=959
03/10/2022 17:21:38 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.89 on epoch=964
03/10/2022 17:21:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.82 on epoch=969
03/10/2022 17:21:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.68 on epoch=974
03/10/2022 17:21:49 - INFO - __main__ - Global step 1950 Train loss 0.75 QA-F1 0.29583333333333334 on epoch=974
03/10/2022 17:21:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.71 on epoch=979
03/10/2022 17:21:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.75 on epoch=984
03/10/2022 17:22:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.72 on epoch=989
03/10/2022 17:22:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.65 on epoch=994
03/10/2022 17:22:11 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.79 on epoch=999
03/10/2022 17:22:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:22:12 - INFO - __main__ - Printing 3 examples
03/10/2022 17:22:12 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/10/2022 17:22:12 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/10/2022 17:22:12 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/10/2022 17:22:12 - INFO - __main__ - ['Up']
03/10/2022 17:22:12 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/10/2022 17:22:12 - INFO - __main__ - ['Norway']
03/10/2022 17:22:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:22:12 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:22:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:22:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:22:12 - INFO - __main__ - Printing 3 examples
03/10/2022 17:22:12 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/10/2022 17:22:12 - INFO - __main__ - ['George Gipp']
03/10/2022 17:22:12 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/10/2022 17:22:12 - INFO - __main__ - ['The Blooming Baroque']
03/10/2022 17:22:12 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/10/2022 17:22:12 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/10/2022 17:22:12 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:22:12 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:22:12 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:22:13 - INFO - __main__ - Global step 2000 Train loss 0.72 QA-F1 0.27604166666666663 on epoch=999
03/10/2022 17:22:13 - INFO - __main__ - save last model!
03/10/2022 17:22:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 17:22:13 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 17:22:13 - INFO - __main__ - Printing 3 examples
03/10/2022 17:22:13 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:22:13 - INFO - __main__ - ['Frankie']
03/10/2022 17:22:13 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:22:13 - INFO - __main__ - ['Frankie']
03/10/2022 17:22:13 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:22:13 - INFO - __main__ - ['Frankie']
03/10/2022 17:22:13 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:22:17 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:22:20 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 17:22:26 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 17:22:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 17:22:27 - INFO - __main__ - Starting training!
03/10/2022 17:23:55 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_13_0.3_8_predictions.txt
03/10/2022 17:23:55 - INFO - __main__ - QA-F1 on test data: 0.2063
03/10/2022 17:23:55 - INFO - __main__ - prefix=quoref_32_13, lr=0.3, bsz=8, dev_performance=0.40312499999999996, test_performance=0.2062655086848635
03/10/2022 17:23:55 - INFO - __main__ - Running ... prefix=quoref_32_13, lr=0.2, bsz=8 ...
03/10/2022 17:23:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:23:56 - INFO - __main__ - Printing 3 examples
03/10/2022 17:23:56 - INFO - __main__ -  [quoref] question: What are the full scientific names of the mangrove tree species that all have the same characteristics? context: Three species of mangrove trees exist in the region: red (Rhizophora mangle), black (Avicennia germinans), and white (Laguncularia racemosa), although all are from different families. All have the same characteristics: they are tolerant of salt, brackish, and fresh water; they grow in oxygen-poor soil; and they can survive drastic water-level changes. Black and white mangroves excrete salt from under their leaves, and red mangroves filter the salinity of sea water. All species are integral to coastline protection during severe storms. Red mangroves, for example, have far-reaching roots that trap sediments. The trees not only stabilize coastlines, but add land as more sand and decaying vegetation is trapped in the root systems. All three mangroves also absorb the energy of waves and storm surges. The estuaries act as fisheries for fry and nurseries for crustaceans. Shrimp, oysters, crabs, whelks, cockles, and snails thrive in these waters, as do primordial horseshoe crabs (Limulus polyphemus). The region supports a $59 million-a-year Tortugas pink shrimp (Farfantepenaeus duorarum) industry, and a $22 million-a-year stone crab (Menippe mercenaria) industry.  Between 80 and 90 percent of species that are harvested commercially in Florida are born or spend time in the shallow waters near the Everglades. Oysters and mangroves work in tandem to build up the coastline. The sand around the coastline has minute white particles of quartz and fine shells. When currents are right, oysters grow in colonies or beds, and deposit their shells, reinforcing the bed. Mangrove seeds, called propagules, are full embryos and float in water until they reach a favorable location and take root, often on oyster beds. They shed skin and litter, ensuring other trees will not compete for space and nutrients.Mangroves also serve as excellent rookeries for birds. Wading birds, such as roseate spoonbills (Platalea ajaja), egrets, and tricolored herons (Egretta tricolor) use the mangroves as a nursery, due to the proximity of food sources and the protection offered from most prey. Thousands of birds can nest in the mangroves at once, making a noisy and messy colony, but their droppings fertilize the mangrove trees. Shorebirds like rails, terns and gulls; diving birds such as pelicans and grebes; and birds of prey such as ospreys, hawks and vultures are among the more than 100 species of birds that use Everglades mangrove trees to raise their young.
03/10/2022 17:23:56 - INFO - __main__ - ['Rhizophora mangle', 'Avicennia germinans', 'Laguncularia racemosa']
03/10/2022 17:23:56 - INFO - __main__ -  [quoref] question: What is the name of the record that R.E.M. hired Pat McCarthy to produce, ending a decade-long collaboration with Scott Litt? context: In April 1997, the band convened at Buck's Kauai vacation home to record demos of material intended for the next album. The band sought to reinvent its sound and intended to incorporate drum loops and percussion experiments. Just as the sessions were due to begin in October, Berry decided, after months of contemplation and discussions with Downs and Mills, to tell the rest of the band that he was quitting. Berry told his bandmates that he would not quit if they would break up as a result, so Stipe, Buck, and Mills agreed to carry on as a three-piece with his blessing. Berry publicly announced his departure three weeks later in October 1997. Berry told the press, "I'm just not as enthusiastic as I have been in the past about doing this anymore . . . I have the best job in the world. But I'm kind of ready to sit back and reflect and maybe not be a pop star anymore." Stipe admitted that the band would be different without a major contributor: "For me, Mike, and Peter, as R.E.M., are we still R.E.M.? I guess a three-legged dog is still a dog. It just has to learn to run differently."The band cancelled its scheduled recording sessions as a result of Berry's departure. "Without Bill it was different, confusing", Mills later said. "We didn't know exactly what to do. We couldn't rehearse without a drummer." The remaining members of R.E.M. resumed work on the album in February 1998 at Toast Studios in San Francisco. The band ended its decade-long collaboration with Scott Litt and hired Pat McCarthy to produce the record. Nigel Godrich was taken on as assistant producer, and drafted in Screaming Trees member Barrett Martin and Beck's touring drummer Joey Waronker. The recording process was plagued with tension, and the group came close to disbanding. Bertis Downs called an emergency meeting where the band members sorted out their problems and agreed to continue as a group. Led off by the single "Daysleeper", Up (1998) debuted in the top ten in the US and UK. However, the album was a relative failure, selling 900,000 copies in the US by mid-1999 and eventually selling just over two million copies worldwide. While R.E.M.'s American sales were declining, the group's commercial base was shifting to the UK, where more R.E.M. records were sold per capita than any other country and the band's singles regularly entered the Top 20.A year after Up's release, R.E.M. wrote the instrumental score to the Andy Kaufman biographical film Man on the Moon, a first for the group. The film took its title from the Automatic for the People song of the same name. The song "The Great Beyond" was released as a single from the Man on the Moon soundtrack album. "The Great Beyond" only reached number 57 on the American pop charts, but was the band's highest-charting single ever in the UK, reaching number three in 2000.
03/10/2022 17:23:56 - INFO - __main__ - ['Up']
03/10/2022 17:23:56 - INFO - __main__ -  [quoref] question: What country did heavy water come from? context: This was followed up by a group of scientists at the Collège de France in Paris: Frédéric Joliot-Curie, Hans von Halban, Lew Kowarski, and Francis Perrin. In February 1939, the Paris Group showed that when fission occurs in uranium, two or three extra neutrons are given off. This important observation suggested that a self-sustaining nuclear chain reaction might be possible. The term "atomic bomb" was already familiar to the British public through the writings of H. G. Wells, in his 1913 novel The World Set Free. It was immediately apparent to many scientists that, in theory at least, an extremely powerful explosive could be created, although most still considered an atomic bomb was an impossibility. Perrin defined a critical mass of uranium to be the smallest amount that could sustain a chain reaction. The neutrons used to cause fission in uranium are considered slow neutrons, but when neutrons are released during a fission reaction they are released as fast neutrons which have much more speed and energy. Thus, in order to create a sustained chain reaction, there existed a need for a neutron moderator to contain and slow the fast neutrons until they reached a usable energy level. The College de France found that both water and graphite could be used as acceptable moderators.Early in 1940, the Paris Group decided on theoretical grounds that heavy water would be an ideal moderator for how they intended to use it. They asked the French Minister of Armaments to obtain as much heavy water as possible from the only source, the large Norsk Hydro hydroelectric station at Vemork in Norway. The French then discovered that Germany had already offered to purchase the entire stock of Norwegian heavy water, indicating that Germany might also be researching an atomic bomb. The French told the Norwegian government of the possible military significance of heavy water. Norway gave the entire stock of 187 litres (41 imp gal; 49 US gal) to a Deuxième Bureau agent, who secretly brought it to France just before Germany invaded Norway in April 1940. On 19 June 1940, following the German invasion of France, it was shipped to England by the Earl of Suffolk and Major Ardale Golding, aboard the steamer Broompark. The heavy water, valued at £22,000, was initially kept at HM Prison Wormwood Scrubs, and was later secretly stored in the library at Windsor Castle. The Paris Group moved to Cambridge, with the exception of Joliot-Curie, who remained in France and became active in the French Resistance.
03/10/2022 17:23:56 - INFO - __main__ - ['Norway']
03/10/2022 17:23:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:23:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:23:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:23:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:23:56 - INFO - __main__ - Printing 3 examples
03/10/2022 17:23:56 - INFO - __main__ -  [quoref] question: What is the full name of the Gipper? context: Lars Knutson Rockne, a carriage builder, moves his family from Norway in 1892, settling in Chicago. His son, Knute, saves up his money and enrolls in college at the Notre Dame campus in South Bend, Indiana, where he plays football. Rockne and teammate Gus Dorais star in Notre Dame's historic 35-13 upset over Army at West Point in 1913.  The game was historically significant as Notre Dame employed the seldom-used forward pass to great effect.  The publicity from the Fighting Irish's surprise win creates Notre Dame football fans around the country. After graduation, Rockne marries sweetheart Bonnie Skiles and stays on at Notre Dame to teach chemistry, work on synthetic rubber in the chemistry lab and in his spare time, serve as an assistant coach of the Fighting Irish  football team under Coach Jesse Harper. An outstanding freshman halfback, George Gipp, leads the Irish to greater gridiron glory. Gipp is stricken with a fatal illness after the final game of the 1920 season, however, and, on his death bed, encourages Rockne at some future date to tell the team to go out and "win one for the Gipper." Notre Dame continues its football success with a backfield of stars dubbed "the Four Horsemen." Rockne, tragically, is killed in a 1931 plane crash on a trip to California, but his legend makes him a campus immortal.
03/10/2022 17:23:56 - INFO - __main__ - ['George Gipp']
03/10/2022 17:23:56 - INFO - __main__ -  [quoref] question: What is the name of the garden that became a permanent landmark after it closed? context: The gardens were to be centered in the north with an Italian terraced garden and were largely completed when Eberhard Louis turned his attention to the south garden. There he laid out a large symmetrical French garden. Charles Eugene filled in the terraces in 1749 to replace them with a large broderie. He then reorganized and expanded the south garden over the next decade. Frederick I again reorganized the south garden in 1797 in a Neoclassical style and Mediterranean theme. He retained the original pathways, but added a canal and fountain to the garden's center. The south garden was divided into four equally sized lawns, with hillocks in their center topped with a large vase crafted by Antonio Isopi. Frederick also expanded the garden east to form an English landscape garden (Lower east) and demolished Charles Eugene's opera house to form a medieval-themed landscape garden (Upper east). Two additional gardens, for Frederick and Charlotte, were laid out adjacent to their palace suites. Also in the fantasy garden is the Emichsburg, a folly built from 1798 to 1802 and named after the fabled ancestor of the House of Württemberg, a knight of the House of Hohenstaufen. William I abandoned Ludwigsburg for Rosenstein Palace in Stuttgart and opened the south garden to the public in 1828. The canal was filled in and an orchard planted on the southern lawns, later used to grow potatoes. In 1947, Albert Schöchle, Director of the State Parks and Gardens Authority, was charged with maintaining the gardens. After visiting the 1951 Bundesgartenschau in Hanover, he decided to restore the gardens. Schöchle convinced Baden-Württemberg's Minister of Finance Karl Frank to help fund the venture in 1952 on the condition that the town of Ludwigsburg also assisted. Ludwigsburg's mayor, Elmar Doch, and the town council agreed to this stipulation. Frank approved the start of work on 23 March 1953, but it lasted late into the year. The restoration of the garden required the moving of 100,000 cubic meters (3,531,467 cu ft) of earth by bulldozers supplied and operated by American soldiers and the planting of tens of thousands of trees and hedges, 22,000 roses, and 400,000 other flowers. The Blooming Baroque (Blühendes Barock) gardens were opened on 23 April 1954 as a special horticultural show and attracted more than 500,000 visitors by the end of May, among them President Theodor Heuss. When the show closed in the fall of 1954, it had recouped all but 150,000 Deutsche Marks of the investment in the restoration of the gardens and became a permanent landmark. The Blooming Baroque gardens, covering an area of 32 hectares (79 acres), attract 520,000 to 550,000 visitors annually.On the far east side is the Fairy-Tale Garden (Märchengarten), opened in 1959. It is made up of some 40 recreations of fairy-tales such as Rapunzel, Sleeping Beauty, and The Frog Prince. The Fairy-Tale Garden was an immediate success and increased revenue by 50% for that year.
03/10/2022 17:23:56 - INFO - __main__ - ['The Blooming Baroque']
03/10/2022 17:23:56 - INFO - __main__ -  [quoref] question: What is the full name of the person that Swoff orders Fergus to shoot? context: In 1989, Anthony "Swoff" Swofford, whose father served in the Vietnam War, attends U.S. Marine Corps training before being stationed at Camp Pendleton. Claiming that he joined the military because he "got lost on the way to college", Swofford finds his time at Camp Pendleton difficult, and struggles to make friends. While Swofford feigns illness to avoid his responsibilities, a "lifer", Staff Sergeant Sykes, takes note of his potential and orders Swofford to attend his Scout Sniper course. After grueling training, the Scout Sniper course is left with eight candidates, among them Swofford, now a sniper, and Swofford's roommate Corporal Alan Troy who becomes his spotter. When Iraq invades Kuwait, Swofford's unit is deployed to the Arabian Peninsula as a part of Operation Desert Shield. Eager for combat, the Marines find themselves bored with remedial training, constant drills, and a routine monotony that feeds their boredom, and prompts them to talk about the unfaithful girlfriends and wives waiting for them at home. They even erect a bulletin board featuring photographs and brief notes telling what perfidies the women had committed. Swofford obtains unauthorized alcohol and organizes an impromptu Christmas party, arranging for Fergus to cover his watch so he can celebrate. Fergus accidentally sets fire to a tent while cooking some sausages and ignites a crate of flares, waking the whole camp and enraging Staff Sergeant Sykes, who demotes Swofford from lance corporal to private and puts him on "shit-burning" detail. The punishments, combined with the heat, the boredom, and Swofford's suspicions of his girlfriend's infidelity, give Swofford a mental breakdown, to the point where he threatens Fergus with a rifle, then orders Fergus to shoot him instead.
03/10/2022 17:23:56 - INFO - __main__ - ['Anthony "Swoff" Swofford']
03/10/2022 17:23:56 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:23:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:23:56 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:24:10 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 17:24:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 17:24:10 - INFO - __main__ - Starting training!
03/10/2022 17:24:15 - INFO - __main__ - Step 10 Global step 10 Train loss 3.11 on epoch=4
03/10/2022 17:24:20 - INFO - __main__ - Step 20 Global step 20 Train loss 2.53 on epoch=9
03/10/2022 17:24:24 - INFO - __main__ - Step 30 Global step 30 Train loss 2.28 on epoch=14
03/10/2022 17:24:28 - INFO - __main__ - Step 40 Global step 40 Train loss 2.11 on epoch=19
03/10/2022 17:24:33 - INFO - __main__ - Step 50 Global step 50 Train loss 1.82 on epoch=24
03/10/2022 17:24:34 - INFO - __main__ - Global step 50 Train loss 2.37 QA-F1 0.4239583333333333 on epoch=24
03/10/2022 17:24:34 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.4239583333333333 on epoch=24, global_step=50
03/10/2022 17:24:39 - INFO - __main__ - Step 60 Global step 60 Train loss 1.76 on epoch=29
03/10/2022 17:24:43 - INFO - __main__ - Step 70 Global step 70 Train loss 1.65 on epoch=34
03/10/2022 17:24:47 - INFO - __main__ - Step 80 Global step 80 Train loss 1.58 on epoch=39
03/10/2022 17:24:52 - INFO - __main__ - Step 90 Global step 90 Train loss 1.47 on epoch=44
03/10/2022 17:24:56 - INFO - __main__ - Step 100 Global step 100 Train loss 1.41 on epoch=49
03/10/2022 17:24:57 - INFO - __main__ - Global step 100 Train loss 1.57 QA-F1 0.29166666666666663 on epoch=49
03/10/2022 17:25:02 - INFO - __main__ - Step 110 Global step 110 Train loss 1.36 on epoch=54
03/10/2022 17:25:06 - INFO - __main__ - Step 120 Global step 120 Train loss 1.44 on epoch=59
03/10/2022 17:25:10 - INFO - __main__ - Step 130 Global step 130 Train loss 1.41 on epoch=64
03/10/2022 17:25:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.21 on epoch=69
03/10/2022 17:25:19 - INFO - __main__ - Step 150 Global step 150 Train loss 1.19 on epoch=74
03/10/2022 17:25:20 - INFO - __main__ - Global step 150 Train loss 1.32 QA-F1 0.3770833333333333 on epoch=74
03/10/2022 17:25:25 - INFO - __main__ - Step 160 Global step 160 Train loss 1.11 on epoch=79
03/10/2022 17:25:29 - INFO - __main__ - Step 170 Global step 170 Train loss 1.06 on epoch=84
03/10/2022 17:25:33 - INFO - __main__ - Step 180 Global step 180 Train loss 1.05 on epoch=89
03/10/2022 17:25:38 - INFO - __main__ - Step 190 Global step 190 Train loss 1.04 on epoch=94
03/10/2022 17:25:42 - INFO - __main__ - Step 200 Global step 200 Train loss 1.07 on epoch=99
03/10/2022 17:25:43 - INFO - __main__ - Global step 200 Train loss 1.07 QA-F1 0.3875 on epoch=99
03/10/2022 17:25:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.94 on epoch=104
03/10/2022 17:25:52 - INFO - __main__ - Step 220 Global step 220 Train loss 0.93 on epoch=109
03/10/2022 17:25:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.90 on epoch=114
03/10/2022 17:26:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.84 on epoch=119
03/10/2022 17:26:05 - INFO - __main__ - Step 250 Global step 250 Train loss 0.82 on epoch=124
03/10/2022 17:26:07 - INFO - __main__ - Global step 250 Train loss 0.89 QA-F1 0.375 on epoch=124
03/10/2022 17:26:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.82 on epoch=129
03/10/2022 17:26:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.75 on epoch=134
03/10/2022 17:26:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.76 on epoch=139
03/10/2022 17:26:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.79 on epoch=144
03/10/2022 17:26:28 - INFO - __main__ - Step 300 Global step 300 Train loss 0.76 on epoch=149
03/10/2022 17:26:30 - INFO - __main__ - Global step 300 Train loss 0.78 QA-F1 0.34375 on epoch=149
03/10/2022 17:26:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.70 on epoch=154
03/10/2022 17:26:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=159
03/10/2022 17:26:43 - INFO - __main__ - Step 330 Global step 330 Train loss 0.69 on epoch=164
03/10/2022 17:26:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.70 on epoch=169
03/10/2022 17:26:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.61 on epoch=174
03/10/2022 17:26:53 - INFO - __main__ - Global step 350 Train loss 0.67 QA-F1 0.359375 on epoch=174
03/10/2022 17:26:58 - INFO - __main__ - Step 360 Global step 360 Train loss 0.58 on epoch=179
03/10/2022 17:27:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=184
03/10/2022 17:27:06 - INFO - __main__ - Step 380 Global step 380 Train loss 0.55 on epoch=189
03/10/2022 17:27:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=194
03/10/2022 17:27:15 - INFO - __main__ - Step 400 Global step 400 Train loss 0.54 on epoch=199
03/10/2022 17:27:16 - INFO - __main__ - Global step 400 Train loss 0.56 QA-F1 0.33541666666666664 on epoch=199
03/10/2022 17:27:21 - INFO - __main__ - Step 410 Global step 410 Train loss 0.54 on epoch=204
03/10/2022 17:27:25 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=209
03/10/2022 17:27:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.53 on epoch=214
03/10/2022 17:27:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.47 on epoch=219
03/10/2022 17:27:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.47 on epoch=224
03/10/2022 17:27:39 - INFO - __main__ - Global step 450 Train loss 0.51 QA-F1 0.29375 on epoch=224
03/10/2022 17:27:44 - INFO - __main__ - Step 460 Global step 460 Train loss 0.54 on epoch=229
03/10/2022 17:27:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=234
03/10/2022 17:27:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=239
03/10/2022 17:27:56 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=244
03/10/2022 17:28:01 - INFO - __main__ - Step 500 Global step 500 Train loss 0.44 on epoch=249
03/10/2022 17:28:02 - INFO - __main__ - Global step 500 Train loss 0.49 QA-F1 0.33854166666666663 on epoch=249
03/10/2022 17:28:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.52 on epoch=254
03/10/2022 17:28:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=259
03/10/2022 17:28:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.45 on epoch=264
03/10/2022 17:28:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.42 on epoch=269
03/10/2022 17:28:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=274
03/10/2022 17:28:25 - INFO - __main__ - Global step 550 Train loss 0.44 QA-F1 0.359375 on epoch=274
03/10/2022 17:28:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.56 on epoch=279
03/10/2022 17:28:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.44 on epoch=284
03/10/2022 17:28:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.45 on epoch=289
03/10/2022 17:28:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=294
03/10/2022 17:28:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=299
03/10/2022 17:28:48 - INFO - __main__ - Global step 600 Train loss 0.45 QA-F1 0.3822916666666667 on epoch=299
03/10/2022 17:28:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.41 on epoch=304
03/10/2022 17:28:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=309
03/10/2022 17:29:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.37 on epoch=314
03/10/2022 17:29:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=319
03/10/2022 17:29:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.32 on epoch=324
03/10/2022 17:29:11 - INFO - __main__ - Global step 650 Train loss 0.36 QA-F1 0.3489583333333333 on epoch=324
03/10/2022 17:29:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=329
03/10/2022 17:29:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.34 on epoch=334
03/10/2022 17:29:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=339
03/10/2022 17:29:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=344
03/10/2022 17:29:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.39 on epoch=349
03/10/2022 17:29:34 - INFO - __main__ - Global step 700 Train loss 0.35 QA-F1 0.38749999999999996 on epoch=349
03/10/2022 17:29:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=354
03/10/2022 17:29:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=359
03/10/2022 17:29:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=364
03/10/2022 17:29:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=369
03/10/2022 17:29:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.28 on epoch=374
03/10/2022 17:29:57 - INFO - __main__ - Global step 750 Train loss 0.33 QA-F1 0.34375 on epoch=374
03/10/2022 17:30:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=379
03/10/2022 17:30:05 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=384
03/10/2022 17:30:10 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=389
03/10/2022 17:30:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
03/10/2022 17:30:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
03/10/2022 17:30:20 - INFO - __main__ - Global step 800 Train loss 0.30 QA-F1 0.328125 on epoch=399
03/10/2022 17:30:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=404
03/10/2022 17:30:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=409
03/10/2022 17:30:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=414
03/10/2022 17:30:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.30 on epoch=419
03/10/2022 17:30:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=424
03/10/2022 17:30:43 - INFO - __main__ - Global step 850 Train loss 0.31 QA-F1 0.328125 on epoch=424
03/10/2022 17:30:47 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
03/10/2022 17:30:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=434
03/10/2022 17:30:56 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=439
03/10/2022 17:31:00 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=444
03/10/2022 17:31:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=449
03/10/2022 17:31:06 - INFO - __main__ - Global step 900 Train loss 0.28 QA-F1 0.27604166666666663 on epoch=449
03/10/2022 17:31:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=454
03/10/2022 17:31:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=459
03/10/2022 17:31:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=464
03/10/2022 17:31:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=469
03/10/2022 17:31:27 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=474
03/10/2022 17:31:29 - INFO - __main__ - Global step 950 Train loss 0.24 QA-F1 0.371875 on epoch=474
03/10/2022 17:31:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=479
03/10/2022 17:31:37 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=484
03/10/2022 17:31:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=489
03/10/2022 17:31:46 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=494
03/10/2022 17:31:50 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=499
03/10/2022 17:31:52 - INFO - __main__ - Global step 1000 Train loss 0.25 QA-F1 0.3614583333333333 on epoch=499
03/10/2022 17:31:56 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=504
03/10/2022 17:32:00 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.29 on epoch=509
03/10/2022 17:32:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=514
03/10/2022 17:32:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.28 on epoch=519
03/10/2022 17:32:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
03/10/2022 17:32:15 - INFO - __main__ - Global step 1050 Train loss 0.26 QA-F1 0.3927083333333333 on epoch=524
03/10/2022 17:32:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=529
03/10/2022 17:32:24 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=534
03/10/2022 17:32:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=539
03/10/2022 17:32:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/10/2022 17:32:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=549
03/10/2022 17:32:38 - INFO - __main__ - Global step 1100 Train loss 0.25 QA-F1 0.36979166666666663 on epoch=549
03/10/2022 17:32:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=554
03/10/2022 17:32:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=559
03/10/2022 17:32:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=564
03/10/2022 17:32:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.20 on epoch=569
03/10/2022 17:33:00 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=574
03/10/2022 17:33:01 - INFO - __main__ - Global step 1150 Train loss 0.24 QA-F1 0.3770833333333333 on epoch=574
03/10/2022 17:33:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=579
03/10/2022 17:33:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.19 on epoch=584
03/10/2022 17:33:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=589
03/10/2022 17:33:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/10/2022 17:33:23 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=599
03/10/2022 17:33:24 - INFO - __main__ - Global step 1200 Train loss 0.21 QA-F1 0.33854166666666663 on epoch=599
03/10/2022 17:33:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=604
03/10/2022 17:33:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=609
03/10/2022 17:33:37 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=614
03/10/2022 17:33:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=619
03/10/2022 17:33:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=624
03/10/2022 17:33:48 - INFO - __main__ - Global step 1250 Train loss 0.23 QA-F1 0.3177083333333333 on epoch=624
03/10/2022 17:33:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=629
03/10/2022 17:33:56 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=634
03/10/2022 17:34:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=639
03/10/2022 17:34:05 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=644
03/10/2022 17:34:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=649
03/10/2022 17:34:11 - INFO - __main__ - Global step 1300 Train loss 0.18 QA-F1 0.359375 on epoch=649
03/10/2022 17:34:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=654
03/10/2022 17:34:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=659
03/10/2022 17:34:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=664
03/10/2022 17:34:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/10/2022 17:34:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=674
03/10/2022 17:34:34 - INFO - __main__ - Global step 1350 Train loss 0.21 QA-F1 0.2864583333333333 on epoch=674
03/10/2022 17:34:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=679
03/10/2022 17:34:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=684
03/10/2022 17:34:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=689
03/10/2022 17:34:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=694
03/10/2022 17:34:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=699
03/10/2022 17:34:58 - INFO - __main__ - Global step 1400 Train loss 0.19 QA-F1 0.3489583333333333 on epoch=699
03/10/2022 17:35:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=704
03/10/2022 17:35:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
03/10/2022 17:35:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.20 on epoch=714
03/10/2022 17:35:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.20 on epoch=719
03/10/2022 17:35:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=724
03/10/2022 17:35:21 - INFO - __main__ - Global step 1450 Train loss 0.18 QA-F1 0.36979166666666663 on epoch=724
03/10/2022 17:35:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
03/10/2022 17:35:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/10/2022 17:35:34 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/10/2022 17:35:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.21 on epoch=744
03/10/2022 17:35:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=749
03/10/2022 17:35:45 - INFO - __main__ - Global step 1500 Train loss 0.20 QA-F1 0.3302083333333333 on epoch=749
03/10/2022 17:35:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
03/10/2022 17:35:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=759
03/10/2022 17:35:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=764
03/10/2022 17:36:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=769
03/10/2022 17:36:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=774
03/10/2022 17:36:08 - INFO - __main__ - Global step 1550 Train loss 0.17 QA-F1 0.36979166666666663 on epoch=774
03/10/2022 17:36:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=779
03/10/2022 17:36:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=784
03/10/2022 17:36:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=789
03/10/2022 17:36:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=794
03/10/2022 17:36:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
03/10/2022 17:36:32 - INFO - __main__ - Global step 1600 Train loss 0.17 QA-F1 0.390625 on epoch=799
03/10/2022 17:36:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=804
03/10/2022 17:36:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=809
03/10/2022 17:36:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.17 on epoch=814
03/10/2022 17:36:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=819
03/10/2022 17:36:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=824
03/10/2022 17:36:55 - INFO - __main__ - Global step 1650 Train loss 0.14 QA-F1 0.3614583333333333 on epoch=824
03/10/2022 17:36:59 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.14 on epoch=829
03/10/2022 17:37:04 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.14 on epoch=834
03/10/2022 17:37:08 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=839
03/10/2022 17:37:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=844
03/10/2022 17:37:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=849
03/10/2022 17:37:18 - INFO - __main__ - Global step 1700 Train loss 0.15 QA-F1 0.375 on epoch=849
03/10/2022 17:37:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=854
03/10/2022 17:37:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=859
03/10/2022 17:37:31 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=864
03/10/2022 17:37:35 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=869
03/10/2022 17:37:40 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=874
03/10/2022 17:37:42 - INFO - __main__ - Global step 1750 Train loss 0.13 QA-F1 0.3927083333333333 on epoch=874
03/10/2022 17:37:46 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=879
03/10/2022 17:37:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=884
03/10/2022 17:37:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.12 on epoch=889
03/10/2022 17:37:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=894
03/10/2022 17:38:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=899
03/10/2022 17:38:05 - INFO - __main__ - Global step 1800 Train loss 0.13 QA-F1 0.35104166666666664 on epoch=899
03/10/2022 17:38:10 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=904
03/10/2022 17:38:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=909
03/10/2022 17:38:18 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.12 on epoch=914
03/10/2022 17:38:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=919
03/10/2022 17:38:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.12 on epoch=924
03/10/2022 17:38:29 - INFO - __main__ - Global step 1850 Train loss 0.12 QA-F1 0.35104166666666664 on epoch=924
03/10/2022 17:38:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=929
03/10/2022 17:38:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/10/2022 17:38:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=939
03/10/2022 17:38:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/10/2022 17:38:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=949
03/10/2022 17:38:53 - INFO - __main__ - Global step 1900 Train loss 0.13 QA-F1 0.3302083333333333 on epoch=949
03/10/2022 17:38:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=954
03/10/2022 17:39:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=959
03/10/2022 17:39:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.12 on epoch=964
03/10/2022 17:39:10 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=969
03/10/2022 17:39:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=974
03/10/2022 17:39:16 - INFO - __main__ - Global step 1950 Train loss 0.12 QA-F1 0.33541666666666664 on epoch=974
03/10/2022 17:39:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=979
03/10/2022 17:39:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=984
03/10/2022 17:39:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.08 on epoch=989
03/10/2022 17:39:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=994
03/10/2022 17:39:38 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=999
03/10/2022 17:39:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:39:39 - INFO - __main__ - Printing 3 examples
03/10/2022 17:39:39 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/10/2022 17:39:39 - INFO - __main__ - ['Allan']
03/10/2022 17:39:39 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/10/2022 17:39:39 - INFO - __main__ - ['Nigro']
03/10/2022 17:39:39 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/10/2022 17:39:39 - INFO - __main__ - ['Minogue']
03/10/2022 17:39:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:39:39 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:39:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:39:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:39:39 - INFO - __main__ - Printing 3 examples
03/10/2022 17:39:39 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/10/2022 17:39:39 - INFO - __main__ - ['Alan']
03/10/2022 17:39:39 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/10/2022 17:39:39 - INFO - __main__ - ['Santo Tomás de Castilla']
03/10/2022 17:39:39 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/10/2022 17:39:39 - INFO - __main__ - ['Albert']
03/10/2022 17:39:39 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:39:39 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:39:39 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:39:40 - INFO - __main__ - Global step 2000 Train loss 0.12 QA-F1 0.3614583333333333 on epoch=999
03/10/2022 17:39:40 - INFO - __main__ - save last model!
03/10/2022 17:39:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 17:39:40 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 17:39:40 - INFO - __main__ - Printing 3 examples
03/10/2022 17:39:40 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:39:40 - INFO - __main__ - ['Frankie']
03/10/2022 17:39:40 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:39:40 - INFO - __main__ - ['Frankie']
03/10/2022 17:39:40 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:39:40 - INFO - __main__ - ['Frankie']
03/10/2022 17:39:40 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:39:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:39:47 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 17:39:52 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 17:39:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 17:39:53 - INFO - __main__ - Starting training!
03/10/2022 17:41:40 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_13_0.2_8_predictions.txt
03/10/2022 17:41:40 - INFO - __main__ - QA-F1 on test data: 0.3024
03/10/2022 17:41:40 - INFO - __main__ - prefix=quoref_32_13, lr=0.2, bsz=8, dev_performance=0.4239583333333333, test_performance=0.30241246208988143
03/10/2022 17:41:40 - INFO - __main__ - Running ... prefix=quoref_32_21, lr=0.5, bsz=8 ...
03/10/2022 17:41:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:41:41 - INFO - __main__ - Printing 3 examples
03/10/2022 17:41:41 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/10/2022 17:41:41 - INFO - __main__ - ['Allan']
03/10/2022 17:41:41 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/10/2022 17:41:41 - INFO - __main__ - ['Nigro']
03/10/2022 17:41:41 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/10/2022 17:41:41 - INFO - __main__ - ['Minogue']
03/10/2022 17:41:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:41:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:41:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:41:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:41:41 - INFO - __main__ - Printing 3 examples
03/10/2022 17:41:41 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/10/2022 17:41:41 - INFO - __main__ - ['Alan']
03/10/2022 17:41:41 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/10/2022 17:41:41 - INFO - __main__ - ['Santo Tomás de Castilla']
03/10/2022 17:41:41 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/10/2022 17:41:41 - INFO - __main__ - ['Albert']
03/10/2022 17:41:41 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:41:41 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:41:41 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:41:55 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 17:41:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 17:41:56 - INFO - __main__ - Starting training!
03/10/2022 17:42:01 - INFO - __main__ - Step 10 Global step 10 Train loss 2.62 on epoch=4
03/10/2022 17:42:05 - INFO - __main__ - Step 20 Global step 20 Train loss 2.10 on epoch=9
03/10/2022 17:42:09 - INFO - __main__ - Step 30 Global step 30 Train loss 1.96 on epoch=14
03/10/2022 17:42:13 - INFO - __main__ - Step 40 Global step 40 Train loss 1.78 on epoch=19
03/10/2022 17:42:18 - INFO - __main__ - Step 50 Global step 50 Train loss 2.76 on epoch=24
03/10/2022 17:42:19 - INFO - __main__ - Global step 50 Train loss 2.25 QA-F1 0.22916666666666663 on epoch=24
03/10/2022 17:42:19 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.22916666666666663 on epoch=24, global_step=50
03/10/2022 17:42:23 - INFO - __main__ - Step 60 Global step 60 Train loss 2.86 on epoch=29
03/10/2022 17:42:27 - INFO - __main__ - Step 70 Global step 70 Train loss 1.92 on epoch=34
03/10/2022 17:42:32 - INFO - __main__ - Step 80 Global step 80 Train loss 1.71 on epoch=39
03/10/2022 17:42:36 - INFO - __main__ - Step 90 Global step 90 Train loss 1.67 on epoch=44
03/10/2022 17:42:40 - INFO - __main__ - Step 100 Global step 100 Train loss 1.67 on epoch=49
03/10/2022 17:42:42 - INFO - __main__ - Global step 100 Train loss 1.97 QA-F1 0.19791666666666666 on epoch=49
03/10/2022 17:42:46 - INFO - __main__ - Step 110 Global step 110 Train loss 1.68 on epoch=54
03/10/2022 17:42:50 - INFO - __main__ - Step 120 Global step 120 Train loss 1.62 on epoch=59
03/10/2022 17:42:54 - INFO - __main__ - Step 130 Global step 130 Train loss 1.58 on epoch=64
03/10/2022 17:42:59 - INFO - __main__ - Step 140 Global step 140 Train loss 1.52 on epoch=69
03/10/2022 17:43:03 - INFO - __main__ - Step 150 Global step 150 Train loss 1.45 on epoch=74
03/10/2022 17:43:04 - INFO - __main__ - Global step 150 Train loss 1.57 QA-F1 0.14583333333333331 on epoch=74
03/10/2022 17:43:09 - INFO - __main__ - Step 160 Global step 160 Train loss 1.44 on epoch=79
03/10/2022 17:43:13 - INFO - __main__ - Step 170 Global step 170 Train loss 1.31 on epoch=84
03/10/2022 17:43:17 - INFO - __main__ - Step 180 Global step 180 Train loss 1.39 on epoch=89
03/10/2022 17:43:22 - INFO - __main__ - Step 190 Global step 190 Train loss 1.30 on epoch=94
03/10/2022 17:43:26 - INFO - __main__ - Step 200 Global step 200 Train loss 1.36 on epoch=99
03/10/2022 17:43:27 - INFO - __main__ - Global step 200 Train loss 1.36 QA-F1 0.10416666666666666 on epoch=99
03/10/2022 17:43:31 - INFO - __main__ - Step 210 Global step 210 Train loss 1.23 on epoch=104
03/10/2022 17:43:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.36 on epoch=109
03/10/2022 17:43:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.25 on epoch=114
03/10/2022 17:43:44 - INFO - __main__ - Step 240 Global step 240 Train loss 1.29 on epoch=119
03/10/2022 17:43:49 - INFO - __main__ - Step 250 Global step 250 Train loss 1.22 on epoch=124
03/10/2022 17:43:50 - INFO - __main__ - Global step 250 Train loss 1.27 QA-F1 0.16666666666666666 on epoch=124
03/10/2022 17:43:54 - INFO - __main__ - Step 260 Global step 260 Train loss 1.22 on epoch=129
03/10/2022 17:43:59 - INFO - __main__ - Step 270 Global step 270 Train loss 1.16 on epoch=134
03/10/2022 17:44:03 - INFO - __main__ - Step 280 Global step 280 Train loss 1.16 on epoch=139
03/10/2022 17:44:07 - INFO - __main__ - Step 290 Global step 290 Train loss 1.11 on epoch=144
03/10/2022 17:44:12 - INFO - __main__ - Step 300 Global step 300 Train loss 1.13 on epoch=149
03/10/2022 17:44:13 - INFO - __main__ - Global step 300 Train loss 1.16 QA-F1 0.16666666666666666 on epoch=149
03/10/2022 17:44:18 - INFO - __main__ - Step 310 Global step 310 Train loss 1.12 on epoch=154
03/10/2022 17:44:22 - INFO - __main__ - Step 320 Global step 320 Train loss 1.11 on epoch=159
03/10/2022 17:44:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.99 on epoch=164
03/10/2022 17:44:30 - INFO - __main__ - Step 340 Global step 340 Train loss 1.02 on epoch=169
03/10/2022 17:44:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.88 on epoch=174
03/10/2022 17:44:36 - INFO - __main__ - Global step 350 Train loss 1.02 QA-F1 0.11458333333333333 on epoch=174
03/10/2022 17:44:41 - INFO - __main__ - Step 360 Global step 360 Train loss 1.01 on epoch=179
03/10/2022 17:44:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.92 on epoch=184
03/10/2022 17:44:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.95 on epoch=189
03/10/2022 17:44:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.92 on epoch=194
03/10/2022 17:44:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.84 on epoch=199
03/10/2022 17:44:59 - INFO - __main__ - Global step 400 Train loss 0.93 QA-F1 0.09375 on epoch=199
03/10/2022 17:45:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.87 on epoch=204
03/10/2022 17:45:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.87 on epoch=209
03/10/2022 17:45:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.89 on epoch=214
03/10/2022 17:45:17 - INFO - __main__ - Step 440 Global step 440 Train loss 0.83 on epoch=219
03/10/2022 17:45:21 - INFO - __main__ - Step 450 Global step 450 Train loss 0.90 on epoch=224
03/10/2022 17:45:23 - INFO - __main__ - Global step 450 Train loss 0.87 QA-F1 0.1875 on epoch=224
03/10/2022 17:45:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=229
03/10/2022 17:45:31 - INFO - __main__ - Step 470 Global step 470 Train loss 0.80 on epoch=234
03/10/2022 17:45:35 - INFO - __main__ - Step 480 Global step 480 Train loss 0.76 on epoch=239
03/10/2022 17:45:40 - INFO - __main__ - Step 490 Global step 490 Train loss 0.75 on epoch=244
03/10/2022 17:45:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.78 on epoch=249
03/10/2022 17:45:46 - INFO - __main__ - Global step 500 Train loss 0.78 QA-F1 0.16666666666666666 on epoch=249
03/10/2022 17:45:50 - INFO - __main__ - Step 510 Global step 510 Train loss 0.71 on epoch=254
03/10/2022 17:45:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.69 on epoch=259
03/10/2022 17:45:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.69 on epoch=264
03/10/2022 17:46:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.65 on epoch=269
03/10/2022 17:46:07 - INFO - __main__ - Step 550 Global step 550 Train loss 0.64 on epoch=274
03/10/2022 17:46:08 - INFO - __main__ - Global step 550 Train loss 0.68 QA-F1 0.1875 on epoch=274
03/10/2022 17:46:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.64 on epoch=279
03/10/2022 17:46:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.62 on epoch=284
03/10/2022 17:46:21 - INFO - __main__ - Step 580 Global step 580 Train loss 0.60 on epoch=289
03/10/2022 17:46:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.67 on epoch=294
03/10/2022 17:46:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=299
03/10/2022 17:46:31 - INFO - __main__ - Global step 600 Train loss 0.61 QA-F1 0.22291666666666665 on epoch=299
03/10/2022 17:46:35 - INFO - __main__ - Step 610 Global step 610 Train loss 0.60 on epoch=304
03/10/2022 17:46:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.49 on epoch=309
03/10/2022 17:46:44 - INFO - __main__ - Step 630 Global step 630 Train loss 0.56 on epoch=314
03/10/2022 17:46:48 - INFO - __main__ - Step 640 Global step 640 Train loss 0.49 on epoch=319
03/10/2022 17:46:52 - INFO - __main__ - Step 650 Global step 650 Train loss 0.56 on epoch=324
03/10/2022 17:46:54 - INFO - __main__ - Global step 650 Train loss 0.54 QA-F1 0.22083333333333333 on epoch=324
03/10/2022 17:46:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=329
03/10/2022 17:47:02 - INFO - __main__ - Step 670 Global step 670 Train loss 0.48 on epoch=334
03/10/2022 17:47:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.40 on epoch=339
03/10/2022 17:47:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=344
03/10/2022 17:47:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=349
03/10/2022 17:47:16 - INFO - __main__ - Global step 700 Train loss 0.42 QA-F1 0.17708333333333331 on epoch=349
03/10/2022 17:47:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=354
03/10/2022 17:47:25 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=359
03/10/2022 17:47:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
03/10/2022 17:47:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=369
03/10/2022 17:47:38 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=374
03/10/2022 17:47:39 - INFO - __main__ - Global step 750 Train loss 0.32 QA-F1 0.17708333333333331 on epoch=374
03/10/2022 17:47:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.32 on epoch=379
03/10/2022 17:47:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=384
03/10/2022 17:47:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
03/10/2022 17:47:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.38 on epoch=394
03/10/2022 17:48:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.29 on epoch=399
03/10/2022 17:48:02 - INFO - __main__ - Global step 800 Train loss 0.32 QA-F1 0.16666666666666666 on epoch=399
03/10/2022 17:48:06 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=404
03/10/2022 17:48:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=409
03/10/2022 17:48:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=414
03/10/2022 17:48:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=419
03/10/2022 17:48:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=424
03/10/2022 17:48:24 - INFO - __main__ - Global step 850 Train loss 0.23 QA-F1 0.09791666666666667 on epoch=424
03/10/2022 17:48:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
03/10/2022 17:48:33 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=434
03/10/2022 17:48:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
03/10/2022 17:48:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=444
03/10/2022 17:48:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=449
03/10/2022 17:48:47 - INFO - __main__ - Global step 900 Train loss 0.19 QA-F1 0.11875 on epoch=449
03/10/2022 17:48:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=454
03/10/2022 17:48:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=459
03/10/2022 17:49:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.16 on epoch=464
03/10/2022 17:49:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=469
03/10/2022 17:49:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.13 on epoch=474
03/10/2022 17:49:09 - INFO - __main__ - Global step 950 Train loss 0.18 QA-F1 0.23333333333333334 on epoch=474
03/10/2022 17:49:09 - INFO - __main__ - Saving model with best QA-F1: 0.22916666666666663 -> 0.23333333333333334 on epoch=474, global_step=950
03/10/2022 17:49:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=479
03/10/2022 17:49:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=484
03/10/2022 17:49:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=489
03/10/2022 17:49:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=494
03/10/2022 17:49:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=499
03/10/2022 17:49:32 - INFO - __main__ - Global step 1000 Train loss 0.15 QA-F1 0.18958333333333333 on epoch=499
03/10/2022 17:49:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.10 on epoch=504
03/10/2022 17:49:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.10 on epoch=509
03/10/2022 17:49:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=514
03/10/2022 17:49:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=519
03/10/2022 17:49:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=524
03/10/2022 17:49:55 - INFO - __main__ - Global step 1050 Train loss 0.11 QA-F1 0.17708333333333331 on epoch=524
03/10/2022 17:49:59 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=529
03/10/2022 17:50:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=534
03/10/2022 17:50:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=539
03/10/2022 17:50:12 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.13 on epoch=544
03/10/2022 17:50:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=549
03/10/2022 17:50:17 - INFO - __main__ - Global step 1100 Train loss 0.15 QA-F1 0.20208333333333334 on epoch=549
03/10/2022 17:50:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=554
03/10/2022 17:50:26 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=559
03/10/2022 17:50:30 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.11 on epoch=564
03/10/2022 17:50:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=569
03/10/2022 17:50:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=574
03/10/2022 17:50:40 - INFO - __main__ - Global step 1150 Train loss 0.12 QA-F1 0.29583333333333334 on epoch=574
03/10/2022 17:50:40 - INFO - __main__ - Saving model with best QA-F1: 0.23333333333333334 -> 0.29583333333333334 on epoch=574, global_step=1150
03/10/2022 17:50:44 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
03/10/2022 17:50:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=584
03/10/2022 17:50:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=589
03/10/2022 17:50:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.11 on epoch=594
03/10/2022 17:51:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=599
03/10/2022 17:51:03 - INFO - __main__ - Global step 1200 Train loss 0.09 QA-F1 0.29583333333333334 on epoch=599
03/10/2022 17:51:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.08 on epoch=604
03/10/2022 17:51:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=609
03/10/2022 17:51:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.05 on epoch=614
03/10/2022 17:51:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=619
03/10/2022 17:51:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
03/10/2022 17:51:25 - INFO - __main__ - Global step 1250 Train loss 0.07 QA-F1 0.26458333333333334 on epoch=624
03/10/2022 17:51:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=629
03/10/2022 17:51:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=634
03/10/2022 17:51:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=639
03/10/2022 17:51:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.08 on epoch=644
03/10/2022 17:51:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=649
03/10/2022 17:51:48 - INFO - __main__ - Global step 1300 Train loss 0.09 QA-F1 0.24999999999999997 on epoch=649
03/10/2022 17:51:52 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=654
03/10/2022 17:51:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=659
03/10/2022 17:52:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
03/10/2022 17:52:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=669
03/10/2022 17:52:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=674
03/10/2022 17:52:11 - INFO - __main__ - Global step 1350 Train loss 0.07 QA-F1 0.21875 on epoch=674
03/10/2022 17:52:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=679
03/10/2022 17:52:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
03/10/2022 17:52:24 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=689
03/10/2022 17:52:28 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
03/10/2022 17:52:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=699
03/10/2022 17:52:34 - INFO - __main__ - Global step 1400 Train loss 0.06 QA-F1 0.24375 on epoch=699
03/10/2022 17:52:38 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=704
03/10/2022 17:52:42 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
03/10/2022 17:52:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
03/10/2022 17:52:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.05 on epoch=719
03/10/2022 17:52:55 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=724
03/10/2022 17:52:56 - INFO - __main__ - Global step 1450 Train loss 0.06 QA-F1 0.2708333333333333 on epoch=724
03/10/2022 17:53:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
03/10/2022 17:53:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.07 on epoch=734
03/10/2022 17:53:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=739
03/10/2022 17:53:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=744
03/10/2022 17:53:18 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
03/10/2022 17:53:19 - INFO - __main__ - Global step 1500 Train loss 0.06 QA-F1 0.14583333333333331 on epoch=749
03/10/2022 17:53:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.10 on epoch=754
03/10/2022 17:53:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
03/10/2022 17:53:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
03/10/2022 17:53:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
03/10/2022 17:53:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=774
03/10/2022 17:53:42 - INFO - __main__ - Global step 1550 Train loss 0.06 QA-F1 0.29166666666666663 on epoch=774
03/10/2022 17:53:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=779
03/10/2022 17:53:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=784
03/10/2022 17:53:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=789
03/10/2022 17:53:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
03/10/2022 17:54:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
03/10/2022 17:54:04 - INFO - __main__ - Global step 1600 Train loss 0.05 QA-F1 0.20833333333333331 on epoch=799
03/10/2022 17:54:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=804
03/10/2022 17:54:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
03/10/2022 17:54:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=814
03/10/2022 17:54:21 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
03/10/2022 17:54:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.06 on epoch=824
03/10/2022 17:54:27 - INFO - __main__ - Global step 1650 Train loss 0.05 QA-F1 0.19791666666666666 on epoch=824
03/10/2022 17:54:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=829
03/10/2022 17:54:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=834
03/10/2022 17:54:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=839
03/10/2022 17:54:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
03/10/2022 17:54:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=849
03/10/2022 17:54:50 - INFO - __main__ - Global step 1700 Train loss 0.05 QA-F1 0.26041666666666663 on epoch=849
03/10/2022 17:54:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
03/10/2022 17:54:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
03/10/2022 17:55:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
03/10/2022 17:55:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
03/10/2022 17:55:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=874
03/10/2022 17:55:12 - INFO - __main__ - Global step 1750 Train loss 0.04 QA-F1 0.20208333333333334 on epoch=874
03/10/2022 17:55:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=879
03/10/2022 17:55:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
03/10/2022 17:55:25 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
03/10/2022 17:55:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
03/10/2022 17:55:34 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
03/10/2022 17:55:35 - INFO - __main__ - Global step 1800 Train loss 0.03 QA-F1 0.2708333333333333 on epoch=899
03/10/2022 17:55:39 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
03/10/2022 17:55:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=909
03/10/2022 17:55:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=914
03/10/2022 17:55:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
03/10/2022 17:55:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
03/10/2022 17:55:58 - INFO - __main__ - Global step 1850 Train loss 0.05 QA-F1 0.2708333333333333 on epoch=924
03/10/2022 17:56:02 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
03/10/2022 17:56:06 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
03/10/2022 17:56:11 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
03/10/2022 17:56:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
03/10/2022 17:56:19 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=949
03/10/2022 17:56:21 - INFO - __main__ - Global step 1900 Train loss 0.04 QA-F1 0.3020833333333333 on epoch=949
03/10/2022 17:56:21 - INFO - __main__ - Saving model with best QA-F1: 0.29583333333333334 -> 0.3020833333333333 on epoch=949, global_step=1900
03/10/2022 17:56:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
03/10/2022 17:56:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
03/10/2022 17:56:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
03/10/2022 17:56:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
03/10/2022 17:56:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
03/10/2022 17:56:43 - INFO - __main__ - Global step 1950 Train loss 0.03 QA-F1 0.3333333333333333 on epoch=974
03/10/2022 17:56:43 - INFO - __main__ - Saving model with best QA-F1: 0.3020833333333333 -> 0.3333333333333333 on epoch=974, global_step=1950
03/10/2022 17:56:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
03/10/2022 17:56:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.06 on epoch=984
03/10/2022 17:56:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
03/10/2022 17:57:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
03/10/2022 17:57:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
03/10/2022 17:57:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:57:06 - INFO - __main__ - Printing 3 examples
03/10/2022 17:57:06 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/10/2022 17:57:06 - INFO - __main__ - ['Allan']
03/10/2022 17:57:06 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/10/2022 17:57:06 - INFO - __main__ - ['Nigro']
03/10/2022 17:57:06 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/10/2022 17:57:06 - INFO - __main__ - ['Minogue']
03/10/2022 17:57:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 17:57:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:57:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:57:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:57:06 - INFO - __main__ - Printing 3 examples
03/10/2022 17:57:06 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/10/2022 17:57:06 - INFO - __main__ - ['Alan']
03/10/2022 17:57:06 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/10/2022 17:57:06 - INFO - __main__ - ['Santo Tomás de Castilla']
03/10/2022 17:57:06 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/10/2022 17:57:06 - INFO - __main__ - ['Albert']
03/10/2022 17:57:06 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:57:06 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:57:06 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:57:06 - INFO - __main__ - Global step 2000 Train loss 0.03 QA-F1 0.17083333333333334 on epoch=999
03/10/2022 17:57:06 - INFO - __main__ - save last model!
03/10/2022 17:57:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 17:57:07 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 17:57:07 - INFO - __main__ - Printing 3 examples
03/10/2022 17:57:07 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:57:07 - INFO - __main__ - ['Frankie']
03/10/2022 17:57:07 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:57:07 - INFO - __main__ - ['Frankie']
03/10/2022 17:57:07 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 17:57:07 - INFO - __main__ - ['Frankie']
03/10/2022 17:57:07 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:57:11 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:57:14 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 17:57:19 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 17:57:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 17:57:20 - INFO - __main__ - Starting training!
03/10/2022 17:59:06 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_21_0.5_8_predictions.txt
03/10/2022 17:59:06 - INFO - __main__ - QA-F1 on test data: 0.2523
03/10/2022 17:59:07 - INFO - __main__ - prefix=quoref_32_21, lr=0.5, bsz=8, dev_performance=0.3333333333333333, test_performance=0.2522668490410426
03/10/2022 17:59:07 - INFO - __main__ - Running ... prefix=quoref_32_21, lr=0.4, bsz=8 ...
03/10/2022 17:59:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:59:08 - INFO - __main__ - Printing 3 examples
03/10/2022 17:59:08 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/10/2022 17:59:08 - INFO - __main__ - ['Allan']
03/10/2022 17:59:08 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/10/2022 17:59:08 - INFO - __main__ - ['Nigro']
03/10/2022 17:59:08 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/10/2022 17:59:08 - INFO - __main__ - ['Minogue']
03/10/2022 17:59:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 17:59:08 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:59:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 17:59:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 17:59:08 - INFO - __main__ - Printing 3 examples
03/10/2022 17:59:08 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/10/2022 17:59:08 - INFO - __main__ - ['Alan']
03/10/2022 17:59:08 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/10/2022 17:59:08 - INFO - __main__ - ['Santo Tomás de Castilla']
03/10/2022 17:59:08 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/10/2022 17:59:08 - INFO - __main__ - ['Albert']
03/10/2022 17:59:08 - INFO - __main__ - Tokenizing Input ...
03/10/2022 17:59:08 - INFO - __main__ - Tokenizing Output ...
03/10/2022 17:59:08 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 17:59:21 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 17:59:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 17:59:22 - INFO - __main__ - Starting training!
03/10/2022 17:59:27 - INFO - __main__ - Step 10 Global step 10 Train loss 2.68 on epoch=4
03/10/2022 17:59:31 - INFO - __main__ - Step 20 Global step 20 Train loss 2.77 on epoch=9
03/10/2022 17:59:35 - INFO - __main__ - Step 30 Global step 30 Train loss 2.49 on epoch=14
03/10/2022 17:59:40 - INFO - __main__ - Step 40 Global step 40 Train loss 2.32 on epoch=19
03/10/2022 17:59:44 - INFO - __main__ - Step 50 Global step 50 Train loss 2.30 on epoch=24
03/10/2022 17:59:45 - INFO - __main__ - Global step 50 Train loss 2.51 QA-F1 0.05208333333333333 on epoch=24
03/10/2022 17:59:45 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.05208333333333333 on epoch=24, global_step=50
03/10/2022 17:59:49 - INFO - __main__ - Step 60 Global step 60 Train loss 2.14 on epoch=29
03/10/2022 17:59:53 - INFO - __main__ - Step 70 Global step 70 Train loss 1.99 on epoch=34
03/10/2022 17:59:58 - INFO - __main__ - Step 80 Global step 80 Train loss 1.93 on epoch=39
03/10/2022 18:00:02 - INFO - __main__ - Step 90 Global step 90 Train loss 1.86 on epoch=44
03/10/2022 18:00:06 - INFO - __main__ - Step 100 Global step 100 Train loss 1.87 on epoch=49
03/10/2022 18:00:08 - INFO - __main__ - Global step 100 Train loss 1.96 QA-F1 0.15625 on epoch=49
03/10/2022 18:00:08 - INFO - __main__ - Saving model with best QA-F1: 0.05208333333333333 -> 0.15625 on epoch=49, global_step=100
03/10/2022 18:00:12 - INFO - __main__ - Step 110 Global step 110 Train loss 1.65 on epoch=54
03/10/2022 18:00:16 - INFO - __main__ - Step 120 Global step 120 Train loss 1.88 on epoch=59
03/10/2022 18:00:20 - INFO - __main__ - Step 130 Global step 130 Train loss 2.79 on epoch=64
03/10/2022 18:00:25 - INFO - __main__ - Step 140 Global step 140 Train loss 4.38 on epoch=69
03/10/2022 18:00:29 - INFO - __main__ - Step 150 Global step 150 Train loss 4.28 on epoch=74
03/10/2022 18:00:31 - INFO - __main__ - Global step 150 Train loss 3.00 QA-F1 0.16666666666666666 on epoch=74
03/10/2022 18:00:31 - INFO - __main__ - Saving model with best QA-F1: 0.15625 -> 0.16666666666666666 on epoch=74, global_step=150
03/10/2022 18:00:35 - INFO - __main__ - Step 160 Global step 160 Train loss 4.44 on epoch=79
03/10/2022 18:00:39 - INFO - __main__ - Step 170 Global step 170 Train loss 4.27 on epoch=84
03/10/2022 18:00:44 - INFO - __main__ - Step 180 Global step 180 Train loss 3.52 on epoch=89
03/10/2022 18:00:48 - INFO - __main__ - Step 190 Global step 190 Train loss 3.12 on epoch=94
03/10/2022 18:00:52 - INFO - __main__ - Step 200 Global step 200 Train loss 2.46 on epoch=99
03/10/2022 18:00:53 - INFO - __main__ - Global step 200 Train loss 3.56 QA-F1 0.14791666666666667 on epoch=99
03/10/2022 18:00:58 - INFO - __main__ - Step 210 Global step 210 Train loss 2.10 on epoch=104
03/10/2022 18:01:02 - INFO - __main__ - Step 220 Global step 220 Train loss 2.09 on epoch=109
03/10/2022 18:01:06 - INFO - __main__ - Step 230 Global step 230 Train loss 2.08 on epoch=114
03/10/2022 18:01:10 - INFO - __main__ - Step 240 Global step 240 Train loss 2.19 on epoch=119
03/10/2022 18:01:15 - INFO - __main__ - Step 250 Global step 250 Train loss 1.96 on epoch=124
03/10/2022 18:01:16 - INFO - __main__ - Global step 250 Train loss 2.08 QA-F1 0.15625 on epoch=124
03/10/2022 18:01:20 - INFO - __main__ - Step 260 Global step 260 Train loss 2.00 on epoch=129
03/10/2022 18:01:25 - INFO - __main__ - Step 270 Global step 270 Train loss 2.01 on epoch=134
03/10/2022 18:01:29 - INFO - __main__ - Step 280 Global step 280 Train loss 2.08 on epoch=139
03/10/2022 18:01:33 - INFO - __main__ - Step 290 Global step 290 Train loss 1.99 on epoch=144
03/10/2022 18:01:37 - INFO - __main__ - Step 300 Global step 300 Train loss 1.93 on epoch=149
03/10/2022 18:01:39 - INFO - __main__ - Global step 300 Train loss 2.00 QA-F1 0.15625 on epoch=149
03/10/2022 18:01:43 - INFO - __main__ - Step 310 Global step 310 Train loss 1.88 on epoch=154
03/10/2022 18:01:47 - INFO - __main__ - Step 320 Global step 320 Train loss 1.88 on epoch=159
03/10/2022 18:01:52 - INFO - __main__ - Step 330 Global step 330 Train loss 1.86 on epoch=164
03/10/2022 18:01:56 - INFO - __main__ - Step 340 Global step 340 Train loss 1.82 on epoch=169
03/10/2022 18:02:00 - INFO - __main__ - Step 350 Global step 350 Train loss 1.73 on epoch=174
03/10/2022 18:02:02 - INFO - __main__ - Global step 350 Train loss 1.83 QA-F1 0.17708333333333331 on epoch=174
03/10/2022 18:02:02 - INFO - __main__ - Saving model with best QA-F1: 0.16666666666666666 -> 0.17708333333333331 on epoch=174, global_step=350
03/10/2022 18:02:06 - INFO - __main__ - Step 360 Global step 360 Train loss 1.78 on epoch=179
03/10/2022 18:02:11 - INFO - __main__ - Step 370 Global step 370 Train loss 1.76 on epoch=184
03/10/2022 18:02:15 - INFO - __main__ - Step 380 Global step 380 Train loss 1.73 on epoch=189
03/10/2022 18:02:19 - INFO - __main__ - Step 390 Global step 390 Train loss 1.63 on epoch=194
03/10/2022 18:02:24 - INFO - __main__ - Step 400 Global step 400 Train loss 1.60 on epoch=199
03/10/2022 18:02:25 - INFO - __main__ - Global step 400 Train loss 1.70 QA-F1 0.20833333333333331 on epoch=199
03/10/2022 18:02:25 - INFO - __main__ - Saving model with best QA-F1: 0.17708333333333331 -> 0.20833333333333331 on epoch=199, global_step=400
03/10/2022 18:02:30 - INFO - __main__ - Step 410 Global step 410 Train loss 1.69 on epoch=204
03/10/2022 18:02:34 - INFO - __main__ - Step 420 Global step 420 Train loss 1.64 on epoch=209
03/10/2022 18:02:38 - INFO - __main__ - Step 430 Global step 430 Train loss 1.61 on epoch=214
03/10/2022 18:02:43 - INFO - __main__ - Step 440 Global step 440 Train loss 1.67 on epoch=219
03/10/2022 18:02:47 - INFO - __main__ - Step 450 Global step 450 Train loss 1.63 on epoch=224
03/10/2022 18:02:48 - INFO - __main__ - Global step 450 Train loss 1.65 QA-F1 0.20833333333333331 on epoch=224
03/10/2022 18:02:53 - INFO - __main__ - Step 460 Global step 460 Train loss 1.67 on epoch=229
03/10/2022 18:02:57 - INFO - __main__ - Step 470 Global step 470 Train loss 1.60 on epoch=234
03/10/2022 18:03:02 - INFO - __main__ - Step 480 Global step 480 Train loss 1.59 on epoch=239
03/10/2022 18:03:06 - INFO - __main__ - Step 490 Global step 490 Train loss 1.59 on epoch=244
03/10/2022 18:03:10 - INFO - __main__ - Step 500 Global step 500 Train loss 1.70 on epoch=249
03/10/2022 18:03:12 - INFO - __main__ - Global step 500 Train loss 1.63 QA-F1 0.20833333333333331 on epoch=249
03/10/2022 18:03:16 - INFO - __main__ - Step 510 Global step 510 Train loss 1.62 on epoch=254
03/10/2022 18:03:20 - INFO - __main__ - Step 520 Global step 520 Train loss 1.66 on epoch=259
03/10/2022 18:03:25 - INFO - __main__ - Step 530 Global step 530 Train loss 1.61 on epoch=264
03/10/2022 18:03:29 - INFO - __main__ - Step 540 Global step 540 Train loss 1.68 on epoch=269
03/10/2022 18:03:34 - INFO - __main__ - Step 550 Global step 550 Train loss 1.58 on epoch=274
03/10/2022 18:03:35 - INFO - __main__ - Global step 550 Train loss 1.63 QA-F1 0.17708333333333331 on epoch=274
03/10/2022 18:03:39 - INFO - __main__ - Step 560 Global step 560 Train loss 1.59 on epoch=279
03/10/2022 18:03:44 - INFO - __main__ - Step 570 Global step 570 Train loss 1.59 on epoch=284
03/10/2022 18:03:48 - INFO - __main__ - Step 580 Global step 580 Train loss 1.75 on epoch=289
03/10/2022 18:03:53 - INFO - __main__ - Step 590 Global step 590 Train loss 1.51 on epoch=294
03/10/2022 18:03:57 - INFO - __main__ - Step 600 Global step 600 Train loss 1.64 on epoch=299
03/10/2022 18:03:58 - INFO - __main__ - Global step 600 Train loss 1.62 QA-F1 0.26041666666666663 on epoch=299
03/10/2022 18:03:58 - INFO - __main__ - Saving model with best QA-F1: 0.20833333333333331 -> 0.26041666666666663 on epoch=299, global_step=600
03/10/2022 18:04:03 - INFO - __main__ - Step 610 Global step 610 Train loss 1.65 on epoch=304
03/10/2022 18:04:07 - INFO - __main__ - Step 620 Global step 620 Train loss 1.55 on epoch=309
03/10/2022 18:04:12 - INFO - __main__ - Step 630 Global step 630 Train loss 1.57 on epoch=314
03/10/2022 18:04:16 - INFO - __main__ - Step 640 Global step 640 Train loss 1.53 on epoch=319
03/10/2022 18:04:20 - INFO - __main__ - Step 650 Global step 650 Train loss 1.55 on epoch=324
03/10/2022 18:04:22 - INFO - __main__ - Global step 650 Train loss 1.57 QA-F1 0.20833333333333331 on epoch=324
03/10/2022 18:04:26 - INFO - __main__ - Step 660 Global step 660 Train loss 1.44 on epoch=329
03/10/2022 18:04:31 - INFO - __main__ - Step 670 Global step 670 Train loss 1.42 on epoch=334
03/10/2022 18:04:35 - INFO - __main__ - Step 680 Global step 680 Train loss 1.45 on epoch=339
03/10/2022 18:04:39 - INFO - __main__ - Step 690 Global step 690 Train loss 1.46 on epoch=344
03/10/2022 18:04:44 - INFO - __main__ - Step 700 Global step 700 Train loss 1.48 on epoch=349
03/10/2022 18:04:45 - INFO - __main__ - Global step 700 Train loss 1.45 QA-F1 0.22916666666666663 on epoch=349
03/10/2022 18:04:50 - INFO - __main__ - Step 710 Global step 710 Train loss 1.39 on epoch=354
03/10/2022 18:04:54 - INFO - __main__ - Step 720 Global step 720 Train loss 1.43 on epoch=359
03/10/2022 18:04:59 - INFO - __main__ - Step 730 Global step 730 Train loss 1.41 on epoch=364
03/10/2022 18:05:03 - INFO - __main__ - Step 740 Global step 740 Train loss 1.43 on epoch=369
03/10/2022 18:05:07 - INFO - __main__ - Step 750 Global step 750 Train loss 1.39 on epoch=374
03/10/2022 18:05:09 - INFO - __main__ - Global step 750 Train loss 1.41 QA-F1 0.23958333333333331 on epoch=374
03/10/2022 18:05:13 - INFO - __main__ - Step 760 Global step 760 Train loss 1.37 on epoch=379
03/10/2022 18:05:18 - INFO - __main__ - Step 770 Global step 770 Train loss 1.42 on epoch=384
03/10/2022 18:05:22 - INFO - __main__ - Step 780 Global step 780 Train loss 1.32 on epoch=389
03/10/2022 18:05:26 - INFO - __main__ - Step 790 Global step 790 Train loss 1.31 on epoch=394
03/10/2022 18:05:31 - INFO - __main__ - Step 800 Global step 800 Train loss 1.40 on epoch=399
03/10/2022 18:05:32 - INFO - __main__ - Global step 800 Train loss 1.36 QA-F1 0.22916666666666663 on epoch=399
03/10/2022 18:05:37 - INFO - __main__ - Step 810 Global step 810 Train loss 1.37 on epoch=404
03/10/2022 18:05:41 - INFO - __main__ - Step 820 Global step 820 Train loss 1.36 on epoch=409
03/10/2022 18:05:45 - INFO - __main__ - Step 830 Global step 830 Train loss 1.37 on epoch=414
03/10/2022 18:05:50 - INFO - __main__ - Step 840 Global step 840 Train loss 1.35 on epoch=419
03/10/2022 18:05:54 - INFO - __main__ - Step 850 Global step 850 Train loss 1.29 on epoch=424
03/10/2022 18:05:55 - INFO - __main__ - Global step 850 Train loss 1.34 QA-F1 0.23958333333333331 on epoch=424
03/10/2022 18:06:00 - INFO - __main__ - Step 860 Global step 860 Train loss 1.34 on epoch=429
03/10/2022 18:06:04 - INFO - __main__ - Step 870 Global step 870 Train loss 1.33 on epoch=434
03/10/2022 18:06:09 - INFO - __main__ - Step 880 Global step 880 Train loss 1.33 on epoch=439
03/10/2022 18:06:13 - INFO - __main__ - Step 890 Global step 890 Train loss 1.30 on epoch=444
03/10/2022 18:06:17 - INFO - __main__ - Step 900 Global step 900 Train loss 1.27 on epoch=449
03/10/2022 18:06:19 - INFO - __main__ - Global step 900 Train loss 1.31 QA-F1 0.12499999999999999 on epoch=449
03/10/2022 18:06:23 - INFO - __main__ - Step 910 Global step 910 Train loss 1.26 on epoch=454
03/10/2022 18:06:28 - INFO - __main__ - Step 920 Global step 920 Train loss 1.25 on epoch=459
03/10/2022 18:06:32 - INFO - __main__ - Step 930 Global step 930 Train loss 1.25 on epoch=464
03/10/2022 18:06:36 - INFO - __main__ - Step 940 Global step 940 Train loss 1.26 on epoch=469
03/10/2022 18:06:41 - INFO - __main__ - Step 950 Global step 950 Train loss 1.23 on epoch=474
03/10/2022 18:06:42 - INFO - __main__ - Global step 950 Train loss 1.25 QA-F1 0.15625 on epoch=474
03/10/2022 18:06:47 - INFO - __main__ - Step 960 Global step 960 Train loss 1.26 on epoch=479
03/10/2022 18:06:51 - INFO - __main__ - Step 970 Global step 970 Train loss 1.13 on epoch=484
03/10/2022 18:06:55 - INFO - __main__ - Step 980 Global step 980 Train loss 1.19 on epoch=489
03/10/2022 18:07:00 - INFO - __main__ - Step 990 Global step 990 Train loss 1.25 on epoch=494
03/10/2022 18:07:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.17 on epoch=499
03/10/2022 18:07:06 - INFO - __main__ - Global step 1000 Train loss 1.20 QA-F1 0.10416666666666666 on epoch=499
03/10/2022 18:07:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.09 on epoch=504
03/10/2022 18:07:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.27 on epoch=509
03/10/2022 18:07:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.17 on epoch=514
03/10/2022 18:07:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.17 on epoch=519
03/10/2022 18:07:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.10 on epoch=524
03/10/2022 18:07:29 - INFO - __main__ - Global step 1050 Train loss 1.16 QA-F1 0.10416666666666666 on epoch=524
03/10/2022 18:07:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.14 on epoch=529
03/10/2022 18:07:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.08 on epoch=534
03/10/2022 18:07:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.18 on epoch=539
03/10/2022 18:07:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.15 on epoch=544
03/10/2022 18:07:51 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.16 on epoch=549
03/10/2022 18:07:52 - INFO - __main__ - Global step 1100 Train loss 1.14 QA-F1 0.11458333333333333 on epoch=549
03/10/2022 18:07:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.17 on epoch=554
03/10/2022 18:08:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.11 on epoch=559
03/10/2022 18:08:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.18 on epoch=564
03/10/2022 18:08:09 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.06 on epoch=569
03/10/2022 18:08:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.08 on epoch=574
03/10/2022 18:08:15 - INFO - __main__ - Global step 1150 Train loss 1.12 QA-F1 0.17708333333333331 on epoch=574
03/10/2022 18:08:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.10 on epoch=579
03/10/2022 18:08:24 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.01 on epoch=584
03/10/2022 18:08:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.15 on epoch=589
03/10/2022 18:08:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.04 on epoch=594
03/10/2022 18:08:37 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.02 on epoch=599
03/10/2022 18:08:39 - INFO - __main__ - Global step 1200 Train loss 1.07 QA-F1 0.13541666666666666 on epoch=599
03/10/2022 18:08:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.06 on epoch=604
03/10/2022 18:08:47 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.04 on epoch=609
03/10/2022 18:08:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.08 on epoch=614
03/10/2022 18:08:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.04 on epoch=619
03/10/2022 18:09:00 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.01 on epoch=624
03/10/2022 18:09:02 - INFO - __main__ - Global step 1250 Train loss 1.05 QA-F1 0.16666666666666666 on epoch=624
03/10/2022 18:09:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.97 on epoch=629
03/10/2022 18:09:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.09 on epoch=634
03/10/2022 18:09:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.95 on epoch=639
03/10/2022 18:09:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.07 on epoch=644
03/10/2022 18:09:23 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.95 on epoch=649
03/10/2022 18:09:25 - INFO - __main__ - Global step 1300 Train loss 1.01 QA-F1 0.15625 on epoch=649
03/10/2022 18:09:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.00 on epoch=654
03/10/2022 18:09:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.95 on epoch=659
03/10/2022 18:09:38 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.98 on epoch=664
03/10/2022 18:09:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.91 on epoch=669
03/10/2022 18:09:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.95 on epoch=674
03/10/2022 18:09:48 - INFO - __main__ - Global step 1350 Train loss 0.96 QA-F1 0.20833333333333331 on epoch=674
03/10/2022 18:09:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.97 on epoch=679
03/10/2022 18:09:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.89 on epoch=684
03/10/2022 18:10:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.94 on epoch=689
03/10/2022 18:10:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.93 on epoch=694
03/10/2022 18:10:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.92 on epoch=699
03/10/2022 18:10:11 - INFO - __main__ - Global step 1400 Train loss 0.93 QA-F1 0.1875 on epoch=699
03/10/2022 18:10:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.93 on epoch=704
03/10/2022 18:10:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.98 on epoch=709
03/10/2022 18:10:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.93 on epoch=714
03/10/2022 18:10:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.92 on epoch=719
03/10/2022 18:10:33 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.86 on epoch=724
03/10/2022 18:10:35 - INFO - __main__ - Global step 1450 Train loss 0.93 QA-F1 0.1875 on epoch=724
03/10/2022 18:10:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.88 on epoch=729
03/10/2022 18:10:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.92 on epoch=734
03/10/2022 18:10:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.82 on epoch=739
03/10/2022 18:10:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.86 on epoch=744
03/10/2022 18:10:56 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.86 on epoch=749
03/10/2022 18:10:58 - INFO - __main__ - Global step 1500 Train loss 0.87 QA-F1 0.23958333333333331 on epoch=749
03/10/2022 18:11:02 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.90 on epoch=754
03/10/2022 18:11:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.87 on epoch=759
03/10/2022 18:11:11 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.80 on epoch=764
03/10/2022 18:11:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.78 on epoch=769
03/10/2022 18:11:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.73 on epoch=774
03/10/2022 18:11:21 - INFO - __main__ - Global step 1550 Train loss 0.81 QA-F1 0.23958333333333331 on epoch=774
03/10/2022 18:11:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.72 on epoch=779
03/10/2022 18:11:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.71 on epoch=784
03/10/2022 18:11:34 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.74 on epoch=789
03/10/2022 18:11:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.69 on epoch=794
03/10/2022 18:11:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.73 on epoch=799
03/10/2022 18:11:44 - INFO - __main__ - Global step 1600 Train loss 0.72 QA-F1 0.3333333333333333 on epoch=799
03/10/2022 18:11:44 - INFO - __main__ - Saving model with best QA-F1: 0.26041666666666663 -> 0.3333333333333333 on epoch=799, global_step=1600
03/10/2022 18:11:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.66 on epoch=804
03/10/2022 18:11:53 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.66 on epoch=809
03/10/2022 18:11:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.63 on epoch=814
03/10/2022 18:12:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.71 on epoch=819
03/10/2022 18:12:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.67 on epoch=824
03/10/2022 18:12:07 - INFO - __main__ - Global step 1650 Train loss 0.66 QA-F1 0.3145833333333333 on epoch=824
03/10/2022 18:12:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.72 on epoch=829
03/10/2022 18:12:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.70 on epoch=834
03/10/2022 18:12:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.62 on epoch=839
03/10/2022 18:12:25 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.63 on epoch=844
03/10/2022 18:12:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.70 on epoch=849
03/10/2022 18:12:31 - INFO - __main__ - Global step 1700 Train loss 0.68 QA-F1 0.28125 on epoch=849
03/10/2022 18:12:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.64 on epoch=854
03/10/2022 18:12:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.62 on epoch=859
03/10/2022 18:12:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.59 on epoch=864
03/10/2022 18:12:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.62 on epoch=869
03/10/2022 18:12:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.61 on epoch=874
03/10/2022 18:12:54 - INFO - __main__ - Global step 1750 Train loss 0.62 QA-F1 0.28125 on epoch=874
03/10/2022 18:12:58 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.56 on epoch=879
03/10/2022 18:13:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.62 on epoch=884
03/10/2022 18:13:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.58 on epoch=889
03/10/2022 18:13:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.60 on epoch=894
03/10/2022 18:13:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.58 on epoch=899
03/10/2022 18:13:17 - INFO - __main__ - Global step 1800 Train loss 0.59 QA-F1 0.22916666666666666 on epoch=899
03/10/2022 18:13:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.63 on epoch=904
03/10/2022 18:13:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.55 on epoch=909
03/10/2022 18:13:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.56 on epoch=914
03/10/2022 18:13:34 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.53 on epoch=919
03/10/2022 18:13:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.55 on epoch=924
03/10/2022 18:13:40 - INFO - __main__ - Global step 1850 Train loss 0.56 QA-F1 0.22916666666666663 on epoch=924
03/10/2022 18:13:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.53 on epoch=929
03/10/2022 18:13:49 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.52 on epoch=934
03/10/2022 18:13:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.52 on epoch=939
03/10/2022 18:13:57 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.52 on epoch=944
03/10/2022 18:14:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.49 on epoch=949
03/10/2022 18:14:03 - INFO - __main__ - Global step 1900 Train loss 0.52 QA-F1 0.26041666666666663 on epoch=949
03/10/2022 18:14:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.49 on epoch=954
03/10/2022 18:14:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.53 on epoch=959
03/10/2022 18:14:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.56 on epoch=964
03/10/2022 18:14:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.53 on epoch=969
03/10/2022 18:14:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.50 on epoch=974
03/10/2022 18:14:26 - INFO - __main__ - Global step 1950 Train loss 0.52 QA-F1 0.3125 on epoch=974
03/10/2022 18:14:31 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.49 on epoch=979
03/10/2022 18:14:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.52 on epoch=984
03/10/2022 18:14:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.55 on epoch=989
03/10/2022 18:14:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=994
03/10/2022 18:14:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=999
03/10/2022 18:14:49 - INFO - __main__ - Global step 2000 Train loss 0.49 QA-F1 0.18749999999999997 on epoch=999
03/10/2022 18:14:49 - INFO - __main__ - save last model!
03/10/2022 18:14:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 18:14:49 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 18:14:49 - INFO - __main__ - Printing 3 examples
03/10/2022 18:14:49 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 18:14:49 - INFO - __main__ - ['Frankie']
03/10/2022 18:14:49 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 18:14:49 - INFO - __main__ - ['Frankie']
03/10/2022 18:14:49 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 18:14:49 - INFO - __main__ - ['Frankie']
03/10/2022 18:14:49 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:14:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:14:50 - INFO - __main__ - Printing 3 examples
03/10/2022 18:14:50 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/10/2022 18:14:50 - INFO - __main__ - ['Allan']
03/10/2022 18:14:50 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/10/2022 18:14:50 - INFO - __main__ - ['Nigro']
03/10/2022 18:14:50 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/10/2022 18:14:50 - INFO - __main__ - ['Minogue']
03/10/2022 18:14:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:14:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:14:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:14:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:14:50 - INFO - __main__ - Printing 3 examples
03/10/2022 18:14:50 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/10/2022 18:14:50 - INFO - __main__ - ['Alan']
03/10/2022 18:14:50 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/10/2022 18:14:50 - INFO - __main__ - ['Santo Tomás de Castilla']
03/10/2022 18:14:50 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/10/2022 18:14:50 - INFO - __main__ - ['Albert']
03/10/2022 18:14:50 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:14:50 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:14:50 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:14:54 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:14:56 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 18:15:03 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 18:15:04 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 18:15:04 - INFO - __main__ - Starting training!
03/10/2022 18:16:43 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_21_0.4_8_predictions.txt
03/10/2022 18:16:43 - INFO - __main__ - QA-F1 on test data: 0.2041
03/10/2022 18:16:43 - INFO - __main__ - prefix=quoref_32_21, lr=0.4, bsz=8, dev_performance=0.3333333333333333, test_performance=0.20406179841663713
03/10/2022 18:16:43 - INFO - __main__ - Running ... prefix=quoref_32_21, lr=0.3, bsz=8 ...
03/10/2022 18:16:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:16:44 - INFO - __main__ - Printing 3 examples
03/10/2022 18:16:44 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/10/2022 18:16:44 - INFO - __main__ - ['Allan']
03/10/2022 18:16:44 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/10/2022 18:16:44 - INFO - __main__ - ['Nigro']
03/10/2022 18:16:44 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/10/2022 18:16:44 - INFO - __main__ - ['Minogue']
03/10/2022 18:16:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:16:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:16:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:16:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:16:44 - INFO - __main__ - Printing 3 examples
03/10/2022 18:16:44 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/10/2022 18:16:44 - INFO - __main__ - ['Alan']
03/10/2022 18:16:44 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/10/2022 18:16:44 - INFO - __main__ - ['Santo Tomás de Castilla']
03/10/2022 18:16:44 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/10/2022 18:16:44 - INFO - __main__ - ['Albert']
03/10/2022 18:16:44 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:16:44 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:16:45 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:16:58 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 18:16:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 18:16:58 - INFO - __main__ - Starting training!
03/10/2022 18:17:03 - INFO - __main__ - Step 10 Global step 10 Train loss 2.46 on epoch=4
03/10/2022 18:17:08 - INFO - __main__ - Step 20 Global step 20 Train loss 1.96 on epoch=9
03/10/2022 18:17:12 - INFO - __main__ - Step 30 Global step 30 Train loss 1.76 on epoch=14
03/10/2022 18:17:16 - INFO - __main__ - Step 40 Global step 40 Train loss 1.61 on epoch=19
03/10/2022 18:17:21 - INFO - __main__ - Step 50 Global step 50 Train loss 1.48 on epoch=24
03/10/2022 18:17:22 - INFO - __main__ - Global step 50 Train loss 1.85 QA-F1 0.2708333333333333 on epoch=24
03/10/2022 18:17:22 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.2708333333333333 on epoch=24, global_step=50
03/10/2022 18:17:27 - INFO - __main__ - Step 60 Global step 60 Train loss 1.43 on epoch=29
03/10/2022 18:17:31 - INFO - __main__ - Step 70 Global step 70 Train loss 1.35 on epoch=34
03/10/2022 18:17:35 - INFO - __main__ - Step 80 Global step 80 Train loss 1.27 on epoch=39
03/10/2022 18:17:40 - INFO - __main__ - Step 90 Global step 90 Train loss 1.33 on epoch=44
03/10/2022 18:17:44 - INFO - __main__ - Step 100 Global step 100 Train loss 1.28 on epoch=49
03/10/2022 18:17:46 - INFO - __main__ - Global step 100 Train loss 1.33 QA-F1 0.22569444444444442 on epoch=49
03/10/2022 18:17:50 - INFO - __main__ - Step 110 Global step 110 Train loss 1.16 on epoch=54
03/10/2022 18:17:55 - INFO - __main__ - Step 120 Global step 120 Train loss 1.18 on epoch=59
03/10/2022 18:17:59 - INFO - __main__ - Step 130 Global step 130 Train loss 1.11 on epoch=64
03/10/2022 18:18:03 - INFO - __main__ - Step 140 Global step 140 Train loss 1.15 on epoch=69
03/10/2022 18:18:08 - INFO - __main__ - Step 150 Global step 150 Train loss 1.04 on epoch=74
03/10/2022 18:18:09 - INFO - __main__ - Global step 150 Train loss 1.13 QA-F1 0.1875 on epoch=74
03/10/2022 18:18:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.96 on epoch=79
03/10/2022 18:18:18 - INFO - __main__ - Step 170 Global step 170 Train loss 0.95 on epoch=84
03/10/2022 18:18:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.98 on epoch=89
03/10/2022 18:18:26 - INFO - __main__ - Step 190 Global step 190 Train loss 0.95 on epoch=94
03/10/2022 18:18:31 - INFO - __main__ - Step 200 Global step 200 Train loss 0.89 on epoch=99
03/10/2022 18:18:32 - INFO - __main__ - Global step 200 Train loss 0.94 QA-F1 0.20833333333333331 on epoch=99
03/10/2022 18:18:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.90 on epoch=104
03/10/2022 18:18:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.88 on epoch=109
03/10/2022 18:18:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.78 on epoch=114
03/10/2022 18:18:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.82 on epoch=119
03/10/2022 18:18:54 - INFO - __main__ - Step 250 Global step 250 Train loss 0.79 on epoch=124
03/10/2022 18:18:56 - INFO - __main__ - Global step 250 Train loss 0.83 QA-F1 0.1875 on epoch=124
03/10/2022 18:19:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.77 on epoch=129
03/10/2022 18:19:05 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=134
03/10/2022 18:19:09 - INFO - __main__ - Step 280 Global step 280 Train loss 0.71 on epoch=139
03/10/2022 18:19:13 - INFO - __main__ - Step 290 Global step 290 Train loss 0.65 on epoch=144
03/10/2022 18:19:18 - INFO - __main__ - Step 300 Global step 300 Train loss 0.66 on epoch=149
03/10/2022 18:19:19 - INFO - __main__ - Global step 300 Train loss 0.69 QA-F1 0.22916666666666663 on epoch=149
03/10/2022 18:19:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.72 on epoch=154
03/10/2022 18:19:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=159
03/10/2022 18:19:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=164
03/10/2022 18:19:37 - INFO - __main__ - Step 340 Global step 340 Train loss 0.64 on epoch=169
03/10/2022 18:19:41 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=174
03/10/2022 18:19:43 - INFO - __main__ - Global step 350 Train loss 0.61 QA-F1 0.20833333333333331 on epoch=174
03/10/2022 18:19:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.63 on epoch=179
03/10/2022 18:19:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.56 on epoch=184
03/10/2022 18:19:56 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=189
03/10/2022 18:20:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.49 on epoch=194
03/10/2022 18:20:05 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=199
03/10/2022 18:20:06 - INFO - __main__ - Global step 400 Train loss 0.50 QA-F1 0.20833333333333331 on epoch=199
03/10/2022 18:20:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=204
03/10/2022 18:20:15 - INFO - __main__ - Step 420 Global step 420 Train loss 0.41 on epoch=209
03/10/2022 18:20:19 - INFO - __main__ - Step 430 Global step 430 Train loss 0.40 on epoch=214
03/10/2022 18:20:24 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
03/10/2022 18:20:28 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=224
03/10/2022 18:20:30 - INFO - __main__ - Global step 450 Train loss 0.39 QA-F1 0.22916666666666663 on epoch=224
03/10/2022 18:20:34 - INFO - __main__ - Step 460 Global step 460 Train loss 0.30 on epoch=229
03/10/2022 18:20:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.35 on epoch=234
03/10/2022 18:20:43 - INFO - __main__ - Step 480 Global step 480 Train loss 0.33 on epoch=239
03/10/2022 18:20:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=244
03/10/2022 18:20:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.28 on epoch=249
03/10/2022 18:20:53 - INFO - __main__ - Global step 500 Train loss 0.31 QA-F1 0.17708333333333331 on epoch=249
03/10/2022 18:20:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=254
03/10/2022 18:21:01 - INFO - __main__ - Step 520 Global step 520 Train loss 0.30 on epoch=259
03/10/2022 18:21:06 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=264
03/10/2022 18:21:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=269
03/10/2022 18:21:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.31 on epoch=274
03/10/2022 18:21:16 - INFO - __main__ - Global step 550 Train loss 0.29 QA-F1 0.16666666666666666 on epoch=274
03/10/2022 18:21:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=279
03/10/2022 18:21:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
03/10/2022 18:21:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=289
03/10/2022 18:21:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=294
03/10/2022 18:21:38 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=299
03/10/2022 18:21:39 - INFO - __main__ - Global step 600 Train loss 0.24 QA-F1 0.19791666666666666 on epoch=299
03/10/2022 18:21:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=304
03/10/2022 18:21:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=309
03/10/2022 18:21:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=314
03/10/2022 18:21:57 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=319
03/10/2022 18:22:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
03/10/2022 18:22:02 - INFO - __main__ - Global step 650 Train loss 0.19 QA-F1 0.23958333333333331 on epoch=324
03/10/2022 18:22:07 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=329
03/10/2022 18:22:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=334
03/10/2022 18:22:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=339
03/10/2022 18:22:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=344
03/10/2022 18:22:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=349
03/10/2022 18:22:26 - INFO - __main__ - Global step 700 Train loss 0.18 QA-F1 0.22916666666666663 on epoch=349
03/10/2022 18:22:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=354
03/10/2022 18:22:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=359
03/10/2022 18:22:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.16 on epoch=364
03/10/2022 18:22:43 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=369
03/10/2022 18:22:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.12 on epoch=374
03/10/2022 18:22:49 - INFO - __main__ - Global step 750 Train loss 0.14 QA-F1 0.20833333333333331 on epoch=374
03/10/2022 18:22:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.14 on epoch=379
03/10/2022 18:22:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.13 on epoch=384
03/10/2022 18:23:02 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=389
03/10/2022 18:23:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=394
03/10/2022 18:23:11 - INFO - __main__ - Step 800 Global step 800 Train loss 0.09 on epoch=399
03/10/2022 18:23:12 - INFO - __main__ - Global step 800 Train loss 0.14 QA-F1 0.16666666666666663 on epoch=399
03/10/2022 18:23:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=404
03/10/2022 18:23:21 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=409
03/10/2022 18:23:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=414
03/10/2022 18:23:30 - INFO - __main__ - Step 840 Global step 840 Train loss 0.11 on epoch=419
03/10/2022 18:23:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=424
03/10/2022 18:23:35 - INFO - __main__ - Global step 850 Train loss 0.14 QA-F1 0.12499999999999999 on epoch=424
03/10/2022 18:23:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.11 on epoch=429
03/10/2022 18:23:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=434
03/10/2022 18:23:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=439
03/10/2022 18:23:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=444
03/10/2022 18:23:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.09 on epoch=449
03/10/2022 18:23:59 - INFO - __main__ - Global step 900 Train loss 0.12 QA-F1 0.11458333333333333 on epoch=449
03/10/2022 18:24:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=454
03/10/2022 18:24:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=459
03/10/2022 18:24:12 - INFO - __main__ - Step 930 Global step 930 Train loss 0.11 on epoch=464
03/10/2022 18:24:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=469
03/10/2022 18:24:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=474
03/10/2022 18:24:22 - INFO - __main__ - Global step 950 Train loss 0.10 QA-F1 0.15625 on epoch=474
03/10/2022 18:24:26 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=479
03/10/2022 18:24:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=484
03/10/2022 18:24:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
03/10/2022 18:24:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=494
03/10/2022 18:24:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=499
03/10/2022 18:24:45 - INFO - __main__ - Global step 1000 Train loss 0.09 QA-F1 0.17708333333333331 on epoch=499
03/10/2022 18:24:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=504
03/10/2022 18:24:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.12 on epoch=509
03/10/2022 18:24:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
03/10/2022 18:25:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=519
03/10/2022 18:25:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.05 on epoch=524
03/10/2022 18:25:08 - INFO - __main__ - Global step 1050 Train loss 0.08 QA-F1 0.23958333333333331 on epoch=524
03/10/2022 18:25:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
03/10/2022 18:25:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.07 on epoch=534
03/10/2022 18:25:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
03/10/2022 18:25:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.06 on epoch=544
03/10/2022 18:25:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
03/10/2022 18:25:32 - INFO - __main__ - Global step 1100 Train loss 0.06 QA-F1 0.11458333333333333 on epoch=549
03/10/2022 18:25:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=554
03/10/2022 18:25:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.09 on epoch=559
03/10/2022 18:25:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
03/10/2022 18:25:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=569
03/10/2022 18:25:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=574
03/10/2022 18:25:55 - INFO - __main__ - Global step 1150 Train loss 0.08 QA-F1 0.1875 on epoch=574
03/10/2022 18:25:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=579
03/10/2022 18:26:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.06 on epoch=584
03/10/2022 18:26:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
03/10/2022 18:26:13 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=594
03/10/2022 18:26:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.08 on epoch=599
03/10/2022 18:26:18 - INFO - __main__ - Global step 1200 Train loss 0.07 QA-F1 0.17708333333333331 on epoch=599
03/10/2022 18:26:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=604
03/10/2022 18:26:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
03/10/2022 18:26:31 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=614
03/10/2022 18:26:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
03/10/2022 18:26:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.08 on epoch=624
03/10/2022 18:26:42 - INFO - __main__ - Global step 1250 Train loss 0.06 QA-F1 0.22916666666666663 on epoch=624
03/10/2022 18:26:46 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=629
03/10/2022 18:26:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
03/10/2022 18:26:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=639
03/10/2022 18:26:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
03/10/2022 18:27:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
03/10/2022 18:27:05 - INFO - __main__ - Global step 1300 Train loss 0.06 QA-F1 0.16666666666666666 on epoch=649
03/10/2022 18:27:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
03/10/2022 18:27:14 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
03/10/2022 18:27:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
03/10/2022 18:27:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=669
03/10/2022 18:27:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
03/10/2022 18:27:29 - INFO - __main__ - Global step 1350 Train loss 0.05 QA-F1 0.1875 on epoch=674
03/10/2022 18:27:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.04 on epoch=679
03/10/2022 18:27:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
03/10/2022 18:27:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
03/10/2022 18:27:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
03/10/2022 18:27:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=699
03/10/2022 18:27:52 - INFO - __main__ - Global step 1400 Train loss 0.04 QA-F1 0.17708333333333331 on epoch=699
03/10/2022 18:27:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
03/10/2022 18:28:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/10/2022 18:28:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
03/10/2022 18:28:09 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
03/10/2022 18:28:14 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
03/10/2022 18:28:15 - INFO - __main__ - Global step 1450 Train loss 0.03 QA-F1 0.23958333333333331 on epoch=724
03/10/2022 18:28:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/10/2022 18:28:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
03/10/2022 18:28:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=739
03/10/2022 18:28:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.08 on epoch=744
03/10/2022 18:28:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
03/10/2022 18:28:38 - INFO - __main__ - Global step 1500 Train loss 0.05 QA-F1 0.17708333333333331 on epoch=749
03/10/2022 18:28:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
03/10/2022 18:28:47 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.06 on epoch=759
03/10/2022 18:28:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
03/10/2022 18:28:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
03/10/2022 18:29:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=774
03/10/2022 18:29:02 - INFO - __main__ - Global step 1550 Train loss 0.04 QA-F1 0.23958333333333331 on epoch=774
03/10/2022 18:29:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.06 on epoch=779
03/10/2022 18:29:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
03/10/2022 18:29:15 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
03/10/2022 18:29:19 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
03/10/2022 18:29:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
03/10/2022 18:29:25 - INFO - __main__ - Global step 1600 Train loss 0.04 QA-F1 0.14583333333333334 on epoch=799
03/10/2022 18:29:29 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
03/10/2022 18:29:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=809
03/10/2022 18:29:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
03/10/2022 18:29:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=819
03/10/2022 18:29:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
03/10/2022 18:29:48 - INFO - __main__ - Global step 1650 Train loss 0.04 QA-F1 0.17708333333333331 on epoch=824
03/10/2022 18:29:53 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
03/10/2022 18:29:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/10/2022 18:30:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
03/10/2022 18:30:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
03/10/2022 18:30:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
03/10/2022 18:30:12 - INFO - __main__ - Global step 1700 Train loss 0.03 QA-F1 0.19791666666666663 on epoch=849
03/10/2022 18:30:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
03/10/2022 18:30:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
03/10/2022 18:30:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=864
03/10/2022 18:30:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
03/10/2022 18:30:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
03/10/2022 18:30:35 - INFO - __main__ - Global step 1750 Train loss 0.03 QA-F1 0.20833333333333331 on epoch=874
03/10/2022 18:30:40 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
03/10/2022 18:30:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
03/10/2022 18:30:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
03/10/2022 18:30:53 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
03/10/2022 18:30:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
03/10/2022 18:30:59 - INFO - __main__ - Global step 1800 Train loss 0.03 QA-F1 0.20833333333333331 on epoch=899
03/10/2022 18:31:03 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
03/10/2022 18:31:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
03/10/2022 18:31:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
03/10/2022 18:31:16 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.05 on epoch=919
03/10/2022 18:31:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
03/10/2022 18:31:22 - INFO - __main__ - Global step 1850 Train loss 0.03 QA-F1 0.1875 on epoch=924
03/10/2022 18:31:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
03/10/2022 18:31:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/10/2022 18:31:35 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
03/10/2022 18:31:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=944
03/10/2022 18:31:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
03/10/2022 18:31:45 - INFO - __main__ - Global step 1900 Train loss 0.03 QA-F1 0.19791666666666663 on epoch=949
03/10/2022 18:31:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/10/2022 18:31:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
03/10/2022 18:31:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
03/10/2022 18:32:03 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/10/2022 18:32:07 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=974
03/10/2022 18:32:09 - INFO - __main__ - Global step 1950 Train loss 0.02 QA-F1 0.17708333333333331 on epoch=974
03/10/2022 18:32:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
03/10/2022 18:32:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=984
03/10/2022 18:32:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
03/10/2022 18:32:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
03/10/2022 18:32:31 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
03/10/2022 18:32:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:32:32 - INFO - __main__ - Printing 3 examples
03/10/2022 18:32:32 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/10/2022 18:32:32 - INFO - __main__ - ['Allan']
03/10/2022 18:32:32 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/10/2022 18:32:32 - INFO - __main__ - ['Nigro']
03/10/2022 18:32:32 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/10/2022 18:32:32 - INFO - __main__ - ['Minogue']
03/10/2022 18:32:32 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:32:32 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:32:32 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:32:32 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:32:32 - INFO - __main__ - Printing 3 examples
03/10/2022 18:32:32 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/10/2022 18:32:32 - INFO - __main__ - ['Alan']
03/10/2022 18:32:32 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/10/2022 18:32:32 - INFO - __main__ - ['Santo Tomás de Castilla']
03/10/2022 18:32:32 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/10/2022 18:32:32 - INFO - __main__ - ['Albert']
03/10/2022 18:32:32 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:32:32 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:32:32 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:32:32 - INFO - __main__ - Global step 2000 Train loss 0.02 QA-F1 0.25 on epoch=999
03/10/2022 18:32:32 - INFO - __main__ - save last model!
03/10/2022 18:32:32 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 18:32:32 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 18:32:32 - INFO - __main__ - Printing 3 examples
03/10/2022 18:32:32 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 18:32:32 - INFO - __main__ - ['Frankie']
03/10/2022 18:32:32 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 18:32:32 - INFO - __main__ - ['Frankie']
03/10/2022 18:32:32 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 18:32:32 - INFO - __main__ - ['Frankie']
03/10/2022 18:32:32 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:32:37 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:32:39 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 18:32:45 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 18:32:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 18:32:46 - INFO - __main__ - Starting training!
03/10/2022 18:34:34 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_21_0.3_8_predictions.txt
03/10/2022 18:34:34 - INFO - __main__ - QA-F1 on test data: 0.2555
03/10/2022 18:34:34 - INFO - __main__ - prefix=quoref_32_21, lr=0.3, bsz=8, dev_performance=0.2708333333333333, test_performance=0.2555332361783975
03/10/2022 18:34:34 - INFO - __main__ - Running ... prefix=quoref_32_21, lr=0.2, bsz=8 ...
03/10/2022 18:34:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:34:35 - INFO - __main__ - Printing 3 examples
03/10/2022 18:34:35 - INFO - __main__ -  [quoref] question: What is the first name of the person the McDonald Ice Rumples is named after? context: Shackleton's first task, on arriving at the Stromness station, was to arrange for his three companions at Peggoty Camp to be picked up. A whaler was sent round the coast, with Worsley aboard to show the way, and by the evening of 21 May all six of the James Caird party were safe.It took four attempts before Shackleton was able to return to Elephant Island to rescue the party stranded there. He first left South Georgia a mere three days after he had arrived in Stromness, after securing the use of a large whaler, The Southern Sky, which was laid up in Husvik Harbour. Shackleton assembled a volunteer crew, which had it ready to sail by the morning of 22 May. As the vessel approached Elephant Island they saw that an impenetrable barrier of pack ice had formed, some 70 miles (110 km) from their destination. The Southern Sky was not built for ice breaking, and retreated to Port Stanley in the Falkland Islands.On reaching Port Stanley, Shackleton informed London by cable of his whereabouts, and requested that a suitable vessel be sent south for the rescue operation. He was informed by the Admiralty that nothing was available before October, which in his view was too late. Then, with the help of the British Minister in Montevideo, Shackleton obtained from the Uruguayan government the loan of a tough trawler, Instituto de Pesca No. 1, which started south on 10 June. Again the pack thwarted them. In search of another ship, Shackleton, Worsley and Crean travelled to Punta Arenas, where they met Allan MacDonald, the British owner of the schooner Emma. McDonald equipped this vessel for a further rescue attempt, which left on 12 July, but with the same negative result—the pack defeated them yet again. Shackleton later named a glacier after McDonald on the Brunt Ice Shelf in the Weddell Sea. After problems arose in identifying this glacier, a nearby ice rise was renamed the McDonald Ice Rumples.By now it was mid-August, more than three months since Shackleton had left Elephant Island. Shackleton begged the Chilean Government to lend him Yelcho, a small steam tug that had assisted Emma during the previous attempt. They agreed; on 25 August, Yelcho—captained by Luis Pardo–set out for Elephant Island. This time, as Shackleton records, providence favoured them. The seas were open, and the ship was able to approach close to the island, in thick fog. At 11:40 a.m. on 30 August, the fog lifted, the camp was spotted and, within an hour, all the Elephant Island party were safely aboard, bound for Punta Arenas.
03/10/2022 18:34:35 - INFO - __main__ - ['Allan']
03/10/2022 18:34:35 - INFO - __main__ -  [quoref] question: What was the last name of the person who halt rescue operations in 7 World Trade Center? context: As the North Tower collapsed on September 11, 2001, heavy debris hit 7 World Trade Center, damaging the south face of the building and starting fires that continued to burn throughout the afternoon. The collapse also caused damage to the southwest corner between Floors 7 and 17 and on the south face between Floor 44 and the roof; other possible structural damage included a large vertical gash near the center of the south face between Floors 24 and 41. The building was equipped with a sprinkler system, but had many single-point vulnerabilities for failure: the sprinkler system required manual initiation of the electrical fire pumps, rather than being a fully automatic system; the floor-level controls had a single connection to the sprinkler water riser; and the sprinkler system required some power for the fire pump to deliver water. Additionally, water pressure was low, with little or no water to feed sprinklers.After the North Tower collapsed, some firefighters entered 7 World Trade Center to search the building. They attempted to extinguish small pockets of fire, but low water pressure hindered their efforts. Over the course of the day, fires burned out of control on several floors of 7 World Trade Center, the flames visible on the east side of the building. During the afternoon, the fire was also seen on floors 6–10, 13–14, 19–22, and 29–30. In particular, the fires on floors 7 through 9 and 11 through 13 continued to burn out of control during the afternoon. At approximately 2:00 pm, firefighters noticed a bulge in the southwest corner of 7 World Trade Center between the 10th and 13th floors, a sign that the building was unstable and might collapse. During the afternoon, firefighters also heard creaking sounds coming from the building. Around 3:30 pm, FDNY Chief Daniel A. Nigro decided to halt rescue operations, surface removal, and searches along the surface of the debris near 7 World Trade Center and evacuate the area due to concerns for the safety of personnel. The fire expanded the girders of the building, causing some to lose their structural integrity. This led column number 79, a critical column supporting a large part of the 13th floor, to buckle, causing the floors above it to collapse to the fifth floor; however, this could not be seen from outside the building. The structure also developed cracks in the facade just before the entire building started to fall. According to FEMA, this collapse started at 5:20:33 pm EDT when the east mechanical penthouse started crumbling. Differing times are given as to what time the building completely collapsed: at 5:21:10 pm EDT according to FEMA, and at 5:20:52 pm EDT according to NIST. There were no casualties associated with the collapse. NIST found no evidence to support conspiracy theories such as the collapse being the result of explosives; it found that a combination of factors including physical damage, fire and the building's unusual construction set off a chain-reaction collapse.
03/10/2022 18:34:35 - INFO - __main__ - ['Nigro']
03/10/2022 18:34:35 - INFO - __main__ -  [quoref] question: What is the last name of the person who embarked on the Let's Get to It Tour? context: Minogue's third album, Rhythm of Love was released in November 1990 and was described as "leaps and bounds more mature" than her previous albums. Her relationship with Michael Hutchence was also seen as part of her departure from her earlier persona. Its lead single, "Better the Devil You Know" peaked at number two in the UK and four in her native Australia. Rhythm of Love's second and fourth single, "Step Back in Time" and "Shocked" were both a top ten hit in the UK and Australia. She then embarked on the Rhythm of Love Tour in February 1991. Minogue's fourth album, Let's Get to It was released in October 1991 and reached number 15 on the UK Albums Chart. It was her first album to fail to reach the top ten. While the first single from the album, "Word Is Out", became her first single to miss the top ten of the UK Singles Chart, subsequent singles "If You Were with Me Now" and "Give Me Just a Little More Time" both reached the top five. In support of the album, she embarked on the Let's Get to It Tour in October. She later expressed her opinion that she was stifled by Stock, Aitken and Waterman, saying, "I was very much a puppet in the beginning. I was blinkered by my record company. I was unable to look left or right." Her first Greatest Hits album was released in August 1992. It reached number one in the United Kingdom and number three in Australia. The singles from the album, "What Kind of Fool" and her cover version of Kool & the Gang's "Celebration" both reached the top twenty of the UK Singles Chart.
03/10/2022 18:34:35 - INFO - __main__ - ['Minogue']
03/10/2022 18:34:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:34:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:34:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:34:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:34:35 - INFO - __main__ - Printing 3 examples
03/10/2022 18:34:35 - INFO - __main__ -  [quoref] question: Who does the police officer instruct about safe driving? context: Idaho police officer Hal Jackson arrives at the funeral for young Frank Dixon, Jr., who has died in a car accident. Hal, a friend of the Dixon family, does not go inside, feeling it would be too difficult to take. Hal finds it hard to believe that, only a few days ago, the Dixons were a relatively regular family. In flashback, he recounts what led to Frank Jr.'s death. Frank Jr. has returned home for the summer to aid his father, Frank Sr. (Harold Agee), on the family farm. He also visits his girlfriend Betty Hutchins. When Frank Sr.'s new tractor arrives at the local train station, Frank Jr.'s brother Alan wishes to drive it, having recently taken a driver's test. His father disallows it, so Frank Jr. drives it home. The next day, Alan discovers that his license has arrived in the mail. Ecstatic, he wishes to drive immediately, asking his family members if they need help with any errands. Later, Hal shows up at the Dixon home. Knowing that Alan's license had been scheduled to arrive, he begins to talk to Alan, telling him about things he should know in order to be able to drive safely. As he finishes giving the advice, Frank Jr. and Betty return home. Alan asks his father if he can drive the car into town. His father lets him, and Frank Jr. and Betty agree to go with him to make sure he arrives safely.
03/10/2022 18:34:35 - INFO - __main__ - ['Alan']
03/10/2022 18:34:35 - INFO - __main__ -  [quoref] question: What port was constructed near Lake Izabal? context: Gil González Dávila set out from the Caribbean island of Hispaniola early in 1524, with the intention of exploring the Caribbean coast of Nicaragua. His course took him to the north coast of Honduras. After founding Puerto de Caballos, Gil Gónzalez sailed west along the coast to the Amatique Bay, and founded a Spanish settlement somewhere near the Dulce River, within modern-day Guatemala, which he named San Gil de Buena Vista. He launched a campaign of conquest in the mountainous region dividing Honduras from Guatemala. González left some of his men under the command of Francisco Riquelme at San Gil de Buena Vista, and sailed back east along the coast to Honduras. The colonists at San Gil did not prosper, and soon set out in search of a more hospitable location. They resettled in the important indigenous town of Nito, near the mouth of the Dulce River. Although they were in a desperate state, and near-starving, they were still there when Cortés passed through en route to Honduras, and were absorbed into his expedition.The Dominicans established themselves in Xocolo on the shore of Lake Izabal in the mid-16th century. Xocolo became infamous among the Dominican missionaries for the practice of witchcraft by its inhabitants. By 1574 it was the most important staging post for European expeditions into the interior, and it remained important in that role until as late as 1630, although it was abandoned in 1631.In 1598 Alfonso Criado de Castilla became governor of the Captaincy General of Guatemala. Owing to the poor state of Puerto de Caballos on the Honduran coast and its exposure to repeated pirate raids he sent a pilot to scout Lake Izabal. As a result of the survey, and after royal permission was granted, Criado de Castilla ordered the construction of a new port, named Santo Tomás de Castilla, at a favourable spot on the Amatique Bay not far from the lake. Work then began on building a highway from the port to the new capital of the colony, modern Antigua Guatemala, following the Motagua Valley into the highlands. Indigenous guides scouting the route from the highlands would not proceed further downriver than three leagues below Quiriguá, because the area was inhabited by the hostile Toquegua.
03/10/2022 18:34:35 - INFO - __main__ - ['Santo Tomás de Castilla']
03/10/2022 18:34:35 - INFO - __main__ -  [quoref] question: Which member of The Moron 5 infiltrates the Pamintuan Residence? context: Half-witted longtime friends Albert, Isaac, Mozart "Mo" (DJ Durano), Michaelangelo "Mike" (Martin Escudero) and  Aristotle "Aris" (Marvin Agustin) were used to living moronic yet pretty normal and hassle-free lives until successful careerwoman Beckie Pamintuan accused them of killing her father and ruin everything for them. The Moron 5 are more than sure of their innocence but for the life of them, they can't find any single satisfactory argument on how to prove it especially when their opponent would do everything to punish them for whim. Spending three miserable years in prison trying different failed comedic attempts to get out, they finally figured a way to escape. They stalked Beckie and tried to understand why she's fighting so hard to have them imprisoned when it's clear as day that what happened three years ago was a nonsense frame-up. An opportunity came when Beckie's driver got fired for having an affair with her maid and Albert volunteered to apply to replace him. He infiltrated the Pamintuan Residence and together with his four crazily daft friends, they've gathered information about the curious family yet to them, it isn't making any sense at all especially Vecky's unexplained hatred to the five of them. Why is Beckie fighting so hard to have them suffer? The Moron 5 will try harder to know and hopefully understand what's really going on although little did they know that by doing so, everything that they hold dear might be at risk.
03/10/2022 18:34:35 - INFO - __main__ - ['Albert']
03/10/2022 18:34:35 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:34:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:34:35 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:34:48 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 18:34:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 18:34:49 - INFO - __main__ - Starting training!
03/10/2022 18:34:56 - INFO - __main__ - Step 10 Global step 10 Train loss 2.63 on epoch=4
03/10/2022 18:35:01 - INFO - __main__ - Step 20 Global step 20 Train loss 2.33 on epoch=9
03/10/2022 18:35:05 - INFO - __main__ - Step 30 Global step 30 Train loss 1.96 on epoch=14
03/10/2022 18:35:09 - INFO - __main__ - Step 40 Global step 40 Train loss 1.69 on epoch=19
03/10/2022 18:35:14 - INFO - __main__ - Step 50 Global step 50 Train loss 1.43 on epoch=24
03/10/2022 18:35:15 - INFO - __main__ - Global step 50 Train loss 2.01 QA-F1 0.24166666666666667 on epoch=24
03/10/2022 18:35:15 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.24166666666666667 on epoch=24, global_step=50
03/10/2022 18:35:19 - INFO - __main__ - Step 60 Global step 60 Train loss 1.36 on epoch=29
03/10/2022 18:35:24 - INFO - __main__ - Step 70 Global step 70 Train loss 1.35 on epoch=34
03/10/2022 18:35:28 - INFO - __main__ - Step 80 Global step 80 Train loss 1.36 on epoch=39
03/10/2022 18:35:32 - INFO - __main__ - Step 90 Global step 90 Train loss 1.25 on epoch=44
03/10/2022 18:35:37 - INFO - __main__ - Step 100 Global step 100 Train loss 1.25 on epoch=49
03/10/2022 18:35:38 - INFO - __main__ - Global step 100 Train loss 1.31 QA-F1 0.17708333333333331 on epoch=49
03/10/2022 18:35:42 - INFO - __main__ - Step 110 Global step 110 Train loss 1.14 on epoch=54
03/10/2022 18:35:47 - INFO - __main__ - Step 120 Global step 120 Train loss 1.11 on epoch=59
03/10/2022 18:35:51 - INFO - __main__ - Step 130 Global step 130 Train loss 1.03 on epoch=64
03/10/2022 18:35:55 - INFO - __main__ - Step 140 Global step 140 Train loss 1.08 on epoch=69
03/10/2022 18:35:59 - INFO - __main__ - Step 150 Global step 150 Train loss 1.06 on epoch=74
03/10/2022 18:36:01 - INFO - __main__ - Global step 150 Train loss 1.09 QA-F1 0.1875 on epoch=74
03/10/2022 18:36:05 - INFO - __main__ - Step 160 Global step 160 Train loss 1.06 on epoch=79
03/10/2022 18:36:09 - INFO - __main__ - Step 170 Global step 170 Train loss 1.10 on epoch=84
03/10/2022 18:36:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.13 on epoch=89
03/10/2022 18:36:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.98 on epoch=94
03/10/2022 18:36:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.99 on epoch=99
03/10/2022 18:36:24 - INFO - __main__ - Global step 200 Train loss 1.05 QA-F1 0.22916666666666663 on epoch=99
03/10/2022 18:36:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.96 on epoch=104
03/10/2022 18:36:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.97 on epoch=109
03/10/2022 18:36:37 - INFO - __main__ - Step 230 Global step 230 Train loss 0.87 on epoch=114
03/10/2022 18:36:41 - INFO - __main__ - Step 240 Global step 240 Train loss 0.88 on epoch=119
03/10/2022 18:36:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.84 on epoch=124
03/10/2022 18:36:47 - INFO - __main__ - Global step 250 Train loss 0.91 QA-F1 0.29166666666666663 on epoch=124
03/10/2022 18:36:47 - INFO - __main__ - Saving model with best QA-F1: 0.24166666666666667 -> 0.29166666666666663 on epoch=124, global_step=250
03/10/2022 18:36:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.87 on epoch=129
03/10/2022 18:36:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.77 on epoch=134
03/10/2022 18:37:00 - INFO - __main__ - Step 280 Global step 280 Train loss 0.84 on epoch=139
03/10/2022 18:37:04 - INFO - __main__ - Step 290 Global step 290 Train loss 0.73 on epoch=144
03/10/2022 18:37:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.76 on epoch=149
03/10/2022 18:37:10 - INFO - __main__ - Global step 300 Train loss 0.79 QA-F1 0.21875 on epoch=149
03/10/2022 18:37:15 - INFO - __main__ - Step 310 Global step 310 Train loss 0.66 on epoch=154
03/10/2022 18:37:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.70 on epoch=159
03/10/2022 18:37:23 - INFO - __main__ - Step 330 Global step 330 Train loss 0.76 on epoch=164
03/10/2022 18:37:28 - INFO - __main__ - Step 340 Global step 340 Train loss 0.65 on epoch=169
03/10/2022 18:37:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.64 on epoch=174
03/10/2022 18:37:33 - INFO - __main__ - Global step 350 Train loss 0.68 QA-F1 0.20833333333333331 on epoch=174
03/10/2022 18:37:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.61 on epoch=179
03/10/2022 18:37:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.60 on epoch=184
03/10/2022 18:37:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=189
03/10/2022 18:37:51 - INFO - __main__ - Step 390 Global step 390 Train loss 0.54 on epoch=194
03/10/2022 18:37:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.53 on epoch=199
03/10/2022 18:37:57 - INFO - __main__ - Global step 400 Train loss 0.56 QA-F1 0.2125 on epoch=199
03/10/2022 18:38:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.45 on epoch=204
03/10/2022 18:38:05 - INFO - __main__ - Step 420 Global step 420 Train loss 0.46 on epoch=209
03/10/2022 18:38:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.55 on epoch=214
03/10/2022 18:38:14 - INFO - __main__ - Step 440 Global step 440 Train loss 0.43 on epoch=219
03/10/2022 18:38:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.45 on epoch=224
03/10/2022 18:38:20 - INFO - __main__ - Global step 450 Train loss 0.47 QA-F1 0.29374999999999996 on epoch=224
03/10/2022 18:38:20 - INFO - __main__ - Saving model with best QA-F1: 0.29166666666666663 -> 0.29374999999999996 on epoch=224, global_step=450
03/10/2022 18:38:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.48 on epoch=229
03/10/2022 18:38:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=234
03/10/2022 18:38:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=239
03/10/2022 18:38:37 - INFO - __main__ - Step 490 Global step 490 Train loss 0.46 on epoch=244
03/10/2022 18:38:41 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=249
03/10/2022 18:38:43 - INFO - __main__ - Global step 500 Train loss 0.44 QA-F1 0.23958333333333331 on epoch=249
03/10/2022 18:38:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
03/10/2022 18:38:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.41 on epoch=259
03/10/2022 18:38:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=264
03/10/2022 18:39:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=269
03/10/2022 18:39:04 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=274
03/10/2022 18:39:06 - INFO - __main__ - Global step 550 Train loss 0.35 QA-F1 0.17708333333333331 on epoch=274
03/10/2022 18:39:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.31 on epoch=279
03/10/2022 18:39:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
03/10/2022 18:39:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
03/10/2022 18:39:23 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=294
03/10/2022 18:39:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
03/10/2022 18:39:29 - INFO - __main__ - Global step 600 Train loss 0.27 QA-F1 0.2520833333333333 on epoch=299
03/10/2022 18:39:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=304
03/10/2022 18:39:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
03/10/2022 18:39:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=314
03/10/2022 18:39:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
03/10/2022 18:39:50 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
03/10/2022 18:39:52 - INFO - __main__ - Global step 650 Train loss 0.26 QA-F1 0.22916666666666663 on epoch=324
03/10/2022 18:39:56 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=329
03/10/2022 18:40:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=334
03/10/2022 18:40:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=339
03/10/2022 18:40:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=344
03/10/2022 18:40:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=349
03/10/2022 18:40:15 - INFO - __main__ - Global step 700 Train loss 0.19 QA-F1 0.19791666666666666 on epoch=349
03/10/2022 18:40:19 - INFO - __main__ - Step 710 Global step 710 Train loss 0.15 on epoch=354
03/10/2022 18:40:23 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=359
03/10/2022 18:40:28 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=364
03/10/2022 18:40:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.15 on epoch=369
03/10/2022 18:40:36 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=374
03/10/2022 18:40:38 - INFO - __main__ - Global step 750 Train loss 0.18 QA-F1 0.22916666666666663 on epoch=374
03/10/2022 18:40:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
03/10/2022 18:40:46 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
03/10/2022 18:40:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.12 on epoch=389
03/10/2022 18:40:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.16 on epoch=394
03/10/2022 18:40:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=399
03/10/2022 18:41:01 - INFO - __main__ - Global step 800 Train loss 0.16 QA-F1 0.19999999999999998 on epoch=399
03/10/2022 18:41:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=404
03/10/2022 18:41:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=409
03/10/2022 18:41:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.15 on epoch=414
03/10/2022 18:41:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=419
03/10/2022 18:41:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=424
03/10/2022 18:41:24 - INFO - __main__ - Global step 850 Train loss 0.17 QA-F1 0.20833333333333331 on epoch=424
03/10/2022 18:41:28 - INFO - __main__ - Step 860 Global step 860 Train loss 0.15 on epoch=429
03/10/2022 18:41:32 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=434
03/10/2022 18:41:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=439
03/10/2022 18:41:41 - INFO - __main__ - Step 890 Global step 890 Train loss 0.10 on epoch=444
03/10/2022 18:41:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=449
03/10/2022 18:41:47 - INFO - __main__ - Global step 900 Train loss 0.13 QA-F1 0.22916666666666663 on epoch=449
03/10/2022 18:41:51 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=454
03/10/2022 18:41:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
03/10/2022 18:42:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=464
03/10/2022 18:42:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.14 on epoch=469
03/10/2022 18:42:08 - INFO - __main__ - Step 950 Global step 950 Train loss 0.11 on epoch=474
03/10/2022 18:42:10 - INFO - __main__ - Global step 950 Train loss 0.11 QA-F1 0.20833333333333331 on epoch=474
03/10/2022 18:42:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.09 on epoch=479
03/10/2022 18:42:19 - INFO - __main__ - Step 970 Global step 970 Train loss 0.12 on epoch=484
03/10/2022 18:42:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=489
03/10/2022 18:42:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=494
03/10/2022 18:42:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=499
03/10/2022 18:42:33 - INFO - __main__ - Global step 1000 Train loss 0.12 QA-F1 0.20833333333333331 on epoch=499
03/10/2022 18:42:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.09 on epoch=504
03/10/2022 18:42:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=509
03/10/2022 18:42:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=514
03/10/2022 18:42:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=519
03/10/2022 18:42:54 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=524
03/10/2022 18:42:56 - INFO - __main__ - Global step 1050 Train loss 0.10 QA-F1 0.2708333333333333 on epoch=524
03/10/2022 18:43:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=529
03/10/2022 18:43:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=534
03/10/2022 18:43:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.07 on epoch=539
03/10/2022 18:43:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.11 on epoch=544
03/10/2022 18:43:17 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.13 on epoch=549
03/10/2022 18:43:19 - INFO - __main__ - Global step 1100 Train loss 0.10 QA-F1 0.24999999999999997 on epoch=549
03/10/2022 18:43:23 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=554
03/10/2022 18:43:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=559
03/10/2022 18:43:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.08 on epoch=564
03/10/2022 18:43:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=569
03/10/2022 18:43:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=574
03/10/2022 18:43:42 - INFO - __main__ - Global step 1150 Train loss 0.09 QA-F1 0.23958333333333331 on epoch=574
03/10/2022 18:43:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.10 on epoch=579
03/10/2022 18:43:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.10 on epoch=584
03/10/2022 18:43:55 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.08 on epoch=589
03/10/2022 18:43:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=594
03/10/2022 18:44:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=599
03/10/2022 18:44:05 - INFO - __main__ - Global step 1200 Train loss 0.09 QA-F1 0.23958333333333331 on epoch=599
03/10/2022 18:44:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
03/10/2022 18:44:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.05 on epoch=609
03/10/2022 18:44:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
03/10/2022 18:44:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
03/10/2022 18:44:27 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.06 on epoch=624
03/10/2022 18:44:28 - INFO - __main__ - Global step 1250 Train loss 0.05 QA-F1 0.22916666666666663 on epoch=624
03/10/2022 18:44:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=629
03/10/2022 18:44:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
03/10/2022 18:44:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
03/10/2022 18:44:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=644
03/10/2022 18:44:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
03/10/2022 18:44:51 - INFO - __main__ - Global step 1300 Train loss 0.06 QA-F1 0.2708333333333333 on epoch=649
03/10/2022 18:44:55 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=654
03/10/2022 18:45:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
03/10/2022 18:45:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
03/10/2022 18:45:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.07 on epoch=669
03/10/2022 18:45:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.06 on epoch=674
03/10/2022 18:45:14 - INFO - __main__ - Global step 1350 Train loss 0.06 QA-F1 0.29166666666666663 on epoch=674
03/10/2022 18:45:19 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=679
03/10/2022 18:45:23 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.05 on epoch=684
03/10/2022 18:45:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
03/10/2022 18:45:32 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
03/10/2022 18:45:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=699
03/10/2022 18:45:37 - INFO - __main__ - Global step 1400 Train loss 0.05 QA-F1 0.3020833333333333 on epoch=699
03/10/2022 18:45:37 - INFO - __main__ - Saving model with best QA-F1: 0.29374999999999996 -> 0.3020833333333333 on epoch=699, global_step=1400
03/10/2022 18:45:42 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
03/10/2022 18:45:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=709
03/10/2022 18:45:50 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.06 on epoch=714
03/10/2022 18:45:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.09 on epoch=719
03/10/2022 18:45:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=724
03/10/2022 18:46:00 - INFO - __main__ - Global step 1450 Train loss 0.06 QA-F1 0.26041666666666663 on epoch=724
03/10/2022 18:46:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=729
03/10/2022 18:46:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
03/10/2022 18:46:13 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.05 on epoch=739
03/10/2022 18:46:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=744
03/10/2022 18:46:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
03/10/2022 18:46:24 - INFO - __main__ - Global step 1500 Train loss 0.05 QA-F1 0.22916666666666663 on epoch=749
03/10/2022 18:46:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
03/10/2022 18:46:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=759
03/10/2022 18:46:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
03/10/2022 18:46:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
03/10/2022 18:46:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.04 on epoch=774
03/10/2022 18:46:47 - INFO - __main__ - Global step 1550 Train loss 0.05 QA-F1 0.23958333333333331 on epoch=774
03/10/2022 18:46:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
03/10/2022 18:46:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=784
03/10/2022 18:47:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
03/10/2022 18:47:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
03/10/2022 18:47:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
03/10/2022 18:47:10 - INFO - __main__ - Global step 1600 Train loss 0.04 QA-F1 0.3020833333333333 on epoch=799
03/10/2022 18:47:14 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
03/10/2022 18:47:19 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.04 on epoch=809
03/10/2022 18:47:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=814
03/10/2022 18:47:27 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
03/10/2022 18:47:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
03/10/2022 18:47:33 - INFO - __main__ - Global step 1650 Train loss 0.04 QA-F1 0.2708333333333333 on epoch=824
03/10/2022 18:47:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=829
03/10/2022 18:47:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=834
03/10/2022 18:47:46 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
03/10/2022 18:47:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
03/10/2022 18:47:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
03/10/2022 18:47:56 - INFO - __main__ - Global step 1700 Train loss 0.04 QA-F1 0.22916666666666666 on epoch=849
03/10/2022 18:48:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.04 on epoch=854
03/10/2022 18:48:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
03/10/2022 18:48:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
03/10/2022 18:48:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
03/10/2022 18:48:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
03/10/2022 18:48:20 - INFO - __main__ - Global step 1750 Train loss 0.03 QA-F1 0.1875 on epoch=874
03/10/2022 18:48:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
03/10/2022 18:48:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
03/10/2022 18:48:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=889
03/10/2022 18:48:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
03/10/2022 18:48:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
03/10/2022 18:48:43 - INFO - __main__ - Global step 1800 Train loss 0.03 QA-F1 0.21875 on epoch=899
03/10/2022 18:48:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
03/10/2022 18:48:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=909
03/10/2022 18:48:56 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
03/10/2022 18:49:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=919
03/10/2022 18:49:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
03/10/2022 18:49:06 - INFO - __main__ - Global step 1850 Train loss 0.04 QA-F1 0.23958333333333331 on epoch=924
03/10/2022 18:49:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
03/10/2022 18:49:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
03/10/2022 18:49:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
03/10/2022 18:49:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
03/10/2022 18:49:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
03/10/2022 18:49:29 - INFO - __main__ - Global step 1900 Train loss 0.03 QA-F1 0.21875 on epoch=949
03/10/2022 18:49:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=954
03/10/2022 18:49:37 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
03/10/2022 18:49:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.05 on epoch=964
03/10/2022 18:49:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
03/10/2022 18:49:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.03 on epoch=974
03/10/2022 18:49:52 - INFO - __main__ - Global step 1950 Train loss 0.03 QA-F1 0.28125 on epoch=974
03/10/2022 18:49:56 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
03/10/2022 18:50:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/10/2022 18:50:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
03/10/2022 18:50:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.04 on epoch=994
03/10/2022 18:50:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.02 on epoch=999
03/10/2022 18:50:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:50:15 - INFO - __main__ - Printing 3 examples
03/10/2022 18:50:15 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/10/2022 18:50:15 - INFO - __main__ - ['Esteban Pasquale']
03/10/2022 18:50:15 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/10/2022 18:50:15 - INFO - __main__ - ['Ronnie J. McGorvey']
03/10/2022 18:50:15 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/10/2022 18:50:15 - INFO - __main__ - ['Sam Harper']
03/10/2022 18:50:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 18:50:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:50:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:50:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:50:15 - INFO - __main__ - Printing 3 examples
03/10/2022 18:50:15 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/10/2022 18:50:15 - INFO - __main__ - ['Red Shadow']
03/10/2022 18:50:15 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/10/2022 18:50:15 - INFO - __main__ - ['Louder Than You Think Studio']
03/10/2022 18:50:15 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/10/2022 18:50:15 - INFO - __main__ - ['Matt', 'Wendy']
03/10/2022 18:50:15 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:50:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:50:15 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:50:15 - INFO - __main__ - Global step 2000 Train loss 0.02 QA-F1 0.26041666666666663 on epoch=999
03/10/2022 18:50:15 - INFO - __main__ - save last model!
03/10/2022 18:50:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 18:50:15 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 18:50:15 - INFO - __main__ - Printing 3 examples
03/10/2022 18:50:15 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 18:50:15 - INFO - __main__ - ['Frankie']
03/10/2022 18:50:15 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 18:50:15 - INFO - __main__ - ['Frankie']
03/10/2022 18:50:15 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 18:50:15 - INFO - __main__ - ['Frankie']
03/10/2022 18:50:15 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:50:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:50:22 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 18:50:28 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 18:50:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 18:50:29 - INFO - __main__ - Starting training!
03/10/2022 18:52:13 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_21_0.2_8_predictions.txt
03/10/2022 18:52:13 - INFO - __main__ - QA-F1 on test data: 0.2725
03/10/2022 18:52:13 - INFO - __main__ - prefix=quoref_32_21, lr=0.2, bsz=8, dev_performance=0.3020833333333333, test_performance=0.2724514434191853
03/10/2022 18:52:13 - INFO - __main__ - Running ... prefix=quoref_32_42, lr=0.5, bsz=8 ...
03/10/2022 18:52:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:52:14 - INFO - __main__ - Printing 3 examples
03/10/2022 18:52:14 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/10/2022 18:52:14 - INFO - __main__ - ['Esteban Pasquale']
03/10/2022 18:52:14 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/10/2022 18:52:14 - INFO - __main__ - ['Ronnie J. McGorvey']
03/10/2022 18:52:14 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/10/2022 18:52:14 - INFO - __main__ - ['Sam Harper']
03/10/2022 18:52:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 18:52:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:52:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 18:52:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 18:52:14 - INFO - __main__ - Printing 3 examples
03/10/2022 18:52:14 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/10/2022 18:52:14 - INFO - __main__ - ['Red Shadow']
03/10/2022 18:52:14 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/10/2022 18:52:14 - INFO - __main__ - ['Louder Than You Think Studio']
03/10/2022 18:52:14 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/10/2022 18:52:14 - INFO - __main__ - ['Matt', 'Wendy']
03/10/2022 18:52:14 - INFO - __main__ - Tokenizing Input ...
03/10/2022 18:52:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 18:52:15 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 18:52:27 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 18:52:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 18:52:28 - INFO - __main__ - Starting training!
03/10/2022 18:52:33 - INFO - __main__ - Step 10 Global step 10 Train loss 3.29 on epoch=4
03/10/2022 18:52:38 - INFO - __main__ - Step 20 Global step 20 Train loss 2.48 on epoch=9
03/10/2022 18:52:42 - INFO - __main__ - Step 30 Global step 30 Train loss 1.72 on epoch=14
03/10/2022 18:52:47 - INFO - __main__ - Step 40 Global step 40 Train loss 1.58 on epoch=19
03/10/2022 18:52:51 - INFO - __main__ - Step 50 Global step 50 Train loss 1.77 on epoch=24
03/10/2022 18:52:52 - INFO - __main__ - Global step 50 Train loss 2.17 QA-F1 0.1875 on epoch=24
03/10/2022 18:52:53 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.1875 on epoch=24, global_step=50
03/10/2022 18:52:57 - INFO - __main__ - Step 60 Global step 60 Train loss 1.56 on epoch=29
03/10/2022 18:53:01 - INFO - __main__ - Step 70 Global step 70 Train loss 1.47 on epoch=34
03/10/2022 18:53:06 - INFO - __main__ - Step 80 Global step 80 Train loss 1.44 on epoch=39
03/10/2022 18:53:10 - INFO - __main__ - Step 90 Global step 90 Train loss 2.02 on epoch=44
03/10/2022 18:53:14 - INFO - __main__ - Step 100 Global step 100 Train loss 1.41 on epoch=49
03/10/2022 18:53:16 - INFO - __main__ - Global step 100 Train loss 1.58 QA-F1 0.14583333333333331 on epoch=49
03/10/2022 18:53:20 - INFO - __main__ - Step 110 Global step 110 Train loss 1.64 on epoch=54
03/10/2022 18:53:25 - INFO - __main__ - Step 120 Global step 120 Train loss 1.52 on epoch=59
03/10/2022 18:53:29 - INFO - __main__ - Step 130 Global step 130 Train loss 2.15 on epoch=64
03/10/2022 18:53:33 - INFO - __main__ - Step 140 Global step 140 Train loss 1.77 on epoch=69
03/10/2022 18:53:37 - INFO - __main__ - Step 150 Global step 150 Train loss 1.55 on epoch=74
03/10/2022 18:53:39 - INFO - __main__ - Global step 150 Train loss 1.73 QA-F1 0.20833333333333331 on epoch=74
03/10/2022 18:53:39 - INFO - __main__ - Saving model with best QA-F1: 0.1875 -> 0.20833333333333331 on epoch=74, global_step=150
03/10/2022 18:53:43 - INFO - __main__ - Step 160 Global step 160 Train loss 1.46 on epoch=79
03/10/2022 18:53:48 - INFO - __main__ - Step 170 Global step 170 Train loss 4.84 on epoch=84
03/10/2022 18:53:52 - INFO - __main__ - Step 180 Global step 180 Train loss 4.35 on epoch=89
03/10/2022 18:53:57 - INFO - __main__ - Step 190 Global step 190 Train loss 3.87 on epoch=94
03/10/2022 18:54:01 - INFO - __main__ - Step 200 Global step 200 Train loss 4.03 on epoch=99
03/10/2022 18:54:02 - INFO - __main__ - Global step 200 Train loss 3.71 QA-F1 0.23437499999999997 on epoch=99
03/10/2022 18:54:02 - INFO - __main__ - Saving model with best QA-F1: 0.20833333333333331 -> 0.23437499999999997 on epoch=99, global_step=200
03/10/2022 18:54:07 - INFO - __main__ - Step 210 Global step 210 Train loss 3.06 on epoch=104
03/10/2022 18:54:11 - INFO - __main__ - Step 220 Global step 220 Train loss 2.66 on epoch=109
03/10/2022 18:54:15 - INFO - __main__ - Step 230 Global step 230 Train loss 2.41 on epoch=114
03/10/2022 18:54:20 - INFO - __main__ - Step 240 Global step 240 Train loss 2.64 on epoch=119
03/10/2022 18:54:24 - INFO - __main__ - Step 250 Global step 250 Train loss 2.47 on epoch=124
03/10/2022 18:54:26 - INFO - __main__ - Global step 250 Train loss 2.65 QA-F1 0.16145833333333331 on epoch=124
03/10/2022 18:54:30 - INFO - __main__ - Step 260 Global step 260 Train loss 2.19 on epoch=129
03/10/2022 18:54:34 - INFO - __main__ - Step 270 Global step 270 Train loss 2.14 on epoch=134
03/10/2022 18:54:39 - INFO - __main__ - Step 280 Global step 280 Train loss 3.34 on epoch=139
03/10/2022 18:54:43 - INFO - __main__ - Step 290 Global step 290 Train loss 3.39 on epoch=144
03/10/2022 18:54:47 - INFO - __main__ - Step 300 Global step 300 Train loss 3.07 on epoch=149
03/10/2022 18:54:49 - INFO - __main__ - Global step 300 Train loss 2.83 QA-F1 0.16666666666666666 on epoch=149
03/10/2022 18:54:53 - INFO - __main__ - Step 310 Global step 310 Train loss 2.64 on epoch=154
03/10/2022 18:54:57 - INFO - __main__ - Step 320 Global step 320 Train loss 2.62 on epoch=159
03/10/2022 18:55:02 - INFO - __main__ - Step 330 Global step 330 Train loss 4.13 on epoch=164
03/10/2022 18:55:06 - INFO - __main__ - Step 340 Global step 340 Train loss 4.17 on epoch=169
03/10/2022 18:55:10 - INFO - __main__ - Step 350 Global step 350 Train loss 3.43 on epoch=174
03/10/2022 18:55:12 - INFO - __main__ - Global step 350 Train loss 3.40 QA-F1 0.19270833333333331 on epoch=174
03/10/2022 18:55:16 - INFO - __main__ - Step 360 Global step 360 Train loss 2.49 on epoch=179
03/10/2022 18:55:21 - INFO - __main__ - Step 370 Global step 370 Train loss 3.61 on epoch=184
03/10/2022 18:55:25 - INFO - __main__ - Step 380 Global step 380 Train loss 4.55 on epoch=189
03/10/2022 18:55:29 - INFO - __main__ - Step 390 Global step 390 Train loss 3.27 on epoch=194
03/10/2022 18:55:34 - INFO - __main__ - Step 400 Global step 400 Train loss 3.68 on epoch=199
03/10/2022 18:55:35 - INFO - __main__ - Global step 400 Train loss 3.52 QA-F1 0.1875 on epoch=199
03/10/2022 18:55:39 - INFO - __main__ - Step 410 Global step 410 Train loss 3.26 on epoch=204
03/10/2022 18:55:44 - INFO - __main__ - Step 420 Global step 420 Train loss 2.29 on epoch=209
03/10/2022 18:55:48 - INFO - __main__ - Step 430 Global step 430 Train loss 2.47 on epoch=214
03/10/2022 18:55:52 - INFO - __main__ - Step 440 Global step 440 Train loss 2.19 on epoch=219
03/10/2022 18:55:57 - INFO - __main__ - Step 450 Global step 450 Train loss 2.84 on epoch=224
03/10/2022 18:55:58 - INFO - __main__ - Global step 450 Train loss 2.61 QA-F1 0.19791666666666663 on epoch=224
03/10/2022 18:56:03 - INFO - __main__ - Step 460 Global step 460 Train loss 2.51 on epoch=229
03/10/2022 18:56:07 - INFO - __main__ - Step 470 Global step 470 Train loss 2.31 on epoch=234
03/10/2022 18:56:11 - INFO - __main__ - Step 480 Global step 480 Train loss 2.05 on epoch=239
03/10/2022 18:56:16 - INFO - __main__ - Step 490 Global step 490 Train loss 2.13 on epoch=244
03/10/2022 18:56:20 - INFO - __main__ - Step 500 Global step 500 Train loss 1.81 on epoch=249
03/10/2022 18:56:21 - INFO - __main__ - Global step 500 Train loss 2.16 QA-F1 0.16145833333333331 on epoch=249
03/10/2022 18:56:26 - INFO - __main__ - Step 510 Global step 510 Train loss 1.95 on epoch=254
03/10/2022 18:56:30 - INFO - __main__ - Step 520 Global step 520 Train loss 1.96 on epoch=259
03/10/2022 18:56:34 - INFO - __main__ - Step 530 Global step 530 Train loss 1.98 on epoch=264
03/10/2022 18:56:39 - INFO - __main__ - Step 540 Global step 540 Train loss 2.04 on epoch=269
03/10/2022 18:56:43 - INFO - __main__ - Step 550 Global step 550 Train loss 2.27 on epoch=274
03/10/2022 18:56:45 - INFO - __main__ - Global step 550 Train loss 2.04 QA-F1 0.13020833333333331 on epoch=274
03/10/2022 18:56:49 - INFO - __main__ - Step 560 Global step 560 Train loss 2.21 on epoch=279
03/10/2022 18:56:53 - INFO - __main__ - Step 570 Global step 570 Train loss 1.95 on epoch=284
03/10/2022 18:56:58 - INFO - __main__ - Step 580 Global step 580 Train loss 2.11 on epoch=289
03/10/2022 18:57:02 - INFO - __main__ - Step 590 Global step 590 Train loss 1.96 on epoch=294
03/10/2022 18:57:06 - INFO - __main__ - Step 600 Global step 600 Train loss 1.85 on epoch=299
03/10/2022 18:57:08 - INFO - __main__ - Global step 600 Train loss 2.02 QA-F1 0.16145833333333331 on epoch=299
03/10/2022 18:57:12 - INFO - __main__ - Step 610 Global step 610 Train loss 2.27 on epoch=304
03/10/2022 18:57:17 - INFO - __main__ - Step 620 Global step 620 Train loss 2.14 on epoch=309
03/10/2022 18:57:21 - INFO - __main__ - Step 630 Global step 630 Train loss 1.98 on epoch=314
03/10/2022 18:57:25 - INFO - __main__ - Step 640 Global step 640 Train loss 1.98 on epoch=319
03/10/2022 18:57:30 - INFO - __main__ - Step 650 Global step 650 Train loss 1.89 on epoch=324
03/10/2022 18:57:31 - INFO - __main__ - Global step 650 Train loss 2.05 QA-F1 0.203125 on epoch=324
03/10/2022 18:57:36 - INFO - __main__ - Step 660 Global step 660 Train loss 1.75 on epoch=329
03/10/2022 18:57:40 - INFO - __main__ - Step 670 Global step 670 Train loss 1.69 on epoch=334
03/10/2022 18:57:44 - INFO - __main__ - Step 680 Global step 680 Train loss 1.64 on epoch=339
03/10/2022 18:57:49 - INFO - __main__ - Step 690 Global step 690 Train loss 1.60 on epoch=344
03/10/2022 18:57:53 - INFO - __main__ - Step 700 Global step 700 Train loss 1.45 on epoch=349
03/10/2022 18:57:54 - INFO - __main__ - Global step 700 Train loss 1.63 QA-F1 0.17708333333333331 on epoch=349
03/10/2022 18:57:59 - INFO - __main__ - Step 710 Global step 710 Train loss 1.50 on epoch=354
03/10/2022 18:58:03 - INFO - __main__ - Step 720 Global step 720 Train loss 1.52 on epoch=359
03/10/2022 18:58:07 - INFO - __main__ - Step 730 Global step 730 Train loss 1.58 on epoch=364
03/10/2022 18:58:12 - INFO - __main__ - Step 740 Global step 740 Train loss 1.52 on epoch=369
03/10/2022 18:58:16 - INFO - __main__ - Step 750 Global step 750 Train loss 1.48 on epoch=374
03/10/2022 18:58:18 - INFO - __main__ - Global step 750 Train loss 1.52 QA-F1 0.17708333333333331 on epoch=374
03/10/2022 18:58:22 - INFO - __main__ - Step 760 Global step 760 Train loss 1.46 on epoch=379
03/10/2022 18:58:26 - INFO - __main__ - Step 770 Global step 770 Train loss 1.57 on epoch=384
03/10/2022 18:58:31 - INFO - __main__ - Step 780 Global step 780 Train loss 1.72 on epoch=389
03/10/2022 18:58:35 - INFO - __main__ - Step 790 Global step 790 Train loss 1.46 on epoch=394
03/10/2022 18:58:39 - INFO - __main__ - Step 800 Global step 800 Train loss 1.42 on epoch=399
03/10/2022 18:58:41 - INFO - __main__ - Global step 800 Train loss 1.53 QA-F1 0.17708333333333331 on epoch=399
03/10/2022 18:58:45 - INFO - __main__ - Step 810 Global step 810 Train loss 1.48 on epoch=404
03/10/2022 18:58:50 - INFO - __main__ - Step 820 Global step 820 Train loss 1.51 on epoch=409
03/10/2022 18:58:54 - INFO - __main__ - Step 830 Global step 830 Train loss 1.47 on epoch=414
03/10/2022 18:58:58 - INFO - __main__ - Step 840 Global step 840 Train loss 1.44 on epoch=419
03/10/2022 18:59:03 - INFO - __main__ - Step 850 Global step 850 Train loss 1.42 on epoch=424
03/10/2022 18:59:04 - INFO - __main__ - Global step 850 Train loss 1.46 QA-F1 0.16666666666666666 on epoch=424
03/10/2022 18:59:09 - INFO - __main__ - Step 860 Global step 860 Train loss 1.42 on epoch=429
03/10/2022 18:59:13 - INFO - __main__ - Step 870 Global step 870 Train loss 1.41 on epoch=434
03/10/2022 18:59:17 - INFO - __main__ - Step 880 Global step 880 Train loss 1.36 on epoch=439
03/10/2022 18:59:22 - INFO - __main__ - Step 890 Global step 890 Train loss 1.39 on epoch=444
03/10/2022 18:59:26 - INFO - __main__ - Step 900 Global step 900 Train loss 1.37 on epoch=449
03/10/2022 18:59:28 - INFO - __main__ - Global step 900 Train loss 1.39 QA-F1 0.14583333333333331 on epoch=449
03/10/2022 18:59:32 - INFO - __main__ - Step 910 Global step 910 Train loss 1.35 on epoch=454
03/10/2022 18:59:36 - INFO - __main__ - Step 920 Global step 920 Train loss 1.36 on epoch=459
03/10/2022 18:59:41 - INFO - __main__ - Step 930 Global step 930 Train loss 1.35 on epoch=464
03/10/2022 18:59:45 - INFO - __main__ - Step 940 Global step 940 Train loss 1.36 on epoch=469
03/10/2022 18:59:49 - INFO - __main__ - Step 950 Global step 950 Train loss 1.27 on epoch=474
03/10/2022 18:59:51 - INFO - __main__ - Global step 950 Train loss 1.34 QA-F1 0.140625 on epoch=474
03/10/2022 18:59:55 - INFO - __main__ - Step 960 Global step 960 Train loss 1.35 on epoch=479
03/10/2022 19:00:00 - INFO - __main__ - Step 970 Global step 970 Train loss 1.43 on epoch=484
03/10/2022 19:00:04 - INFO - __main__ - Step 980 Global step 980 Train loss 1.29 on epoch=489
03/10/2022 19:00:08 - INFO - __main__ - Step 990 Global step 990 Train loss 1.38 on epoch=494
03/10/2022 19:00:13 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.34 on epoch=499
03/10/2022 19:00:14 - INFO - __main__ - Global step 1000 Train loss 1.36 QA-F1 0.16145833333333331 on epoch=499
03/10/2022 19:00:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.29 on epoch=504
03/10/2022 19:00:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.31 on epoch=509
03/10/2022 19:00:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.29 on epoch=514
03/10/2022 19:00:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.22 on epoch=519
03/10/2022 19:00:36 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.34 on epoch=524
03/10/2022 19:00:38 - INFO - __main__ - Global step 1050 Train loss 1.29 QA-F1 0.16145833333333331 on epoch=524
03/10/2022 19:00:42 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.24 on epoch=529
03/10/2022 19:00:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.27 on epoch=534
03/10/2022 19:00:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.30 on epoch=539
03/10/2022 19:00:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.30 on epoch=544
03/10/2022 19:00:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.26 on epoch=549
03/10/2022 19:01:01 - INFO - __main__ - Global step 1100 Train loss 1.28 QA-F1 0.16145833333333331 on epoch=549
03/10/2022 19:01:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.27 on epoch=554
03/10/2022 19:01:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.17 on epoch=559
03/10/2022 19:01:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.24 on epoch=564
03/10/2022 19:01:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.26 on epoch=569
03/10/2022 19:01:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.19 on epoch=574
03/10/2022 19:01:24 - INFO - __main__ - Global step 1150 Train loss 1.23 QA-F1 0.140625 on epoch=574
03/10/2022 19:01:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.23 on epoch=579
03/10/2022 19:01:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.25 on epoch=584
03/10/2022 19:01:37 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.20 on epoch=589
03/10/2022 19:01:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.18 on epoch=594
03/10/2022 19:01:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.17 on epoch=599
03/10/2022 19:01:47 - INFO - __main__ - Global step 1200 Train loss 1.21 QA-F1 0.19791666666666666 on epoch=599
03/10/2022 19:01:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.23 on epoch=604
03/10/2022 19:01:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.10 on epoch=609
03/10/2022 19:02:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.09 on epoch=614
03/10/2022 19:02:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.17 on epoch=619
03/10/2022 19:02:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.16 on epoch=624
03/10/2022 19:02:11 - INFO - __main__ - Global step 1250 Train loss 1.15 QA-F1 0.19791666666666666 on epoch=624
03/10/2022 19:02:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.22 on epoch=629
03/10/2022 19:02:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.09 on epoch=634
03/10/2022 19:02:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.11 on epoch=639
03/10/2022 19:02:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.15 on epoch=644
03/10/2022 19:02:32 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.10 on epoch=649
03/10/2022 19:02:34 - INFO - __main__ - Global step 1300 Train loss 1.13 QA-F1 0.19791666666666666 on epoch=649
03/10/2022 19:02:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.12 on epoch=654
03/10/2022 19:02:43 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.15 on epoch=659
03/10/2022 19:02:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.14 on epoch=664
03/10/2022 19:02:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.16 on epoch=669
03/10/2022 19:02:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.13 on epoch=674
03/10/2022 19:02:57 - INFO - __main__ - Global step 1350 Train loss 1.14 QA-F1 0.19791666666666666 on epoch=674
03/10/2022 19:03:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.03 on epoch=679
03/10/2022 19:03:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.07 on epoch=684
03/10/2022 19:03:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.14 on epoch=689
03/10/2022 19:03:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.07 on epoch=694
03/10/2022 19:03:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.10 on epoch=699
03/10/2022 19:03:20 - INFO - __main__ - Global step 1400 Train loss 1.08 QA-F1 0.19791666666666666 on epoch=699
03/10/2022 19:03:24 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.10 on epoch=704
03/10/2022 19:03:29 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.02 on epoch=709
03/10/2022 19:03:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.09 on epoch=714
03/10/2022 19:03:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.03 on epoch=719
03/10/2022 19:03:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.03 on epoch=724
03/10/2022 19:03:43 - INFO - __main__ - Global step 1450 Train loss 1.05 QA-F1 0.19791666666666666 on epoch=724
03/10/2022 19:03:48 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.09 on epoch=729
03/10/2022 19:03:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.99 on epoch=734
03/10/2022 19:03:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.91 on epoch=739
03/10/2022 19:04:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.97 on epoch=744
03/10/2022 19:04:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.02 on epoch=749
03/10/2022 19:04:07 - INFO - __main__ - Global step 1500 Train loss 1.00 QA-F1 0.17708333333333331 on epoch=749
03/10/2022 19:04:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.94 on epoch=754
03/10/2022 19:04:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.05 on epoch=759
03/10/2022 19:04:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.94 on epoch=764
03/10/2022 19:04:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.01 on epoch=769
03/10/2022 19:04:28 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.02 on epoch=774
03/10/2022 19:04:30 - INFO - __main__ - Global step 1550 Train loss 0.99 QA-F1 0.16666666666666666 on epoch=774
03/10/2022 19:04:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.96 on epoch=779
03/10/2022 19:04:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.04 on epoch=784
03/10/2022 19:04:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.93 on epoch=789
03/10/2022 19:04:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.90 on epoch=794
03/10/2022 19:04:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.08 on epoch=799
03/10/2022 19:04:53 - INFO - __main__ - Global step 1600 Train loss 0.98 QA-F1 0.17708333333333331 on epoch=799
03/10/2022 19:04:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.99 on epoch=804
03/10/2022 19:05:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.91 on epoch=809
03/10/2022 19:05:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.92 on epoch=814
03/10/2022 19:05:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.89 on epoch=819
03/10/2022 19:05:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.85 on epoch=824
03/10/2022 19:05:16 - INFO - __main__ - Global step 1650 Train loss 0.91 QA-F1 0.21354166666666663 on epoch=824
03/10/2022 19:05:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.96 on epoch=829
03/10/2022 19:05:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.90 on epoch=834
03/10/2022 19:05:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.93 on epoch=839
03/10/2022 19:05:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.90 on epoch=844
03/10/2022 19:05:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.78 on epoch=849
03/10/2022 19:05:40 - INFO - __main__ - Global step 1700 Train loss 0.90 QA-F1 0.14583333333333331 on epoch=849
03/10/2022 19:05:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.81 on epoch=854
03/10/2022 19:05:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.78 on epoch=859
03/10/2022 19:05:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.86 on epoch=864
03/10/2022 19:05:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.84 on epoch=869
03/10/2022 19:06:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.80 on epoch=874
03/10/2022 19:06:03 - INFO - __main__ - Global step 1750 Train loss 0.82 QA-F1 0.1875 on epoch=874
03/10/2022 19:06:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.83 on epoch=879
03/10/2022 19:06:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.73 on epoch=884
03/10/2022 19:06:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.81 on epoch=889
03/10/2022 19:06:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.75 on epoch=894
03/10/2022 19:06:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.80 on epoch=899
03/10/2022 19:06:26 - INFO - __main__ - Global step 1800 Train loss 0.78 QA-F1 0.16666666666666666 on epoch=899
03/10/2022 19:06:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.74 on epoch=904
03/10/2022 19:06:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.73 on epoch=909
03/10/2022 19:06:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.71 on epoch=914
03/10/2022 19:06:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.80 on epoch=919
03/10/2022 19:06:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.71 on epoch=924
03/10/2022 19:06:49 - INFO - __main__ - Global step 1850 Train loss 0.74 QA-F1 0.14583333333333331 on epoch=924
03/10/2022 19:06:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.80 on epoch=929
03/10/2022 19:06:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.84 on epoch=934
03/10/2022 19:07:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.74 on epoch=939
03/10/2022 19:07:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.68 on epoch=944
03/10/2022 19:07:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.65 on epoch=949
03/10/2022 19:07:12 - INFO - __main__ - Global step 1900 Train loss 0.74 QA-F1 0.18229166666666666 on epoch=949
03/10/2022 19:07:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.67 on epoch=954
03/10/2022 19:07:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.71 on epoch=959
03/10/2022 19:07:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.66 on epoch=964
03/10/2022 19:07:29 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.74 on epoch=969
03/10/2022 19:07:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.73 on epoch=974
03/10/2022 19:07:35 - INFO - __main__ - Global step 1950 Train loss 0.70 QA-F1 0.21875 on epoch=974
03/10/2022 19:07:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.74 on epoch=979
03/10/2022 19:07:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.68 on epoch=984
03/10/2022 19:07:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.55 on epoch=989
03/10/2022 19:07:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.69 on epoch=994
03/10/2022 19:07:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.66 on epoch=999
03/10/2022 19:07:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:07:58 - INFO - __main__ - Printing 3 examples
03/10/2022 19:07:58 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/10/2022 19:07:58 - INFO - __main__ - ['Esteban Pasquale']
03/10/2022 19:07:58 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/10/2022 19:07:58 - INFO - __main__ - ['Ronnie J. McGorvey']
03/10/2022 19:07:58 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/10/2022 19:07:58 - INFO - __main__ - ['Sam Harper']
03/10/2022 19:07:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 19:07:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:07:58 - INFO - __main__ - Loaded 32 examples from train data
03/10/2022 19:07:58 - INFO - __main__ - Global step 2000 Train loss 0.66 QA-F1 0.16666666666666666 on epoch=999
use DistributedSampler
03/10/2022 19:07:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:07:58 - INFO - __main__ - Printing 3 examples
03/10/2022 19:07:58 - INFO - __main__ - save last model!
03/10/2022 19:07:58 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/10/2022 19:07:58 - INFO - __main__ - ['Red Shadow']
03/10/2022 19:07:58 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/10/2022 19:07:58 - INFO - __main__ - ['Louder Than You Think Studio']
03/10/2022 19:07:58 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/10/2022 19:07:58 - INFO - __main__ - ['Matt', 'Wendy']
03/10/2022 19:07:58 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:07:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 19:07:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:07:58 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:07:59 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 19:07:59 - INFO - __main__ - Printing 3 examples
03/10/2022 19:07:59 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 19:07:59 - INFO - __main__ - ['Frankie']
03/10/2022 19:07:59 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 19:07:59 - INFO - __main__ - ['Frankie']
03/10/2022 19:07:59 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 19:07:59 - INFO - __main__ - ['Frankie']
03/10/2022 19:07:59 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:08:03 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:08:05 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 19:08:12 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 19:08:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 19:08:13 - INFO - __main__ - Starting training!
03/10/2022 19:09:47 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_42_0.5_8_predictions.txt
03/10/2022 19:09:47 - INFO - __main__ - QA-F1 on test data: 0.1857
03/10/2022 19:09:48 - INFO - __main__ - prefix=quoref_32_42, lr=0.5, bsz=8, dev_performance=0.23437499999999997, test_performance=0.1856512662964276
03/10/2022 19:09:48 - INFO - __main__ - Running ... prefix=quoref_32_42, lr=0.4, bsz=8 ...
03/10/2022 19:09:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:09:49 - INFO - __main__ - Printing 3 examples
03/10/2022 19:09:49 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/10/2022 19:09:49 - INFO - __main__ - ['Esteban Pasquale']
03/10/2022 19:09:49 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/10/2022 19:09:49 - INFO - __main__ - ['Ronnie J. McGorvey']
03/10/2022 19:09:49 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/10/2022 19:09:49 - INFO - __main__ - ['Sam Harper']
03/10/2022 19:09:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 19:09:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:09:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:09:49 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:09:49 - INFO - __main__ - Printing 3 examples
03/10/2022 19:09:49 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/10/2022 19:09:49 - INFO - __main__ - ['Red Shadow']
03/10/2022 19:09:49 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/10/2022 19:09:49 - INFO - __main__ - ['Louder Than You Think Studio']
03/10/2022 19:09:49 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/10/2022 19:09:49 - INFO - __main__ - ['Matt', 'Wendy']
03/10/2022 19:09:49 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:09:49 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:09:49 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:10:03 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 19:10:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 19:10:03 - INFO - __main__ - Starting training!
03/10/2022 19:10:08 - INFO - __main__ - Step 10 Global step 10 Train loss 3.25 on epoch=4
03/10/2022 19:10:13 - INFO - __main__ - Step 20 Global step 20 Train loss 2.46 on epoch=9
03/10/2022 19:10:17 - INFO - __main__ - Step 30 Global step 30 Train loss 2.24 on epoch=14
03/10/2022 19:10:21 - INFO - __main__ - Step 40 Global step 40 Train loss 1.99 on epoch=19
03/10/2022 19:10:26 - INFO - __main__ - Step 50 Global step 50 Train loss 1.66 on epoch=24
03/10/2022 19:10:27 - INFO - __main__ - Global step 50 Train loss 2.32 QA-F1 0.13541666666666666 on epoch=24
03/10/2022 19:10:27 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.13541666666666666 on epoch=24, global_step=50
03/10/2022 19:10:32 - INFO - __main__ - Step 60 Global step 60 Train loss 1.68 on epoch=29
03/10/2022 19:10:36 - INFO - __main__ - Step 70 Global step 70 Train loss 1.86 on epoch=34
03/10/2022 19:10:40 - INFO - __main__ - Step 80 Global step 80 Train loss 1.68 on epoch=39
03/10/2022 19:10:45 - INFO - __main__ - Step 90 Global step 90 Train loss 1.56 on epoch=44
03/10/2022 19:10:49 - INFO - __main__ - Step 100 Global step 100 Train loss 1.54 on epoch=49
03/10/2022 19:10:50 - INFO - __main__ - Global step 100 Train loss 1.67 QA-F1 0.16666666666666666 on epoch=49
03/10/2022 19:10:50 - INFO - __main__ - Saving model with best QA-F1: 0.13541666666666666 -> 0.16666666666666666 on epoch=49, global_step=100
03/10/2022 19:10:55 - INFO - __main__ - Step 110 Global step 110 Train loss 1.90 on epoch=54
03/10/2022 19:10:59 - INFO - __main__ - Step 120 Global step 120 Train loss 1.61 on epoch=59
03/10/2022 19:11:03 - INFO - __main__ - Step 130 Global step 130 Train loss 1.75 on epoch=64
03/10/2022 19:11:08 - INFO - __main__ - Step 140 Global step 140 Train loss 1.59 on epoch=69
03/10/2022 19:11:12 - INFO - __main__ - Step 150 Global step 150 Train loss 1.69 on epoch=74
03/10/2022 19:11:14 - INFO - __main__ - Global step 150 Train loss 1.71 QA-F1 0.140625 on epoch=74
03/10/2022 19:11:18 - INFO - __main__ - Step 160 Global step 160 Train loss 1.55 on epoch=79
03/10/2022 19:11:22 - INFO - __main__ - Step 170 Global step 170 Train loss 1.84 on epoch=84
03/10/2022 19:11:27 - INFO - __main__ - Step 180 Global step 180 Train loss 1.82 on epoch=89
03/10/2022 19:11:31 - INFO - __main__ - Step 190 Global step 190 Train loss 1.60 on epoch=94
03/10/2022 19:11:35 - INFO - __main__ - Step 200 Global step 200 Train loss 1.67 on epoch=99
03/10/2022 19:11:37 - INFO - __main__ - Global step 200 Train loss 1.69 QA-F1 0.21354166666666666 on epoch=99
03/10/2022 19:11:37 - INFO - __main__ - Saving model with best QA-F1: 0.16666666666666666 -> 0.21354166666666666 on epoch=99, global_step=200
03/10/2022 19:11:41 - INFO - __main__ - Step 210 Global step 210 Train loss 1.62 on epoch=104
03/10/2022 19:11:45 - INFO - __main__ - Step 220 Global step 220 Train loss 1.57 on epoch=109
03/10/2022 19:11:50 - INFO - __main__ - Step 230 Global step 230 Train loss 1.59 on epoch=114
03/10/2022 19:11:54 - INFO - __main__ - Step 240 Global step 240 Train loss 1.59 on epoch=119
03/10/2022 19:11:58 - INFO - __main__ - Step 250 Global step 250 Train loss 1.52 on epoch=124
03/10/2022 19:12:00 - INFO - __main__ - Global step 250 Train loss 1.58 QA-F1 0.18229166666666666 on epoch=124
03/10/2022 19:12:04 - INFO - __main__ - Step 260 Global step 260 Train loss 1.58 on epoch=129
03/10/2022 19:12:09 - INFO - __main__ - Step 270 Global step 270 Train loss 1.54 on epoch=134
03/10/2022 19:12:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.51 on epoch=139
03/10/2022 19:12:17 - INFO - __main__ - Step 290 Global step 290 Train loss 1.65 on epoch=144
03/10/2022 19:12:22 - INFO - __main__ - Step 300 Global step 300 Train loss 1.60 on epoch=149
03/10/2022 19:12:23 - INFO - __main__ - Global step 300 Train loss 1.58 QA-F1 0.18229166666666666 on epoch=149
03/10/2022 19:12:27 - INFO - __main__ - Step 310 Global step 310 Train loss 1.55 on epoch=154
03/10/2022 19:12:32 - INFO - __main__ - Step 320 Global step 320 Train loss 1.48 on epoch=159
03/10/2022 19:12:36 - INFO - __main__ - Step 330 Global step 330 Train loss 1.55 on epoch=164
03/10/2022 19:12:40 - INFO - __main__ - Step 340 Global step 340 Train loss 1.48 on epoch=169
03/10/2022 19:12:45 - INFO - __main__ - Step 350 Global step 350 Train loss 1.40 on epoch=174
03/10/2022 19:12:46 - INFO - __main__ - Global step 350 Train loss 1.49 QA-F1 0.13541666666666666 on epoch=174
03/10/2022 19:12:51 - INFO - __main__ - Step 360 Global step 360 Train loss 1.42 on epoch=179
03/10/2022 19:12:55 - INFO - __main__ - Step 370 Global step 370 Train loss 1.37 on epoch=184
03/10/2022 19:12:59 - INFO - __main__ - Step 380 Global step 380 Train loss 1.37 on epoch=189
03/10/2022 19:13:03 - INFO - __main__ - Step 390 Global step 390 Train loss 1.41 on epoch=194
03/10/2022 19:13:08 - INFO - __main__ - Step 400 Global step 400 Train loss 1.36 on epoch=199
03/10/2022 19:13:09 - INFO - __main__ - Global step 400 Train loss 1.39 QA-F1 0.14583333333333331 on epoch=199
03/10/2022 19:13:14 - INFO - __main__ - Step 410 Global step 410 Train loss 1.29 on epoch=204
03/10/2022 19:13:18 - INFO - __main__ - Step 420 Global step 420 Train loss 1.39 on epoch=209
03/10/2022 19:13:22 - INFO - __main__ - Step 430 Global step 430 Train loss 1.47 on epoch=214
03/10/2022 19:13:27 - INFO - __main__ - Step 440 Global step 440 Train loss 1.31 on epoch=219
03/10/2022 19:13:31 - INFO - __main__ - Step 450 Global step 450 Train loss 1.23 on epoch=224
03/10/2022 19:13:32 - INFO - __main__ - Global step 450 Train loss 1.34 QA-F1 0.09895833333333333 on epoch=224
03/10/2022 19:13:37 - INFO - __main__ - Step 460 Global step 460 Train loss 1.32 on epoch=229
03/10/2022 19:13:41 - INFO - __main__ - Step 470 Global step 470 Train loss 1.27 on epoch=234
03/10/2022 19:13:45 - INFO - __main__ - Step 480 Global step 480 Train loss 1.20 on epoch=239
03/10/2022 19:13:50 - INFO - __main__ - Step 490 Global step 490 Train loss 1.23 on epoch=244
03/10/2022 19:13:54 - INFO - __main__ - Step 500 Global step 500 Train loss 1.30 on epoch=249
03/10/2022 19:13:56 - INFO - __main__ - Global step 500 Train loss 1.26 QA-F1 0.13541666666666666 on epoch=249
03/10/2022 19:14:00 - INFO - __main__ - Step 510 Global step 510 Train loss 1.27 on epoch=254
03/10/2022 19:14:04 - INFO - __main__ - Step 520 Global step 520 Train loss 1.22 on epoch=259
03/10/2022 19:14:09 - INFO - __main__ - Step 530 Global step 530 Train loss 1.07 on epoch=264
03/10/2022 19:14:13 - INFO - __main__ - Step 540 Global step 540 Train loss 1.04 on epoch=269
03/10/2022 19:14:17 - INFO - __main__ - Step 550 Global step 550 Train loss 1.13 on epoch=274
03/10/2022 19:14:19 - INFO - __main__ - Global step 550 Train loss 1.14 QA-F1 0.13541666666666666 on epoch=274
03/10/2022 19:14:23 - INFO - __main__ - Step 560 Global step 560 Train loss 1.14 on epoch=279
03/10/2022 19:14:27 - INFO - __main__ - Step 570 Global step 570 Train loss 1.07 on epoch=284
03/10/2022 19:14:32 - INFO - __main__ - Step 580 Global step 580 Train loss 1.12 on epoch=289
03/10/2022 19:14:36 - INFO - __main__ - Step 590 Global step 590 Train loss 1.00 on epoch=294
03/10/2022 19:14:40 - INFO - __main__ - Step 600 Global step 600 Train loss 1.03 on epoch=299
03/10/2022 19:14:42 - INFO - __main__ - Global step 600 Train loss 1.07 QA-F1 0.13541666666666666 on epoch=299
03/10/2022 19:14:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.98 on epoch=304
03/10/2022 19:14:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.91 on epoch=309
03/10/2022 19:14:55 - INFO - __main__ - Step 630 Global step 630 Train loss 1.04 on epoch=314
03/10/2022 19:14:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.92 on epoch=319
03/10/2022 19:15:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.91 on epoch=324
03/10/2022 19:15:05 - INFO - __main__ - Global step 650 Train loss 0.95 QA-F1 0.13541666666666666 on epoch=324
03/10/2022 19:15:09 - INFO - __main__ - Step 660 Global step 660 Train loss 1.01 on epoch=329
03/10/2022 19:15:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.92 on epoch=334
03/10/2022 19:15:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.99 on epoch=339
03/10/2022 19:15:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.91 on epoch=344
03/10/2022 19:15:26 - INFO - __main__ - Step 700 Global step 700 Train loss 0.89 on epoch=349
03/10/2022 19:15:28 - INFO - __main__ - Global step 700 Train loss 0.95 QA-F1 0.15104166666666666 on epoch=349
03/10/2022 19:15:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.86 on epoch=354
03/10/2022 19:15:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.75 on epoch=359
03/10/2022 19:15:41 - INFO - __main__ - Step 730 Global step 730 Train loss 0.89 on epoch=364
03/10/2022 19:15:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.88 on epoch=369
03/10/2022 19:15:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.74 on epoch=374
03/10/2022 19:15:51 - INFO - __main__ - Global step 750 Train loss 0.82 QA-F1 0.22916666666666666 on epoch=374
03/10/2022 19:15:51 - INFO - __main__ - Saving model with best QA-F1: 0.21354166666666666 -> 0.22916666666666666 on epoch=374, global_step=750
03/10/2022 19:15:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.85 on epoch=379
03/10/2022 19:15:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.70 on epoch=384
03/10/2022 19:16:04 - INFO - __main__ - Step 780 Global step 780 Train loss 0.85 on epoch=389
03/10/2022 19:16:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.77 on epoch=394
03/10/2022 19:16:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.73 on epoch=399
03/10/2022 19:16:14 - INFO - __main__ - Global step 800 Train loss 0.78 QA-F1 0.11979166666666666 on epoch=399
03/10/2022 19:16:18 - INFO - __main__ - Step 810 Global step 810 Train loss 0.66 on epoch=404
03/10/2022 19:16:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.68 on epoch=409
03/10/2022 19:16:27 - INFO - __main__ - Step 830 Global step 830 Train loss 0.77 on epoch=414
03/10/2022 19:16:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.69 on epoch=419
03/10/2022 19:16:36 - INFO - __main__ - Step 850 Global step 850 Train loss 0.68 on epoch=424
03/10/2022 19:16:37 - INFO - __main__ - Global step 850 Train loss 0.70 QA-F1 0.18229166666666666 on epoch=424
03/10/2022 19:16:41 - INFO - __main__ - Step 860 Global step 860 Train loss 0.69 on epoch=429
03/10/2022 19:16:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.56 on epoch=434
03/10/2022 19:16:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.59 on epoch=439
03/10/2022 19:16:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.49 on epoch=444
03/10/2022 19:16:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.65 on epoch=449
03/10/2022 19:17:00 - INFO - __main__ - Global step 900 Train loss 0.60 QA-F1 0.11979166666666666 on epoch=449
03/10/2022 19:17:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.58 on epoch=454
03/10/2022 19:17:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.49 on epoch=459
03/10/2022 19:17:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=464
03/10/2022 19:17:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.56 on epoch=469
03/10/2022 19:17:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.49 on epoch=474
03/10/2022 19:17:23 - INFO - __main__ - Global step 950 Train loss 0.52 QA-F1 0.08854166666666666 on epoch=474
03/10/2022 19:17:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.56 on epoch=479
03/10/2022 19:17:31 - INFO - __main__ - Step 970 Global step 970 Train loss 0.49 on epoch=484
03/10/2022 19:17:36 - INFO - __main__ - Step 980 Global step 980 Train loss 0.53 on epoch=489
03/10/2022 19:17:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=494
03/10/2022 19:17:44 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.51 on epoch=499
03/10/2022 19:17:46 - INFO - __main__ - Global step 1000 Train loss 0.51 QA-F1 0.140625 on epoch=499
03/10/2022 19:17:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.59 on epoch=504
03/10/2022 19:17:54 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=509
03/10/2022 19:17:59 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.46 on epoch=514
03/10/2022 19:18:03 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.52 on epoch=519
03/10/2022 19:18:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=524
03/10/2022 19:18:09 - INFO - __main__ - Global step 1050 Train loss 0.50 QA-F1 0.16666666666666666 on epoch=524
03/10/2022 19:18:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=529
03/10/2022 19:18:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=534
03/10/2022 19:18:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.44 on epoch=539
03/10/2022 19:18:26 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.41 on epoch=544
03/10/2022 19:18:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.49 on epoch=549
03/10/2022 19:18:32 - INFO - __main__ - Global step 1100 Train loss 0.43 QA-F1 0.11979166666666666 on epoch=549
03/10/2022 19:18:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.53 on epoch=554
03/10/2022 19:18:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.50 on epoch=559
03/10/2022 19:18:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.45 on epoch=564
03/10/2022 19:18:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.52 on epoch=569
03/10/2022 19:18:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=574
03/10/2022 19:18:55 - INFO - __main__ - Global step 1150 Train loss 0.47 QA-F1 0.09375 on epoch=574
03/10/2022 19:18:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.36 on epoch=579
03/10/2022 19:19:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=584
03/10/2022 19:19:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.43 on epoch=589
03/10/2022 19:19:12 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=594
03/10/2022 19:19:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.40 on epoch=599
03/10/2022 19:19:18 - INFO - __main__ - Global step 1200 Train loss 0.40 QA-F1 0.14583333333333331 on epoch=599
03/10/2022 19:19:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.42 on epoch=604
03/10/2022 19:19:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.29 on epoch=609
03/10/2022 19:19:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=614
03/10/2022 19:19:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=619
03/10/2022 19:19:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=624
03/10/2022 19:19:40 - INFO - __main__ - Global step 1250 Train loss 0.33 QA-F1 0.140625 on epoch=624
03/10/2022 19:19:45 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=629
03/10/2022 19:19:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=634
03/10/2022 19:19:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.42 on epoch=639
03/10/2022 19:19:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=644
03/10/2022 19:20:02 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/10/2022 19:20:03 - INFO - __main__ - Global step 1300 Train loss 0.32 QA-F1 0.15625 on epoch=649
03/10/2022 19:20:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=654
03/10/2022 19:20:12 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=659
03/10/2022 19:20:16 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.30 on epoch=664
03/10/2022 19:20:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.32 on epoch=669
03/10/2022 19:20:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=674
03/10/2022 19:20:26 - INFO - __main__ - Global step 1350 Train loss 0.32 QA-F1 0.19791666666666666 on epoch=674
03/10/2022 19:20:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.29 on epoch=679
03/10/2022 19:20:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.28 on epoch=684
03/10/2022 19:20:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=689
03/10/2022 19:20:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=694
03/10/2022 19:20:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=699
03/10/2022 19:20:49 - INFO - __main__ - Global step 1400 Train loss 0.28 QA-F1 0.14583333333333331 on epoch=699
03/10/2022 19:20:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=704
03/10/2022 19:20:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=709
03/10/2022 19:21:02 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=714
03/10/2022 19:21:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=719
03/10/2022 19:21:11 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=724
03/10/2022 19:21:12 - INFO - __main__ - Global step 1450 Train loss 0.28 QA-F1 0.13541666666666666 on epoch=724
03/10/2022 19:21:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=729
03/10/2022 19:21:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.20 on epoch=734
03/10/2022 19:21:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=739
03/10/2022 19:21:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=744
03/10/2022 19:21:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=749
03/10/2022 19:21:35 - INFO - __main__ - Global step 1500 Train loss 0.26 QA-F1 0.16145833333333331 on epoch=749
03/10/2022 19:21:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.27 on epoch=754
03/10/2022 19:21:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=759
03/10/2022 19:21:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=764
03/10/2022 19:21:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=769
03/10/2022 19:21:57 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=774
03/10/2022 19:21:58 - INFO - __main__ - Global step 1550 Train loss 0.24 QA-F1 0.18229166666666666 on epoch=774
03/10/2022 19:22:03 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=779
03/10/2022 19:22:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=784
03/10/2022 19:22:11 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=789
03/10/2022 19:22:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=794
03/10/2022 19:22:20 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
03/10/2022 19:22:21 - INFO - __main__ - Global step 1600 Train loss 0.25 QA-F1 0.22395833333333331 on epoch=799
03/10/2022 19:22:26 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.30 on epoch=804
03/10/2022 19:22:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=809
03/10/2022 19:22:34 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=814
03/10/2022 19:22:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=819
03/10/2022 19:22:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=824
03/10/2022 19:22:44 - INFO - __main__ - Global step 1650 Train loss 0.23 QA-F1 0.13020833333333331 on epoch=824
03/10/2022 19:22:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=829
03/10/2022 19:22:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=834
03/10/2022 19:22:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
03/10/2022 19:23:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=844
03/10/2022 19:23:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.24 on epoch=849
03/10/2022 19:23:07 - INFO - __main__ - Global step 1700 Train loss 0.25 QA-F1 0.17708333333333331 on epoch=849
03/10/2022 19:23:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=854
03/10/2022 19:23:16 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=859
03/10/2022 19:23:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=864
03/10/2022 19:23:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=869
03/10/2022 19:23:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=874
03/10/2022 19:23:30 - INFO - __main__ - Global step 1750 Train loss 0.20 QA-F1 0.11458333333333333 on epoch=874
03/10/2022 19:23:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=879
03/10/2022 19:23:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=884
03/10/2022 19:23:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=889
03/10/2022 19:23:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.21 on epoch=894
03/10/2022 19:23:52 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.25 on epoch=899
03/10/2022 19:23:53 - INFO - __main__ - Global step 1800 Train loss 0.22 QA-F1 0.18229166666666666 on epoch=899
03/10/2022 19:23:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=904
03/10/2022 19:24:02 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=909
03/10/2022 19:24:06 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=914
03/10/2022 19:24:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=919
03/10/2022 19:24:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=924
03/10/2022 19:24:16 - INFO - __main__ - Global step 1850 Train loss 0.18 QA-F1 0.19270833333333331 on epoch=924
03/10/2022 19:24:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=929
03/10/2022 19:24:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=934
03/10/2022 19:24:29 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=939
03/10/2022 19:24:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/10/2022 19:24:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=949
03/10/2022 19:24:39 - INFO - __main__ - Global step 1900 Train loss 0.19 QA-F1 0.109375 on epoch=949
03/10/2022 19:24:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=954
03/10/2022 19:24:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=959
03/10/2022 19:24:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=964
03/10/2022 19:24:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.17 on epoch=969
03/10/2022 19:25:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.16 on epoch=974
03/10/2022 19:25:02 - INFO - __main__ - Global step 1950 Train loss 0.18 QA-F1 0.13020833333333331 on epoch=974
03/10/2022 19:25:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=979
03/10/2022 19:25:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.14 on epoch=984
03/10/2022 19:25:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=989
03/10/2022 19:25:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.22 on epoch=994
03/10/2022 19:25:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.18 on epoch=999
03/10/2022 19:25:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:25:25 - INFO - __main__ - Printing 3 examples
03/10/2022 19:25:25 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/10/2022 19:25:25 - INFO - __main__ - ['Esteban Pasquale']
03/10/2022 19:25:25 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/10/2022 19:25:25 - INFO - __main__ - ['Ronnie J. McGorvey']
03/10/2022 19:25:25 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/10/2022 19:25:25 - INFO - __main__ - ['Sam Harper']
03/10/2022 19:25:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 19:25:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:25:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:25:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:25:25 - INFO - __main__ - Printing 3 examples
03/10/2022 19:25:25 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/10/2022 19:25:25 - INFO - __main__ - ['Red Shadow']
03/10/2022 19:25:25 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/10/2022 19:25:25 - INFO - __main__ - ['Louder Than You Think Studio']
03/10/2022 19:25:25 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/10/2022 19:25:25 - INFO - __main__ - ['Matt', 'Wendy']
03/10/2022 19:25:25 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:25:25 - INFO - __main__ - Global step 2000 Train loss 0.17 QA-F1 0.16145833333333331 on epoch=999
03/10/2022 19:25:25 - INFO - __main__ - save last model!
03/10/2022 19:25:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 19:25:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:25:25 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 19:25:25 - INFO - __main__ - Printing 3 examples
03/10/2022 19:25:25 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 19:25:25 - INFO - __main__ - ['Frankie']
03/10/2022 19:25:25 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 19:25:25 - INFO - __main__ - ['Frankie']
03/10/2022 19:25:25 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 19:25:25 - INFO - __main__ - ['Frankie']
03/10/2022 19:25:25 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:25:25 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:25:29 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:25:32 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 19:25:38 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 19:25:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 19:25:39 - INFO - __main__ - Starting training!
03/10/2022 19:27:15 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_42_0.4_8_predictions.txt
03/10/2022 19:27:15 - INFO - __main__ - QA-F1 on test data: 0.2109
03/10/2022 19:27:15 - INFO - __main__ - prefix=quoref_32_42, lr=0.4, bsz=8, dev_performance=0.22916666666666666, test_performance=0.21093189964157702
03/10/2022 19:27:15 - INFO - __main__ - Running ... prefix=quoref_32_42, lr=0.3, bsz=8 ...
03/10/2022 19:27:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:27:16 - INFO - __main__ - Printing 3 examples
03/10/2022 19:27:16 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/10/2022 19:27:16 - INFO - __main__ - ['Esteban Pasquale']
03/10/2022 19:27:16 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/10/2022 19:27:16 - INFO - __main__ - ['Ronnie J. McGorvey']
03/10/2022 19:27:16 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/10/2022 19:27:16 - INFO - __main__ - ['Sam Harper']
03/10/2022 19:27:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 19:27:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:27:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:27:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:27:16 - INFO - __main__ - Printing 3 examples
03/10/2022 19:27:16 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/10/2022 19:27:16 - INFO - __main__ - ['Red Shadow']
03/10/2022 19:27:16 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/10/2022 19:27:16 - INFO - __main__ - ['Louder Than You Think Studio']
03/10/2022 19:27:16 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/10/2022 19:27:16 - INFO - __main__ - ['Matt', 'Wendy']
03/10/2022 19:27:16 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:27:16 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:27:17 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:27:30 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 19:27:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 19:27:30 - INFO - __main__ - Starting training!
03/10/2022 19:27:35 - INFO - __main__ - Step 10 Global step 10 Train loss 3.31 on epoch=4
03/10/2022 19:27:40 - INFO - __main__ - Step 20 Global step 20 Train loss 3.00 on epoch=9
03/10/2022 19:27:44 - INFO - __main__ - Step 30 Global step 30 Train loss 2.57 on epoch=14
03/10/2022 19:27:48 - INFO - __main__ - Step 40 Global step 40 Train loss 2.18 on epoch=19
03/10/2022 19:27:53 - INFO - __main__ - Step 50 Global step 50 Train loss 2.07 on epoch=24
03/10/2022 19:27:54 - INFO - __main__ - Global step 50 Train loss 2.63 QA-F1 0.19791666666666663 on epoch=24
03/10/2022 19:27:54 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.19791666666666663 on epoch=24, global_step=50
03/10/2022 19:27:59 - INFO - __main__ - Step 60 Global step 60 Train loss 2.16 on epoch=29
03/10/2022 19:28:03 - INFO - __main__ - Step 70 Global step 70 Train loss 1.98 on epoch=34
03/10/2022 19:28:07 - INFO - __main__ - Step 80 Global step 80 Train loss 1.90 on epoch=39
03/10/2022 19:28:12 - INFO - __main__ - Step 90 Global step 90 Train loss 1.87 on epoch=44
03/10/2022 19:28:16 - INFO - __main__ - Step 100 Global step 100 Train loss 1.79 on epoch=49
03/10/2022 19:28:17 - INFO - __main__ - Global step 100 Train loss 1.94 QA-F1 0.14583333333333331 on epoch=49
03/10/2022 19:28:22 - INFO - __main__ - Step 110 Global step 110 Train loss 1.75 on epoch=54
03/10/2022 19:28:26 - INFO - __main__ - Step 120 Global step 120 Train loss 1.66 on epoch=59
03/10/2022 19:28:30 - INFO - __main__ - Step 130 Global step 130 Train loss 1.64 on epoch=64
03/10/2022 19:28:35 - INFO - __main__ - Step 140 Global step 140 Train loss 1.67 on epoch=69
03/10/2022 19:28:39 - INFO - __main__ - Step 150 Global step 150 Train loss 1.51 on epoch=74
03/10/2022 19:28:41 - INFO - __main__ - Global step 150 Train loss 1.65 QA-F1 0.15625 on epoch=74
03/10/2022 19:28:45 - INFO - __main__ - Step 160 Global step 160 Train loss 1.54 on epoch=79
03/10/2022 19:28:49 - INFO - __main__ - Step 170 Global step 170 Train loss 1.58 on epoch=84
03/10/2022 19:28:54 - INFO - __main__ - Step 180 Global step 180 Train loss 1.40 on epoch=89
03/10/2022 19:28:58 - INFO - __main__ - Step 190 Global step 190 Train loss 1.43 on epoch=94
03/10/2022 19:29:02 - INFO - __main__ - Step 200 Global step 200 Train loss 1.36 on epoch=99
03/10/2022 19:29:04 - INFO - __main__ - Global step 200 Train loss 1.46 QA-F1 0.19791666666666663 on epoch=99
03/10/2022 19:29:08 - INFO - __main__ - Step 210 Global step 210 Train loss 1.32 on epoch=104
03/10/2022 19:29:12 - INFO - __main__ - Step 220 Global step 220 Train loss 1.34 on epoch=109
03/10/2022 19:29:17 - INFO - __main__ - Step 230 Global step 230 Train loss 1.25 on epoch=114
03/10/2022 19:29:21 - INFO - __main__ - Step 240 Global step 240 Train loss 1.26 on epoch=119
03/10/2022 19:29:26 - INFO - __main__ - Step 250 Global step 250 Train loss 1.19 on epoch=124
03/10/2022 19:29:27 - INFO - __main__ - Global step 250 Train loss 1.27 QA-F1 0.14583333333333331 on epoch=124
03/10/2022 19:29:31 - INFO - __main__ - Step 260 Global step 260 Train loss 1.16 on epoch=129
03/10/2022 19:29:36 - INFO - __main__ - Step 270 Global step 270 Train loss 1.07 on epoch=134
03/10/2022 19:29:40 - INFO - __main__ - Step 280 Global step 280 Train loss 1.17 on epoch=139
03/10/2022 19:29:44 - INFO - __main__ - Step 290 Global step 290 Train loss 1.08 on epoch=144
03/10/2022 19:29:49 - INFO - __main__ - Step 300 Global step 300 Train loss 1.04 on epoch=149
03/10/2022 19:29:50 - INFO - __main__ - Global step 300 Train loss 1.10 QA-F1 0.17708333333333331 on epoch=149
03/10/2022 19:29:55 - INFO - __main__ - Step 310 Global step 310 Train loss 1.03 on epoch=154
03/10/2022 19:29:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.97 on epoch=159
03/10/2022 19:30:03 - INFO - __main__ - Step 330 Global step 330 Train loss 0.86 on epoch=164
03/10/2022 19:30:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.90 on epoch=169
03/10/2022 19:30:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.92 on epoch=174
03/10/2022 19:30:13 - INFO - __main__ - Global step 350 Train loss 0.93 QA-F1 0.16666666666666666 on epoch=174
03/10/2022 19:30:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.80 on epoch=179
03/10/2022 19:30:22 - INFO - __main__ - Step 370 Global step 370 Train loss 0.83 on epoch=184
03/10/2022 19:30:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.78 on epoch=189
03/10/2022 19:30:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.77 on epoch=194
03/10/2022 19:30:35 - INFO - __main__ - Step 400 Global step 400 Train loss 0.76 on epoch=199
03/10/2022 19:30:37 - INFO - __main__ - Global step 400 Train loss 0.79 QA-F1 0.15625 on epoch=199
03/10/2022 19:30:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.68 on epoch=204
03/10/2022 19:30:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.66 on epoch=209
03/10/2022 19:30:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=214
03/10/2022 19:30:54 - INFO - __main__ - Step 440 Global step 440 Train loss 0.67 on epoch=219
03/10/2022 19:30:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.64 on epoch=224
03/10/2022 19:31:00 - INFO - __main__ - Global step 450 Train loss 0.70 QA-F1 0.16666666666666666 on epoch=224
03/10/2022 19:31:04 - INFO - __main__ - Step 460 Global step 460 Train loss 0.58 on epoch=229
03/10/2022 19:31:09 - INFO - __main__ - Step 470 Global step 470 Train loss 0.67 on epoch=234
03/10/2022 19:31:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=239
03/10/2022 19:31:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.58 on epoch=244
03/10/2022 19:31:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.56 on epoch=249
03/10/2022 19:31:23 - INFO - __main__ - Global step 500 Train loss 0.58 QA-F1 0.13541666666666666 on epoch=249
03/10/2022 19:31:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.69 on epoch=254
03/10/2022 19:31:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.54 on epoch=259
03/10/2022 19:31:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.53 on epoch=264
03/10/2022 19:31:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.71 on epoch=269
03/10/2022 19:31:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.57 on epoch=274
03/10/2022 19:31:46 - INFO - __main__ - Global step 550 Train loss 0.61 QA-F1 0.14583333333333331 on epoch=274
03/10/2022 19:31:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=279
03/10/2022 19:31:55 - INFO - __main__ - Step 570 Global step 570 Train loss 0.55 on epoch=284
03/10/2022 19:31:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=289
03/10/2022 19:32:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.49 on epoch=294
03/10/2022 19:32:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.52 on epoch=299
03/10/2022 19:32:10 - INFO - __main__ - Global step 600 Train loss 0.49 QA-F1 0.13541666666666666 on epoch=299
03/10/2022 19:32:14 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=304
03/10/2022 19:32:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=309
03/10/2022 19:32:23 - INFO - __main__ - Step 630 Global step 630 Train loss 0.39 on epoch=314
03/10/2022 19:32:27 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=319
03/10/2022 19:32:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=324
03/10/2022 19:32:33 - INFO - __main__ - Global step 650 Train loss 0.40 QA-F1 0.18229166666666666 on epoch=324
03/10/2022 19:32:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=329
03/10/2022 19:32:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=334
03/10/2022 19:32:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.42 on epoch=339
03/10/2022 19:32:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=344
03/10/2022 19:32:55 - INFO - __main__ - Step 700 Global step 700 Train loss 0.38 on epoch=349
03/10/2022 19:32:56 - INFO - __main__ - Global step 700 Train loss 0.37 QA-F1 0.11458333333333333 on epoch=349
03/10/2022 19:33:00 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=354
03/10/2022 19:33:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=359
03/10/2022 19:33:09 - INFO - __main__ - Step 730 Global step 730 Train loss 0.64 on epoch=364
03/10/2022 19:33:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.80 on epoch=369
03/10/2022 19:33:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=374
03/10/2022 19:33:19 - INFO - __main__ - Global step 750 Train loss 0.51 QA-F1 0.234375 on epoch=374
03/10/2022 19:33:19 - INFO - __main__ - Saving model with best QA-F1: 0.19791666666666663 -> 0.234375 on epoch=374, global_step=750
03/10/2022 19:33:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=379
03/10/2022 19:33:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=384
03/10/2022 19:33:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=389
03/10/2022 19:33:37 - INFO - __main__ - Step 790 Global step 790 Train loss 0.37 on epoch=394
03/10/2022 19:33:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=399
03/10/2022 19:33:42 - INFO - __main__ - Global step 800 Train loss 0.34 QA-F1 0.10416666666666666 on epoch=399
03/10/2022 19:33:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=404
03/10/2022 19:33:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=409
03/10/2022 19:33:55 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=414
03/10/2022 19:34:00 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=419
03/10/2022 19:34:04 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=424
03/10/2022 19:34:06 - INFO - __main__ - Global step 850 Train loss 0.32 QA-F1 0.11458333333333333 on epoch=424
03/10/2022 19:34:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=429
03/10/2022 19:34:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=434
03/10/2022 19:34:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.29 on epoch=439
03/10/2022 19:34:23 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=444
03/10/2022 19:34:27 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
03/10/2022 19:34:29 - INFO - __main__ - Global step 900 Train loss 0.32 QA-F1 0.16666666666666666 on epoch=449
03/10/2022 19:34:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
03/10/2022 19:34:37 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=459
03/10/2022 19:34:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=464
03/10/2022 19:34:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.30 on epoch=469
03/10/2022 19:34:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=474
03/10/2022 19:34:52 - INFO - __main__ - Global step 950 Train loss 0.25 QA-F1 0.125 on epoch=474
03/10/2022 19:34:56 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=479
03/10/2022 19:35:01 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=484
03/10/2022 19:35:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=489
03/10/2022 19:35:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=494
03/10/2022 19:35:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=499
03/10/2022 19:35:15 - INFO - __main__ - Global step 1000 Train loss 0.23 QA-F1 0.15104166666666666 on epoch=499
03/10/2022 19:35:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=504
03/10/2022 19:35:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.27 on epoch=509
03/10/2022 19:35:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=514
03/10/2022 19:35:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=519
03/10/2022 19:35:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.21 on epoch=524
03/10/2022 19:35:38 - INFO - __main__ - Global step 1050 Train loss 0.20 QA-F1 0.10416666666666666 on epoch=524
03/10/2022 19:35:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.21 on epoch=529
03/10/2022 19:35:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=534
03/10/2022 19:35:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=539
03/10/2022 19:35:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.17 on epoch=544
03/10/2022 19:36:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=549
03/10/2022 19:36:01 - INFO - __main__ - Global step 1100 Train loss 0.20 QA-F1 0.12499999999999999 on epoch=549
03/10/2022 19:36:06 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=554
03/10/2022 19:36:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=559
03/10/2022 19:36:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=564
03/10/2022 19:36:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=569
03/10/2022 19:36:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=574
03/10/2022 19:36:25 - INFO - __main__ - Global step 1150 Train loss 0.17 QA-F1 0.13020833333333331 on epoch=574
03/10/2022 19:36:29 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=579
03/10/2022 19:36:34 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=584
03/10/2022 19:36:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=589
03/10/2022 19:36:42 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=594
03/10/2022 19:36:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.12 on epoch=599
03/10/2022 19:36:48 - INFO - __main__ - Global step 1200 Train loss 0.15 QA-F1 0.14583333333333331 on epoch=599
03/10/2022 19:36:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=604
03/10/2022 19:36:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=609
03/10/2022 19:37:01 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=614
03/10/2022 19:37:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.13 on epoch=619
03/10/2022 19:37:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=624
03/10/2022 19:37:12 - INFO - __main__ - Global step 1250 Train loss 0.18 QA-F1 0.15104166666666666 on epoch=624
03/10/2022 19:37:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=629
03/10/2022 19:37:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.15 on epoch=634
03/10/2022 19:37:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=639
03/10/2022 19:37:29 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=644
03/10/2022 19:37:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=649
03/10/2022 19:37:35 - INFO - __main__ - Global step 1300 Train loss 0.16 QA-F1 0.15625 on epoch=649
03/10/2022 19:37:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.17 on epoch=654
03/10/2022 19:37:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=659
03/10/2022 19:37:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.13 on epoch=664
03/10/2022 19:37:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.14 on epoch=669
03/10/2022 19:37:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=674
03/10/2022 19:37:58 - INFO - __main__ - Global step 1350 Train loss 0.15 QA-F1 0.20833333333333331 on epoch=674
03/10/2022 19:38:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=679
03/10/2022 19:38:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=684
03/10/2022 19:38:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
03/10/2022 19:38:15 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=694
03/10/2022 19:38:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=699
03/10/2022 19:38:21 - INFO - __main__ - Global step 1400 Train loss 0.13 QA-F1 0.16666666666666666 on epoch=699
03/10/2022 19:38:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.09 on epoch=704
03/10/2022 19:38:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.11 on epoch=709
03/10/2022 19:38:34 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.17 on epoch=714
03/10/2022 19:38:38 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=719
03/10/2022 19:38:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=724
03/10/2022 19:38:44 - INFO - __main__ - Global step 1450 Train loss 0.12 QA-F1 0.1875 on epoch=724
03/10/2022 19:38:49 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.13 on epoch=729
03/10/2022 19:38:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.11 on epoch=734
03/10/2022 19:38:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=739
03/10/2022 19:39:02 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=744
03/10/2022 19:39:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=749
03/10/2022 19:39:07 - INFO - __main__ - Global step 1500 Train loss 0.12 QA-F1 0.22395833333333331 on epoch=749
03/10/2022 19:39:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=754
03/10/2022 19:39:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.11 on epoch=759
03/10/2022 19:39:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=764
03/10/2022 19:39:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=769
03/10/2022 19:39:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=774
03/10/2022 19:39:30 - INFO - __main__ - Global step 1550 Train loss 0.11 QA-F1 0.11979166666666666 on epoch=774
03/10/2022 19:39:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.09 on epoch=779
03/10/2022 19:39:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=784
03/10/2022 19:39:43 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=789
03/10/2022 19:39:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=794
03/10/2022 19:39:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=799
03/10/2022 19:39:54 - INFO - __main__ - Global step 1600 Train loss 0.12 QA-F1 0.16666666666666666 on epoch=799
03/10/2022 19:39:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.12 on epoch=804
03/10/2022 19:40:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=809
03/10/2022 19:40:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=814
03/10/2022 19:40:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=819
03/10/2022 19:40:15 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=824
03/10/2022 19:40:17 - INFO - __main__ - Global step 1650 Train loss 0.11 QA-F1 0.18229166666666666 on epoch=824
03/10/2022 19:40:21 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.10 on epoch=829
03/10/2022 19:40:25 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=834
03/10/2022 19:40:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=839
03/10/2022 19:40:34 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.08 on epoch=844
03/10/2022 19:40:38 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.09 on epoch=849
03/10/2022 19:40:40 - INFO - __main__ - Global step 1700 Train loss 0.09 QA-F1 0.16666666666666666 on epoch=849
03/10/2022 19:40:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=854
03/10/2022 19:40:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.09 on epoch=859
03/10/2022 19:40:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.09 on epoch=864
03/10/2022 19:40:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=869
03/10/2022 19:41:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.09 on epoch=874
03/10/2022 19:41:03 - INFO - __main__ - Global step 1750 Train loss 0.10 QA-F1 0.16666666666666666 on epoch=874
03/10/2022 19:41:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.09 on epoch=879
03/10/2022 19:41:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.09 on epoch=884
03/10/2022 19:41:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=889
03/10/2022 19:41:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=894
03/10/2022 19:41:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=899
03/10/2022 19:41:26 - INFO - __main__ - Global step 1800 Train loss 0.08 QA-F1 0.13541666666666666 on epoch=899
03/10/2022 19:41:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
03/10/2022 19:41:34 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.06 on epoch=909
03/10/2022 19:41:39 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=914
03/10/2022 19:41:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.08 on epoch=919
03/10/2022 19:41:47 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.09 on epoch=924
03/10/2022 19:41:49 - INFO - __main__ - Global step 1850 Train loss 0.12 QA-F1 0.18229166666666666 on epoch=924
03/10/2022 19:41:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.11 on epoch=929
03/10/2022 19:41:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=934
03/10/2022 19:42:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=939
03/10/2022 19:42:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
03/10/2022 19:42:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=949
03/10/2022 19:42:12 - INFO - __main__ - Global step 1900 Train loss 0.08 QA-F1 0.23958333333333331 on epoch=949
03/10/2022 19:42:12 - INFO - __main__ - Saving model with best QA-F1: 0.234375 -> 0.23958333333333331 on epoch=949, global_step=1900
03/10/2022 19:42:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.07 on epoch=954
03/10/2022 19:42:21 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=959
03/10/2022 19:42:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=964
03/10/2022 19:42:30 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=969
03/10/2022 19:42:34 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.12 on epoch=974
03/10/2022 19:42:35 - INFO - __main__ - Global step 1950 Train loss 0.09 QA-F1 0.31101190476190477 on epoch=974
03/10/2022 19:42:35 - INFO - __main__ - Saving model with best QA-F1: 0.23958333333333331 -> 0.31101190476190477 on epoch=974, global_step=1950
03/10/2022 19:42:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=979
03/10/2022 19:42:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=984
03/10/2022 19:42:48 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=989
03/10/2022 19:42:53 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.11 on epoch=994
03/10/2022 19:42:57 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
03/10/2022 19:42:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:42:58 - INFO - __main__ - Printing 3 examples
03/10/2022 19:42:58 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/10/2022 19:42:58 - INFO - __main__ - ['Esteban Pasquale']
03/10/2022 19:42:58 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/10/2022 19:42:58 - INFO - __main__ - ['Ronnie J. McGorvey']
03/10/2022 19:42:58 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/10/2022 19:42:58 - INFO - __main__ - ['Sam Harper']
03/10/2022 19:42:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 19:42:59 - INFO - __main__ - Global step 2000 Train loss 0.09 QA-F1 0.22247023809523808 on epoch=999
03/10/2022 19:42:59 - INFO - __main__ - save last model!
03/10/2022 19:42:59 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:42:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 19:42:59 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 19:42:59 - INFO - __main__ - Printing 3 examples
03/10/2022 19:42:59 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 19:42:59 - INFO - __main__ - ['Frankie']
03/10/2022 19:42:59 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:42:59 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 19:42:59 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:42:59 - INFO - __main__ - Printing 3 examples
03/10/2022 19:42:59 - INFO - __main__ - ['Frankie']
03/10/2022 19:42:59 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/10/2022 19:42:59 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 19:42:59 - INFO - __main__ - ['Red Shadow']
03/10/2022 19:42:59 - INFO - __main__ - ['Frankie']
03/10/2022 19:42:59 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/10/2022 19:42:59 - INFO - __main__ - ['Louder Than You Think Studio']
03/10/2022 19:42:59 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:42:59 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/10/2022 19:42:59 - INFO - __main__ - ['Matt', 'Wendy']
03/10/2022 19:42:59 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:42:59 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:42:59 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:43:03 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:43:06 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 19:43:13 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 19:43:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 19:43:13 - INFO - __main__ - Starting training!
03/10/2022 19:44:50 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_42_0.3_8_predictions.txt
03/10/2022 19:44:50 - INFO - __main__ - QA-F1 on test data: 0.2887
03/10/2022 19:44:51 - INFO - __main__ - prefix=quoref_32_42, lr=0.3, bsz=8, dev_performance=0.31101190476190477, test_performance=0.28867455722294433
03/10/2022 19:44:51 - INFO - __main__ - Running ... prefix=quoref_32_42, lr=0.2, bsz=8 ...
03/10/2022 19:44:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:44:52 - INFO - __main__ - Printing 3 examples
03/10/2022 19:44:52 - INFO - __main__ -  [quoref] question: What's the name of the person El Zorro kills? context: Don Diego Vega is urgently called home by his father. To all outward appearances, he is the foppish son of wealthy ranchero and former Alcade Don Alejandro Vega, having returned to California after his military education in Spain.  Don Diego is horrified at the way the common people are now mistreated by the corrupt Alcalde, Luis Quintero, who had forced his father from the position of Alcalde. Don Diego adopts the guise of El Zorro ("The Fox"), a masked outlaw dressed entirely in black, who becomes the defender of the common people and a champion for justice.  In the meantime he romances the Alcalde's beautiful and innocent niece, Lolita, whom he grows to love. As part of his plan, Don Diego simultaneously flirts with the Alcalde's wife Inez, filling her head with tales of Madrid fashion and culture and raising her desire to move there with her corrupt husband, Luis.  In both his guises Don Diego must contend with the governor's ablest henchman, the malevolent Captain Esteban Pasquale. He eventually dispatches the Captain in a fast-moving rapier duel-to-the-death, forcing a regime change; Don Diego's plan all along.
03/10/2022 19:44:52 - INFO - __main__ - ['Esteban Pasquale']
03/10/2022 19:44:52 - INFO - __main__ -  [quoref] question: What is the full name of the person with pedophilia? context: Sarah Pierce is a hapless, stay-at-home mother in a small suburb of Boston. She had been working on a doctorate in English, but set aside her work to marry Richard, and raise their 3-year-old daughter, Lucy. Her marriage falls apart when she discovers that Richard is addicted to online pornography. Sarah meets Brad Adamson, a law student who brings his 4-year-old son, Aaron, to the park. Brad is married to Kathy, and although their marriage is loving and amicable, it has been lacking intimacy. When Brad is supposed to be studying for the bar exam, he instead plays on a local football team or sits and watches teenagers skateboard outside his house, fantasizing about being young and carefree again. Brad and Sarah become friendly and, on a dare, kiss in the park, scandalizing the other park parents. They are instantly attracted to each other, but resolve to keep their relationship platonic. One day, several parents panic when they see sex offender Ronnie J. McGorvey, who was recently released from prison, swimming in the pool with the children. After Ronnie is escorted away by the police, it begins to rain. Sarah and Brad take Lucy and Aaron back to her house and put the kids to bed. Brad looks at one of Sarah's books and finds a photo of him in it. While Sarah is drying towels in her basement, Brad kisses her and they have sex. Brad's friend, Larry Hedges, is a former police officer who was forced to retire when he accidentally shot a teenager at a local mall. Now he is estranged from his wife and spends much of his time harassing Ronnie. Ronnie lives with his mother, May, who believes that meeting a woman his own age would cure him of his pedophilia. Ronnie knows this is futile, but agrees to go on a date May has arranged for him with a woman named Sheila.
03/10/2022 19:44:52 - INFO - __main__ - ['Ronnie J. McGorvey']
03/10/2022 19:44:52 - INFO - __main__ -  [quoref] question: What is the name of the person that is handed the shaving kit from the lawyer? context: Sam Harper, a struggling corporate trader in New York City, is in trouble after one of his deals violates federal law and the Federal Trade Commission threatens him with an investigation. Sam's boss urges him to bribe federal officials, at Sam's own expense. Returning home, Sam learns from his girlfriend Hannah that Jerry, his estranged father, has died in L.A. of cancer. Sam tries to avoid attending the funeral, but Hannah insists on making arrangements. After flying home to L.A., he stays with Hannah at Jerry's house and has a tense reunion with his mother Lillian. Sam meets with his father's lawyer and friend, who tells him that the will leaves Sam no money. However, the lawyer hands him a shaving kit. Inside is $150,000 in cash and a note stipulating that the money be delivered to "Josh Davis." Josh turns out to be a troubled 11-year-old whose single mother, Frankie Davis, is a recovering alcoholic and bartender. Sam secretly follows Frankie to an Alcoholics Anonymous meeting, where she reveals to the group that she is Jerry's illegitimate daughter. Sam realizes that Frankie is his paternal half-sister, and Josh his nephew. Sam tells Hannah the news, and his intention of keeping the money for himself. This disgusts her, and she returns to New York, leaving Sam with Lillian.
03/10/2022 19:44:52 - INFO - __main__ - ['Sam Harper']
03/10/2022 19:44:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 19:44:52 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:44:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 19:44:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 19:44:53 - INFO - __main__ - Printing 3 examples
03/10/2022 19:44:53 - INFO - __main__ -  [quoref] question: What is the alias name of the person who loses the respect of the Riffs? context: French General Birabeau has been sent to Morocco to root out and destroy the Riffs, a band of Arab rebels, who threaten the safety of the French outpost in the Moroccan desert.  Their dashing, daredevil leader is the mysterious "Red Shadow".  Margot Bonvalet, a lovely, sassy French girl, is soon to be married at the fort to Birabeau's right-hand man, Captain Fontaine.  Birabeau's son Pierre, in reality the Red Shadow, loves Margot, but pretends to be a milksop to preserve his secret identity.  Margot tells Pierre that she secretly yearns to be swept into the arms of some bold, dashing sheik, perhaps even the Red Shadow himself.  Pierre, as the Red Shadow, kidnaps Margot and declares his love for her. To her surprise, Margot's mysterious abductor treats her with every Western consideration.  When the Red Shadow comes face to face with General Birabeau, the old man challenges the rebel leader to a duel.  Of course Pierre will not kill his own father, so he refuses to fight, losing the respect of the Riffs.  Azuri, the sinuous and secretive native dancing girl, might be persuaded to answer some of these riddles if only she can be persuaded by Captain Fontaine.  Meanwhile, two other characters, Benny (a reporter) and Susan provide comic relief.  Eventually, the Red Shadow's identity is discovered, a deal is struck with the Riffs, and Pierre and Margot live happily ever after.
03/10/2022 19:44:53 - INFO - __main__ - ['Red Shadow']
03/10/2022 19:44:53 - INFO - __main__ -  [quoref] question: What was the name of the studio that Slay Tracks was recorded? context: Pavement was formed in 1989 in Stockton, California, by Stephen Malkmus and Scott Kannberg. Malkmus and Kannberg had previously performed together in the band Bag O' Bones. Pavement had its start playing at open mike nights at clubs and bars. The songs the band played during this time were mostly covers, although they also performed many original songs that would later be released on Slay Tracks. Malkmus recalls, "It was pretty reasonable to be able to make a single for $1,000, so we decided to go for it. We didn't have any real plans because we weren't a real band." Two local studios existed in Stockton, the cheaper and less professionally minded of which was Gary Young's Louder Than You Think Studio. The band decided to record at Young's studio due to their admiration of other local punk bands who had recorded there, including The Young Pioneers and The Authorities. Kannberg reportedly borrowed $800 from his father to record Slay Tracks.Slay Tracks was recorded during a four-hour session on January 17, 1989, at Young's studio. Kannberg, describing the studio and the recording process, said, "You go into his house and it's stuff everywhere, old dogs lying around, big pot plants everywhere, and Gary tells us that he got all his equipment by selling pot! It was us going in and pretty much just laying down the songs with a guide guitar and a detuned guitar through a bass amp and then we'd play drums over the top." Young, though bewildered by the band's sound, contributed by playing drums. He recalled, "[Malkmus and Kannberg] come in and they play this weird guitar noise and it just sounds like noise, with no background. My drums were in there so I said, 'Should I drum?' and they said 'Okay.'" Kannberg said, "We did it really fast. We probably spent one day tracking and one day mixing it." The title of the EP had been decided prior to its recording, and the pseudonyms S.M. and Spiral Stairs were used to credit Malkmus and Kannberg respectively.
03/10/2022 19:44:53 - INFO - __main__ - ['Louder Than You Think Studio']
03/10/2022 19:44:53 - INFO - __main__ -  [quoref] question: Who rides to the party in the convertible Barry steals? context: Matt Franklin is a recent MIT graduate who works at a Los Angeles Suncoast Video store in 1988 while trying to figure out what he wants to do with his life, something that his police officer father has grown impatient with. While working one day, Matt's high school crush, Tori Frederking walks into the store. After pretending that he doesn't work there and saying that he works at Goldman Sachs in an effort to impress her, Tori invites Matt to a Labor Day party, hosted by Matt's twin sister Wendy's boyfriend, Kyle Masterson, at his hillside home. Later that night, Matt, Wendy, and Matt's best friend, Barry Nathan, head to the party. On the drive over, Barry steals a brand new Mercedes-Benz convertible from the car dealership he got fired from earlier that day, justifying his actions by saying that Matt needs the convertible if he really wants to impress Tori. The trio arrive at the party. While there, Matt catches up with an old classmate (who actually works at Goldman Sachs) and then awkwardly tries to woo Tori. Barry snorts some cocaine he found in the glove box of the stolen convertible and gets involved in a dance-off, and Wendy's boyfriend proposes to her in front of everyone at the party. She says yes, upsetting Matt, who doesn't think that Kyle will support her in her dream to attend graduate school at the University of Cambridge. Tori eventually invites Matt and Barry to another party her boss is hosting in Beverly Hills. Matt takes Tori there in the Mercedes, while Barry rides with her two friends in another car, using the cocaine as an enticement to let him go along. Barry has a wild sexual encounter with an older woman while Matt and Tori continue to mingle with each other, after Matt's successful 'put down' of Tori's boss, a habitual sexual harasser. They leave the party to go into a neighbor's backyard where they jump on a trampoline, play truth or dare, and end up having sex.
03/10/2022 19:44:53 - INFO - __main__ - ['Matt', 'Wendy']
03/10/2022 19:44:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 19:44:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 19:44:53 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 19:45:06 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 19:45:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 19:45:06 - INFO - __main__ - Starting training!
03/10/2022 19:45:11 - INFO - __main__ - Step 10 Global step 10 Train loss 3.44 on epoch=4
03/10/2022 19:45:16 - INFO - __main__ - Step 20 Global step 20 Train loss 2.95 on epoch=9
03/10/2022 19:45:20 - INFO - __main__ - Step 30 Global step 30 Train loss 2.57 on epoch=14
03/10/2022 19:45:24 - INFO - __main__ - Step 40 Global step 40 Train loss 2.38 on epoch=19
03/10/2022 19:45:29 - INFO - __main__ - Step 50 Global step 50 Train loss 2.03 on epoch=24
03/10/2022 19:45:30 - INFO - __main__ - Global step 50 Train loss 2.67 QA-F1 0.19270833333333331 on epoch=24
03/10/2022 19:45:30 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.19270833333333331 on epoch=24, global_step=50
03/10/2022 19:45:35 - INFO - __main__ - Step 60 Global step 60 Train loss 2.04 on epoch=29
03/10/2022 19:45:39 - INFO - __main__ - Step 70 Global step 70 Train loss 2.03 on epoch=34
03/10/2022 19:45:43 - INFO - __main__ - Step 80 Global step 80 Train loss 1.78 on epoch=39
03/10/2022 19:45:48 - INFO - __main__ - Step 90 Global step 90 Train loss 1.75 on epoch=44
03/10/2022 19:45:52 - INFO - __main__ - Step 100 Global step 100 Train loss 1.69 on epoch=49
03/10/2022 19:45:54 - INFO - __main__ - Global step 100 Train loss 1.86 QA-F1 0.11979166666666666 on epoch=49
03/10/2022 19:45:58 - INFO - __main__ - Step 110 Global step 110 Train loss 1.54 on epoch=54
03/10/2022 19:46:02 - INFO - __main__ - Step 120 Global step 120 Train loss 1.57 on epoch=59
03/10/2022 19:46:07 - INFO - __main__ - Step 130 Global step 130 Train loss 1.41 on epoch=64
03/10/2022 19:46:11 - INFO - __main__ - Step 140 Global step 140 Train loss 1.49 on epoch=69
03/10/2022 19:46:16 - INFO - __main__ - Step 150 Global step 150 Train loss 1.43 on epoch=74
03/10/2022 19:46:17 - INFO - __main__ - Global step 150 Train loss 1.49 QA-F1 0.13541666666666666 on epoch=74
03/10/2022 19:46:22 - INFO - __main__ - Step 160 Global step 160 Train loss 1.31 on epoch=79
03/10/2022 19:46:26 - INFO - __main__ - Step 170 Global step 170 Train loss 1.32 on epoch=84
03/10/2022 19:46:30 - INFO - __main__ - Step 180 Global step 180 Train loss 1.25 on epoch=89
03/10/2022 19:46:35 - INFO - __main__ - Step 190 Global step 190 Train loss 1.18 on epoch=94
03/10/2022 19:46:39 - INFO - __main__ - Step 200 Global step 200 Train loss 1.34 on epoch=99
03/10/2022 19:46:40 - INFO - __main__ - Global step 200 Train loss 1.28 QA-F1 0.15625 on epoch=99
03/10/2022 19:46:45 - INFO - __main__ - Step 210 Global step 210 Train loss 1.16 on epoch=104
03/10/2022 19:46:49 - INFO - __main__ - Step 220 Global step 220 Train loss 1.15 on epoch=109
03/10/2022 19:46:54 - INFO - __main__ - Step 230 Global step 230 Train loss 1.09 on epoch=114
03/10/2022 19:46:58 - INFO - __main__ - Step 240 Global step 240 Train loss 1.16 on epoch=119
03/10/2022 19:47:02 - INFO - __main__ - Step 250 Global step 250 Train loss 1.17 on epoch=124
03/10/2022 19:47:04 - INFO - __main__ - Global step 250 Train loss 1.15 QA-F1 0.10937499999999999 on epoch=124
03/10/2022 19:47:08 - INFO - __main__ - Step 260 Global step 260 Train loss 1.10 on epoch=129
03/10/2022 19:47:13 - INFO - __main__ - Step 270 Global step 270 Train loss 1.18 on epoch=134
03/10/2022 19:47:17 - INFO - __main__ - Step 280 Global step 280 Train loss 1.09 on epoch=139
03/10/2022 19:47:21 - INFO - __main__ - Step 290 Global step 290 Train loss 1.08 on epoch=144
03/10/2022 19:47:26 - INFO - __main__ - Step 300 Global step 300 Train loss 1.10 on epoch=149
03/10/2022 19:47:27 - INFO - __main__ - Global step 300 Train loss 1.11 QA-F1 0.10937499999999999 on epoch=149
03/10/2022 19:47:32 - INFO - __main__ - Step 310 Global step 310 Train loss 1.11 on epoch=154
03/10/2022 19:47:36 - INFO - __main__ - Step 320 Global step 320 Train loss 1.07 on epoch=159
03/10/2022 19:47:40 - INFO - __main__ - Step 330 Global step 330 Train loss 1.05 on epoch=164
03/10/2022 19:47:45 - INFO - __main__ - Step 340 Global step 340 Train loss 1.22 on epoch=169
03/10/2022 19:47:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.98 on epoch=174
03/10/2022 19:47:51 - INFO - __main__ - Global step 350 Train loss 1.09 QA-F1 0.12499999999999999 on epoch=174
03/10/2022 19:47:55 - INFO - __main__ - Step 360 Global step 360 Train loss 1.08 on epoch=179
03/10/2022 19:47:59 - INFO - __main__ - Step 370 Global step 370 Train loss 1.07 on epoch=184
03/10/2022 19:48:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.91 on epoch=189
03/10/2022 19:48:08 - INFO - __main__ - Step 390 Global step 390 Train loss 1.00 on epoch=194
03/10/2022 19:48:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.90 on epoch=199
03/10/2022 19:48:14 - INFO - __main__ - Global step 400 Train loss 0.99 QA-F1 0.10416666666666666 on epoch=199
03/10/2022 19:48:18 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=204
03/10/2022 19:48:23 - INFO - __main__ - Step 420 Global step 420 Train loss 0.95 on epoch=209
03/10/2022 19:48:27 - INFO - __main__ - Step 430 Global step 430 Train loss 0.91 on epoch=214
03/10/2022 19:48:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.99 on epoch=219
03/10/2022 19:48:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.89 on epoch=224
03/10/2022 19:48:37 - INFO - __main__ - Global step 450 Train loss 0.93 QA-F1 0.10416666666666666 on epoch=224
03/10/2022 19:48:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.97 on epoch=229
03/10/2022 19:48:46 - INFO - __main__ - Step 470 Global step 470 Train loss 0.90 on epoch=234
03/10/2022 19:48:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.92 on epoch=239
03/10/2022 19:48:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.87 on epoch=244
03/10/2022 19:48:59 - INFO - __main__ - Step 500 Global step 500 Train loss 0.80 on epoch=249
03/10/2022 19:49:01 - INFO - __main__ - Global step 500 Train loss 0.89 QA-F1 0.109375 on epoch=249
03/10/2022 19:49:05 - INFO - __main__ - Step 510 Global step 510 Train loss 0.85 on epoch=254
03/10/2022 19:49:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=259
03/10/2022 19:49:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.89 on epoch=264
03/10/2022 19:49:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.81 on epoch=269
03/10/2022 19:49:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.90 on epoch=274
03/10/2022 19:49:24 - INFO - __main__ - Global step 550 Train loss 0.86 QA-F1 0.13541666666666666 on epoch=274
03/10/2022 19:49:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.82 on epoch=279
03/10/2022 19:49:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.80 on epoch=284
03/10/2022 19:49:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.79 on epoch=289
03/10/2022 19:49:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.78 on epoch=294
03/10/2022 19:49:46 - INFO - __main__ - Step 600 Global step 600 Train loss 0.87 on epoch=299
03/10/2022 19:49:47 - INFO - __main__ - Global step 600 Train loss 0.81 QA-F1 0.12499999999999999 on epoch=299
03/10/2022 19:49:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.77 on epoch=304
03/10/2022 19:49:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=309
03/10/2022 19:50:00 - INFO - __main__ - Step 630 Global step 630 Train loss 0.65 on epoch=314
03/10/2022 19:50:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.74 on epoch=319
03/10/2022 19:50:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=324
03/10/2022 19:50:11 - INFO - __main__ - Global step 650 Train loss 0.76 QA-F1 0.11458333333333333 on epoch=324
03/10/2022 19:50:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.74 on epoch=329
03/10/2022 19:50:19 - INFO - __main__ - Step 670 Global step 670 Train loss 0.65 on epoch=334
03/10/2022 19:50:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.75 on epoch=339
03/10/2022 19:50:28 - INFO - __main__ - Step 690 Global step 690 Train loss 0.85 on epoch=344
03/10/2022 19:50:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.74 on epoch=349
03/10/2022 19:50:34 - INFO - __main__ - Global step 700 Train loss 0.75 QA-F1 0.15625 on epoch=349
03/10/2022 19:50:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.69 on epoch=354
03/10/2022 19:50:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.85 on epoch=359
03/10/2022 19:50:47 - INFO - __main__ - Step 730 Global step 730 Train loss 0.80 on epoch=364
03/10/2022 19:50:52 - INFO - __main__ - Step 740 Global step 740 Train loss 0.74 on epoch=369
03/10/2022 19:50:56 - INFO - __main__ - Step 750 Global step 750 Train loss 0.66 on epoch=374
03/10/2022 19:50:57 - INFO - __main__ - Global step 750 Train loss 0.75 QA-F1 0.11979166666666666 on epoch=374
03/10/2022 19:51:02 - INFO - __main__ - Step 760 Global step 760 Train loss 0.70 on epoch=379
03/10/2022 19:51:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.69 on epoch=384
03/10/2022 19:51:11 - INFO - __main__ - Step 780 Global step 780 Train loss 0.72 on epoch=389
03/10/2022 19:51:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.69 on epoch=394
03/10/2022 19:51:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.74 on epoch=399
03/10/2022 19:51:21 - INFO - __main__ - Global step 800 Train loss 0.71 QA-F1 0.14583333333333331 on epoch=399
03/10/2022 19:51:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.64 on epoch=404
03/10/2022 19:51:29 - INFO - __main__ - Step 820 Global step 820 Train loss 0.71 on epoch=409
03/10/2022 19:51:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.77 on epoch=414
03/10/2022 19:51:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.68 on epoch=419
03/10/2022 19:51:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.82 on epoch=424
03/10/2022 19:51:44 - INFO - __main__ - Global step 850 Train loss 0.73 QA-F1 0.15104166666666666 on epoch=424
03/10/2022 19:51:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.81 on epoch=429
03/10/2022 19:51:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.81 on epoch=434
03/10/2022 19:51:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.60 on epoch=439
03/10/2022 19:52:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.66 on epoch=444
03/10/2022 19:52:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.68 on epoch=449
03/10/2022 19:52:07 - INFO - __main__ - Global step 900 Train loss 0.71 QA-F1 0.13541666666666666 on epoch=449
03/10/2022 19:52:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.58 on epoch=454
03/10/2022 19:52:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.66 on epoch=459
03/10/2022 19:52:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.56 on epoch=464
03/10/2022 19:52:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.69 on epoch=469
03/10/2022 19:52:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.61 on epoch=474
03/10/2022 19:52:30 - INFO - __main__ - Global step 950 Train loss 0.62 QA-F1 0.13541666666666666 on epoch=474
03/10/2022 19:52:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.61 on epoch=479
03/10/2022 19:52:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.55 on epoch=484
03/10/2022 19:52:44 - INFO - __main__ - Step 980 Global step 980 Train loss 0.66 on epoch=489
03/10/2022 19:52:48 - INFO - __main__ - Step 990 Global step 990 Train loss 0.56 on epoch=494
03/10/2022 19:52:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.73 on epoch=499
03/10/2022 19:52:54 - INFO - __main__ - Global step 1000 Train loss 0.62 QA-F1 0.13541666666666666 on epoch=499
03/10/2022 19:52:58 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.53 on epoch=504
03/10/2022 19:53:02 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.59 on epoch=509
03/10/2022 19:53:07 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.62 on epoch=514
03/10/2022 19:53:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.59 on epoch=519
03/10/2022 19:53:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.64 on epoch=524
03/10/2022 19:53:17 - INFO - __main__ - Global step 1050 Train loss 0.60 QA-F1 0.15625 on epoch=524
03/10/2022 19:53:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.67 on epoch=529
03/10/2022 19:53:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.61 on epoch=534
03/10/2022 19:53:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.50 on epoch=539
03/10/2022 19:53:34 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.50 on epoch=544
03/10/2022 19:53:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.57 on epoch=549
03/10/2022 19:53:40 - INFO - __main__ - Global step 1100 Train loss 0.57 QA-F1 0.13541666666666666 on epoch=549
03/10/2022 19:53:44 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.60 on epoch=554
03/10/2022 19:53:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.67 on epoch=559
03/10/2022 19:53:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.69 on epoch=564
03/10/2022 19:53:57 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.53 on epoch=569
03/10/2022 19:54:02 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.49 on epoch=574
03/10/2022 19:54:03 - INFO - __main__ - Global step 1150 Train loss 0.59 QA-F1 0.14583333333333331 on epoch=574
03/10/2022 19:54:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.67 on epoch=579
03/10/2022 19:54:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.48 on epoch=584
03/10/2022 19:54:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.55 on epoch=589
03/10/2022 19:54:21 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.58 on epoch=594
03/10/2022 19:54:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.56 on epoch=599
03/10/2022 19:54:26 - INFO - __main__ - Global step 1200 Train loss 0.57 QA-F1 0.1875 on epoch=599
03/10/2022 19:54:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.55 on epoch=604
03/10/2022 19:54:35 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.48 on epoch=609
03/10/2022 19:54:39 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.58 on epoch=614
03/10/2022 19:54:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.47 on epoch=619
03/10/2022 19:54:48 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.43 on epoch=624
03/10/2022 19:54:50 - INFO - __main__ - Global step 1250 Train loss 0.50 QA-F1 0.15625 on epoch=624
03/10/2022 19:54:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=629
03/10/2022 19:54:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.58 on epoch=634
03/10/2022 19:55:03 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.62 on epoch=639
03/10/2022 19:55:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.53 on epoch=644
03/10/2022 19:55:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.43 on epoch=649
03/10/2022 19:55:13 - INFO - __main__ - Global step 1300 Train loss 0.53 QA-F1 0.22916666666666666 on epoch=649
03/10/2022 19:55:13 - INFO - __main__ - Saving model with best QA-F1: 0.19270833333333331 -> 0.22916666666666666 on epoch=649, global_step=1300
03/10/2022 19:55:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.53 on epoch=654
03/10/2022 19:55:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.57 on epoch=659
03/10/2022 19:55:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.41 on epoch=664
03/10/2022 19:55:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.37 on epoch=669
03/10/2022 19:55:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.49 on epoch=674
03/10/2022 19:55:36 - INFO - __main__ - Global step 1350 Train loss 0.47 QA-F1 0.17708333333333331 on epoch=674
03/10/2022 19:55:40 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.58 on epoch=679
03/10/2022 19:55:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.50 on epoch=684
03/10/2022 19:55:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.51 on epoch=689
03/10/2022 19:55:53 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.44 on epoch=694
03/10/2022 19:55:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.49 on epoch=699
03/10/2022 19:55:59 - INFO - __main__ - Global step 1400 Train loss 0.50 QA-F1 0.19791666666666666 on epoch=699
03/10/2022 19:56:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.49 on epoch=704
03/10/2022 19:56:07 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=709
03/10/2022 19:56:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.35 on epoch=714
03/10/2022 19:56:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=719
03/10/2022 19:56:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=724
03/10/2022 19:56:22 - INFO - __main__ - Global step 1450 Train loss 0.43 QA-F1 0.17708333333333331 on epoch=724
03/10/2022 19:56:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.49 on epoch=729
03/10/2022 19:56:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.45 on epoch=734
03/10/2022 19:56:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.53 on epoch=739
03/10/2022 19:56:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=744
03/10/2022 19:56:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.51 on epoch=749
03/10/2022 19:56:45 - INFO - __main__ - Global step 1500 Train loss 0.49 QA-F1 0.19791666666666666 on epoch=749
03/10/2022 19:56:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.46 on epoch=754
03/10/2022 19:56:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.44 on epoch=759
03/10/2022 19:56:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.48 on epoch=764
03/10/2022 19:57:02 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.49 on epoch=769
03/10/2022 19:57:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.37 on epoch=774
03/10/2022 19:57:08 - INFO - __main__ - Global step 1550 Train loss 0.45 QA-F1 0.22916666666666666 on epoch=774
03/10/2022 19:57:12 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.41 on epoch=779
03/10/2022 19:57:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=784
03/10/2022 19:57:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=789
03/10/2022 19:57:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.43 on epoch=794
03/10/2022 19:57:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=799
03/10/2022 19:57:31 - INFO - __main__ - Global step 1600 Train loss 0.39 QA-F1 0.16666666666666666 on epoch=799
03/10/2022 19:57:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.30 on epoch=804
03/10/2022 19:57:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
03/10/2022 19:57:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=814
03/10/2022 19:57:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
03/10/2022 19:57:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=824
03/10/2022 19:57:54 - INFO - __main__ - Global step 1650 Train loss 0.38 QA-F1 0.16666666666666666 on epoch=824
03/10/2022 19:57:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.35 on epoch=829
03/10/2022 19:58:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.33 on epoch=834
03/10/2022 19:58:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
03/10/2022 19:58:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.31 on epoch=844
03/10/2022 19:58:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=849
03/10/2022 19:58:17 - INFO - __main__ - Global step 1700 Train loss 0.36 QA-F1 0.16666666666666666 on epoch=849
03/10/2022 19:58:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.38 on epoch=854
03/10/2022 19:58:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=859
03/10/2022 19:58:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.28 on epoch=864
03/10/2022 19:58:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.32 on epoch=869
03/10/2022 19:58:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.30 on epoch=874
03/10/2022 19:58:40 - INFO - __main__ - Global step 1750 Train loss 0.33 QA-F1 0.1875 on epoch=874
03/10/2022 19:58:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=879
03/10/2022 19:58:49 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.37 on epoch=884
03/10/2022 19:58:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
03/10/2022 19:58:57 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.27 on epoch=894
03/10/2022 19:59:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
03/10/2022 19:59:03 - INFO - __main__ - Global step 1800 Train loss 0.33 QA-F1 0.16666666666666666 on epoch=899
03/10/2022 19:59:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=904
03/10/2022 19:59:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.30 on epoch=909
03/10/2022 19:59:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=914
03/10/2022 19:59:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.39 on epoch=919
03/10/2022 19:59:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
03/10/2022 19:59:26 - INFO - __main__ - Global step 1850 Train loss 0.34 QA-F1 0.16666666666666666 on epoch=924
03/10/2022 19:59:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=929
03/10/2022 19:59:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.32 on epoch=934
03/10/2022 19:59:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
03/10/2022 19:59:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=944
03/10/2022 19:59:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.40 on epoch=949
03/10/2022 19:59:49 - INFO - __main__ - Global step 1900 Train loss 0.33 QA-F1 0.1875 on epoch=949
03/10/2022 19:59:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=954
03/10/2022 19:59:58 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
03/10/2022 20:00:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
03/10/2022 20:00:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.29 on epoch=969
03/10/2022 20:00:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=974
03/10/2022 20:00:12 - INFO - __main__ - Global step 1950 Train loss 0.28 QA-F1 0.1875 on epoch=974
03/10/2022 20:00:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=979
03/10/2022 20:00:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.36 on epoch=984
03/10/2022 20:00:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.28 on epoch=989
03/10/2022 20:00:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.35 on epoch=994
03/10/2022 20:00:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.37 on epoch=999
03/10/2022 20:00:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:00:35 - INFO - __main__ - Printing 3 examples
03/10/2022 20:00:35 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/10/2022 20:00:35 - INFO - __main__ - ['Elaine Benson']
03/10/2022 20:00:35 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/10/2022 20:00:35 - INFO - __main__ - ['Richard Boston']
03/10/2022 20:00:35 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/10/2022 20:00:35 - INFO - __main__ - ['The Knight Errant']
03/10/2022 20:00:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:00:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:00:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:00:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:00:35 - INFO - __main__ - Printing 3 examples
03/10/2022 20:00:35 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/10/2022 20:00:35 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/10/2022 20:00:35 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/10/2022 20:00:35 - INFO - __main__ - ['Captain England']
03/10/2022 20:00:35 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/10/2022 20:00:35 - INFO - __main__ - ['Saint-Saëns']
03/10/2022 20:00:35 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:00:36 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:00:36 - INFO - __main__ - Global step 2000 Train loss 0.34 QA-F1 0.1875 on epoch=999
03/10/2022 20:00:36 - INFO - __main__ - save last model!
03/10/2022 20:00:36 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:00:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 20:00:36 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 20:00:36 - INFO - __main__ - Printing 3 examples
03/10/2022 20:00:36 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:00:36 - INFO - __main__ - ['Frankie']
03/10/2022 20:00:36 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:00:36 - INFO - __main__ - ['Frankie']
03/10/2022 20:00:36 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:00:36 - INFO - __main__ - ['Frankie']
03/10/2022 20:00:36 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:00:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:00:43 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 20:00:49 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 20:00:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 20:00:49 - INFO - __main__ - Starting training!
03/10/2022 20:02:26 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_42_0.2_8_predictions.txt
03/10/2022 20:02:26 - INFO - __main__ - QA-F1 on test data: 0.2484
03/10/2022 20:02:26 - INFO - __main__ - prefix=quoref_32_42, lr=0.2, bsz=8, dev_performance=0.22916666666666666, test_performance=0.2483595257788806
03/10/2022 20:02:26 - INFO - __main__ - Running ... prefix=quoref_32_87, lr=0.5, bsz=8 ...
03/10/2022 20:02:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:02:27 - INFO - __main__ - Printing 3 examples
03/10/2022 20:02:27 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/10/2022 20:02:27 - INFO - __main__ - ['Elaine Benson']
03/10/2022 20:02:27 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/10/2022 20:02:27 - INFO - __main__ - ['Richard Boston']
03/10/2022 20:02:27 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/10/2022 20:02:27 - INFO - __main__ - ['The Knight Errant']
03/10/2022 20:02:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:02:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:02:27 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:02:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:02:27 - INFO - __main__ - Printing 3 examples
03/10/2022 20:02:27 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/10/2022 20:02:27 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/10/2022 20:02:27 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/10/2022 20:02:27 - INFO - __main__ - ['Captain England']
03/10/2022 20:02:27 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/10/2022 20:02:27 - INFO - __main__ - ['Saint-Saëns']
03/10/2022 20:02:27 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:02:27 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:02:27 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:02:40 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 20:02:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 20:02:41 - INFO - __main__ - Starting training!
03/10/2022 20:02:46 - INFO - __main__ - Step 10 Global step 10 Train loss 2.61 on epoch=4
03/10/2022 20:02:50 - INFO - __main__ - Step 20 Global step 20 Train loss 2.00 on epoch=9
03/10/2022 20:02:55 - INFO - __main__ - Step 30 Global step 30 Train loss 1.62 on epoch=14
03/10/2022 20:02:59 - INFO - __main__ - Step 40 Global step 40 Train loss 1.48 on epoch=19
03/10/2022 20:03:03 - INFO - __main__ - Step 50 Global step 50 Train loss 1.51 on epoch=24
03/10/2022 20:03:05 - INFO - __main__ - Global step 50 Train loss 1.84 QA-F1 0.22395833333333331 on epoch=24
03/10/2022 20:03:05 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.22395833333333331 on epoch=24, global_step=50
03/10/2022 20:03:10 - INFO - __main__ - Step 60 Global step 60 Train loss 1.48 on epoch=29
03/10/2022 20:03:14 - INFO - __main__ - Step 70 Global step 70 Train loss 1.53 on epoch=34
03/10/2022 20:03:18 - INFO - __main__ - Step 80 Global step 80 Train loss 1.29 on epoch=39
03/10/2022 20:03:22 - INFO - __main__ - Step 90 Global step 90 Train loss 1.38 on epoch=44
03/10/2022 20:03:27 - INFO - __main__ - Step 100 Global step 100 Train loss 1.35 on epoch=49
03/10/2022 20:03:28 - INFO - __main__ - Global step 100 Train loss 1.41 QA-F1 0.29166666666666663 on epoch=49
03/10/2022 20:03:29 - INFO - __main__ - Saving model with best QA-F1: 0.22395833333333331 -> 0.29166666666666663 on epoch=49, global_step=100
03/10/2022 20:03:33 - INFO - __main__ - Step 110 Global step 110 Train loss 1.54 on epoch=54
03/10/2022 20:03:37 - INFO - __main__ - Step 120 Global step 120 Train loss 1.62 on epoch=59
03/10/2022 20:03:41 - INFO - __main__ - Step 130 Global step 130 Train loss 1.92 on epoch=64
03/10/2022 20:03:46 - INFO - __main__ - Step 140 Global step 140 Train loss 1.71 on epoch=69
03/10/2022 20:03:50 - INFO - __main__ - Step 150 Global step 150 Train loss 1.73 on epoch=74
03/10/2022 20:03:52 - INFO - __main__ - Global step 150 Train loss 1.70 QA-F1 0.23958333333333331 on epoch=74
03/10/2022 20:03:56 - INFO - __main__ - Step 160 Global step 160 Train loss 2.21 on epoch=79
03/10/2022 20:04:00 - INFO - __main__ - Step 170 Global step 170 Train loss 3.41 on epoch=84
03/10/2022 20:04:05 - INFO - __main__ - Step 180 Global step 180 Train loss 3.72 on epoch=89
03/10/2022 20:04:09 - INFO - __main__ - Step 190 Global step 190 Train loss 3.54 on epoch=94
03/10/2022 20:04:13 - INFO - __main__ - Step 200 Global step 200 Train loss 2.42 on epoch=99
03/10/2022 20:04:15 - INFO - __main__ - Global step 200 Train loss 3.06 QA-F1 0.19270833333333331 on epoch=99
03/10/2022 20:04:19 - INFO - __main__ - Step 210 Global step 210 Train loss 2.66 on epoch=104
03/10/2022 20:04:24 - INFO - __main__ - Step 220 Global step 220 Train loss 2.63 on epoch=109
03/10/2022 20:04:28 - INFO - __main__ - Step 230 Global step 230 Train loss 2.38 on epoch=114
03/10/2022 20:04:32 - INFO - __main__ - Step 240 Global step 240 Train loss 2.10 on epoch=119
03/10/2022 20:04:36 - INFO - __main__ - Step 250 Global step 250 Train loss 2.09 on epoch=124
03/10/2022 20:04:38 - INFO - __main__ - Global step 250 Train loss 2.37 QA-F1 0.2708333333333333 on epoch=124
03/10/2022 20:04:42 - INFO - __main__ - Step 260 Global step 260 Train loss 2.05 on epoch=129
03/10/2022 20:04:47 - INFO - __main__ - Step 270 Global step 270 Train loss 1.94 on epoch=134
03/10/2022 20:04:51 - INFO - __main__ - Step 280 Global step 280 Train loss 1.91 on epoch=139
03/10/2022 20:04:55 - INFO - __main__ - Step 290 Global step 290 Train loss 1.78 on epoch=144
03/10/2022 20:05:00 - INFO - __main__ - Step 300 Global step 300 Train loss 1.72 on epoch=149
03/10/2022 20:05:02 - INFO - __main__ - Global step 300 Train loss 1.88 QA-F1 0.20833333333333331 on epoch=149
03/10/2022 20:05:06 - INFO - __main__ - Step 310 Global step 310 Train loss 1.66 on epoch=154
03/10/2022 20:05:10 - INFO - __main__ - Step 320 Global step 320 Train loss 1.61 on epoch=159
03/10/2022 20:05:15 - INFO - __main__ - Step 330 Global step 330 Train loss 1.63 on epoch=164
03/10/2022 20:05:19 - INFO - __main__ - Step 340 Global step 340 Train loss 1.51 on epoch=169
03/10/2022 20:05:23 - INFO - __main__ - Step 350 Global step 350 Train loss 1.43 on epoch=174
03/10/2022 20:05:25 - INFO - __main__ - Global step 350 Train loss 1.57 QA-F1 0.23958333333333331 on epoch=174
03/10/2022 20:05:30 - INFO - __main__ - Step 360 Global step 360 Train loss 1.48 on epoch=179
03/10/2022 20:05:34 - INFO - __main__ - Step 370 Global step 370 Train loss 1.38 on epoch=184
03/10/2022 20:05:38 - INFO - __main__ - Step 380 Global step 380 Train loss 1.45 on epoch=189
03/10/2022 20:05:42 - INFO - __main__ - Step 390 Global step 390 Train loss 1.44 on epoch=194
03/10/2022 20:05:47 - INFO - __main__ - Step 400 Global step 400 Train loss 1.42 on epoch=199
03/10/2022 20:05:48 - INFO - __main__ - Global step 400 Train loss 1.43 QA-F1 0.23958333333333331 on epoch=199
03/10/2022 20:05:53 - INFO - __main__ - Step 410 Global step 410 Train loss 1.46 on epoch=204
03/10/2022 20:05:57 - INFO - __main__ - Step 420 Global step 420 Train loss 1.35 on epoch=209
03/10/2022 20:06:01 - INFO - __main__ - Step 430 Global step 430 Train loss 1.41 on epoch=214
03/10/2022 20:06:06 - INFO - __main__ - Step 440 Global step 440 Train loss 1.30 on epoch=219
03/10/2022 20:06:10 - INFO - __main__ - Step 450 Global step 450 Train loss 1.34 on epoch=224
03/10/2022 20:06:12 - INFO - __main__ - Global step 450 Train loss 1.37 QA-F1 0.22916666666666663 on epoch=224
03/10/2022 20:06:16 - INFO - __main__ - Step 460 Global step 460 Train loss 1.38 on epoch=229
03/10/2022 20:06:20 - INFO - __main__ - Step 470 Global step 470 Train loss 1.27 on epoch=234
03/10/2022 20:06:25 - INFO - __main__ - Step 480 Global step 480 Train loss 1.24 on epoch=239
03/10/2022 20:06:29 - INFO - __main__ - Step 490 Global step 490 Train loss 1.21 on epoch=244
03/10/2022 20:06:33 - INFO - __main__ - Step 500 Global step 500 Train loss 1.20 on epoch=249
03/10/2022 20:06:35 - INFO - __main__ - Global step 500 Train loss 1.26 QA-F1 0.27604166666666663 on epoch=249
03/10/2022 20:06:40 - INFO - __main__ - Step 510 Global step 510 Train loss 1.20 on epoch=254
03/10/2022 20:06:44 - INFO - __main__ - Step 520 Global step 520 Train loss 1.28 on epoch=259
03/10/2022 20:06:48 - INFO - __main__ - Step 530 Global step 530 Train loss 1.18 on epoch=264
03/10/2022 20:06:53 - INFO - __main__ - Step 540 Global step 540 Train loss 1.14 on epoch=269
03/10/2022 20:06:57 - INFO - __main__ - Step 550 Global step 550 Train loss 1.14 on epoch=274
03/10/2022 20:06:59 - INFO - __main__ - Global step 550 Train loss 1.19 QA-F1 0.234375 on epoch=274
03/10/2022 20:07:03 - INFO - __main__ - Step 560 Global step 560 Train loss 1.17 on epoch=279
03/10/2022 20:07:08 - INFO - __main__ - Step 570 Global step 570 Train loss 1.19 on epoch=284
03/10/2022 20:07:12 - INFO - __main__ - Step 580 Global step 580 Train loss 1.81 on epoch=289
03/10/2022 20:07:16 - INFO - __main__ - Step 590 Global step 590 Train loss 2.74 on epoch=294
03/10/2022 20:07:21 - INFO - __main__ - Step 600 Global step 600 Train loss 1.52 on epoch=299
03/10/2022 20:07:22 - INFO - __main__ - Global step 600 Train loss 1.69 QA-F1 0.19270833333333331 on epoch=299
03/10/2022 20:07:27 - INFO - __main__ - Step 610 Global step 610 Train loss 1.70 on epoch=304
03/10/2022 20:07:31 - INFO - __main__ - Step 620 Global step 620 Train loss 1.37 on epoch=309
03/10/2022 20:07:35 - INFO - __main__ - Step 630 Global step 630 Train loss 1.31 on epoch=314
03/10/2022 20:07:40 - INFO - __main__ - Step 640 Global step 640 Train loss 1.27 on epoch=319
03/10/2022 20:07:44 - INFO - __main__ - Step 650 Global step 650 Train loss 1.36 on epoch=324
03/10/2022 20:07:46 - INFO - __main__ - Global step 650 Train loss 1.40 QA-F1 0.265625 on epoch=324
03/10/2022 20:07:50 - INFO - __main__ - Step 660 Global step 660 Train loss 2.52 on epoch=329
03/10/2022 20:07:54 - INFO - __main__ - Step 670 Global step 670 Train loss 1.73 on epoch=334
03/10/2022 20:07:59 - INFO - __main__ - Step 680 Global step 680 Train loss 2.08 on epoch=339
03/10/2022 20:08:03 - INFO - __main__ - Step 690 Global step 690 Train loss 1.71 on epoch=344
03/10/2022 20:08:07 - INFO - __main__ - Step 700 Global step 700 Train loss 1.46 on epoch=349
03/10/2022 20:08:09 - INFO - __main__ - Global step 700 Train loss 1.90 QA-F1 0.296875 on epoch=349
03/10/2022 20:08:09 - INFO - __main__ - Saving model with best QA-F1: 0.29166666666666663 -> 0.296875 on epoch=349, global_step=700
03/10/2022 20:08:13 - INFO - __main__ - Step 710 Global step 710 Train loss 1.42 on epoch=354
03/10/2022 20:08:18 - INFO - __main__ - Step 720 Global step 720 Train loss 1.34 on epoch=359
03/10/2022 20:08:22 - INFO - __main__ - Step 730 Global step 730 Train loss 1.33 on epoch=364
03/10/2022 20:08:26 - INFO - __main__ - Step 740 Global step 740 Train loss 1.35 on epoch=369
03/10/2022 20:08:31 - INFO - __main__ - Step 750 Global step 750 Train loss 1.39 on epoch=374
03/10/2022 20:08:32 - INFO - __main__ - Global step 750 Train loss 1.36 QA-F1 0.296875 on epoch=374
03/10/2022 20:08:37 - INFO - __main__ - Step 760 Global step 760 Train loss 1.59 on epoch=379
03/10/2022 20:08:41 - INFO - __main__ - Step 770 Global step 770 Train loss 1.60 on epoch=384
03/10/2022 20:08:45 - INFO - __main__ - Step 780 Global step 780 Train loss 1.47 on epoch=389
03/10/2022 20:08:50 - INFO - __main__ - Step 790 Global step 790 Train loss 1.44 on epoch=394
03/10/2022 20:08:54 - INFO - __main__ - Step 800 Global step 800 Train loss 1.38 on epoch=399
03/10/2022 20:08:56 - INFO - __main__ - Global step 800 Train loss 1.50 QA-F1 0.296875 on epoch=399
03/10/2022 20:09:00 - INFO - __main__ - Step 810 Global step 810 Train loss 1.40 on epoch=404
03/10/2022 20:09:04 - INFO - __main__ - Step 820 Global step 820 Train loss 1.41 on epoch=409
03/10/2022 20:09:09 - INFO - __main__ - Step 830 Global step 830 Train loss 1.33 on epoch=414
03/10/2022 20:09:13 - INFO - __main__ - Step 840 Global step 840 Train loss 1.41 on epoch=419
03/10/2022 20:09:17 - INFO - __main__ - Step 850 Global step 850 Train loss 1.32 on epoch=424
03/10/2022 20:09:19 - INFO - __main__ - Global step 850 Train loss 1.38 QA-F1 0.30729166666666663 on epoch=424
03/10/2022 20:09:19 - INFO - __main__ - Saving model with best QA-F1: 0.296875 -> 0.30729166666666663 on epoch=424, global_step=850
03/10/2022 20:09:23 - INFO - __main__ - Step 860 Global step 860 Train loss 1.38 on epoch=429
03/10/2022 20:09:28 - INFO - __main__ - Step 870 Global step 870 Train loss 1.33 on epoch=434
03/10/2022 20:09:32 - INFO - __main__ - Step 880 Global step 880 Train loss 1.39 on epoch=439
03/10/2022 20:09:36 - INFO - __main__ - Step 890 Global step 890 Train loss 1.34 on epoch=444
03/10/2022 20:09:41 - INFO - __main__ - Step 900 Global step 900 Train loss 1.25 on epoch=449
03/10/2022 20:09:42 - INFO - __main__ - Global step 900 Train loss 1.34 QA-F1 0.21874999999999997 on epoch=449
03/10/2022 20:09:47 - INFO - __main__ - Step 910 Global step 910 Train loss 1.25 on epoch=454
03/10/2022 20:09:51 - INFO - __main__ - Step 920 Global step 920 Train loss 1.16 on epoch=459
03/10/2022 20:09:55 - INFO - __main__ - Step 930 Global step 930 Train loss 1.29 on epoch=464
03/10/2022 20:10:00 - INFO - __main__ - Step 940 Global step 940 Train loss 1.16 on epoch=469
03/10/2022 20:10:04 - INFO - __main__ - Step 950 Global step 950 Train loss 1.15 on epoch=474
03/10/2022 20:10:06 - INFO - __main__ - Global step 950 Train loss 1.20 QA-F1 0.15625 on epoch=474
03/10/2022 20:10:10 - INFO - __main__ - Step 960 Global step 960 Train loss 1.16 on epoch=479
03/10/2022 20:10:14 - INFO - __main__ - Step 970 Global step 970 Train loss 1.17 on epoch=484
03/10/2022 20:10:19 - INFO - __main__ - Step 980 Global step 980 Train loss 1.22 on epoch=489
03/10/2022 20:10:23 - INFO - __main__ - Step 990 Global step 990 Train loss 1.20 on epoch=494
03/10/2022 20:10:27 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.15 on epoch=499
03/10/2022 20:10:29 - INFO - __main__ - Global step 1000 Train loss 1.18 QA-F1 0.1875 on epoch=499
03/10/2022 20:10:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.11 on epoch=504
03/10/2022 20:10:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.05 on epoch=509
03/10/2022 20:10:42 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.06 on epoch=514
03/10/2022 20:10:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.15 on epoch=519
03/10/2022 20:10:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.22 on epoch=524
03/10/2022 20:10:52 - INFO - __main__ - Global step 1050 Train loss 1.12 QA-F1 0.234375 on epoch=524
03/10/2022 20:10:57 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.11 on epoch=529
03/10/2022 20:11:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.08 on epoch=534
03/10/2022 20:11:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.07 on epoch=539
03/10/2022 20:11:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.21 on epoch=544
03/10/2022 20:11:14 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.14 on epoch=549
03/10/2022 20:11:16 - INFO - __main__ - Global step 1100 Train loss 1.12 QA-F1 0.234375 on epoch=549
03/10/2022 20:11:20 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.02 on epoch=554
03/10/2022 20:11:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.07 on epoch=559
03/10/2022 20:11:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.06 on epoch=564
03/10/2022 20:11:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.99 on epoch=569
03/10/2022 20:11:37 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.04 on epoch=574
03/10/2022 20:11:39 - INFO - __main__ - Global step 1150 Train loss 1.04 QA-F1 0.26041666666666663 on epoch=574
03/10/2022 20:11:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.08 on epoch=579
03/10/2022 20:11:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.10 on epoch=584
03/10/2022 20:11:52 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.97 on epoch=589
03/10/2022 20:11:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.93 on epoch=594
03/10/2022 20:12:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.01 on epoch=599
03/10/2022 20:12:02 - INFO - __main__ - Global step 1200 Train loss 1.02 QA-F1 0.14583333333333331 on epoch=599
03/10/2022 20:12:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.03 on epoch=604
03/10/2022 20:12:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.94 on epoch=609
03/10/2022 20:12:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.03 on epoch=614
03/10/2022 20:12:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.96 on epoch=619
03/10/2022 20:12:24 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.02 on epoch=624
03/10/2022 20:12:25 - INFO - __main__ - Global step 1250 Train loss 1.00 QA-F1 0.17708333333333331 on epoch=624
03/10/2022 20:12:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.05 on epoch=629
03/10/2022 20:12:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.95 on epoch=634
03/10/2022 20:12:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.01 on epoch=639
03/10/2022 20:12:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.09 on epoch=644
03/10/2022 20:12:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.94 on epoch=649
03/10/2022 20:12:49 - INFO - __main__ - Global step 1300 Train loss 1.01 QA-F1 0.20833333333333331 on epoch=649
03/10/2022 20:12:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.93 on epoch=654
03/10/2022 20:12:57 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.98 on epoch=659
03/10/2022 20:13:02 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.94 on epoch=664
03/10/2022 20:13:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.94 on epoch=669
03/10/2022 20:13:10 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.86 on epoch=674
03/10/2022 20:13:12 - INFO - __main__ - Global step 1350 Train loss 0.93 QA-F1 0.11458333333333331 on epoch=674
03/10/2022 20:13:16 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.90 on epoch=679
03/10/2022 20:13:21 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.90 on epoch=684
03/10/2022 20:13:25 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.90 on epoch=689
03/10/2022 20:13:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.94 on epoch=694
03/10/2022 20:13:34 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.90 on epoch=699
03/10/2022 20:13:35 - INFO - __main__ - Global step 1400 Train loss 0.91 QA-F1 0.14583333333333331 on epoch=699
03/10/2022 20:13:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.91 on epoch=704
03/10/2022 20:13:44 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.97 on epoch=709
03/10/2022 20:13:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.95 on epoch=714
03/10/2022 20:13:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.85 on epoch=719
03/10/2022 20:13:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.84 on epoch=724
03/10/2022 20:13:59 - INFO - __main__ - Global step 1450 Train loss 0.90 QA-F1 0.14583333333333331 on epoch=724
03/10/2022 20:14:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.86 on epoch=729
03/10/2022 20:14:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.81 on epoch=734
03/10/2022 20:14:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.86 on epoch=739
03/10/2022 20:14:16 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.88 on epoch=744
03/10/2022 20:14:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.81 on epoch=749
03/10/2022 20:14:22 - INFO - __main__ - Global step 1500 Train loss 0.85 QA-F1 0.09375 on epoch=749
03/10/2022 20:14:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.77 on epoch=754
03/10/2022 20:14:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.79 on epoch=759
03/10/2022 20:14:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.82 on epoch=764
03/10/2022 20:14:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.78 on epoch=769
03/10/2022 20:14:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.81 on epoch=774
03/10/2022 20:14:45 - INFO - __main__ - Global step 1550 Train loss 0.79 QA-F1 0.14583333333333331 on epoch=774
03/10/2022 20:14:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.87 on epoch=779
03/10/2022 20:14:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.82 on epoch=784
03/10/2022 20:14:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.78 on epoch=789
03/10/2022 20:15:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.84 on epoch=794
03/10/2022 20:15:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.81 on epoch=799
03/10/2022 20:15:09 - INFO - __main__ - Global step 1600 Train loss 0.83 QA-F1 0.16666666666666663 on epoch=799
03/10/2022 20:15:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.74 on epoch=804
03/10/2022 20:15:17 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.76 on epoch=809
03/10/2022 20:15:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.71 on epoch=814
03/10/2022 20:15:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.75 on epoch=819
03/10/2022 20:15:30 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.73 on epoch=824
03/10/2022 20:15:32 - INFO - __main__ - Global step 1650 Train loss 0.74 QA-F1 0.14583333333333331 on epoch=824
03/10/2022 20:15:36 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.70 on epoch=829
03/10/2022 20:15:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.65 on epoch=834
03/10/2022 20:15:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.73 on epoch=839
03/10/2022 20:15:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.69 on epoch=844
03/10/2022 20:15:54 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.76 on epoch=849
03/10/2022 20:15:55 - INFO - __main__ - Global step 1700 Train loss 0.71 QA-F1 0.11458333333333333 on epoch=849
03/10/2022 20:16:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.67 on epoch=854
03/10/2022 20:16:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.68 on epoch=859
03/10/2022 20:16:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.64 on epoch=864
03/10/2022 20:16:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.74 on epoch=869
03/10/2022 20:16:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.67 on epoch=874
03/10/2022 20:16:19 - INFO - __main__ - Global step 1750 Train loss 0.68 QA-F1 0.11458333333333333 on epoch=874
03/10/2022 20:16:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.62 on epoch=879
03/10/2022 20:16:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.70 on epoch=884
03/10/2022 20:16:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.60 on epoch=889
03/10/2022 20:16:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.61 on epoch=894
03/10/2022 20:16:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.76 on epoch=899
03/10/2022 20:16:42 - INFO - __main__ - Global step 1800 Train loss 0.66 QA-F1 0.14583333333333331 on epoch=899
03/10/2022 20:16:46 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.66 on epoch=904
03/10/2022 20:16:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.64 on epoch=909
03/10/2022 20:16:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.69 on epoch=914
03/10/2022 20:16:59 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.66 on epoch=919
03/10/2022 20:17:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.69 on epoch=924
03/10/2022 20:17:05 - INFO - __main__ - Global step 1850 Train loss 0.67 QA-F1 0.12499999999999999 on epoch=924
03/10/2022 20:17:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.61 on epoch=929
03/10/2022 20:17:14 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.63 on epoch=934
03/10/2022 20:17:18 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.58 on epoch=939
03/10/2022 20:17:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.62 on epoch=944
03/10/2022 20:17:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.55 on epoch=949
03/10/2022 20:17:29 - INFO - __main__ - Global step 1900 Train loss 0.60 QA-F1 0.14583333333333331 on epoch=949
03/10/2022 20:17:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.55 on epoch=954
03/10/2022 20:17:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.59 on epoch=959
03/10/2022 20:17:42 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.54 on epoch=964
03/10/2022 20:17:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.60 on epoch=969
03/10/2022 20:17:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.63 on epoch=974
03/10/2022 20:17:52 - INFO - __main__ - Global step 1950 Train loss 0.58 QA-F1 0.171875 on epoch=974
03/10/2022 20:17:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.50 on epoch=979
03/10/2022 20:18:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.49 on epoch=984
03/10/2022 20:18:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.47 on epoch=989
03/10/2022 20:18:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.51 on epoch=994
03/10/2022 20:18:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.45 on epoch=999
03/10/2022 20:18:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:18:15 - INFO - __main__ - Printing 3 examples
03/10/2022 20:18:15 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/10/2022 20:18:15 - INFO - __main__ - ['Elaine Benson']
03/10/2022 20:18:15 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/10/2022 20:18:15 - INFO - __main__ - ['Richard Boston']
03/10/2022 20:18:15 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/10/2022 20:18:15 - INFO - __main__ - ['The Knight Errant']
03/10/2022 20:18:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:18:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:18:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:18:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:18:15 - INFO - __main__ - Printing 3 examples
03/10/2022 20:18:15 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/10/2022 20:18:15 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/10/2022 20:18:15 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/10/2022 20:18:15 - INFO - __main__ - ['Captain England']
03/10/2022 20:18:15 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/10/2022 20:18:15 - INFO - __main__ - ['Saint-Saëns']
03/10/2022 20:18:15 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:18:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:18:15 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:18:15 - INFO - __main__ - Global step 2000 Train loss 0.49 QA-F1 0.1875 on epoch=999
03/10/2022 20:18:15 - INFO - __main__ - save last model!
03/10/2022 20:18:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 20:18:16 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 20:18:16 - INFO - __main__ - Printing 3 examples
03/10/2022 20:18:16 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:18:16 - INFO - __main__ - ['Frankie']
03/10/2022 20:18:16 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:18:16 - INFO - __main__ - ['Frankie']
03/10/2022 20:18:16 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:18:16 - INFO - __main__ - ['Frankie']
03/10/2022 20:18:16 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:18:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:18:23 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 20:18:28 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 20:18:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 20:18:29 - INFO - __main__ - Starting training!
03/10/2022 20:20:12 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_87_0.5_8_predictions.txt
03/10/2022 20:20:12 - INFO - __main__ - QA-F1 on test data: 0.2651
03/10/2022 20:20:13 - INFO - __main__ - prefix=quoref_32_87, lr=0.5, bsz=8, dev_performance=0.30729166666666663, test_performance=0.26505835860674565
03/10/2022 20:20:13 - INFO - __main__ - Running ... prefix=quoref_32_87, lr=0.4, bsz=8 ...
03/10/2022 20:20:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:20:14 - INFO - __main__ - Printing 3 examples
03/10/2022 20:20:14 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/10/2022 20:20:14 - INFO - __main__ - ['Elaine Benson']
03/10/2022 20:20:14 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/10/2022 20:20:14 - INFO - __main__ - ['Richard Boston']
03/10/2022 20:20:14 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/10/2022 20:20:14 - INFO - __main__ - ['The Knight Errant']
03/10/2022 20:20:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:20:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:20:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:20:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:20:14 - INFO - __main__ - Printing 3 examples
03/10/2022 20:20:14 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/10/2022 20:20:14 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/10/2022 20:20:14 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/10/2022 20:20:14 - INFO - __main__ - ['Captain England']
03/10/2022 20:20:14 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/10/2022 20:20:14 - INFO - __main__ - ['Saint-Saëns']
03/10/2022 20:20:14 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:20:14 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:20:14 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:20:27 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 20:20:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 20:20:28 - INFO - __main__ - Starting training!
03/10/2022 20:20:33 - INFO - __main__ - Step 10 Global step 10 Train loss 2.46 on epoch=4
03/10/2022 20:20:37 - INFO - __main__ - Step 20 Global step 20 Train loss 2.02 on epoch=9
03/10/2022 20:20:42 - INFO - __main__ - Step 30 Global step 30 Train loss 1.88 on epoch=14
03/10/2022 20:20:46 - INFO - __main__ - Step 40 Global step 40 Train loss 1.90 on epoch=19
03/10/2022 20:20:50 - INFO - __main__ - Step 50 Global step 50 Train loss 2.10 on epoch=24
03/10/2022 20:21:01 - INFO - __main__ - Global step 50 Train loss 2.07 QA-F1 0.38622835497835495 on epoch=24
03/10/2022 20:21:01 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.38622835497835495 on epoch=24, global_step=50
03/10/2022 20:21:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.79 on epoch=29
03/10/2022 20:21:10 - INFO - __main__ - Step 70 Global step 70 Train loss 1.65 on epoch=34
03/10/2022 20:21:14 - INFO - __main__ - Step 80 Global step 80 Train loss 1.58 on epoch=39
03/10/2022 20:21:19 - INFO - __main__ - Step 90 Global step 90 Train loss 1.68 on epoch=44
03/10/2022 20:21:23 - INFO - __main__ - Step 100 Global step 100 Train loss 1.59 on epoch=49
03/10/2022 20:21:26 - INFO - __main__ - Global step 100 Train loss 1.66 QA-F1 0.3641826923076923 on epoch=49
03/10/2022 20:21:30 - INFO - __main__ - Step 110 Global step 110 Train loss 1.46 on epoch=54
03/10/2022 20:21:34 - INFO - __main__ - Step 120 Global step 120 Train loss 1.48 on epoch=59
03/10/2022 20:21:38 - INFO - __main__ - Step 130 Global step 130 Train loss 1.45 on epoch=64
03/10/2022 20:21:43 - INFO - __main__ - Step 140 Global step 140 Train loss 1.42 on epoch=69
03/10/2022 20:21:47 - INFO - __main__ - Step 150 Global step 150 Train loss 1.35 on epoch=74
03/10/2022 20:21:49 - INFO - __main__ - Global step 150 Train loss 1.43 QA-F1 0.32291666666666663 on epoch=74
03/10/2022 20:21:53 - INFO - __main__ - Step 160 Global step 160 Train loss 1.31 on epoch=79
03/10/2022 20:21:57 - INFO - __main__ - Step 170 Global step 170 Train loss 1.30 on epoch=84
03/10/2022 20:22:02 - INFO - __main__ - Step 180 Global step 180 Train loss 1.28 on epoch=89
03/10/2022 20:22:06 - INFO - __main__ - Step 190 Global step 190 Train loss 1.43 on epoch=94
03/10/2022 20:22:10 - INFO - __main__ - Step 200 Global step 200 Train loss 1.25 on epoch=99
03/10/2022 20:22:12 - INFO - __main__ - Global step 200 Train loss 1.31 QA-F1 0.22916666666666666 on epoch=99
03/10/2022 20:22:16 - INFO - __main__ - Step 210 Global step 210 Train loss 1.17 on epoch=104
03/10/2022 20:22:20 - INFO - __main__ - Step 220 Global step 220 Train loss 1.16 on epoch=109
03/10/2022 20:22:25 - INFO - __main__ - Step 230 Global step 230 Train loss 1.16 on epoch=114
03/10/2022 20:22:29 - INFO - __main__ - Step 240 Global step 240 Train loss 1.13 on epoch=119
03/10/2022 20:22:33 - INFO - __main__ - Step 250 Global step 250 Train loss 1.08 on epoch=124
03/10/2022 20:22:35 - INFO - __main__ - Global step 250 Train loss 1.14 QA-F1 0.21354166666666663 on epoch=124
03/10/2022 20:22:39 - INFO - __main__ - Step 260 Global step 260 Train loss 1.11 on epoch=129
03/10/2022 20:22:43 - INFO - __main__ - Step 270 Global step 270 Train loss 1.12 on epoch=134
03/10/2022 20:22:48 - INFO - __main__ - Step 280 Global step 280 Train loss 1.12 on epoch=139
03/10/2022 20:22:52 - INFO - __main__ - Step 290 Global step 290 Train loss 1.02 on epoch=144
03/10/2022 20:22:56 - INFO - __main__ - Step 300 Global step 300 Train loss 1.02 on epoch=149
03/10/2022 20:22:58 - INFO - __main__ - Global step 300 Train loss 1.08 QA-F1 0.19791666666666666 on epoch=149
03/10/2022 20:23:02 - INFO - __main__ - Step 310 Global step 310 Train loss 1.04 on epoch=154
03/10/2022 20:23:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.99 on epoch=159
03/10/2022 20:23:11 - INFO - __main__ - Step 330 Global step 330 Train loss 1.02 on epoch=164
03/10/2022 20:23:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.94 on epoch=169
03/10/2022 20:23:20 - INFO - __main__ - Step 350 Global step 350 Train loss 1.05 on epoch=174
03/10/2022 20:23:21 - INFO - __main__ - Global step 350 Train loss 1.01 QA-F1 0.19791666666666666 on epoch=174
03/10/2022 20:23:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.96 on epoch=179
03/10/2022 20:23:30 - INFO - __main__ - Step 370 Global step 370 Train loss 1.00 on epoch=184
03/10/2022 20:23:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.90 on epoch=189
03/10/2022 20:23:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.95 on epoch=194
03/10/2022 20:23:43 - INFO - __main__ - Step 400 Global step 400 Train loss 1.01 on epoch=199
03/10/2022 20:23:44 - INFO - __main__ - Global step 400 Train loss 0.96 QA-F1 0.27604166666666663 on epoch=199
03/10/2022 20:23:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.93 on epoch=204
03/10/2022 20:23:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.89 on epoch=209
03/10/2022 20:23:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.85 on epoch=214
03/10/2022 20:24:02 - INFO - __main__ - Step 440 Global step 440 Train loss 0.94 on epoch=219
03/10/2022 20:24:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.84 on epoch=224
03/10/2022 20:24:08 - INFO - __main__ - Global step 450 Train loss 0.89 QA-F1 0.30729166666666663 on epoch=224
03/10/2022 20:24:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=229
03/10/2022 20:24:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.81 on epoch=234
03/10/2022 20:24:21 - INFO - __main__ - Step 480 Global step 480 Train loss 0.85 on epoch=239
03/10/2022 20:24:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.94 on epoch=244
03/10/2022 20:24:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.87 on epoch=249
03/10/2022 20:24:31 - INFO - __main__ - Global step 500 Train loss 0.86 QA-F1 0.30729166666666663 on epoch=249
03/10/2022 20:24:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.81 on epoch=254
03/10/2022 20:24:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.82 on epoch=259
03/10/2022 20:24:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.80 on epoch=264
03/10/2022 20:24:48 - INFO - __main__ - Step 540 Global step 540 Train loss 0.77 on epoch=269
03/10/2022 20:24:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.72 on epoch=274
03/10/2022 20:24:54 - INFO - __main__ - Global step 550 Train loss 0.79 QA-F1 0.18229166666666666 on epoch=274
03/10/2022 20:24:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=279
03/10/2022 20:25:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.80 on epoch=284
03/10/2022 20:25:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.74 on epoch=289
03/10/2022 20:25:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.73 on epoch=294
03/10/2022 20:25:16 - INFO - __main__ - Step 600 Global step 600 Train loss 0.73 on epoch=299
03/10/2022 20:25:17 - INFO - __main__ - Global step 600 Train loss 0.75 QA-F1 0.18229166666666666 on epoch=299
03/10/2022 20:25:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.71 on epoch=304
03/10/2022 20:25:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.72 on epoch=309
03/10/2022 20:25:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.70 on epoch=314
03/10/2022 20:25:34 - INFO - __main__ - Step 640 Global step 640 Train loss 0.66 on epoch=319
03/10/2022 20:25:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.67 on epoch=324
03/10/2022 20:25:40 - INFO - __main__ - Global step 650 Train loss 0.69 QA-F1 0.18229166666666666 on epoch=324
03/10/2022 20:25:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.64 on epoch=329
03/10/2022 20:25:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.56 on epoch=334
03/10/2022 20:25:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.59 on epoch=339
03/10/2022 20:25:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=344
03/10/2022 20:26:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.60 on epoch=349
03/10/2022 20:26:04 - INFO - __main__ - Global step 700 Train loss 0.59 QA-F1 0.18229166666666666 on epoch=349
03/10/2022 20:26:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.62 on epoch=354
03/10/2022 20:26:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.56 on epoch=359
03/10/2022 20:26:16 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=364
03/10/2022 20:26:21 - INFO - __main__ - Step 740 Global step 740 Train loss 0.67 on epoch=369
03/10/2022 20:26:25 - INFO - __main__ - Step 750 Global step 750 Train loss 0.52 on epoch=374
03/10/2022 20:26:27 - INFO - __main__ - Global step 750 Train loss 0.58 QA-F1 0.19791666666666663 on epoch=374
03/10/2022 20:26:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=379
03/10/2022 20:26:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.53 on epoch=384
03/10/2022 20:26:40 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=389
03/10/2022 20:26:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.55 on epoch=394
03/10/2022 20:26:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.44 on epoch=399
03/10/2022 20:26:50 - INFO - __main__ - Global step 800 Train loss 0.49 QA-F1 0.18229166666666666 on epoch=399
03/10/2022 20:26:54 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
03/10/2022 20:26:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.50 on epoch=409
03/10/2022 20:27:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.50 on epoch=414
03/10/2022 20:27:07 - INFO - __main__ - Step 840 Global step 840 Train loss 0.41 on epoch=419
03/10/2022 20:27:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=424
03/10/2022 20:27:13 - INFO - __main__ - Global step 850 Train loss 0.46 QA-F1 0.22604166666666664 on epoch=424
03/10/2022 20:27:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
03/10/2022 20:27:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=434
03/10/2022 20:27:26 - INFO - __main__ - Step 880 Global step 880 Train loss 0.44 on epoch=439
03/10/2022 20:27:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=444
03/10/2022 20:27:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=449
03/10/2022 20:27:36 - INFO - __main__ - Global step 900 Train loss 0.39 QA-F1 0.34374999999999994 on epoch=449
03/10/2022 20:27:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.42 on epoch=454
03/10/2022 20:27:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/10/2022 20:27:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
03/10/2022 20:27:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
03/10/2022 20:27:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=474
03/10/2022 20:27:59 - INFO - __main__ - Global step 950 Train loss 0.39 QA-F1 0.375 on epoch=474
03/10/2022 20:28:03 - INFO - __main__ - Step 960 Global step 960 Train loss 0.34 on epoch=479
03/10/2022 20:28:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=484
03/10/2022 20:28:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=489
03/10/2022 20:28:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=494
03/10/2022 20:28:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=499
03/10/2022 20:28:22 - INFO - __main__ - Global step 1000 Train loss 0.32 QA-F1 0.2864583333333333 on epoch=499
03/10/2022 20:28:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=504
03/10/2022 20:28:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=509
03/10/2022 20:28:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=514
03/10/2022 20:28:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=519
03/10/2022 20:28:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.30 on epoch=524
03/10/2022 20:28:45 - INFO - __main__ - Global step 1050 Train loss 0.30 QA-F1 0.35416666666666663 on epoch=524
03/10/2022 20:28:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=529
03/10/2022 20:28:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=534
03/10/2022 20:28:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.32 on epoch=539
03/10/2022 20:29:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=544
03/10/2022 20:29:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=549
03/10/2022 20:29:08 - INFO - __main__ - Global step 1100 Train loss 0.28 QA-F1 0.3020833333333333 on epoch=549
03/10/2022 20:29:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=554
03/10/2022 20:29:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.29 on epoch=559
03/10/2022 20:29:21 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=564
03/10/2022 20:29:25 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=569
03/10/2022 20:29:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=574
03/10/2022 20:29:31 - INFO - __main__ - Global step 1150 Train loss 0.28 QA-F1 0.27604166666666663 on epoch=574
03/10/2022 20:29:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=579
03/10/2022 20:29:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=584
03/10/2022 20:29:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=589
03/10/2022 20:29:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=594
03/10/2022 20:29:53 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=599
03/10/2022 20:29:54 - INFO - __main__ - Global step 1200 Train loss 0.24 QA-F1 0.2708333333333333 on epoch=599
03/10/2022 20:29:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.27 on epoch=604
03/10/2022 20:30:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=609
03/10/2022 20:30:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=614
03/10/2022 20:30:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=619
03/10/2022 20:30:16 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=624
03/10/2022 20:30:18 - INFO - __main__ - Global step 1250 Train loss 0.22 QA-F1 0.3645833333333333 on epoch=624
03/10/2022 20:30:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=629
03/10/2022 20:30:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=634
03/10/2022 20:30:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.14 on epoch=639
03/10/2022 20:30:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=644
03/10/2022 20:30:39 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/10/2022 20:30:41 - INFO - __main__ - Global step 1300 Train loss 0.22 QA-F1 0.38541666666666663 on epoch=649
03/10/2022 20:30:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=654
03/10/2022 20:30:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=659
03/10/2022 20:30:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=664
03/10/2022 20:30:58 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=669
03/10/2022 20:31:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.17 on epoch=674
03/10/2022 20:31:04 - INFO - __main__ - Global step 1350 Train loss 0.20 QA-F1 0.3020833333333333 on epoch=674
03/10/2022 20:31:08 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=679
03/10/2022 20:31:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=684
03/10/2022 20:31:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.29 on epoch=689
03/10/2022 20:31:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=694
03/10/2022 20:31:26 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=699
03/10/2022 20:31:27 - INFO - __main__ - Global step 1400 Train loss 0.28 QA-F1 0.328125 on epoch=699
03/10/2022 20:31:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=704
03/10/2022 20:31:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.16 on epoch=709
03/10/2022 20:31:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=714
03/10/2022 20:31:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.21 on epoch=719
03/10/2022 20:31:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=724
03/10/2022 20:31:50 - INFO - __main__ - Global step 1450 Train loss 0.22 QA-F1 0.43229166666666663 on epoch=724
03/10/2022 20:31:51 - INFO - __main__ - Saving model with best QA-F1: 0.38622835497835495 -> 0.43229166666666663 on epoch=724, global_step=1450
03/10/2022 20:31:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
03/10/2022 20:31:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=734
03/10/2022 20:32:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=739
03/10/2022 20:32:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=744
03/10/2022 20:32:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=749
03/10/2022 20:32:14 - INFO - __main__ - Global step 1500 Train loss 0.23 QA-F1 0.29166666666666663 on epoch=749
03/10/2022 20:32:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
03/10/2022 20:32:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=759
03/10/2022 20:32:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=764
03/10/2022 20:32:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=769
03/10/2022 20:32:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.16 on epoch=774
03/10/2022 20:32:38 - INFO - __main__ - Global step 1550 Train loss 0.18 QA-F1 0.27604166666666663 on epoch=774
03/10/2022 20:32:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=779
03/10/2022 20:32:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.54 on epoch=784
03/10/2022 20:32:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 5.10 on epoch=789
03/10/2022 20:32:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 5.17 on epoch=794
03/10/2022 20:32:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 4.09 on epoch=799
03/10/2022 20:33:01 - INFO - __main__ - Global step 1600 Train loss 3.42 QA-F1 0.26041666666666663 on epoch=799
03/10/2022 20:33:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.55 on epoch=804
03/10/2022 20:33:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.88 on epoch=809
03/10/2022 20:33:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.93 on epoch=814
03/10/2022 20:33:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.61 on epoch=819
03/10/2022 20:33:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.55 on epoch=824
03/10/2022 20:33:25 - INFO - __main__ - Global step 1650 Train loss 2.10 QA-F1 0.22916666666666663 on epoch=824
03/10/2022 20:33:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.66 on epoch=829
03/10/2022 20:33:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.44 on epoch=834
03/10/2022 20:33:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=839
03/10/2022 20:33:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.94 on epoch=844
03/10/2022 20:33:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.98 on epoch=849
03/10/2022 20:33:49 - INFO - __main__ - Global step 1700 Train loss 0.87 QA-F1 0.29166666666666663 on epoch=849
03/10/2022 20:33:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
03/10/2022 20:33:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=859
03/10/2022 20:34:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.63 on epoch=864
03/10/2022 20:34:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.61 on epoch=869
03/10/2022 20:34:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.68 on epoch=874
03/10/2022 20:34:13 - INFO - __main__ - Global step 1750 Train loss 0.54 QA-F1 0.28125 on epoch=874
03/10/2022 20:34:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.86 on epoch=879
03/10/2022 20:34:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.32 on epoch=884
03/10/2022 20:34:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.89 on epoch=889
03/10/2022 20:34:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.89 on epoch=894
03/10/2022 20:34:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.80 on epoch=899
03/10/2022 20:34:37 - INFO - __main__ - Global step 1800 Train loss 0.95 QA-F1 0.25 on epoch=899
03/10/2022 20:34:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.46 on epoch=904
03/10/2022 20:34:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=909
03/10/2022 20:34:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=914
03/10/2022 20:34:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
03/10/2022 20:34:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.43 on epoch=924
03/10/2022 20:35:00 - INFO - __main__ - Global step 1850 Train loss 0.33 QA-F1 0.3125 on epoch=924
03/10/2022 20:35:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.44 on epoch=929
03/10/2022 20:35:09 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.50 on epoch=934
03/10/2022 20:35:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=939
03/10/2022 20:35:17 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.27 on epoch=944
03/10/2022 20:35:22 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=949
03/10/2022 20:35:24 - INFO - __main__ - Global step 1900 Train loss 0.35 QA-F1 0.234375 on epoch=949
03/10/2022 20:35:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/10/2022 20:35:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=959
03/10/2022 20:35:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=964
03/10/2022 20:35:41 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.22 on epoch=969
03/10/2022 20:35:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.27 on epoch=974
03/10/2022 20:35:47 - INFO - __main__ - Global step 1950 Train loss 0.24 QA-F1 0.265625 on epoch=974
03/10/2022 20:35:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=979
03/10/2022 20:35:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.26 on epoch=984
03/10/2022 20:36:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=989
03/10/2022 20:36:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=994
03/10/2022 20:36:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=999
03/10/2022 20:36:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:36:10 - INFO - __main__ - Printing 3 examples
03/10/2022 20:36:10 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/10/2022 20:36:10 - INFO - __main__ - ['Elaine Benson']
03/10/2022 20:36:10 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/10/2022 20:36:10 - INFO - __main__ - ['Richard Boston']
03/10/2022 20:36:10 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/10/2022 20:36:10 - INFO - __main__ - ['The Knight Errant']
03/10/2022 20:36:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:36:10 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:36:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:36:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:36:10 - INFO - __main__ - Printing 3 examples
03/10/2022 20:36:10 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/10/2022 20:36:10 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/10/2022 20:36:10 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/10/2022 20:36:10 - INFO - __main__ - ['Captain England']
03/10/2022 20:36:10 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/10/2022 20:36:10 - INFO - __main__ - ['Saint-Saëns']
03/10/2022 20:36:10 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:36:10 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:36:10 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:36:11 - INFO - __main__ - Global step 2000 Train loss 0.25 QA-F1 0.21354166666666666 on epoch=999
03/10/2022 20:36:11 - INFO - __main__ - save last model!
03/10/2022 20:36:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 20:36:11 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 20:36:11 - INFO - __main__ - Printing 3 examples
03/10/2022 20:36:11 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:36:11 - INFO - __main__ - ['Frankie']
03/10/2022 20:36:11 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:36:11 - INFO - __main__ - ['Frankie']
03/10/2022 20:36:11 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:36:11 - INFO - __main__ - ['Frankie']
03/10/2022 20:36:11 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:36:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:36:18 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 20:36:24 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 20:36:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 20:36:25 - INFO - __main__ - Starting training!
03/10/2022 20:38:11 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_87_0.4_8_predictions.txt
03/10/2022 20:38:11 - INFO - __main__ - QA-F1 on test data: 0.3180
03/10/2022 20:38:12 - INFO - __main__ - prefix=quoref_32_87, lr=0.4, bsz=8, dev_performance=0.43229166666666663, test_performance=0.31799386396160595
03/10/2022 20:38:12 - INFO - __main__ - Running ... prefix=quoref_32_87, lr=0.3, bsz=8 ...
03/10/2022 20:38:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:38:12 - INFO - __main__ - Printing 3 examples
03/10/2022 20:38:12 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/10/2022 20:38:12 - INFO - __main__ - ['Elaine Benson']
03/10/2022 20:38:12 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/10/2022 20:38:12 - INFO - __main__ - ['Richard Boston']
03/10/2022 20:38:12 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/10/2022 20:38:12 - INFO - __main__ - ['The Knight Errant']
03/10/2022 20:38:12 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:38:13 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:38:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:38:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:38:13 - INFO - __main__ - Printing 3 examples
03/10/2022 20:38:13 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/10/2022 20:38:13 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/10/2022 20:38:13 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/10/2022 20:38:13 - INFO - __main__ - ['Captain England']
03/10/2022 20:38:13 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/10/2022 20:38:13 - INFO - __main__ - ['Saint-Saëns']
03/10/2022 20:38:13 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:38:13 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:38:13 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:38:26 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 20:38:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 20:38:27 - INFO - __main__ - Starting training!
03/10/2022 20:38:32 - INFO - __main__ - Step 10 Global step 10 Train loss 2.38 on epoch=4
03/10/2022 20:38:36 - INFO - __main__ - Step 20 Global step 20 Train loss 1.99 on epoch=9
03/10/2022 20:38:40 - INFO - __main__ - Step 30 Global step 30 Train loss 1.73 on epoch=14
03/10/2022 20:38:45 - INFO - __main__ - Step 40 Global step 40 Train loss 1.55 on epoch=19
03/10/2022 20:38:49 - INFO - __main__ - Step 50 Global step 50 Train loss 1.45 on epoch=24
03/10/2022 20:38:50 - INFO - __main__ - Global step 50 Train loss 1.82 QA-F1 0.35937499999999994 on epoch=24
03/10/2022 20:38:50 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.35937499999999994 on epoch=24, global_step=50
03/10/2022 20:38:55 - INFO - __main__ - Step 60 Global step 60 Train loss 1.25 on epoch=29
03/10/2022 20:38:59 - INFO - __main__ - Step 70 Global step 70 Train loss 1.21 on epoch=34
03/10/2022 20:39:03 - INFO - __main__ - Step 80 Global step 80 Train loss 1.25 on epoch=39
03/10/2022 20:39:08 - INFO - __main__ - Step 90 Global step 90 Train loss 1.15 on epoch=44
03/10/2022 20:39:12 - INFO - __main__ - Step 100 Global step 100 Train loss 1.08 on epoch=49
03/10/2022 20:39:14 - INFO - __main__ - Global step 100 Train loss 1.19 QA-F1 0.3125 on epoch=49
03/10/2022 20:39:18 - INFO - __main__ - Step 110 Global step 110 Train loss 1.33 on epoch=54
03/10/2022 20:39:22 - INFO - __main__ - Step 120 Global step 120 Train loss 1.15 on epoch=59
03/10/2022 20:39:27 - INFO - __main__ - Step 130 Global step 130 Train loss 1.13 on epoch=64
03/10/2022 20:39:31 - INFO - __main__ - Step 140 Global step 140 Train loss 1.02 on epoch=69
03/10/2022 20:39:35 - INFO - __main__ - Step 150 Global step 150 Train loss 1.08 on epoch=74
03/10/2022 20:39:37 - INFO - __main__ - Global step 150 Train loss 1.14 QA-F1 0.28125 on epoch=74
03/10/2022 20:39:41 - INFO - __main__ - Step 160 Global step 160 Train loss 1.00 on epoch=79
03/10/2022 20:39:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.91 on epoch=84
03/10/2022 20:39:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.97 on epoch=89
03/10/2022 20:39:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.94 on epoch=94
03/10/2022 20:39:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.90 on epoch=99
03/10/2022 20:40:00 - INFO - __main__ - Global step 200 Train loss 0.94 QA-F1 0.26041666666666663 on epoch=99
03/10/2022 20:40:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.87 on epoch=104
03/10/2022 20:40:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.86 on epoch=109
03/10/2022 20:40:13 - INFO - __main__ - Step 230 Global step 230 Train loss 0.83 on epoch=114
03/10/2022 20:40:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.83 on epoch=119
03/10/2022 20:40:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.77 on epoch=124
03/10/2022 20:40:23 - INFO - __main__ - Global step 250 Train loss 0.83 QA-F1 0.23958333333333331 on epoch=124
03/10/2022 20:40:28 - INFO - __main__ - Step 260 Global step 260 Train loss 0.65 on epoch=129
03/10/2022 20:40:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.66 on epoch=134
03/10/2022 20:40:36 - INFO - __main__ - Step 280 Global step 280 Train loss 0.73 on epoch=139
03/10/2022 20:40:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.69 on epoch=144
03/10/2022 20:40:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.63 on epoch=149
03/10/2022 20:40:46 - INFO - __main__ - Global step 300 Train loss 0.67 QA-F1 0.234375 on epoch=149
03/10/2022 20:40:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.71 on epoch=154
03/10/2022 20:40:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.68 on epoch=159
03/10/2022 20:40:59 - INFO - __main__ - Step 330 Global step 330 Train loss 0.59 on epoch=164
03/10/2022 20:41:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.59 on epoch=169
03/10/2022 20:41:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.52 on epoch=174
03/10/2022 20:41:10 - INFO - __main__ - Global step 350 Train loss 0.62 QA-F1 0.23958333333333331 on epoch=174
03/10/2022 20:41:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.53 on epoch=179
03/10/2022 20:41:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.54 on epoch=184
03/10/2022 20:41:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.54 on epoch=189
03/10/2022 20:41:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.54 on epoch=194
03/10/2022 20:41:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.42 on epoch=199
03/10/2022 20:41:33 - INFO - __main__ - Global step 400 Train loss 0.51 QA-F1 0.21354166666666666 on epoch=199
03/10/2022 20:41:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.47 on epoch=204
03/10/2022 20:41:41 - INFO - __main__ - Step 420 Global step 420 Train loss 0.47 on epoch=209
03/10/2022 20:41:45 - INFO - __main__ - Step 430 Global step 430 Train loss 0.50 on epoch=214
03/10/2022 20:41:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.50 on epoch=219
03/10/2022 20:41:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=224
03/10/2022 20:41:56 - INFO - __main__ - Global step 450 Train loss 0.47 QA-F1 0.23958333333333331 on epoch=224
03/10/2022 20:42:00 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=229
03/10/2022 20:42:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.47 on epoch=234
03/10/2022 20:42:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=239
03/10/2022 20:42:13 - INFO - __main__ - Step 490 Global step 490 Train loss 0.43 on epoch=244
03/10/2022 20:42:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=249
03/10/2022 20:42:19 - INFO - __main__ - Global step 500 Train loss 0.42 QA-F1 0.24479166666666666 on epoch=249
03/10/2022 20:42:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
03/10/2022 20:42:27 - INFO - __main__ - Step 520 Global step 520 Train loss 0.43 on epoch=259
03/10/2022 20:42:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=264
03/10/2022 20:42:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
03/10/2022 20:42:40 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=274
03/10/2022 20:42:42 - INFO - __main__ - Global step 550 Train loss 0.35 QA-F1 0.23958333333333331 on epoch=274
03/10/2022 20:42:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.29 on epoch=279
03/10/2022 20:42:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=284
03/10/2022 20:42:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=289
03/10/2022 20:42:59 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/10/2022 20:43:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
03/10/2022 20:43:05 - INFO - __main__ - Global step 600 Train loss 0.31 QA-F1 0.265625 on epoch=299
03/10/2022 20:43:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=304
03/10/2022 20:43:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.28 on epoch=309
03/10/2022 20:43:18 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=314
03/10/2022 20:43:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=319
03/10/2022 20:43:27 - INFO - __main__ - Step 650 Global step 650 Train loss 0.27 on epoch=324
03/10/2022 20:43:28 - INFO - __main__ - Global step 650 Train loss 0.28 QA-F1 0.2708333333333333 on epoch=324
03/10/2022 20:43:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=329
03/10/2022 20:43:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=334
03/10/2022 20:43:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.28 on epoch=339
03/10/2022 20:43:46 - INFO - __main__ - Step 690 Global step 690 Train loss 0.69 on epoch=344
03/10/2022 20:43:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.57 on epoch=349
03/10/2022 20:43:51 - INFO - __main__ - Global step 700 Train loss 0.41 QA-F1 0.14583333333333331 on epoch=349
03/10/2022 20:43:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.57 on epoch=354
03/10/2022 20:44:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=359
03/10/2022 20:44:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.53 on epoch=364
03/10/2022 20:44:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.63 on epoch=369
03/10/2022 20:44:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=374
03/10/2022 20:44:15 - INFO - __main__ - Global step 750 Train loss 0.51 QA-F1 0.21875 on epoch=374
03/10/2022 20:44:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.56 on epoch=379
03/10/2022 20:44:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=384
03/10/2022 20:44:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=389
03/10/2022 20:44:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=394
03/10/2022 20:44:37 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=399
03/10/2022 20:44:38 - INFO - __main__ - Global step 800 Train loss 0.36 QA-F1 0.375 on epoch=399
03/10/2022 20:44:38 - INFO - __main__ - Saving model with best QA-F1: 0.35937499999999994 -> 0.375 on epoch=399, global_step=800
03/10/2022 20:44:42 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
03/10/2022 20:44:47 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=409
03/10/2022 20:44:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=414
03/10/2022 20:44:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/10/2022 20:45:00 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=424
03/10/2022 20:45:01 - INFO - __main__ - Global step 850 Train loss 0.23 QA-F1 0.3125 on epoch=424
03/10/2022 20:45:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=429
03/10/2022 20:45:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.17 on epoch=434
03/10/2022 20:45:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=439
03/10/2022 20:45:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=444
03/10/2022 20:45:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=449
03/10/2022 20:45:25 - INFO - __main__ - Global step 900 Train loss 0.18 QA-F1 0.22916666666666666 on epoch=449
03/10/2022 20:45:29 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=454
03/10/2022 20:45:33 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=459
03/10/2022 20:45:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.20 on epoch=464
03/10/2022 20:45:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=469
03/10/2022 20:45:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=474
03/10/2022 20:45:48 - INFO - __main__ - Global step 950 Train loss 0.19 QA-F1 0.296875 on epoch=474
03/10/2022 20:45:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=479
03/10/2022 20:45:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=484
03/10/2022 20:46:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=489
03/10/2022 20:46:05 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=494
03/10/2022 20:46:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
03/10/2022 20:46:11 - INFO - __main__ - Global step 1000 Train loss 0.20 QA-F1 0.25 on epoch=499
03/10/2022 20:46:16 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=504
03/10/2022 20:46:20 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=509
03/10/2022 20:46:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.17 on epoch=514
03/10/2022 20:46:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=519
03/10/2022 20:46:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=524
03/10/2022 20:46:34 - INFO - __main__ - Global step 1050 Train loss 0.16 QA-F1 0.27604166666666663 on epoch=524
03/10/2022 20:46:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=529
03/10/2022 20:46:43 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.17 on epoch=534
03/10/2022 20:46:47 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=539
03/10/2022 20:46:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=544
03/10/2022 20:46:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.18 on epoch=549
03/10/2022 20:46:57 - INFO - __main__ - Global step 1100 Train loss 0.16 QA-F1 0.22916666666666666 on epoch=549
03/10/2022 20:47:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.18 on epoch=554
03/10/2022 20:47:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=559
03/10/2022 20:47:10 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=564
03/10/2022 20:47:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=569
03/10/2022 20:47:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.15 on epoch=574
03/10/2022 20:47:21 - INFO - __main__ - Global step 1150 Train loss 0.18 QA-F1 0.32291666666666663 on epoch=574
03/10/2022 20:47:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=579
03/10/2022 20:47:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=584
03/10/2022 20:47:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=589
03/10/2022 20:47:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=594
03/10/2022 20:47:42 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=599
03/10/2022 20:47:44 - INFO - __main__ - Global step 1200 Train loss 0.15 QA-F1 0.3020833333333333 on epoch=599
03/10/2022 20:47:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.16 on epoch=604
03/10/2022 20:47:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.16 on epoch=609
03/10/2022 20:47:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.11 on epoch=614
03/10/2022 20:48:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.15 on epoch=619
03/10/2022 20:48:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=624
03/10/2022 20:48:07 - INFO - __main__ - Global step 1250 Train loss 0.14 QA-F1 0.32291666666666663 on epoch=624
03/10/2022 20:48:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=629
03/10/2022 20:48:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.14 on epoch=634
03/10/2022 20:48:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=639
03/10/2022 20:48:25 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=644
03/10/2022 20:48:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=649
03/10/2022 20:48:30 - INFO - __main__ - Global step 1300 Train loss 0.14 QA-F1 0.2864583333333333 on epoch=649
03/10/2022 20:48:35 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.13 on epoch=654
03/10/2022 20:48:39 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.11 on epoch=659
03/10/2022 20:48:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=664
03/10/2022 20:48:48 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.11 on epoch=669
03/10/2022 20:48:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.18 on epoch=674
03/10/2022 20:48:53 - INFO - __main__ - Global step 1350 Train loss 0.13 QA-F1 0.35416666666666663 on epoch=674
03/10/2022 20:48:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.13 on epoch=679
03/10/2022 20:49:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=684
03/10/2022 20:49:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=689
03/10/2022 20:49:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=694
03/10/2022 20:49:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=699
03/10/2022 20:49:16 - INFO - __main__ - Global step 1400 Train loss 0.14 QA-F1 0.24479166666666666 on epoch=699
03/10/2022 20:49:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=704
03/10/2022 20:49:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=709
03/10/2022 20:49:29 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=714
03/10/2022 20:49:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.12 on epoch=719
03/10/2022 20:49:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=724
03/10/2022 20:49:40 - INFO - __main__ - Global step 1450 Train loss 0.13 QA-F1 0.19791666666666666 on epoch=724
03/10/2022 20:49:44 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=729
03/10/2022 20:49:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=734
03/10/2022 20:49:53 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=739
03/10/2022 20:49:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=744
03/10/2022 20:50:01 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=749
03/10/2022 20:50:03 - INFO - __main__ - Global step 1500 Train loss 0.15 QA-F1 0.25 on epoch=749
03/10/2022 20:50:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=754
03/10/2022 20:50:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.10 on epoch=759
03/10/2022 20:50:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=764
03/10/2022 20:50:20 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=769
03/10/2022 20:50:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=774
03/10/2022 20:50:26 - INFO - __main__ - Global step 1550 Train loss 0.13 QA-F1 0.36458333333333326 on epoch=774
03/10/2022 20:50:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=779
03/10/2022 20:50:35 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.12 on epoch=784
03/10/2022 20:50:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=789
03/10/2022 20:50:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=794
03/10/2022 20:50:48 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.10 on epoch=799
03/10/2022 20:50:49 - INFO - __main__ - Global step 1600 Train loss 0.12 QA-F1 0.3125 on epoch=799
03/10/2022 20:50:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.11 on epoch=804
03/10/2022 20:50:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=809
03/10/2022 20:51:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=814
03/10/2022 20:51:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=819
03/10/2022 20:51:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=824
03/10/2022 20:51:13 - INFO - __main__ - Global step 1650 Train loss 0.11 QA-F1 0.3333333333333333 on epoch=824
03/10/2022 20:51:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=829
03/10/2022 20:51:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=834
03/10/2022 20:51:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=839
03/10/2022 20:51:30 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
03/10/2022 20:51:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=849
03/10/2022 20:51:36 - INFO - __main__ - Global step 1700 Train loss 0.11 QA-F1 0.28125 on epoch=849
03/10/2022 20:51:40 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=854
03/10/2022 20:51:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=859
03/10/2022 20:51:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=864
03/10/2022 20:51:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=869
03/10/2022 20:51:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=874
03/10/2022 20:51:59 - INFO - __main__ - Global step 1750 Train loss 0.13 QA-F1 0.3125 on epoch=874
03/10/2022 20:52:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=879
03/10/2022 20:52:08 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=884
03/10/2022 20:52:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=889
03/10/2022 20:52:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=894
03/10/2022 20:52:21 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.12 on epoch=899
03/10/2022 20:52:23 - INFO - __main__ - Global step 1800 Train loss 0.11 QA-F1 0.3489583333333333 on epoch=899
03/10/2022 20:52:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=904
03/10/2022 20:52:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=909
03/10/2022 20:52:36 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.09 on epoch=914
03/10/2022 20:52:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=919
03/10/2022 20:52:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=924
03/10/2022 20:52:46 - INFO - __main__ - Global step 1850 Train loss 0.10 QA-F1 0.35416666666666663 on epoch=924
03/10/2022 20:52:50 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=929
03/10/2022 20:52:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/10/2022 20:52:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=939
03/10/2022 20:53:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.09 on epoch=944
03/10/2022 20:53:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=949
03/10/2022 20:53:09 - INFO - __main__ - Global step 1900 Train loss 0.12 QA-F1 0.38541666666666663 on epoch=949
03/10/2022 20:53:09 - INFO - __main__ - Saving model with best QA-F1: 0.375 -> 0.38541666666666663 on epoch=949, global_step=1900
03/10/2022 20:53:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
03/10/2022 20:53:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=959
03/10/2022 20:53:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.08 on epoch=964
03/10/2022 20:53:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.09 on epoch=969
03/10/2022 20:53:31 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=974
03/10/2022 20:53:33 - INFO - __main__ - Global step 1950 Train loss 0.09 QA-F1 0.3333333333333333 on epoch=974
03/10/2022 20:53:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.09 on epoch=979
03/10/2022 20:53:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=984
03/10/2022 20:53:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=989
03/10/2022 20:53:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.10 on epoch=994
03/10/2022 20:53:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=999
03/10/2022 20:53:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:53:56 - INFO - __main__ - Printing 3 examples
03/10/2022 20:53:56 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/10/2022 20:53:56 - INFO - __main__ - ['Elaine Benson']
03/10/2022 20:53:56 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/10/2022 20:53:56 - INFO - __main__ - ['Richard Boston']
03/10/2022 20:53:56 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/10/2022 20:53:56 - INFO - __main__ - ['The Knight Errant']
03/10/2022 20:53:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 20:53:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:53:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:53:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:53:56 - INFO - __main__ - Printing 3 examples
03/10/2022 20:53:56 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/10/2022 20:53:56 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/10/2022 20:53:56 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/10/2022 20:53:56 - INFO - __main__ - ['Captain England']
03/10/2022 20:53:56 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/10/2022 20:53:56 - INFO - __main__ - ['Saint-Saëns']
03/10/2022 20:53:56 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:53:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:53:56 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:53:56 - INFO - __main__ - Global step 2000 Train loss 0.10 QA-F1 0.28125 on epoch=999
03/10/2022 20:53:56 - INFO - __main__ - save last model!
03/10/2022 20:53:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 20:53:56 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 20:53:56 - INFO - __main__ - Printing 3 examples
03/10/2022 20:53:56 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:53:56 - INFO - __main__ - ['Frankie']
03/10/2022 20:53:56 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:53:56 - INFO - __main__ - ['Frankie']
03/10/2022 20:53:56 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 20:53:56 - INFO - __main__ - ['Frankie']
03/10/2022 20:53:56 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:54:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:54:03 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 20:54:12 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 20:54:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 20:54:13 - INFO - __main__ - Starting training!
03/10/2022 20:55:56 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_87_0.3_8_predictions.txt
03/10/2022 20:55:56 - INFO - __main__ - QA-F1 on test data: 0.3495
03/10/2022 20:55:57 - INFO - __main__ - prefix=quoref_32_87, lr=0.3, bsz=8, dev_performance=0.38541666666666663, test_performance=0.3494706905997228
03/10/2022 20:55:57 - INFO - __main__ - Running ... prefix=quoref_32_87, lr=0.2, bsz=8 ...
03/10/2022 20:55:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:55:57 - INFO - __main__ - Printing 3 examples
03/10/2022 20:55:57 - INFO - __main__ -  [quoref] question: What is the full name of the person developing a romantic interest in Phil Fletcher? context: Music student Nancy, the 19-year-old daughter of Frank, real estate broker, and Elaine Benson (Bob Hope and Jane Wyman), wants to marry fellow music student David, the 20-year-old son of Oliver Poe, record producer. What the bride doesn't know is that her parents are about to get a divorce. Poe is opposed to marriage and doesn't want the kids to get married. At the church, when the wedding is in progress, he exposes the Bensons' secret. Nancy and David decide marriage isn't necessary. They will live together instead, travel around the country with a rock band and heed the advice and wisdom of a Persian mystic called the Baba Zeba. Frank and Elaine are seeing other people. He is involved with a divorcee, Lois Grey, while she is developing an interest in Phil Fletcher, who also is recently divorced. Poe, meanwhile, continues to see, LaVerne Baker, his live in girl friend. Then one day, Nancy finds out she is pregnant. The Baba Zeba persuades her to put up the baby for adoption, paid off by Oliver. Frank and Elaine conspire behind their daughter's back to adopt their own grandchild. Complications arise, resulting in Frank trying to bribe the guru and even disguising himself as one of the Baba Zeba's robed followers. By the end, all is resolved; the Bensons get back together, David and Nancy have their baby, even Poe and LaVerne have married giving the film a thriced blessed happy ending.
03/10/2022 20:55:57 - INFO - __main__ - ['Elaine Benson']
03/10/2022 20:55:57 - INFO - __main__ -  [quoref] question: What is the full name of the man that wrote about the person that studied at St. Ronan's preparatory school? context: Lancaster was born in London in 1908, the only child of Robert Lancaster (1880–1917) and his wife, Clare Bracebridge, née Manger. His paternal grandfather, Sir William Lancaster, rose from modest beginnings to become the chief executive of the Prudential Assurance Company, Lord of the manor of East Winch, Norfolk, and a philanthropist in the field of education. Osbert's mother was an artist, known for her paintings of flowers, who had exhibited regularly at the Royal Academy; his father was a publisher, who volunteered for the army on the outbreak of the First World War, was commissioned as a second lieutenant in the Norfolk Regiment, and was killed at the Battle of Arras in April 1917. Elgin Crescent, Notting Hill, where Lancaster was born and raised, was an upper-middle class area. The family maintained a staff of servants, including a cook and a nurse. Such was the mixed nature of London in the early years of the 20th century that a short distance away were the deprived and dangerous Notting Dale and the Portobello Road, where, as Lancaster recalled in his 1953 memoirs, it was said to be impossible for a well-dressed man to walk and emerge intact. From an early age Lancaster was aware of the variety of classes, nationalities, and social attitudes around him.In 1918 Lancaster was sent to St Ronan's preparatory school, Worthing. The régime at the school leaned heavily towards sport, in which he was neither interested nor proficient. The headmaster, Stanley Harris, was a celebrated amateur footballer and occasional first class cricketer, but he was reasonably tolerant of Lancaster's disdain for games, and on the whole Lancaster enjoyed his time at the school. His education there was, he later commented, of more importance to him than anything he learned later in his school and university career. He left St Ronan's in 1921, aged thirteen, and went to Charterhouse, where his father and uncles had all been sent. There he was shocked by the bullying and bad language, but in addition to its sporty, philistine "bloods", the school had an intellectual and aesthetic tradition. Lancaster's biographer Richard Boston writes, "The hearty Baden-Powell, for example, was offset by Ralph Vaughan Williams and Robert Graves, while talented Carthusian artists had included Thackeray, Leech, Lovat Fraser and Max Beerbohm". The art master, P. J. ("Purple") Johnson, encouraged Lancaster, insisting that a sound technique was a prerequisite for effective self-expression in drawing or painting; in that respect the boy's time at the school was valuable, though otherwise the headmaster found him "irretrievably gauche ... a sad disappointment". Lancaster shared Beerbohm's view that being an old boy of the school was more pleasurable than being a pupil there.
03/10/2022 20:55:57 - INFO - __main__ - ['Richard Boston']
03/10/2022 20:55:57 - INFO - __main__ -  [quoref] question: What was the name of the work that was likely influenced by Musidora? context: By the time Etty exhibited Musidora, the theme was becoming something of a cliche, such that by 1850 it was described by The Literary Gazette as "a favourite subject for a dip of the brush". As interest in studies of Musidora waned, its role as a pretext for nude paintings by English artists was replaced by Lady Godiva, who had become a topic of increased interest owing to Alfred, Lord Tennyson's poem Godiva. After the death of William Wordsworth in 1850, James Thomson ceased to be a major influence on writers. From the 1870s his popularity with readers waned, and by the end of the 20th century his works other than Rule, Britannia! were little known.When Etty died in 1849, despite having worked and exhibited until his death, he was still  regarded by many as a pornographer. Charles Robert Leslie observed shortly after Etty's death that "[Etty] himself, thinking and meaning no evil, was not aware of the manner in which his works were regarded by grosser minds". Interest in him declined as new movements came to characterise painting in Britain, and by the end of the 19th century the value of his paintings had fallen. It is likely that the composition and style of John Everett Millais's controversial The Knight Errant was influenced by Musidora, but other than Millais, and Etty's admirer and imitator William Edward Frost, few other artists were directly influenced by Etty's work. In 1882 Vanity Fair commented on Musidora that "I know only too well how the rough and his female companion behave in front of pictures such as Etty's bather. I have seen the gangs of workmen strolling round, and I know that their artistic interest in studies of the nude is emphatically embarrassing." By the early 20th century Victorian styles of art and literature fell dramatically out of fashion in Britain, and by 1915 the word "Victorian" had become a derogatory term. Frederick Mentone's The Human Form in Art (1944) was one of the few 20th-century academic works to favourably view Musidora.
03/10/2022 20:55:57 - INFO - __main__ - ['The Knight Errant']
03/10/2022 20:55:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 20:55:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:55:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 20:55:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 20:55:58 - INFO - __main__ - Printing 3 examples
03/10/2022 20:55:58 - INFO - __main__ -  [quoref] question: The mention of the author's brief voluntary institutionalization results in friction between which two characters? context: Writer David Lipsky is dismayed to hear about the suicide of novelist David Foster Wallace in 2008. He had interviewed the author over a period of days twelve years earlier, following the publication of Wallace's novel Infinite Jest, which received critical praise and became an international bestseller, a touchstone for numerous readers. He listens to the recordings he made during their time together. The film returns to the period shortly after the book's release. Although initially skeptical of the high praise Wallace's book is receiving, Lipsky – a writer having only marginal success – is awestruck after reading it. He persuades his editor at Rolling Stone magazine to give him an assignment to interview Wallace during his book tour. The journalist travels to meet Wallace at his home on the outskirts of Bloomington-Normal, Illinois (near Illinois State University where the author teaches writing). Lipsky finds the young author unassuming and amiable, but indifferent to being interviewed. Wallace permits Lipsky to tape-record their conversations, with the proviso that Lipsky won't use any direct quotes which Wallace asks to have taken "off the record" five minutes later. Wallace opens up to Lipsky on a variety of subjects, ranging from dogs to television to fame and self-identity, but remains somewhat guarded. He tacitly admits to alcoholism, but offers few details of his experience. Lipsky's mention of Wallace's brief voluntary institutionalization under a suicide watch causes some friction between them.
03/10/2022 20:55:58 - INFO - __main__ - ['David Lipsky', 'David Foster Wallace']
03/10/2022 20:55:58 - INFO - __main__ -  [quoref] question: Who was the captain of the Nimrod? context: On arriving in McMurdo Sound on 29 January 1908, Nimrod's progress southward to the Discovery base at Hut Point was blocked by frozen sea. Shackleton decided to wait a few days in the hope that the ice would break up. During this delay, second officer Aeneas Mackintosh suffered an accident that led to the loss of his right eye. After emergency surgery by Marshall and Mackay, he was forced to relinquish his shore party place and go back to New Zealand with Nimrod. He recovered sufficiently to return with the ship in the following season.On 3 February Shackleton decided not to wait for the ice to shift but to make his headquarters at the nearest practicable landing place, Cape Royds. Late that evening the ship was moored, and a suitable site for the expedition's prefabricated hut was selected. The site was separated from Hut Point by 20 nautical miles (37 km; 23 mi) of sea, with no landward route to the south. Shackleton believed the party was "fortunate to get winter quarters as near as this to our starting point for the south."The following days were occupied with the landing of stores and equipment. This work was hampered by poor weather and by the caution of Captain England, who frequently took the ship out into the bay until ice conditions at the landing ground were in his view safer. The next fortnight followed this pattern, leading to sharp dissent between Shackleton and the captain. At one point, Shackleton asked England to stand down on the grounds that he was ill, but England refused. The task of unloading became, in Riffenburgh's description, "mind-numbingly difficult" but was finally completed on 22 February. Nimrod at last sailed away north, England unaware that ship's engineer Harry Dunlop was carrying a letter from Shackleton to the expedition's New Zealand agent, requesting a replacement captain for the return voyage next year. This knowledge was an open secret among the shore party; Marshall recorded in his diary that he was "glad to see the last of [England] ... whole thing damned disgrace to name of country!"
03/10/2022 20:55:58 - INFO - __main__ - ['Captain England']
03/10/2022 20:55:58 - INFO - __main__ -  [quoref] question: What is the last name of the person whose organ professor's pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet? context: In 1848, at the age of thirteen, Saint-Saëns was admitted to the Paris Conservatoire, France's foremost music academy. The director, Daniel Auber, had succeeded Luigi Cherubini in 1842, and brought a more relaxed regime than that of his martinet predecessor, though the curriculum remained conservative. Students, even outstanding pianists like Saint-Saëns, were encouraged to specialise in organ studies, because a career as a church organist was seen to offer more opportunities than that of a solo pianist. His organ professor was François Benoist, whom Saint-Saëns considered a mediocre organist but a first-rate teacher; his pupils included Adolphe Adam, César Franck, Charles Alkan, Louis Lefébure-Wély and Georges Bizet. In 1851 Saint-Saëns won the Conservatoire's top prize for organists, and in the same year he began formal composition studies. His professor was a protégé of Cherubini, Fromental Halévy, whose pupils included Charles Gounod and Bizet.Saint-Saëns's student compositions included a symphony in A major (1850) and a choral piece, Les Djinns (1850), after an eponymous poem by Victor Hugo. He competed for France's premier musical award, the Prix de Rome, in 1852 but was unsuccessful. Auber believed that the prize should have gone to Saint-Saëns, considering him to have more promise than the winner, Léonce Cohen, who made little mark during the rest of his career. In the same year Saint-Saëns had greater success in a competition organised by the Société Sainte-Cécile, Paris, with his Ode à Sainte-Cécile, for which the judges unanimously voted him the first prize. The first piece the composer acknowledged as a mature work and gave an opus number was Trois Morceaux for harmonium (1852).
03/10/2022 20:55:58 - INFO - __main__ - ['Saint-Saëns']
03/10/2022 20:55:58 - INFO - __main__ - Tokenizing Input ...
03/10/2022 20:55:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 20:55:58 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 20:56:11 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 20:56:12 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 20:56:12 - INFO - __main__ - Starting training!
03/10/2022 20:56:17 - INFO - __main__ - Step 10 Global step 10 Train loss 2.55 on epoch=4
03/10/2022 20:56:21 - INFO - __main__ - Step 20 Global step 20 Train loss 2.34 on epoch=9
03/10/2022 20:56:25 - INFO - __main__ - Step 30 Global step 30 Train loss 2.16 on epoch=14
03/10/2022 20:56:30 - INFO - __main__ - Step 40 Global step 40 Train loss 1.94 on epoch=19
03/10/2022 20:56:34 - INFO - __main__ - Step 50 Global step 50 Train loss 1.77 on epoch=24
03/10/2022 20:56:38 - INFO - __main__ - Global step 50 Train loss 2.15 QA-F1 0.2934027777777778 on epoch=24
03/10/2022 20:56:38 - INFO - __main__ - Saving model with best QA-F1: -1.0 -> 0.2934027777777778 on epoch=24, global_step=50
03/10/2022 20:56:42 - INFO - __main__ - Step 60 Global step 60 Train loss 1.69 on epoch=29
03/10/2022 20:56:46 - INFO - __main__ - Step 70 Global step 70 Train loss 1.61 on epoch=34
03/10/2022 20:56:51 - INFO - __main__ - Step 80 Global step 80 Train loss 1.59 on epoch=39
03/10/2022 20:56:55 - INFO - __main__ - Step 90 Global step 90 Train loss 1.50 on epoch=44
03/10/2022 20:56:59 - INFO - __main__ - Step 100 Global step 100 Train loss 1.42 on epoch=49
03/10/2022 20:57:01 - INFO - __main__ - Global step 100 Train loss 1.56 QA-F1 0.32812499999999994 on epoch=49
03/10/2022 20:57:02 - INFO - __main__ - Saving model with best QA-F1: 0.2934027777777778 -> 0.32812499999999994 on epoch=49, global_step=100
03/10/2022 20:57:06 - INFO - __main__ - Step 110 Global step 110 Train loss 1.43 on epoch=54
03/10/2022 20:57:10 - INFO - __main__ - Step 120 Global step 120 Train loss 1.33 on epoch=59
03/10/2022 20:57:14 - INFO - __main__ - Step 130 Global step 130 Train loss 1.33 on epoch=64
03/10/2022 20:57:19 - INFO - __main__ - Step 140 Global step 140 Train loss 1.37 on epoch=69
03/10/2022 20:57:23 - INFO - __main__ - Step 150 Global step 150 Train loss 1.31 on epoch=74
03/10/2022 20:57:25 - INFO - __main__ - Global step 150 Train loss 1.35 QA-F1 0.296875 on epoch=74
03/10/2022 20:57:29 - INFO - __main__ - Step 160 Global step 160 Train loss 1.34 on epoch=79
03/10/2022 20:57:33 - INFO - __main__ - Step 170 Global step 170 Train loss 1.20 on epoch=84
03/10/2022 20:57:38 - INFO - __main__ - Step 180 Global step 180 Train loss 1.16 on epoch=89
03/10/2022 20:57:42 - INFO - __main__ - Step 190 Global step 190 Train loss 1.12 on epoch=94
03/10/2022 20:57:46 - INFO - __main__ - Step 200 Global step 200 Train loss 1.28 on epoch=99
03/10/2022 20:57:48 - INFO - __main__ - Global step 200 Train loss 1.22 QA-F1 0.234375 on epoch=99
03/10/2022 20:57:52 - INFO - __main__ - Step 210 Global step 210 Train loss 1.19 on epoch=104
03/10/2022 20:57:57 - INFO - __main__ - Step 220 Global step 220 Train loss 1.20 on epoch=109
03/10/2022 20:58:01 - INFO - __main__ - Step 230 Global step 230 Train loss 1.20 on epoch=114
03/10/2022 20:58:05 - INFO - __main__ - Step 240 Global step 240 Train loss 1.12 on epoch=119
03/10/2022 20:58:10 - INFO - __main__ - Step 250 Global step 250 Train loss 1.13 on epoch=124
03/10/2022 20:58:11 - INFO - __main__ - Global step 250 Train loss 1.17 QA-F1 0.265625 on epoch=124
03/10/2022 20:58:16 - INFO - __main__ - Step 260 Global step 260 Train loss 1.07 on epoch=129
03/10/2022 20:58:20 - INFO - __main__ - Step 270 Global step 270 Train loss 1.13 on epoch=134
03/10/2022 20:58:24 - INFO - __main__ - Step 280 Global step 280 Train loss 1.13 on epoch=139
03/10/2022 20:58:29 - INFO - __main__ - Step 290 Global step 290 Train loss 1.07 on epoch=144
03/10/2022 20:58:33 - INFO - __main__ - Step 300 Global step 300 Train loss 1.06 on epoch=149
03/10/2022 20:58:35 - INFO - __main__ - Global step 300 Train loss 1.09 QA-F1 0.26041666666666663 on epoch=149
03/10/2022 20:58:39 - INFO - __main__ - Step 310 Global step 310 Train loss 1.04 on epoch=154
03/10/2022 20:58:43 - INFO - __main__ - Step 320 Global step 320 Train loss 1.12 on epoch=159
03/10/2022 20:58:48 - INFO - __main__ - Step 330 Global step 330 Train loss 1.08 on epoch=164
03/10/2022 20:58:52 - INFO - __main__ - Step 340 Global step 340 Train loss 0.98 on epoch=169
03/10/2022 20:58:56 - INFO - __main__ - Step 350 Global step 350 Train loss 0.96 on epoch=174
03/10/2022 20:58:58 - INFO - __main__ - Global step 350 Train loss 1.04 QA-F1 0.28125 on epoch=174
03/10/2022 20:59:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.97 on epoch=179
03/10/2022 20:59:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.97 on epoch=184
03/10/2022 20:59:11 - INFO - __main__ - Step 380 Global step 380 Train loss 0.99 on epoch=189
03/10/2022 20:59:16 - INFO - __main__ - Step 390 Global step 390 Train loss 0.96 on epoch=194
03/10/2022 20:59:20 - INFO - __main__ - Step 400 Global step 400 Train loss 0.90 on epoch=199
03/10/2022 20:59:22 - INFO - __main__ - Global step 400 Train loss 0.96 QA-F1 0.21875 on epoch=199
03/10/2022 20:59:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.91 on epoch=204
03/10/2022 20:59:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.90 on epoch=209
03/10/2022 20:59:35 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=214
03/10/2022 20:59:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.86 on epoch=219
03/10/2022 20:59:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.78 on epoch=224
03/10/2022 20:59:45 - INFO - __main__ - Global step 450 Train loss 0.86 QA-F1 0.2708333333333333 on epoch=224
03/10/2022 20:59:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.80 on epoch=229
03/10/2022 20:59:53 - INFO - __main__ - Step 470 Global step 470 Train loss 0.84 on epoch=234
03/10/2022 20:59:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.82 on epoch=239
03/10/2022 21:00:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.76 on epoch=244
03/10/2022 21:00:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.69 on epoch=249
03/10/2022 21:00:08 - INFO - __main__ - Global step 500 Train loss 0.78 QA-F1 0.203125 on epoch=249
03/10/2022 21:00:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.72 on epoch=254
03/10/2022 21:00:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.70 on epoch=259
03/10/2022 21:00:21 - INFO - __main__ - Step 530 Global step 530 Train loss 0.75 on epoch=264
03/10/2022 21:00:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.74 on epoch=269
03/10/2022 21:00:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.76 on epoch=274
03/10/2022 21:00:32 - INFO - __main__ - Global step 550 Train loss 0.74 QA-F1 0.140625 on epoch=274
03/10/2022 21:00:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.69 on epoch=279
03/10/2022 21:00:40 - INFO - __main__ - Step 570 Global step 570 Train loss 0.74 on epoch=284
03/10/2022 21:00:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.83 on epoch=289
03/10/2022 21:00:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.66 on epoch=294
03/10/2022 21:00:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.62 on epoch=299
03/10/2022 21:00:55 - INFO - __main__ - Global step 600 Train loss 0.71 QA-F1 0.21354166666666666 on epoch=299
03/10/2022 21:00:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.62 on epoch=304
03/10/2022 21:01:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.67 on epoch=309
03/10/2022 21:01:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.61 on epoch=314
03/10/2022 21:01:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.61 on epoch=319
03/10/2022 21:01:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.64 on epoch=324
03/10/2022 21:01:18 - INFO - __main__ - Global step 650 Train loss 0.63 QA-F1 0.22395833333333331 on epoch=324
03/10/2022 21:01:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.57 on epoch=329
03/10/2022 21:01:27 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=334
03/10/2022 21:01:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.60 on epoch=339
03/10/2022 21:01:36 - INFO - __main__ - Step 690 Global step 690 Train loss 0.55 on epoch=344
03/10/2022 21:01:40 - INFO - __main__ - Step 700 Global step 700 Train loss 0.52 on epoch=349
03/10/2022 21:01:41 - INFO - __main__ - Global step 700 Train loss 0.57 QA-F1 0.22395833333333331 on epoch=349
03/10/2022 21:01:46 - INFO - __main__ - Step 710 Global step 710 Train loss 0.56 on epoch=354
03/10/2022 21:01:50 - INFO - __main__ - Step 720 Global step 720 Train loss 0.55 on epoch=359
03/10/2022 21:01:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.59 on epoch=364
03/10/2022 21:01:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.52 on epoch=369
03/10/2022 21:02:03 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=374
03/10/2022 21:02:04 - INFO - __main__ - Global step 750 Train loss 0.54 QA-F1 0.1875 on epoch=374
03/10/2022 21:02:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.48 on epoch=379
03/10/2022 21:02:13 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=384
03/10/2022 21:02:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.60 on epoch=389
03/10/2022 21:02:22 - INFO - __main__ - Step 790 Global step 790 Train loss 0.47 on epoch=394
03/10/2022 21:02:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=399
03/10/2022 21:02:27 - INFO - __main__ - Global step 800 Train loss 0.48 QA-F1 0.21354166666666666 on epoch=399
03/10/2022 21:02:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.44 on epoch=404
03/10/2022 21:02:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=409
03/10/2022 21:02:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=414
03/10/2022 21:02:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.51 on epoch=419
03/10/2022 21:02:49 - INFO - __main__ - Step 850 Global step 850 Train loss 0.39 on epoch=424
03/10/2022 21:02:50 - INFO - __main__ - Global step 850 Train loss 0.46 QA-F1 0.16145833333333331 on epoch=424
03/10/2022 21:02:55 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
03/10/2022 21:02:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.45 on epoch=434
03/10/2022 21:03:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.39 on epoch=439
03/10/2022 21:03:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=444
03/10/2022 21:03:12 - INFO - __main__ - Step 900 Global step 900 Train loss 0.39 on epoch=449
03/10/2022 21:03:14 - INFO - __main__ - Global step 900 Train loss 0.41 QA-F1 0.18229166666666666 on epoch=449
03/10/2022 21:03:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.41 on epoch=454
03/10/2022 21:03:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.48 on epoch=459
03/10/2022 21:03:27 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=464
03/10/2022 21:03:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=469
03/10/2022 21:03:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.43 on epoch=474
03/10/2022 21:03:37 - INFO - __main__ - Global step 950 Train loss 0.42 QA-F1 0.171875 on epoch=474
03/10/2022 21:03:41 - INFO - __main__ - Step 960 Global step 960 Train loss 0.45 on epoch=479
03/10/2022 21:03:45 - INFO - __main__ - Step 970 Global step 970 Train loss 0.42 on epoch=484
03/10/2022 21:03:50 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=489
03/10/2022 21:03:54 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=494
03/10/2022 21:03:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.39 on epoch=499
03/10/2022 21:04:00 - INFO - __main__ - Global step 1000 Train loss 0.42 QA-F1 0.21875 on epoch=499
03/10/2022 21:04:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=504
03/10/2022 21:04:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=509
03/10/2022 21:04:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.38 on epoch=514
03/10/2022 21:04:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.43 on epoch=519
03/10/2022 21:04:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=524
03/10/2022 21:04:23 - INFO - __main__ - Global step 1050 Train loss 0.37 QA-F1 0.234375 on epoch=524
03/10/2022 21:04:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.31 on epoch=529
03/10/2022 21:04:32 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=534
03/10/2022 21:04:36 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=539
03/10/2022 21:04:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=544
03/10/2022 21:04:45 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=549
03/10/2022 21:04:47 - INFO - __main__ - Global step 1100 Train loss 0.33 QA-F1 0.171875 on epoch=549
03/10/2022 21:04:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.31 on epoch=554
03/10/2022 21:04:55 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=559
03/10/2022 21:05:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.34 on epoch=564
03/10/2022 21:05:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.33 on epoch=569
03/10/2022 21:05:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=574
03/10/2022 21:05:10 - INFO - __main__ - Global step 1150 Train loss 0.34 QA-F1 0.22395833333333331 on epoch=574
03/10/2022 21:05:14 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=579
03/10/2022 21:05:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.29 on epoch=584
03/10/2022 21:05:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=589
03/10/2022 21:05:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.31 on epoch=594
03/10/2022 21:05:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=599
03/10/2022 21:05:33 - INFO - __main__ - Global step 1200 Train loss 0.30 QA-F1 0.18229166666666663 on epoch=599
03/10/2022 21:05:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=604
03/10/2022 21:05:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=609
03/10/2022 21:05:46 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.31 on epoch=614
03/10/2022 21:05:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=619
03/10/2022 21:05:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.29 on epoch=624
03/10/2022 21:05:56 - INFO - __main__ - Global step 1250 Train loss 0.30 QA-F1 0.30729166666666663 on epoch=624
03/10/2022 21:06:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.28 on epoch=629
03/10/2022 21:06:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.30 on epoch=634
03/10/2022 21:06:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=639
03/10/2022 21:06:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=644
03/10/2022 21:06:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.26 on epoch=649
03/10/2022 21:06:20 - INFO - __main__ - Global step 1300 Train loss 0.27 QA-F1 0.24479166666666663 on epoch=649
03/10/2022 21:06:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.28 on epoch=654
03/10/2022 21:06:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=659
03/10/2022 21:06:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=664
03/10/2022 21:06:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=669
03/10/2022 21:06:42 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.31 on epoch=674
03/10/2022 21:06:43 - INFO - __main__ - Global step 1350 Train loss 0.25 QA-F1 0.24479166666666663 on epoch=674
03/10/2022 21:06:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=679
03/10/2022 21:06:52 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=684
03/10/2022 21:06:56 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.27 on epoch=689
03/10/2022 21:07:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=694
03/10/2022 21:07:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.31 on epoch=699
03/10/2022 21:07:06 - INFO - __main__ - Global step 1400 Train loss 0.27 QA-F1 0.27604166666666663 on epoch=699
03/10/2022 21:07:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=704
03/10/2022 21:07:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=709
03/10/2022 21:07:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
03/10/2022 21:07:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=719
03/10/2022 21:07:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=724
03/10/2022 21:07:30 - INFO - __main__ - Global step 1450 Train loss 0.25 QA-F1 0.27604166666666663 on epoch=724
03/10/2022 21:07:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=729
03/10/2022 21:07:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=734
03/10/2022 21:07:43 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=739
03/10/2022 21:07:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=744
03/10/2022 21:07:52 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=749
03/10/2022 21:07:53 - INFO - __main__ - Global step 1500 Train loss 0.22 QA-F1 0.33854166666666663 on epoch=749
03/10/2022 21:07:53 - INFO - __main__ - Saving model with best QA-F1: 0.32812499999999994 -> 0.33854166666666663 on epoch=749, global_step=1500
03/10/2022 21:07:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=754
03/10/2022 21:08:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=759
03/10/2022 21:08:06 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=764
03/10/2022 21:08:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.16 on epoch=769
03/10/2022 21:08:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=774
03/10/2022 21:08:17 - INFO - __main__ - Global step 1550 Train loss 0.22 QA-F1 0.30729166666666663 on epoch=774
03/10/2022 21:08:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=779
03/10/2022 21:08:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=784
03/10/2022 21:08:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.33 on epoch=789
03/10/2022 21:08:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=794
03/10/2022 21:08:38 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=799
03/10/2022 21:08:40 - INFO - __main__ - Global step 1600 Train loss 0.25 QA-F1 0.27604166666666663 on epoch=799
03/10/2022 21:08:44 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.27 on epoch=804
03/10/2022 21:08:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=809
03/10/2022 21:08:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.18 on epoch=814
03/10/2022 21:08:57 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=819
03/10/2022 21:09:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=824
03/10/2022 21:09:03 - INFO - __main__ - Global step 1650 Train loss 0.22 QA-F1 0.2552083333333333 on epoch=824
03/10/2022 21:09:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=829
03/10/2022 21:09:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=834
03/10/2022 21:09:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.17 on epoch=839
03/10/2022 21:09:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=844
03/10/2022 21:09:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=849
03/10/2022 21:09:27 - INFO - __main__ - Global step 1700 Train loss 0.19 QA-F1 0.2552083333333333 on epoch=849
03/10/2022 21:09:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=854
03/10/2022 21:09:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.18 on epoch=859
03/10/2022 21:09:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=864
03/10/2022 21:09:44 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.17 on epoch=869
03/10/2022 21:09:48 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.17 on epoch=874
03/10/2022 21:09:50 - INFO - __main__ - Global step 1750 Train loss 0.17 QA-F1 0.2552083333333333 on epoch=874
03/10/2022 21:09:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.21 on epoch=879
03/10/2022 21:09:59 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=884
03/10/2022 21:10:03 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.17 on epoch=889
03/10/2022 21:10:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=894
03/10/2022 21:10:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.16 on epoch=899
03/10/2022 21:10:13 - INFO - __main__ - Global step 1800 Train loss 0.18 QA-F1 0.33854166666666663 on epoch=899
03/10/2022 21:10:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=904
03/10/2022 21:10:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=909
03/10/2022 21:10:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=914
03/10/2022 21:10:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=919
03/10/2022 21:10:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=924
03/10/2022 21:10:37 - INFO - __main__ - Global step 1850 Train loss 0.18 QA-F1 0.265625 on epoch=924
03/10/2022 21:10:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.17 on epoch=929
03/10/2022 21:10:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=934
03/10/2022 21:10:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=939
03/10/2022 21:10:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=944
03/10/2022 21:10:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=949
03/10/2022 21:11:00 - INFO - __main__ - Global step 1900 Train loss 0.17 QA-F1 0.2864583333333333 on epoch=949
03/10/2022 21:11:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=954
03/10/2022 21:11:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=959
03/10/2022 21:11:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=964
03/10/2022 21:11:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=969
03/10/2022 21:11:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=974
03/10/2022 21:11:24 - INFO - __main__ - Global step 1950 Train loss 0.17 QA-F1 0.296875 on epoch=974
03/10/2022 21:11:28 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=979
03/10/2022 21:11:32 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.16 on epoch=984
03/10/2022 21:11:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=989
03/10/2022 21:11:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=994
03/10/2022 21:11:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=999
03/10/2022 21:11:47 - INFO - __main__ - Global step 2000 Train loss 0.15 QA-F1 0.375 on epoch=999
03/10/2022 21:11:47 - INFO - __main__ - Saving model with best QA-F1: 0.33854166666666663 -> 0.375 on epoch=999, global_step=2000
03/10/2022 21:11:47 - INFO - __main__ - save last model!
03/10/2022 21:11:47 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 21:11:47 - INFO - __main__ - Start tokenizing ... 2418 instances
03/10/2022 21:11:47 - INFO - __main__ - Printing 3 examples
03/10/2022 21:11:47 - INFO - __main__ -  [quoref] question: What is the first name of the person who purchases a revolver? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 21:11:47 - INFO - __main__ - ['Frankie']
03/10/2022 21:11:47 - INFO - __main__ -  [quoref] question: What is the first name of the person who has until New Year's Eve to perform a hit? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 21:11:47 - INFO - __main__ - ['Frankie']
03/10/2022 21:11:47 - INFO - __main__ -  [quoref] question: What is the first name of the person whose contract will be reneged if they are spotted before the hit is performed? context: Frankie Bono, a mentally disturbed hitman from Cleveland, comes back to his hometown in New York City during Christmas week to kill a middle-management mobster, Troiano. The assassination will be risky, with Frankie being warned by a fellow enforcer that should he be spotted before the hit is performed, the contract will be reneged. First he follows his target to select the best possible location, but opts to wait until Troiano isn't being accompanied by his bodyguards. Next, he goes to purchase a revolver from Big Ralph, an obese gun runner who keeps sewer rats as pets. The encounter with this old acquaintance leaves Frankie feeling disgusted. With several hours left before the hit is to be performed, Frankie decides to kill time in the city, where he is plagued by memories of past trauma during his time living there. While sitting alone for a drink, Frankie is reunited with childhood friend Petey, who invites the reluctant Frankie to a Christmas party, where Frankie later encounters his old flame, Lori. The following day Frankie goes to see Lori at her apartment to get better reacquainted with her, but the visit ends in disaster when an at first vulnerable Frankie suddenly attempts to sexually assault her. Lori forgives Frankie for his actions and calmly asks him to leave, to which he obliges. That same day, Frankie tails Troiano and his mistress to a Jazz club in Greenwich village. However, he is spotted by Big Ralph, who decides to blackmail Frankie out of the hit. In turn, Frankie stalks Ralph back to his tenement and strangles him to death following a violent brawl between the two. Losing his nerve, Frankie calls up his employers to tell them he wants to quit the job. Unsympathetic, the supervisor tells him he has until New Year's Eve to perform the hit.
03/10/2022 21:11:47 - INFO - __main__ - ['Frankie']
03/10/2022 21:11:47 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:11:51 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:11:54 - INFO - __main__ - Loaded 2418 examples from test data
03/10/2022 21:13:44 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-quoref/quoref_32_87_0.2_8_predictions.txt
03/10/2022 21:13:44 - INFO - __main__ - QA-F1 on test data: 0.3229
03/10/2022 21:13:44 - INFO - __main__ - prefix=quoref_32_87, lr=0.2, bsz=8, dev_performance=0.375, test_performance=0.32290636484184876
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00045228004455566406 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15191", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 21322, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15192", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 21322, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 21322, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15602): No such process
Task: amazon_polarity, Checkpoint: models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_fomaml_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24568
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_e2_5gdw_/none_a3d0jylo
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24568
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_e2_5gdw_/none_a3d0jylo/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_e2_5gdw_/none_a3d0jylo/attempt_0/1/error.json
03/10/2022 21:13:49 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-fomaml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/10/2022 21:13:49 - INFO - __main__ - models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity
03/10/2022 21:13:49 - INFO - __main__ - Namespace(task_dir='data/amazon_polarity/', task_name='amazon_polarity', identifier='T5-large-fomaml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/10/2022 21:13:49 - INFO - __main__ - models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity
03/10/2022 21:13:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/10/2022 21:13:51 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/10/2022 21:13:51 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/10/2022 21:13:51 - INFO - __main__ - args.device: cuda:0
03/10/2022 21:13:51 - INFO - __main__ - Using 2 gpus
03/10/2022 21:13:51 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/10/2022 21:13:51 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/10/2022 21:13:51 - INFO - __main__ - args.device: cuda:1
03/10/2022 21:13:51 - INFO - __main__ - Using 2 gpus
03/10/2022 21:13:51 - INFO - __main__ - Fine-tuning the following samples: ['amazon_polarity_16_100', 'amazon_polarity_16_13', 'amazon_polarity_16_21', 'amazon_polarity_16_42', 'amazon_polarity_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/10/2022 21:13:57 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.5, bsz=8 ...
03/10/2022 21:13:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:13:57 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:13:57 - INFO - __main__ - Printing 3 examples
03/10/2022 21:13:57 - INFO - __main__ - Printing 3 examples
03/10/2022 21:13:57 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 21:13:57 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 21:13:57 - INFO - __main__ - ['positive']
03/10/2022 21:13:57 - INFO - __main__ - ['positive']
03/10/2022 21:13:57 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 21:13:57 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 21:13:57 - INFO - __main__ - ['positive']
03/10/2022 21:13:57 - INFO - __main__ - ['positive']
03/10/2022 21:13:57 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 21:13:57 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 21:13:57 - INFO - __main__ - ['positive']
03/10/2022 21:13:57 - INFO - __main__ - ['positive']
03/10/2022 21:13:57 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:13:57 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:13:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:13:57 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:13:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:13:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:13:58 - INFO - __main__ - Printing 3 examples
03/10/2022 21:13:58 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 21:13:58 - INFO - __main__ - ['positive']
03/10/2022 21:13:58 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 21:13:58 - INFO - __main__ - ['positive']
03/10/2022 21:13:58 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 21:13:58 - INFO - __main__ - ['positive']
03/10/2022 21:13:58 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:13:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:13:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:13:58 - INFO - __main__ - Printing 3 examples
03/10/2022 21:13:58 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 21:13:58 - INFO - __main__ - ['positive']
03/10/2022 21:13:58 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 21:13:58 - INFO - __main__ - ['positive']
03/10/2022 21:13:58 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 21:13:58 - INFO - __main__ - ['positive']
03/10/2022 21:13:58 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:13:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:13:58 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:13:58 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:13:58 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:14:13 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:14:14 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:14:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:14:15 - INFO - __main__ - Starting training!
03/10/2022 21:14:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:14:21 - INFO - __main__ - Starting training!
03/10/2022 21:14:25 - INFO - __main__ - Step 10 Global step 10 Train loss 0.91 on epoch=4
03/10/2022 21:14:27 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=9
03/10/2022 21:14:30 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=14
03/10/2022 21:14:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=19
03/10/2022 21:14:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/10/2022 21:14:36 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 21:14:36 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 21:14:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=29
03/10/2022 21:14:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=34
03/10/2022 21:14:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
03/10/2022 21:14:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.35 on epoch=44
03/10/2022 21:14:49 - INFO - __main__ - Step 100 Global step 100 Train loss 0.37 on epoch=49
03/10/2022 21:14:50 - INFO - __main__ - Global step 100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 21:14:53 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
03/10/2022 21:14:55 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=59
03/10/2022 21:14:58 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=64
03/10/2022 21:15:00 - INFO - __main__ - Step 140 Global step 140 Train loss 0.42 on epoch=69
03/10/2022 21:15:03 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=74
03/10/2022 21:15:04 - INFO - __main__ - Global step 150 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 21:15:06 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=79
03/10/2022 21:15:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=84
03/10/2022 21:15:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
03/10/2022 21:15:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
03/10/2022 21:15:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
03/10/2022 21:15:18 - INFO - __main__ - Global step 200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 21:15:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
03/10/2022 21:15:23 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=109
03/10/2022 21:15:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.36 on epoch=114
03/10/2022 21:15:28 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
03/10/2022 21:15:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.33 on epoch=124
03/10/2022 21:15:31 - INFO - __main__ - Global step 250 Train loss 0.35 Classification-F1 0.7408906882591093 on epoch=124
03/10/2022 21:15:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.7408906882591093 on epoch=124, global_step=250
03/10/2022 21:15:34 - INFO - __main__ - Step 260 Global step 260 Train loss 0.33 on epoch=129
03/10/2022 21:15:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=134
03/10/2022 21:15:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=139
03/10/2022 21:15:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
03/10/2022 21:15:44 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=149
03/10/2022 21:15:45 - INFO - __main__ - Global step 300 Train loss 0.32 Classification-F1 0.7046153846153846 on epoch=149
03/10/2022 21:15:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
03/10/2022 21:15:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=159
03/10/2022 21:15:53 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=164
03/10/2022 21:15:56 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=169
03/10/2022 21:15:58 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=174
03/10/2022 21:15:59 - INFO - __main__ - Global step 350 Train loss 0.30 Classification-F1 0.8423645320197044 on epoch=174
03/10/2022 21:15:59 - INFO - __main__ - Saving model with best Classification-F1: 0.7408906882591093 -> 0.8423645320197044 on epoch=174, global_step=350
03/10/2022 21:16:01 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
03/10/2022 21:16:04 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
03/10/2022 21:16:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
03/10/2022 21:16:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=194
03/10/2022 21:16:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
03/10/2022 21:16:13 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.9372549019607843 on epoch=199
03/10/2022 21:16:13 - INFO - __main__ - Saving model with best Classification-F1: 0.8423645320197044 -> 0.9372549019607843 on epoch=199, global_step=400
03/10/2022 21:16:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/10/2022 21:16:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=209
03/10/2022 21:16:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.14 on epoch=214
03/10/2022 21:16:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.17 on epoch=219
03/10/2022 21:16:26 - INFO - __main__ - Step 450 Global step 450 Train loss 0.15 on epoch=224
03/10/2022 21:16:29 - INFO - __main__ - Global step 450 Train loss 0.19 Classification-F1 1.0 on epoch=224
03/10/2022 21:16:29 - INFO - __main__ - Saving model with best Classification-F1: 0.9372549019607843 -> 1.0 on epoch=224, global_step=450
03/10/2022 21:16:31 - INFO - __main__ - Step 460 Global step 460 Train loss 0.11 on epoch=229
03/10/2022 21:16:34 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=234
03/10/2022 21:16:37 - INFO - __main__ - Step 480 Global step 480 Train loss 0.06 on epoch=239
03/10/2022 21:16:39 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
03/10/2022 21:16:42 - INFO - __main__ - Step 500 Global step 500 Train loss 0.14 on epoch=249
03/10/2022 21:16:45 - INFO - __main__ - Global step 500 Train loss 0.08 Classification-F1 0.9687194525904204 on epoch=249
03/10/2022 21:16:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.07 on epoch=254
03/10/2022 21:16:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.06 on epoch=259
03/10/2022 21:16:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
03/10/2022 21:16:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
03/10/2022 21:16:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.05 on epoch=274
03/10/2022 21:17:04 - INFO - __main__ - Global step 550 Train loss 0.04 Classification-F1 0.9687194525904204 on epoch=274
03/10/2022 21:17:07 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
03/10/2022 21:17:10 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/10/2022 21:17:12 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/10/2022 21:17:15 - INFO - __main__ - Step 590 Global step 590 Train loss 0.05 on epoch=294
03/10/2022 21:17:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.07 on epoch=299
03/10/2022 21:17:22 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=299
03/10/2022 21:17:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
03/10/2022 21:17:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/10/2022 21:17:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.01 on epoch=314
03/10/2022 21:17:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/10/2022 21:17:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/10/2022 21:17:38 - INFO - __main__ - Global step 650 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=324
03/10/2022 21:17:40 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/10/2022 21:17:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/10/2022 21:17:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/10/2022 21:17:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/10/2022 21:17:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/10/2022 21:17:56 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 1.0 on epoch=349
03/10/2022 21:17:59 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/10/2022 21:18:01 - INFO - __main__ - Step 720 Global step 720 Train loss 0.00 on epoch=359
03/10/2022 21:18:04 - INFO - __main__ - Step 730 Global step 730 Train loss 0.00 on epoch=364
03/10/2022 21:18:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.02 on epoch=369
03/10/2022 21:18:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
03/10/2022 21:18:12 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=374
03/10/2022 21:18:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/10/2022 21:18:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/10/2022 21:18:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/10/2022 21:18:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/10/2022 21:18:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
03/10/2022 21:18:30 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 1.0 on epoch=399
03/10/2022 21:18:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.00 on epoch=404
03/10/2022 21:18:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/10/2022 21:18:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/10/2022 21:18:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
03/10/2022 21:18:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
03/10/2022 21:18:46 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=424
03/10/2022 21:18:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/10/2022 21:18:51 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/10/2022 21:18:54 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
03/10/2022 21:18:56 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/10/2022 21:18:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
03/10/2022 21:19:03 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=449
03/10/2022 21:19:06 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/10/2022 21:19:08 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/10/2022 21:19:11 - INFO - __main__ - Step 930 Global step 930 Train loss 0.04 on epoch=464
03/10/2022 21:19:14 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/10/2022 21:19:16 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/10/2022 21:19:24 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=474
03/10/2022 21:19:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/10/2022 21:19:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/10/2022 21:19:32 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
03/10/2022 21:19:34 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/10/2022 21:19:37 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/10/2022 21:19:40 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 1.0 on epoch=499
03/10/2022 21:19:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/10/2022 21:19:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/10/2022 21:19:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/10/2022 21:19:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/10/2022 21:19:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/10/2022 21:19:56 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/10/2022 21:19:58 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/10/2022 21:20:01 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/10/2022 21:20:03 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/10/2022 21:20:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/10/2022 21:20:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/10/2022 21:20:11 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/10/2022 21:20:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/10/2022 21:20:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/10/2022 21:20:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/10/2022 21:20:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/10/2022 21:20:25 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
03/10/2022 21:20:27 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=574
03/10/2022 21:20:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/10/2022 21:20:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/10/2022 21:20:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/10/2022 21:20:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
03/10/2022 21:20:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/10/2022 21:20:42 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=599
03/10/2022 21:20:45 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/10/2022 21:20:48 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/10/2022 21:20:50 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
03/10/2022 21:20:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/10/2022 21:20:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/10/2022 21:21:01 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/10/2022 21:21:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/10/2022 21:21:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/10/2022 21:21:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/10/2022 21:21:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/10/2022 21:21:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/10/2022 21:21:21 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/10/2022 21:21:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/10/2022 21:21:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/10/2022 21:21:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/10/2022 21:21:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/10/2022 21:21:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/10/2022 21:21:36 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/10/2022 21:21:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/10/2022 21:21:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/10/2022 21:21:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/10/2022 21:21:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/10/2022 21:21:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/10/2022 21:21:52 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/10/2022 21:21:54 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/10/2022 21:21:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/10/2022 21:22:00 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/10/2022 21:22:02 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/10/2022 21:22:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/10/2022 21:22:11 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 1.0 on epoch=724
03/10/2022 21:22:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/10/2022 21:22:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/10/2022 21:22:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/10/2022 21:22:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/10/2022 21:22:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/10/2022 21:22:26 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/10/2022 21:22:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/10/2022 21:22:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/10/2022 21:22:34 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/10/2022 21:22:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/10/2022 21:22:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/10/2022 21:22:41 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 1.0 on epoch=774
03/10/2022 21:22:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/10/2022 21:22:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/10/2022 21:22:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/10/2022 21:22:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/10/2022 21:22:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/10/2022 21:22:56 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/10/2022 21:22:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/10/2022 21:23:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/10/2022 21:23:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/10/2022 21:23:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/10/2022 21:23:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/10/2022 21:23:11 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/10/2022 21:23:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/10/2022 21:23:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/10/2022 21:23:19 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/10/2022 21:23:21 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/10/2022 21:23:24 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/10/2022 21:23:26 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/10/2022 21:23:28 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/10/2022 21:23:31 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/10/2022 21:23:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/10/2022 21:23:36 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/10/2022 21:23:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/10/2022 21:23:40 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/10/2022 21:23:43 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/10/2022 21:23:46 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/10/2022 21:23:48 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/10/2022 21:23:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/10/2022 21:23:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/10/2022 21:23:56 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/10/2022 21:23:58 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/10/2022 21:24:01 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/10/2022 21:24:04 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/10/2022 21:24:06 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/10/2022 21:24:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/10/2022 21:24:10 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/10/2022 21:24:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/10/2022 21:24:16 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/10/2022 21:24:19 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/10/2022 21:24:21 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/10/2022 21:24:24 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/10/2022 21:24:25 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/10/2022 21:24:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/10/2022 21:24:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/10/2022 21:24:33 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/10/2022 21:24:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/10/2022 21:24:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/10/2022 21:24:40 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/10/2022 21:24:43 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/10/2022 21:24:46 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/10/2022 21:24:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/10/2022 21:24:51 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/10/2022 21:24:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/10/2022 21:24:55 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/10/2022 21:24:55 - INFO - __main__ - save last model!
03/10/2022 21:24:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 21:24:55 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:24:55 - INFO - __main__ - Printing 3 examples
03/10/2022 21:24:55 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:24:55 - INFO - __main__ - ['negative']
03/10/2022 21:24:55 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:24:55 - INFO - __main__ - ['negative']
03/10/2022 21:24:55 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:24:55 - INFO - __main__ - ['negative']
03/10/2022 21:24:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:24:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:24:55 - INFO - __main__ - Printing 3 examples
03/10/2022 21:24:55 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 21:24:55 - INFO - __main__ - ['positive']
03/10/2022 21:24:55 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 21:24:55 - INFO - __main__ - ['positive']
03/10/2022 21:24:55 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 21:24:55 - INFO - __main__ - ['positive']
03/10/2022 21:24:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:24:55 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:24:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:24:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:24:55 - INFO - __main__ - Printing 3 examples
03/10/2022 21:24:55 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 21:24:55 - INFO - __main__ - ['positive']
03/10/2022 21:24:55 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 21:24:55 - INFO - __main__ - ['positive']
03/10/2022 21:24:55 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 21:24:55 - INFO - __main__ - ['positive']
03/10/2022 21:24:55 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:24:55 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:24:55 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:24:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:24:57 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:25:10 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:25:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:25:11 - INFO - __main__ - Starting training!
03/10/2022 21:25:32 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_100_0.5_8_predictions.txt
03/10/2022 21:25:32 - INFO - __main__ - Classification-F1 on test data: 0.9580
03/10/2022 21:25:33 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.5, bsz=8, dev_performance=1.0, test_performance=0.9579892452467831
03/10/2022 21:25:33 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.4, bsz=8 ...
03/10/2022 21:25:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:25:33 - INFO - __main__ - Printing 3 examples
03/10/2022 21:25:33 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 21:25:33 - INFO - __main__ - ['positive']
03/10/2022 21:25:33 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 21:25:33 - INFO - __main__ - ['positive']
03/10/2022 21:25:33 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 21:25:33 - INFO - __main__ - ['positive']
03/10/2022 21:25:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:25:33 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:25:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:25:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:25:33 - INFO - __main__ - Printing 3 examples
03/10/2022 21:25:33 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 21:25:33 - INFO - __main__ - ['positive']
03/10/2022 21:25:33 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 21:25:33 - INFO - __main__ - ['positive']
03/10/2022 21:25:33 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 21:25:33 - INFO - __main__ - ['positive']
03/10/2022 21:25:33 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:25:33 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:25:34 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:25:47 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:25:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:25:47 - INFO - __main__ - Starting training!
03/10/2022 21:25:54 - INFO - __main__ - Step 10 Global step 10 Train loss 1.24 on epoch=4
03/10/2022 21:25:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=9
03/10/2022 21:25:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.44 on epoch=14
03/10/2022 21:26:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.38 on epoch=19
03/10/2022 21:26:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=24
03/10/2022 21:26:05 - INFO - __main__ - Global step 50 Train loss 0.60 Classification-F1 0.3191489361702127 on epoch=24
03/10/2022 21:26:05 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3191489361702127 on epoch=24, global_step=50
03/10/2022 21:26:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=29
03/10/2022 21:26:10 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=34
03/10/2022 21:26:13 - INFO - __main__ - Step 80 Global step 80 Train loss 0.37 on epoch=39
03/10/2022 21:26:15 - INFO - __main__ - Step 90 Global step 90 Train loss 0.36 on epoch=44
03/10/2022 21:26:18 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
03/10/2022 21:26:18 - INFO - __main__ - Global step 100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 21:26:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3191489361702127 -> 0.3333333333333333 on epoch=49, global_step=100
03/10/2022 21:26:21 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=54
03/10/2022 21:26:24 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=59
03/10/2022 21:26:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=64
03/10/2022 21:26:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=69
03/10/2022 21:26:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=74
03/10/2022 21:26:32 - INFO - __main__ - Global step 150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 21:26:35 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
03/10/2022 21:26:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=84
03/10/2022 21:26:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
03/10/2022 21:26:42 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=94
03/10/2022 21:26:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
03/10/2022 21:26:46 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3816425120772947 on epoch=99
03/10/2022 21:26:46 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3816425120772947 on epoch=99, global_step=200
03/10/2022 21:26:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
03/10/2022 21:26:51 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=109
03/10/2022 21:26:54 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
03/10/2022 21:26:56 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=119
03/10/2022 21:26:59 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=124
03/10/2022 21:26:59 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=124
03/10/2022 21:27:02 - INFO - __main__ - Step 260 Global step 260 Train loss 0.41 on epoch=129
03/10/2022 21:27:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
03/10/2022 21:27:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
03/10/2022 21:27:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
03/10/2022 21:27:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
03/10/2022 21:27:13 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.5901477832512315 on epoch=149
03/10/2022 21:27:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.5901477832512315 on epoch=149, global_step=300
03/10/2022 21:27:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=154
03/10/2022 21:27:18 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=159
03/10/2022 21:27:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.48 on epoch=164
03/10/2022 21:27:23 - INFO - __main__ - Step 340 Global step 340 Train loss 0.37 on epoch=169
03/10/2022 21:27:26 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=174
03/10/2022 21:27:27 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.6389743589743591 on epoch=174
03/10/2022 21:27:27 - INFO - __main__ - Saving model with best Classification-F1: 0.5901477832512315 -> 0.6389743589743591 on epoch=174, global_step=350
03/10/2022 21:27:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=179
03/10/2022 21:27:32 - INFO - __main__ - Step 370 Global step 370 Train loss 0.35 on epoch=184
03/10/2022 21:27:34 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=189
03/10/2022 21:27:37 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=194
03/10/2022 21:27:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=199
03/10/2022 21:27:40 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.539313399778516 on epoch=199
03/10/2022 21:27:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=204
03/10/2022 21:27:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=209
03/10/2022 21:27:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.39 on epoch=214
03/10/2022 21:27:51 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=219
03/10/2022 21:27:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=224
03/10/2022 21:27:54 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.6761133603238867 on epoch=224
03/10/2022 21:27:54 - INFO - __main__ - Saving model with best Classification-F1: 0.6389743589743591 -> 0.6761133603238867 on epoch=224, global_step=450
03/10/2022 21:27:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=229
03/10/2022 21:27:59 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=234
03/10/2022 21:28:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=239
03/10/2022 21:28:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=244
03/10/2022 21:28:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.34 on epoch=249
03/10/2022 21:28:07 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.5636363636363637 on epoch=249
03/10/2022 21:28:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
03/10/2022 21:28:13 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
03/10/2022 21:28:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=264
03/10/2022 21:28:18 - INFO - __main__ - Step 540 Global step 540 Train loss 0.33 on epoch=269
03/10/2022 21:28:20 - INFO - __main__ - Step 550 Global step 550 Train loss 0.36 on epoch=274
03/10/2022 21:28:21 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.7184750733137829 on epoch=274
03/10/2022 21:28:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6761133603238867 -> 0.7184750733137829 on epoch=274, global_step=550
03/10/2022 21:28:24 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=279
03/10/2022 21:28:26 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=284
03/10/2022 21:28:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=289
03/10/2022 21:28:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=294
03/10/2022 21:28:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=299
03/10/2022 21:28:35 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.6761133603238867 on epoch=299
03/10/2022 21:28:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=304
03/10/2022 21:28:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=309
03/10/2022 21:28:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=314
03/10/2022 21:28:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=319
03/10/2022 21:28:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=324
03/10/2022 21:28:48 - INFO - __main__ - Global step 650 Train loss 0.36 Classification-F1 0.539313399778516 on epoch=324
03/10/2022 21:28:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.31 on epoch=329
03/10/2022 21:28:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=334
03/10/2022 21:28:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=339
03/10/2022 21:28:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=344
03/10/2022 21:29:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=349
03/10/2022 21:29:02 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.4385964912280702 on epoch=349
03/10/2022 21:29:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=354
03/10/2022 21:29:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=359
03/10/2022 21:29:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=364
03/10/2022 21:29:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=369
03/10/2022 21:29:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=374
03/10/2022 21:29:15 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.5733333333333335 on epoch=374
03/10/2022 21:29:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=379
03/10/2022 21:29:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.30 on epoch=384
03/10/2022 21:29:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=389
03/10/2022 21:29:26 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=394
03/10/2022 21:29:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=399
03/10/2022 21:29:29 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.3992490613266583 on epoch=399
03/10/2022 21:29:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=404
03/10/2022 21:29:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.28 on epoch=409
03/10/2022 21:29:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=414
03/10/2022 21:29:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=419
03/10/2022 21:29:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=424
03/10/2022 21:29:43 - INFO - __main__ - Global step 850 Train loss 0.30 Classification-F1 0.3992490613266583 on epoch=424
03/10/2022 21:29:45 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=429
03/10/2022 21:29:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=434
03/10/2022 21:29:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=439
03/10/2022 21:29:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.39 on epoch=444
03/10/2022 21:29:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=449
03/10/2022 21:29:56 - INFO - __main__ - Global step 900 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=449
03/10/2022 21:29:59 - INFO - __main__ - Step 910 Global step 910 Train loss 0.31 on epoch=454
03/10/2022 21:30:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=459
03/10/2022 21:30:04 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=464
03/10/2022 21:30:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=469
03/10/2022 21:30:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.35 on epoch=474
03/10/2022 21:30:10 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.5636363636363637 on epoch=474
03/10/2022 21:30:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=479
03/10/2022 21:30:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.34 on epoch=484
03/10/2022 21:30:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=489
03/10/2022 21:30:20 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=494
03/10/2022 21:30:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=499
03/10/2022 21:30:23 - INFO - __main__ - Global step 1000 Train loss 0.34 Classification-F1 0.46843853820598 on epoch=499
03/10/2022 21:30:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=504
03/10/2022 21:30:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=509
03/10/2022 21:30:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=514
03/10/2022 21:30:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.35 on epoch=519
03/10/2022 21:30:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=524
03/10/2022 21:30:37 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.4458874458874459 on epoch=524
03/10/2022 21:30:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.33 on epoch=529
03/10/2022 21:30:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.28 on epoch=534
03/10/2022 21:30:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.31 on epoch=539
03/10/2022 21:30:48 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=544
03/10/2022 21:30:50 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.31 on epoch=549
03/10/2022 21:30:51 - INFO - __main__ - Global step 1100 Train loss 0.31 Classification-F1 0.4554554554554554 on epoch=549
03/10/2022 21:30:54 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
03/10/2022 21:30:56 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.32 on epoch=559
03/10/2022 21:30:59 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=564
03/10/2022 21:31:01 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=569
03/10/2022 21:31:04 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=574
03/10/2022 21:31:05 - INFO - __main__ - Global step 1150 Train loss 0.31 Classification-F1 0.43529411764705883 on epoch=574
03/10/2022 21:31:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.29 on epoch=579
03/10/2022 21:31:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=584
03/10/2022 21:31:13 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=589
03/10/2022 21:31:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/10/2022 21:31:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.28 on epoch=599
03/10/2022 21:31:18 - INFO - __main__ - Global step 1200 Train loss 0.27 Classification-F1 0.4420512820512821 on epoch=599
03/10/2022 21:31:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=604
03/10/2022 21:31:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.31 on epoch=609
03/10/2022 21:31:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.29 on epoch=614
03/10/2022 21:31:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.30 on epoch=619
03/10/2022 21:31:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=624
03/10/2022 21:31:32 - INFO - __main__ - Global step 1250 Train loss 0.29 Classification-F1 0.3816425120772947 on epoch=624
03/10/2022 21:31:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=629
03/10/2022 21:31:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=634
03/10/2022 21:31:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=639
03/10/2022 21:31:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.27 on epoch=644
03/10/2022 21:31:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/10/2022 21:31:46 - INFO - __main__ - Global step 1300 Train loss 0.25 Classification-F1 0.39756367663344405 on epoch=649
03/10/2022 21:31:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.19 on epoch=654
03/10/2022 21:31:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=659
03/10/2022 21:31:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=664
03/10/2022 21:31:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=669
03/10/2022 21:31:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=674
03/10/2022 21:31:59 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.40566959921798634 on epoch=674
03/10/2022 21:32:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=679
03/10/2022 21:32:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=684
03/10/2022 21:32:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=689
03/10/2022 21:32:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.12 on epoch=694
03/10/2022 21:32:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=699
03/10/2022 21:32:13 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.4682306940371457 on epoch=699
03/10/2022 21:32:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=704
03/10/2022 21:32:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=709
03/10/2022 21:32:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.09 on epoch=714
03/10/2022 21:32:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=719
03/10/2022 21:32:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.09 on epoch=724
03/10/2022 21:32:27 - INFO - __main__ - Global step 1450 Train loss 0.14 Classification-F1 0.4375 on epoch=724
03/10/2022 21:32:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.09 on epoch=729
03/10/2022 21:32:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=734
03/10/2022 21:32:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.11 on epoch=739
03/10/2022 21:32:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=744
03/10/2022 21:32:40 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=749
03/10/2022 21:32:41 - INFO - __main__ - Global step 1500 Train loss 0.10 Classification-F1 0.4009852216748768 on epoch=749
03/10/2022 21:32:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
03/10/2022 21:32:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=759
03/10/2022 21:32:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=764
03/10/2022 21:32:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
03/10/2022 21:32:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=774
03/10/2022 21:32:54 - INFO - __main__ - Global step 1550 Train loss 0.07 Classification-F1 0.4009852216748768 on epoch=774
03/10/2022 21:32:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.05 on epoch=779
03/10/2022 21:33:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
03/10/2022 21:33:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=789
03/10/2022 21:33:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.05 on epoch=794
03/10/2022 21:33:08 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
03/10/2022 21:33:08 - INFO - __main__ - Global step 1600 Train loss 0.06 Classification-F1 0.4980392156862745 on epoch=799
03/10/2022 21:33:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.03 on epoch=804
03/10/2022 21:33:13 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.03 on epoch=809
03/10/2022 21:33:16 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=814
03/10/2022 21:33:19 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=819
03/10/2022 21:33:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
03/10/2022 21:33:22 - INFO - __main__ - Global step 1650 Train loss 0.04 Classification-F1 0.30158730158730157 on epoch=824
03/10/2022 21:33:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=829
03/10/2022 21:33:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=834
03/10/2022 21:33:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=839
03/10/2022 21:33:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
03/10/2022 21:33:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=849
03/10/2022 21:33:36 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.40566959921798634 on epoch=849
03/10/2022 21:33:38 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
03/10/2022 21:33:41 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
03/10/2022 21:33:44 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=864
03/10/2022 21:33:46 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
03/10/2022 21:33:49 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
03/10/2022 21:33:49 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.4009852216748768 on epoch=874
03/10/2022 21:33:52 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=879
03/10/2022 21:33:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.05 on epoch=884
03/10/2022 21:33:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
03/10/2022 21:34:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
03/10/2022 21:34:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=899
03/10/2022 21:34:05 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.3107692307692308 on epoch=899
03/10/2022 21:34:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
03/10/2022 21:34:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
03/10/2022 21:34:13 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=914
03/10/2022 21:34:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
03/10/2022 21:34:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
03/10/2022 21:34:19 - INFO - __main__ - Global step 1850 Train loss 0.03 Classification-F1 0.3273273273273273 on epoch=924
03/10/2022 21:34:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=929
03/10/2022 21:34:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.09 on epoch=934
03/10/2022 21:34:26 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.05 on epoch=939
03/10/2022 21:34:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.02 on epoch=944
03/10/2022 21:34:32 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/10/2022 21:34:32 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.40566959921798634 on epoch=949
03/10/2022 21:34:35 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/10/2022 21:34:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
03/10/2022 21:34:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
03/10/2022 21:34:43 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=969
03/10/2022 21:34:45 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/10/2022 21:34:46 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.43529411764705883 on epoch=974
03/10/2022 21:34:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
03/10/2022 21:34:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/10/2022 21:34:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/10/2022 21:34:57 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.08 on epoch=994
03/10/2022 21:34:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
03/10/2022 21:35:00 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.3764102564102564 on epoch=999
03/10/2022 21:35:00 - INFO - __main__ - save last model!
03/10/2022 21:35:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 21:35:00 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:35:00 - INFO - __main__ - Printing 3 examples
03/10/2022 21:35:00 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:35:00 - INFO - __main__ - ['negative']
03/10/2022 21:35:00 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:35:00 - INFO - __main__ - ['negative']
03/10/2022 21:35:00 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:35:00 - INFO - __main__ - ['negative']
03/10/2022 21:35:00 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:35:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:35:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:35:01 - INFO - __main__ - Printing 3 examples
03/10/2022 21:35:01 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 21:35:01 - INFO - __main__ - ['positive']
03/10/2022 21:35:01 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 21:35:01 - INFO - __main__ - ['positive']
03/10/2022 21:35:01 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 21:35:01 - INFO - __main__ - ['positive']
03/10/2022 21:35:01 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:35:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:35:01 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:35:01 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:35:01 - INFO - __main__ - Printing 3 examples
03/10/2022 21:35:01 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 21:35:01 - INFO - __main__ - ['positive']
03/10/2022 21:35:01 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 21:35:01 - INFO - __main__ - ['positive']
03/10/2022 21:35:01 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 21:35:01 - INFO - __main__ - ['positive']
03/10/2022 21:35:01 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:35:01 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:35:01 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:35:02 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:35:15 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:35:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:35:16 - INFO - __main__ - Starting training!
03/10/2022 21:35:23 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_100_0.4_8_predictions.txt
03/10/2022 21:35:23 - INFO - __main__ - Classification-F1 on test data: 0.4992
03/10/2022 21:35:24 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.4, bsz=8, dev_performance=0.7184750733137829, test_performance=0.4991902229007387
03/10/2022 21:35:24 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.3, bsz=8 ...
03/10/2022 21:35:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:35:25 - INFO - __main__ - Printing 3 examples
03/10/2022 21:35:25 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 21:35:25 - INFO - __main__ - ['positive']
03/10/2022 21:35:25 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 21:35:25 - INFO - __main__ - ['positive']
03/10/2022 21:35:25 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 21:35:25 - INFO - __main__ - ['positive']
03/10/2022 21:35:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:35:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:35:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:35:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:35:25 - INFO - __main__ - Printing 3 examples
03/10/2022 21:35:25 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 21:35:25 - INFO - __main__ - ['positive']
03/10/2022 21:35:25 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 21:35:25 - INFO - __main__ - ['positive']
03/10/2022 21:35:25 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 21:35:25 - INFO - __main__ - ['positive']
03/10/2022 21:35:25 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:35:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:35:25 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:35:38 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:35:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:35:39 - INFO - __main__ - Starting training!
03/10/2022 21:35:42 - INFO - __main__ - Step 10 Global step 10 Train loss 1.33 on epoch=4
03/10/2022 21:35:45 - INFO - __main__ - Step 20 Global step 20 Train loss 0.65 on epoch=9
03/10/2022 21:35:47 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=14
03/10/2022 21:35:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=19
03/10/2022 21:35:53 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=24
03/10/2022 21:35:53 - INFO - __main__ - Global step 50 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 21:35:53 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 21:35:56 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=29
03/10/2022 21:35:58 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=34
03/10/2022 21:36:01 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=39
03/10/2022 21:36:04 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
03/10/2022 21:36:06 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=49
03/10/2022 21:36:07 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 21:36:10 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
03/10/2022 21:36:12 - INFO - __main__ - Step 120 Global step 120 Train loss 0.48 on epoch=59
03/10/2022 21:36:15 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=64
03/10/2022 21:36:17 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=69
03/10/2022 21:36:20 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=74
03/10/2022 21:36:21 - INFO - __main__ - Global step 150 Train loss 0.40 Classification-F1 0.5151515151515151 on epoch=74
03/10/2022 21:36:21 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5151515151515151 on epoch=74, global_step=150
03/10/2022 21:36:23 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
03/10/2022 21:36:26 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=84
03/10/2022 21:36:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.33 on epoch=89
03/10/2022 21:36:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.37 on epoch=94
03/10/2022 21:36:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
03/10/2022 21:36:34 - INFO - __main__ - Global step 200 Train loss 0.36 Classification-F1 0.7184750733137829 on epoch=99
03/10/2022 21:36:35 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.7184750733137829 on epoch=99, global_step=200
03/10/2022 21:36:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
03/10/2022 21:36:40 - INFO - __main__ - Step 220 Global step 220 Train loss 1.00 on epoch=109
03/10/2022 21:36:42 - INFO - __main__ - Step 230 Global step 230 Train loss 0.45 on epoch=114
03/10/2022 21:36:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=119
03/10/2022 21:36:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=124
03/10/2022 21:36:48 - INFO - __main__ - Global step 250 Train loss 0.51 Classification-F1 0.746031746031746 on epoch=124
03/10/2022 21:36:48 - INFO - __main__ - Saving model with best Classification-F1: 0.7184750733137829 -> 0.746031746031746 on epoch=124, global_step=250
03/10/2022 21:36:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.35 on epoch=129
03/10/2022 21:36:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.34 on epoch=134
03/10/2022 21:36:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=139
03/10/2022 21:36:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=144
03/10/2022 21:37:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=149
03/10/2022 21:37:02 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.7702564102564102 on epoch=149
03/10/2022 21:37:02 - INFO - __main__ - Saving model with best Classification-F1: 0.746031746031746 -> 0.7702564102564102 on epoch=149, global_step=300
03/10/2022 21:37:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=154
03/10/2022 21:37:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.43 on epoch=159
03/10/2022 21:37:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=164
03/10/2022 21:37:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=169
03/10/2022 21:37:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
03/10/2022 21:37:16 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.746031746031746 on epoch=174
03/10/2022 21:37:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=179
03/10/2022 21:37:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=184
03/10/2022 21:37:24 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=189
03/10/2022 21:37:27 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=194
03/10/2022 21:37:29 - INFO - __main__ - Step 400 Global step 400 Train loss 0.36 on epoch=199
03/10/2022 21:37:30 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.7184750733137829 on epoch=199
03/10/2022 21:37:32 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=204
03/10/2022 21:37:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=209
03/10/2022 21:37:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=214
03/10/2022 21:37:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.34 on epoch=219
03/10/2022 21:37:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=224
03/10/2022 21:37:44 - INFO - __main__ - Global step 450 Train loss 0.36 Classification-F1 0.6761133603238867 on epoch=224
03/10/2022 21:37:46 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=229
03/10/2022 21:37:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=234
03/10/2022 21:37:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.44 on epoch=239
03/10/2022 21:37:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.35 on epoch=244
03/10/2022 21:37:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=249
03/10/2022 21:37:57 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.6862745098039216 on epoch=249
03/10/2022 21:38:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.39 on epoch=254
03/10/2022 21:38:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=259
03/10/2022 21:38:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.42 on epoch=264
03/10/2022 21:38:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.43 on epoch=269
03/10/2022 21:38:11 - INFO - __main__ - Step 550 Global step 550 Train loss 0.50 on epoch=274
03/10/2022 21:38:11 - INFO - __main__ - Global step 550 Train loss 0.43 Classification-F1 0.49090909090909085 on epoch=274
03/10/2022 21:38:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.45 on epoch=279
03/10/2022 21:38:17 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=284
03/10/2022 21:38:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=289
03/10/2022 21:38:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=294
03/10/2022 21:38:25 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=299
03/10/2022 21:38:25 - INFO - __main__ - Global step 600 Train loss 0.40 Classification-F1 0.6532019704433498 on epoch=299
03/10/2022 21:38:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=304
03/10/2022 21:38:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.42 on epoch=309
03/10/2022 21:38:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.65 on epoch=314
03/10/2022 21:38:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.68 on epoch=319
03/10/2022 21:38:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.68 on epoch=324
03/10/2022 21:38:39 - INFO - __main__ - Global step 650 Train loss 0.57 Classification-F1 0.6235294117647059 on epoch=324
03/10/2022 21:38:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.82 on epoch=329
03/10/2022 21:38:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=334
03/10/2022 21:38:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.65 on epoch=339
03/10/2022 21:38:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.54 on epoch=344
03/10/2022 21:38:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.64 on epoch=349
03/10/2022 21:38:53 - INFO - __main__ - Global step 700 Train loss 0.66 Classification-F1 0.6113360323886641 on epoch=349
03/10/2022 21:38:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.53 on epoch=354
03/10/2022 21:38:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.58 on epoch=359
03/10/2022 21:39:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.48 on epoch=364
03/10/2022 21:39:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.49 on epoch=369
03/10/2022 21:39:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.59 on epoch=374
03/10/2022 21:39:07 - INFO - __main__ - Global step 750 Train loss 0.53 Classification-F1 0.6559139784946237 on epoch=374
03/10/2022 21:39:10 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=379
03/10/2022 21:39:12 - INFO - __main__ - Step 770 Global step 770 Train loss 0.48 on epoch=384
03/10/2022 21:39:15 - INFO - __main__ - Step 780 Global step 780 Train loss 0.45 on epoch=389
03/10/2022 21:39:18 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=394
03/10/2022 21:39:20 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=399
03/10/2022 21:39:21 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.7408906882591093 on epoch=399
03/10/2022 21:39:24 - INFO - __main__ - Step 810 Global step 810 Train loss 0.47 on epoch=404
03/10/2022 21:39:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.48 on epoch=409
03/10/2022 21:39:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
03/10/2022 21:39:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=419
03/10/2022 21:39:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=424
03/10/2022 21:39:35 - INFO - __main__ - Global step 850 Train loss 0.46 Classification-F1 0.6101882613510521 on epoch=424
03/10/2022 21:39:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.46 on epoch=429
03/10/2022 21:39:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.42 on epoch=434
03/10/2022 21:39:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=439
03/10/2022 21:39:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.51 on epoch=444
03/10/2022 21:39:48 - INFO - __main__ - Step 900 Global step 900 Train loss 0.51 on epoch=449
03/10/2022 21:39:49 - INFO - __main__ - Global step 900 Train loss 0.47 Classification-F1 0.6559139784946237 on epoch=449
03/10/2022 21:39:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.51 on epoch=454
03/10/2022 21:39:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.45 on epoch=459
03/10/2022 21:39:57 - INFO - __main__ - Step 930 Global step 930 Train loss 0.43 on epoch=464
03/10/2022 21:40:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.43 on epoch=469
03/10/2022 21:40:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.48 on epoch=474
03/10/2022 21:40:03 - INFO - __main__ - Global step 950 Train loss 0.46 Classification-F1 0.6000000000000001 on epoch=474
03/10/2022 21:40:06 - INFO - __main__ - Step 960 Global step 960 Train loss 0.44 on epoch=479
03/10/2022 21:40:08 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
03/10/2022 21:40:11 - INFO - __main__ - Step 980 Global step 980 Train loss 0.43 on epoch=489
03/10/2022 21:40:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.47 on epoch=494
03/10/2022 21:40:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.44 on epoch=499
03/10/2022 21:40:17 - INFO - __main__ - Global step 1000 Train loss 0.44 Classification-F1 0.6825396825396826 on epoch=499
03/10/2022 21:40:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.43 on epoch=504
03/10/2022 21:40:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=509
03/10/2022 21:40:25 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.39 on epoch=514
03/10/2022 21:40:27 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=519
03/10/2022 21:40:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.47 on epoch=524
03/10/2022 21:40:31 - INFO - __main__ - Global step 1050 Train loss 0.42 Classification-F1 0.6101882613510521 on epoch=524
03/10/2022 21:40:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.40 on epoch=529
03/10/2022 21:40:36 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.42 on epoch=534
03/10/2022 21:40:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=539
03/10/2022 21:40:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=544
03/10/2022 21:40:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
03/10/2022 21:40:45 - INFO - __main__ - Global step 1100 Train loss 0.41 Classification-F1 0.7408906882591093 on epoch=549
03/10/2022 21:40:47 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.37 on epoch=554
03/10/2022 21:40:50 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
03/10/2022 21:40:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=564
03/10/2022 21:40:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
03/10/2022 21:40:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.43 on epoch=574
03/10/2022 21:40:59 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.5636363636363637 on epoch=574
03/10/2022 21:41:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.40 on epoch=579
03/10/2022 21:41:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.44 on epoch=584
03/10/2022 21:41:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=589
03/10/2022 21:41:09 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.45 on epoch=594
03/10/2022 21:41:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=599
03/10/2022 21:41:13 - INFO - __main__ - Global step 1200 Train loss 0.42 Classification-F1 0.4589371980676329 on epoch=599
03/10/2022 21:41:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.44 on epoch=604
03/10/2022 21:41:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=609
03/10/2022 21:41:21 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.42 on epoch=614
03/10/2022 21:41:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=619
03/10/2022 21:41:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=624
03/10/2022 21:41:27 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3992490613266583 on epoch=624
03/10/2022 21:41:29 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.37 on epoch=629
03/10/2022 21:41:32 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=634
03/10/2022 21:41:35 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=639
03/10/2022 21:41:37 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.39 on epoch=644
03/10/2022 21:41:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.45 on epoch=649
03/10/2022 21:41:41 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.5636363636363637 on epoch=649
03/10/2022 21:41:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=654
03/10/2022 21:41:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=659
03/10/2022 21:41:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.42 on epoch=664
03/10/2022 21:41:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=669
03/10/2022 21:41:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.42 on epoch=674
03/10/2022 21:41:54 - INFO - __main__ - Global step 1350 Train loss 0.40 Classification-F1 0.4589371980676329 on epoch=674
03/10/2022 21:41:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.45 on epoch=679
03/10/2022 21:42:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.40 on epoch=684
03/10/2022 21:42:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.46 on epoch=689
03/10/2022 21:42:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.41 on epoch=694
03/10/2022 21:42:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.40 on epoch=699
03/10/2022 21:42:08 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.4589371980676329 on epoch=699
03/10/2022 21:42:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
03/10/2022 21:42:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.38 on epoch=709
03/10/2022 21:42:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=714
03/10/2022 21:42:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=719
03/10/2022 21:42:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=724
03/10/2022 21:42:22 - INFO - __main__ - Global step 1450 Train loss 0.38 Classification-F1 0.6101882613510521 on epoch=724
03/10/2022 21:42:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.44 on epoch=729
03/10/2022 21:42:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=734
03/10/2022 21:42:30 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.40 on epoch=739
03/10/2022 21:42:33 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.45 on epoch=744
03/10/2022 21:42:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.39 on epoch=749
03/10/2022 21:42:36 - INFO - __main__ - Global step 1500 Train loss 0.42 Classification-F1 0.5844155844155844 on epoch=749
03/10/2022 21:42:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
03/10/2022 21:42:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=759
03/10/2022 21:42:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=764
03/10/2022 21:42:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=769
03/10/2022 21:42:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=774
03/10/2022 21:42:50 - INFO - __main__ - Global step 1550 Train loss 0.37 Classification-F1 0.4589371980676329 on epoch=774
03/10/2022 21:42:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.39 on epoch=779
03/10/2022 21:42:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
03/10/2022 21:42:58 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.41 on epoch=789
03/10/2022 21:43:01 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.40 on epoch=794
03/10/2022 21:43:03 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.36 on epoch=799
03/10/2022 21:43:04 - INFO - __main__ - Global step 1600 Train loss 0.39 Classification-F1 0.6190476190476191 on epoch=799
03/10/2022 21:43:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.38 on epoch=804
03/10/2022 21:43:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.36 on epoch=809
03/10/2022 21:43:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=814
03/10/2022 21:43:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=819
03/10/2022 21:43:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=824
03/10/2022 21:43:18 - INFO - __main__ - Global step 1650 Train loss 0.38 Classification-F1 0.5835835835835835 on epoch=824
03/10/2022 21:43:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
03/10/2022 21:43:23 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.39 on epoch=834
03/10/2022 21:43:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.38 on epoch=839
03/10/2022 21:43:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.38 on epoch=844
03/10/2022 21:43:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
03/10/2022 21:43:32 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.5333333333333333 on epoch=849
03/10/2022 21:43:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
03/10/2022 21:43:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.35 on epoch=859
03/10/2022 21:43:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.39 on epoch=864
03/10/2022 21:43:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=869
03/10/2022 21:43:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
03/10/2022 21:43:46 - INFO - __main__ - Global step 1750 Train loss 0.37 Classification-F1 0.5333333333333333 on epoch=874
03/10/2022 21:43:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.38 on epoch=879
03/10/2022 21:43:51 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=884
03/10/2022 21:43:54 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=889
03/10/2022 21:43:56 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=894
03/10/2022 21:43:59 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.37 on epoch=899
03/10/2022 21:43:59 - INFO - __main__ - Global step 1800 Train loss 0.39 Classification-F1 0.5844155844155844 on epoch=899
03/10/2022 21:44:02 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=904
03/10/2022 21:44:05 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
03/10/2022 21:44:07 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.36 on epoch=914
03/10/2022 21:44:10 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=919
03/10/2022 21:44:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
03/10/2022 21:44:13 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.5636363636363637 on epoch=924
03/10/2022 21:44:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.46 on epoch=929
03/10/2022 21:44:19 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=934
03/10/2022 21:44:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.39 on epoch=939
03/10/2022 21:44:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.38 on epoch=944
03/10/2022 21:44:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.43 on epoch=949
03/10/2022 21:44:27 - INFO - __main__ - Global step 1900 Train loss 0.40 Classification-F1 0.5134502923976608 on epoch=949
03/10/2022 21:44:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.37 on epoch=954
03/10/2022 21:44:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
03/10/2022 21:44:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.41 on epoch=964
03/10/2022 21:44:38 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.37 on epoch=969
03/10/2022 21:44:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.40 on epoch=974
03/10/2022 21:44:41 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.5151515151515151 on epoch=974
03/10/2022 21:44:44 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.39 on epoch=979
03/10/2022 21:44:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=984
03/10/2022 21:44:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=989
03/10/2022 21:44:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.38 on epoch=994
03/10/2022 21:44:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.34 on epoch=999
03/10/2022 21:44:55 - INFO - __main__ - Global step 2000 Train loss 0.37 Classification-F1 0.5733333333333335 on epoch=999
03/10/2022 21:44:55 - INFO - __main__ - save last model!
03/10/2022 21:44:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 21:44:55 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:44:55 - INFO - __main__ - Printing 3 examples
03/10/2022 21:44:55 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:44:55 - INFO - __main__ - ['negative']
03/10/2022 21:44:55 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:44:55 - INFO - __main__ - ['negative']
03/10/2022 21:44:55 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:44:55 - INFO - __main__ - ['negative']
03/10/2022 21:44:55 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:44:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:44:56 - INFO - __main__ - Printing 3 examples
03/10/2022 21:44:56 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 21:44:56 - INFO - __main__ - ['positive']
03/10/2022 21:44:56 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 21:44:56 - INFO - __main__ - ['positive']
03/10/2022 21:44:56 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 21:44:56 - INFO - __main__ - ['positive']
03/10/2022 21:44:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:44:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:44:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:44:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:44:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:44:56 - INFO - __main__ - Printing 3 examples
03/10/2022 21:44:56 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 21:44:56 - INFO - __main__ - ['positive']
03/10/2022 21:44:56 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 21:44:56 - INFO - __main__ - ['positive']
03/10/2022 21:44:56 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 21:44:56 - INFO - __main__ - ['positive']
03/10/2022 21:44:56 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:44:56 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:44:56 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:44:57 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:45:10 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:45:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:45:11 - INFO - __main__ - Starting training!
03/10/2022 21:45:17 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_100_0.3_8_predictions.txt
03/10/2022 21:45:17 - INFO - __main__ - Classification-F1 on test data: 0.6314
03/10/2022 21:45:19 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.3, bsz=8, dev_performance=0.7702564102564102, test_performance=0.631446730706647
03/10/2022 21:45:19 - INFO - __main__ - Running ... prefix=amazon_polarity_16_100, lr=0.2, bsz=8 ...
03/10/2022 21:45:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:45:20 - INFO - __main__ - Printing 3 examples
03/10/2022 21:45:20 - INFO - __main__ -  [amazon_polarity] title: Sound so good I never imagine [SEP] content: i bought this Bose 3-2-1 GSX DVD home entertainment system - DVD surround system - radio / DVD - silver a couple mothns ago and satisfied to the sound and portability i set up good because you did not set up good the sound is awful you cannot hear the bass and sound so weird when you not set up good , you must read the manual first, so I read the manual and i set up in my own now it soud great in my DVD and the in my compilation of my CDs in the hardrive feels so great because all my cd collection board in my compilation and the iI put the HDMI in my Tv wow its so nice to hear the sound quality of the BOSE 3-2-1 GSX you will apprreciate it , I know thi is a small room only not for the bigger room in my living , not like my own component in my living room with the big speakers and pioneer receiver yeah it is better sound because it design to the bigger space but this bose is only for the small space room that is great for.. iI recommended this product
03/10/2022 21:45:20 - INFO - __main__ - ['positive']
03/10/2022 21:45:20 - INFO - __main__ -  [amazon_polarity] title: Good product and great Amazon service [SEP] content: I had been looking for a decent 2nd. carving set for a while when this one came up on special. Excellent quality for the price. Cannot fault Amazon's service for quickly sending me a replacement set when the 1st. one got lost somewhere in the wilds of Australia.
03/10/2022 21:45:20 - INFO - __main__ - ['positive']
03/10/2022 21:45:20 - INFO - __main__ -  [amazon_polarity] title: I like it... [SEP] content: This was not a need but a want. I am a gadget freak and I also like good quality kitchen appliances.Works very well...to make MY Egg M?Muffins. I wish it had a lid..but it does not and so I use a small plate to cover.T-Fal...are you listening....how about a simple cover...with a simple handle... thanks :) This would give it 5 stars.
03/10/2022 21:45:20 - INFO - __main__ - ['positive']
03/10/2022 21:45:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:45:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:45:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:45:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:45:20 - INFO - __main__ - Printing 3 examples
03/10/2022 21:45:20 - INFO - __main__ -  [amazon_polarity] title: Considering the price, these are fantastic! [SEP] content: When my good headphones got broken, I needed a quick and cheap fix until I could afford some more high quality ones. I'm not a fan of ear buds, but after reading reviews here I decided to pick a pair of these up. I am not disappointed.Despite their size and price tag, these ear buds pack quite a punch. I was downright amazed by the sound quality, which is simply incredible. For under ten dollars, you can't get better sound. Also, the winding case is a very nice touch. I can keep my headphones from getting tangled, and it only takes about five seconds to get them back in the case.My only real gripes are that the cord is only a meter, far too short if you want to have any freedom of movement when listening. Also, they pop out of your ears too easily, especially because they're short. However, such a problem is minor considering the price and quality of these ear buds.
03/10/2022 21:45:20 - INFO - __main__ - ['positive']
03/10/2022 21:45:20 - INFO - __main__ -  [amazon_polarity] title: great atmospheric gangster movie [SEP] content: I REALLY LIKED THE MOVIE LE SAMOURAI. STARRING ALAIN DELON AS ASSASSIN JEF COSTELLO.. I ALWAYS LIKED ALAIN DELON FOR SUCH A PRETTY BOY HE SURE PLAYS GREAT BADGUYS AND THATS HARD TO DO IF YOUR AS GOOD-LOOKING AS HE IS. ALAIN DELON PLAYS A HITMAN, A LONE WOLF, WHEN HE KILLS A NIGHTCLUB OWNER, A BEAUTIFUL BLACK PIANIST SEES HIM CLOSE UP BUT PROTECTS HIS ALIBI WHEN SHE IS QUESTIONED BY THE POLICE. ALAIN DELONS GIRLFRIEND IN THE MOVIE AND AT THE TIME ALAINS REAL LIFE WIFE, NATHALIE DELON PROTECTS HIS ALIBI TOO. JEF COSTELLO (ALAIN DELON) FINDS THE POLICE AND DOUBLE CROSSING MOB ASSOCIATES HOT ON HIS TAIL. I WONT RUIN THE ENDING AND THE GREAT CHASE SCENES BUT IT WAS A VERY GOOD FILM.
03/10/2022 21:45:20 - INFO - __main__ - ['positive']
03/10/2022 21:45:20 - INFO - __main__ -  [amazon_polarity] title: The best Morandi title under $200.00 [SEP] content: The great strength of this book is its inclusive nature and the quality of printing. Morandi, like many painters, worked out in drawing what would he would find later in the paintings, his best known medium. There are enough water-colors and etchings to enhance the understanding of his paintings by featuring his interests in color and tone in the former, shape and tonal areas in the latter. All these and clearer reproduction (especially of brush strokes, surface textures) make it a better buy than the also useful K. Wilkin book on Morandi. Morandi fans won't mind owning both books; even among the paintings there is not great overlap in particular works. Where there is a double, the differences help remind you of the limitations of reproductions, no matter what book they're in. (PS the Morandi Museum in Bologna, Italy is worth a trip to that city in itself.)
03/10/2022 21:45:20 - INFO - __main__ - ['positive']
03/10/2022 21:45:20 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:45:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:45:20 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:45:33 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:45:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:45:33 - INFO - __main__ - Starting training!
03/10/2022 21:45:37 - INFO - __main__ - Step 10 Global step 10 Train loss 1.56 on epoch=4
03/10/2022 21:45:40 - INFO - __main__ - Step 20 Global step 20 Train loss 0.68 on epoch=9
03/10/2022 21:45:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=14
03/10/2022 21:45:45 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=19
03/10/2022 21:45:48 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=24
03/10/2022 21:45:49 - INFO - __main__ - Global step 50 Train loss 0.73 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 21:45:49 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 21:45:52 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=29
03/10/2022 21:45:54 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
03/10/2022 21:45:57 - INFO - __main__ - Step 80 Global step 80 Train loss 0.45 on epoch=39
03/10/2022 21:46:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=44
03/10/2022 21:46:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.36 on epoch=49
03/10/2022 21:46:03 - INFO - __main__ - Global step 100 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 21:46:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
03/10/2022 21:46:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=59
03/10/2022 21:46:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.41 on epoch=64
03/10/2022 21:46:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.36 on epoch=69
03/10/2022 21:46:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
03/10/2022 21:46:17 - INFO - __main__ - Global step 150 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 21:46:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
03/10/2022 21:46:22 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=84
03/10/2022 21:46:25 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
03/10/2022 21:46:28 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
03/10/2022 21:46:30 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=99
03/10/2022 21:46:31 - INFO - __main__ - Global step 200 Train loss 0.37 Classification-F1 0.7117117117117117 on epoch=99
03/10/2022 21:46:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.7117117117117117 on epoch=99, global_step=200
03/10/2022 21:46:34 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=104
03/10/2022 21:46:37 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=109
03/10/2022 21:46:39 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
03/10/2022 21:46:42 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=119
03/10/2022 21:46:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
03/10/2022 21:46:45 - INFO - __main__ - Global step 250 Train loss 0.35 Classification-F1 0.5835835835835835 on epoch=124
03/10/2022 21:46:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
03/10/2022 21:46:51 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=134
03/10/2022 21:46:53 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=139
03/10/2022 21:46:56 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
03/10/2022 21:46:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
03/10/2022 21:46:59 - INFO - __main__ - Global step 300 Train loss 0.35 Classification-F1 0.716256157635468 on epoch=149
03/10/2022 21:47:00 - INFO - __main__ - Saving model with best Classification-F1: 0.7117117117117117 -> 0.716256157635468 on epoch=149, global_step=300
03/10/2022 21:47:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
03/10/2022 21:47:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.32 on epoch=159
03/10/2022 21:47:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
03/10/2022 21:47:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=169
03/10/2022 21:47:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=174
03/10/2022 21:47:14 - INFO - __main__ - Global step 350 Train loss 0.32 Classification-F1 0.5151515151515151 on epoch=174
03/10/2022 21:47:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.30 on epoch=179
03/10/2022 21:47:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=184
03/10/2022 21:47:22 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=189
03/10/2022 21:47:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=194
03/10/2022 21:47:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=199
03/10/2022 21:47:28 - INFO - __main__ - Global step 400 Train loss 0.30 Classification-F1 0.7810361681329424 on epoch=199
03/10/2022 21:47:28 - INFO - __main__ - Saving model with best Classification-F1: 0.716256157635468 -> 0.7810361681329424 on epoch=199, global_step=400
03/10/2022 21:47:31 - INFO - __main__ - Step 410 Global step 410 Train loss 0.29 on epoch=204
03/10/2022 21:47:33 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=209
03/10/2022 21:47:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
03/10/2022 21:47:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
03/10/2022 21:47:41 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
03/10/2022 21:47:42 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.6825396825396826 on epoch=224
03/10/2022 21:47:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/10/2022 21:47:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.26 on epoch=234
03/10/2022 21:47:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=239
03/10/2022 21:47:53 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
03/10/2022 21:47:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=249
03/10/2022 21:47:57 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.8125 on epoch=249
03/10/2022 21:47:57 - INFO - __main__ - Saving model with best Classification-F1: 0.7810361681329424 -> 0.8125 on epoch=249, global_step=500
03/10/2022 21:47:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=254
03/10/2022 21:48:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
03/10/2022 21:48:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=264
03/10/2022 21:48:07 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=269
03/10/2022 21:48:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=274
03/10/2022 21:48:11 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.8125 on epoch=274
03/10/2022 21:48:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
03/10/2022 21:48:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
03/10/2022 21:48:19 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=289
03/10/2022 21:48:22 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=294
03/10/2022 21:48:24 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
03/10/2022 21:48:25 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.7757757757757757 on epoch=299
03/10/2022 21:48:28 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
03/10/2022 21:48:30 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=309
03/10/2022 21:48:33 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=314
03/10/2022 21:48:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.14 on epoch=319
03/10/2022 21:48:39 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=324
03/10/2022 21:48:39 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.8095238095238095 on epoch=324
03/10/2022 21:48:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=329
03/10/2022 21:48:45 - INFO - __main__ - Step 670 Global step 670 Train loss 0.14 on epoch=334
03/10/2022 21:48:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=339
03/10/2022 21:48:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.09 on epoch=344
03/10/2022 21:48:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
03/10/2022 21:48:54 - INFO - __main__ - Global step 700 Train loss 0.13 Classification-F1 0.9375 on epoch=349
03/10/2022 21:48:54 - INFO - __main__ - Saving model with best Classification-F1: 0.8125 -> 0.9375 on epoch=349, global_step=700
03/10/2022 21:48:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.07 on epoch=354
03/10/2022 21:49:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.06 on epoch=359
03/10/2022 21:49:02 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
03/10/2022 21:49:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=369
03/10/2022 21:49:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=374
03/10/2022 21:49:10 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.9375 on epoch=374
03/10/2022 21:49:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=379
03/10/2022 21:49:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.05 on epoch=384
03/10/2022 21:49:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
03/10/2022 21:49:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.07 on epoch=394
03/10/2022 21:49:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
03/10/2022 21:49:25 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.9375 on epoch=399
03/10/2022 21:49:28 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
03/10/2022 21:49:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.07 on epoch=409
03/10/2022 21:49:34 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
03/10/2022 21:49:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
03/10/2022 21:49:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
03/10/2022 21:49:42 - INFO - __main__ - Global step 850 Train loss 0.06 Classification-F1 0.9687194525904204 on epoch=424
03/10/2022 21:49:42 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=424, global_step=850
03/10/2022 21:49:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
03/10/2022 21:49:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=434
03/10/2022 21:49:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.03 on epoch=439
03/10/2022 21:49:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
03/10/2022 21:49:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
03/10/2022 21:49:58 - INFO - __main__ - Global step 900 Train loss 0.05 Classification-F1 0.9375 on epoch=449
03/10/2022 21:50:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
03/10/2022 21:50:03 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
03/10/2022 21:50:06 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
03/10/2022 21:50:08 - INFO - __main__ - Step 940 Global step 940 Train loss 0.03 on epoch=469
03/10/2022 21:50:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
03/10/2022 21:50:15 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=474
03/10/2022 21:50:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=479
03/10/2022 21:50:21 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
03/10/2022 21:50:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
03/10/2022 21:50:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/10/2022 21:50:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/10/2022 21:50:33 - INFO - __main__ - Global step 1000 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=499
03/10/2022 21:50:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
03/10/2022 21:50:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/10/2022 21:50:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
03/10/2022 21:50:44 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.03 on epoch=519
03/10/2022 21:50:47 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/10/2022 21:50:51 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=524
03/10/2022 21:50:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/10/2022 21:50:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
03/10/2022 21:50:59 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/10/2022 21:51:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
03/10/2022 21:51:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/10/2022 21:51:09 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=549
03/10/2022 21:51:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/10/2022 21:51:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/10/2022 21:51:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.02 on epoch=564
03/10/2022 21:51:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.03 on epoch=569
03/10/2022 21:51:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/10/2022 21:51:25 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=574
03/10/2022 21:51:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
03/10/2022 21:51:31 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/10/2022 21:51:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
03/10/2022 21:51:36 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.02 on epoch=594
03/10/2022 21:51:38 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/10/2022 21:51:43 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=599
03/10/2022 21:51:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/10/2022 21:51:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/10/2022 21:51:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/10/2022 21:51:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/10/2022 21:51:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
03/10/2022 21:52:01 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=624
03/10/2022 21:52:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
03/10/2022 21:52:07 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/10/2022 21:52:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/10/2022 21:52:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
03/10/2022 21:52:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
03/10/2022 21:52:17 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 1.0 on epoch=649
03/10/2022 21:52:17 - INFO - __main__ - Saving model with best Classification-F1: 0.9687194525904204 -> 1.0 on epoch=649, global_step=1300
03/10/2022 21:52:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/10/2022 21:52:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/10/2022 21:52:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/10/2022 21:52:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/10/2022 21:52:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/10/2022 21:52:35 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 1.0 on epoch=674
03/10/2022 21:52:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/10/2022 21:52:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/10/2022 21:52:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/10/2022 21:52:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/10/2022 21:52:49 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/10/2022 21:52:53 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 1.0 on epoch=699
03/10/2022 21:52:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/10/2022 21:52:59 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/10/2022 21:53:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/10/2022 21:53:04 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/10/2022 21:53:07 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/10/2022 21:53:11 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 1.0 on epoch=724
03/10/2022 21:53:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/10/2022 21:53:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/10/2022 21:53:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/10/2022 21:53:21 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/10/2022 21:53:24 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/10/2022 21:53:28 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 1.0 on epoch=749
03/10/2022 21:53:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/10/2022 21:53:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/10/2022 21:53:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/10/2022 21:53:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
03/10/2022 21:53:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/10/2022 21:53:46 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 1.0 on epoch=774
03/10/2022 21:53:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/10/2022 21:53:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/10/2022 21:53:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/10/2022 21:53:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/10/2022 21:53:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/10/2022 21:54:04 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/10/2022 21:54:06 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/10/2022 21:54:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/10/2022 21:54:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/10/2022 21:54:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/10/2022 21:54:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/10/2022 21:54:21 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/10/2022 21:54:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/10/2022 21:54:26 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/10/2022 21:54:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/10/2022 21:54:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/10/2022 21:54:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/10/2022 21:54:39 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/10/2022 21:54:41 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/10/2022 21:54:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/10/2022 21:54:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/10/2022 21:54:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/10/2022 21:54:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/10/2022 21:54:56 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 1.0 on epoch=874
03/10/2022 21:54:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/10/2022 21:55:01 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/10/2022 21:55:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/10/2022 21:55:07 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/10/2022 21:55:09 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/10/2022 21:55:13 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 1.0 on epoch=899
03/10/2022 21:55:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/10/2022 21:55:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/10/2022 21:55:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/10/2022 21:55:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
03/10/2022 21:55:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/10/2022 21:55:31 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=924
03/10/2022 21:55:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/10/2022 21:55:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/10/2022 21:55:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/10/2022 21:55:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/10/2022 21:55:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/10/2022 21:55:48 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/10/2022 21:55:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=954
03/10/2022 21:55:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
03/10/2022 21:55:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/10/2022 21:55:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/10/2022 21:56:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=974
03/10/2022 21:56:03 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=974
03/10/2022 21:56:06 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/10/2022 21:56:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/10/2022 21:56:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/10/2022 21:56:14 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/10/2022 21:56:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/10/2022 21:56:19 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 1.0 on epoch=999
03/10/2022 21:56:19 - INFO - __main__ - save last model!
03/10/2022 21:56:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 21:56:20 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 21:56:20 - INFO - __main__ - Printing 3 examples
03/10/2022 21:56:20 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 21:56:20 - INFO - __main__ - ['negative']
03/10/2022 21:56:20 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 21:56:20 - INFO - __main__ - ['negative']
03/10/2022 21:56:20 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 21:56:20 - INFO - __main__ - ['negative']
03/10/2022 21:56:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:56:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:56:20 - INFO - __main__ - Printing 3 examples
03/10/2022 21:56:20 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 21:56:20 - INFO - __main__ - ['negative']
03/10/2022 21:56:20 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 21:56:20 - INFO - __main__ - ['negative']
03/10/2022 21:56:20 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 21:56:20 - INFO - __main__ - ['negative']
03/10/2022 21:56:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 21:56:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:56:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:56:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:56:20 - INFO - __main__ - Printing 3 examples
03/10/2022 21:56:20 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 21:56:20 - INFO - __main__ - ['negative']
03/10/2022 21:56:20 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 21:56:20 - INFO - __main__ - ['negative']
03/10/2022 21:56:20 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 21:56:20 - INFO - __main__ - ['negative']
03/10/2022 21:56:20 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:56:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:56:20 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:56:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:56:21 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 21:56:34 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:56:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:56:35 - INFO - __main__ - Starting training!
03/10/2022 21:57:51 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_100_0.2_8_predictions.txt
03/10/2022 21:57:51 - INFO - __main__ - Classification-F1 on test data: 0.9550
03/10/2022 21:57:52 - INFO - __main__ - prefix=amazon_polarity_16_100, lr=0.2, bsz=8, dev_performance=1.0, test_performance=0.9549963547047311
03/10/2022 21:57:52 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.5, bsz=8 ...
03/10/2022 21:57:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:57:53 - INFO - __main__ - Printing 3 examples
03/10/2022 21:57:53 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 21:57:53 - INFO - __main__ - ['negative']
03/10/2022 21:57:53 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 21:57:53 - INFO - __main__ - ['negative']
03/10/2022 21:57:53 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 21:57:53 - INFO - __main__ - ['negative']
03/10/2022 21:57:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 21:57:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:57:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 21:57:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 21:57:53 - INFO - __main__ - Printing 3 examples
03/10/2022 21:57:53 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 21:57:53 - INFO - __main__ - ['negative']
03/10/2022 21:57:53 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 21:57:53 - INFO - __main__ - ['negative']
03/10/2022 21:57:53 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 21:57:53 - INFO - __main__ - ['negative']
03/10/2022 21:57:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 21:57:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 21:57:53 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 21:58:06 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 21:58:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 21:58:07 - INFO - __main__ - Starting training!
03/10/2022 21:58:10 - INFO - __main__ - Step 10 Global step 10 Train loss 1.04 on epoch=4
03/10/2022 21:58:13 - INFO - __main__ - Step 20 Global step 20 Train loss 0.60 on epoch=9
03/10/2022 21:58:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=14
03/10/2022 21:58:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=19
03/10/2022 21:58:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.39 on epoch=24
03/10/2022 21:58:22 - INFO - __main__ - Global step 50 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 21:58:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 21:58:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=29
03/10/2022 21:58:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=34
03/10/2022 21:58:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=39
03/10/2022 21:58:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.37 on epoch=44
03/10/2022 21:58:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
03/10/2022 21:58:37 - INFO - __main__ - Global step 100 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 21:58:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.36 on epoch=54
03/10/2022 21:58:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=59
03/10/2022 21:58:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=64
03/10/2022 21:58:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=69
03/10/2022 21:58:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
03/10/2022 21:58:52 - INFO - __main__ - Global step 150 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 21:58:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
03/10/2022 21:58:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=84
03/10/2022 21:59:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.39 on epoch=89
03/10/2022 21:59:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
03/10/2022 21:59:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
03/10/2022 21:59:07 - INFO - __main__ - Global step 200 Train loss 0.37 Classification-F1 0.4589371980676329 on epoch=99
03/10/2022 21:59:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4589371980676329 on epoch=99, global_step=200
03/10/2022 21:59:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=104
03/10/2022 21:59:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=109
03/10/2022 21:59:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.56 on epoch=114
03/10/2022 21:59:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
03/10/2022 21:59:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.40 on epoch=124
03/10/2022 21:59:22 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.5134502923976608 on epoch=124
03/10/2022 21:59:22 - INFO - __main__ - Saving model with best Classification-F1: 0.4589371980676329 -> 0.5134502923976608 on epoch=124, global_step=250
03/10/2022 21:59:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=129
03/10/2022 21:59:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=134
03/10/2022 21:59:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.44 on epoch=139
03/10/2022 21:59:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.91 on epoch=144
03/10/2022 21:59:37 - INFO - __main__ - Step 300 Global step 300 Train loss 1.06 on epoch=149
03/10/2022 21:59:37 - INFO - __main__ - Global step 300 Train loss 0.64 Classification-F1 0.5607843137254902 on epoch=149
03/10/2022 21:59:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.5607843137254902 on epoch=149, global_step=300
03/10/2022 21:59:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.41 on epoch=154
03/10/2022 21:59:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.40 on epoch=159
03/10/2022 21:59:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=164
03/10/2022 21:59:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.39 on epoch=169
03/10/2022 21:59:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.43 on epoch=174
03/10/2022 21:59:52 - INFO - __main__ - Global step 350 Train loss 0.41 Classification-F1 0.539313399778516 on epoch=174
03/10/2022 21:59:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=179
03/10/2022 21:59:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=184
03/10/2022 22:00:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=189
03/10/2022 22:00:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.53 on epoch=194
03/10/2022 22:00:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.43 on epoch=199
03/10/2022 22:00:07 - INFO - __main__ - Global step 400 Train loss 0.46 Classification-F1 0.3992490613266583 on epoch=199
03/10/2022 22:00:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=204
03/10/2022 22:00:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.40 on epoch=209
03/10/2022 22:00:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=214
03/10/2022 22:00:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
03/10/2022 22:00:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.44 on epoch=224
03/10/2022 22:00:22 - INFO - __main__ - Global step 450 Train loss 0.41 Classification-F1 0.3992490613266583 on epoch=224
03/10/2022 22:00:25 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=229
03/10/2022 22:00:28 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=234
03/10/2022 22:00:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.43 on epoch=239
03/10/2022 22:00:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=244
03/10/2022 22:00:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.43 on epoch=249
03/10/2022 22:00:37 - INFO - __main__ - Global step 500 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=249
03/10/2022 22:00:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
03/10/2022 22:00:43 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=259
03/10/2022 22:00:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=264
03/10/2022 22:00:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
03/10/2022 22:00:51 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=274
03/10/2022 22:00:52 - INFO - __main__ - Global step 550 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=274
03/10/2022 22:00:55 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=279
03/10/2022 22:00:58 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=284
03/10/2022 22:01:01 - INFO - __main__ - Step 580 Global step 580 Train loss 0.42 on epoch=289
03/10/2022 22:01:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.36 on epoch=294
03/10/2022 22:01:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.36 on epoch=299
03/10/2022 22:01:07 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=299
03/10/2022 22:01:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=304
03/10/2022 22:01:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=309
03/10/2022 22:01:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=314
03/10/2022 22:01:19 - INFO - __main__ - Step 640 Global step 640 Train loss 0.38 on epoch=319
03/10/2022 22:01:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.37 on epoch=324
03/10/2022 22:01:22 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=324
03/10/2022 22:01:25 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=329
03/10/2022 22:01:28 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=334
03/10/2022 22:01:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=339
03/10/2022 22:01:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=344
03/10/2022 22:01:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=349
03/10/2022 22:01:37 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=349
03/10/2022 22:01:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=354
03/10/2022 22:01:43 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=359
03/10/2022 22:01:46 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=364
03/10/2022 22:01:48 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=369
03/10/2022 22:01:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=374
03/10/2022 22:01:52 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=374
03/10/2022 22:01:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=379
03/10/2022 22:01:58 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=384
03/10/2022 22:02:00 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=389
03/10/2022 22:02:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=394
03/10/2022 22:02:06 - INFO - __main__ - Step 800 Global step 800 Train loss 0.42 on epoch=399
03/10/2022 22:02:07 - INFO - __main__ - Global step 800 Train loss 0.38 Classification-F1 0.4589371980676329 on epoch=399
03/10/2022 22:02:10 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=404
03/10/2022 22:02:12 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=409
03/10/2022 22:02:15 - INFO - __main__ - Step 830 Global step 830 Train loss 0.35 on epoch=414
03/10/2022 22:02:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=419
03/10/2022 22:02:21 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=424
03/10/2022 22:02:22 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.4589371980676329 on epoch=424
03/10/2022 22:02:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=429
03/10/2022 22:02:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.35 on epoch=434
03/10/2022 22:02:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.35 on epoch=439
03/10/2022 22:02:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=444
03/10/2022 22:02:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=449
03/10/2022 22:02:37 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=449
03/10/2022 22:02:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=454
03/10/2022 22:02:42 - INFO - __main__ - Step 920 Global step 920 Train loss 0.35 on epoch=459
03/10/2022 22:02:45 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=464
03/10/2022 22:02:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.36 on epoch=469
03/10/2022 22:02:51 - INFO - __main__ - Step 950 Global step 950 Train loss 0.38 on epoch=474
03/10/2022 22:02:51 - INFO - __main__ - Global step 950 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=474
03/10/2022 22:02:54 - INFO - __main__ - Step 960 Global step 960 Train loss 0.36 on epoch=479
03/10/2022 22:02:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=484
03/10/2022 22:03:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=489
03/10/2022 22:03:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=494
03/10/2022 22:03:06 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.34 on epoch=499
03/10/2022 22:03:06 - INFO - __main__ - Global step 1000 Train loss 0.35 Classification-F1 0.4589371980676329 on epoch=499
03/10/2022 22:03:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.36 on epoch=504
03/10/2022 22:03:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=509
03/10/2022 22:03:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=514
03/10/2022 22:03:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=519
03/10/2022 22:03:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=524
03/10/2022 22:03:21 - INFO - __main__ - Global step 1050 Train loss 0.34 Classification-F1 0.3992490613266583 on epoch=524
03/10/2022 22:03:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=529
03/10/2022 22:03:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.36 on epoch=534
03/10/2022 22:03:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.37 on epoch=539
03/10/2022 22:03:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.39 on epoch=544
03/10/2022 22:03:36 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=549
03/10/2022 22:03:36 - INFO - __main__ - Global step 1100 Train loss 0.36 Classification-F1 0.3992490613266583 on epoch=549
03/10/2022 22:03:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
03/10/2022 22:03:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=559
03/10/2022 22:03:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.38 on epoch=564
03/10/2022 22:03:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.32 on epoch=569
03/10/2022 22:03:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=574
03/10/2022 22:03:51 - INFO - __main__ - Global step 1150 Train loss 0.34 Classification-F1 0.4589371980676329 on epoch=574
03/10/2022 22:03:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=579
03/10/2022 22:03:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=584
03/10/2022 22:04:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=589
03/10/2022 22:04:02 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=594
03/10/2022 22:04:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.34 on epoch=599
03/10/2022 22:04:06 - INFO - __main__ - Global step 1200 Train loss 0.35 Classification-F1 0.6101882613510521 on epoch=599
03/10/2022 22:04:06 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.6101882613510521 on epoch=599, global_step=1200
03/10/2022 22:04:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
03/10/2022 22:04:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=609
03/10/2022 22:04:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.37 on epoch=614
03/10/2022 22:04:17 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.35 on epoch=619
03/10/2022 22:04:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.31 on epoch=624
03/10/2022 22:04:21 - INFO - __main__ - Global step 1250 Train loss 0.34 Classification-F1 0.6536796536796536 on epoch=624
03/10/2022 22:04:21 - INFO - __main__ - Saving model with best Classification-F1: 0.6101882613510521 -> 0.6536796536796536 on epoch=624, global_step=1250
03/10/2022 22:04:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=629
03/10/2022 22:04:27 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.33 on epoch=634
03/10/2022 22:04:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.27 on epoch=639
03/10/2022 22:04:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=644
03/10/2022 22:04:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=649
03/10/2022 22:04:36 - INFO - __main__ - Global step 1300 Train loss 0.30 Classification-F1 0.8435972629521017 on epoch=649
03/10/2022 22:04:36 - INFO - __main__ - Saving model with best Classification-F1: 0.6536796536796536 -> 0.8435972629521017 on epoch=649, global_step=1300
03/10/2022 22:04:39 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.32 on epoch=654
03/10/2022 22:04:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=659
03/10/2022 22:04:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=664
03/10/2022 22:04:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=669
03/10/2022 22:04:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=674
03/10/2022 22:04:52 - INFO - __main__ - Global step 1350 Train loss 0.26 Classification-F1 0.873015873015873 on epoch=674
03/10/2022 22:04:52 - INFO - __main__ - Saving model with best Classification-F1: 0.8435972629521017 -> 0.873015873015873 on epoch=674, global_step=1350
03/10/2022 22:04:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=679
03/10/2022 22:04:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=684
03/10/2022 22:05:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=689
03/10/2022 22:05:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
03/10/2022 22:05:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=699
03/10/2022 22:05:07 - INFO - __main__ - Global step 1400 Train loss 0.15 Classification-F1 0.8435972629521017 on epoch=699
03/10/2022 22:05:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.10 on epoch=704
03/10/2022 22:05:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=709
03/10/2022 22:05:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=714
03/10/2022 22:05:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=719
03/10/2022 22:05:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=724
03/10/2022 22:05:23 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.8745098039215686 on epoch=724
03/10/2022 22:05:23 - INFO - __main__ - Saving model with best Classification-F1: 0.873015873015873 -> 0.8745098039215686 on epoch=724, global_step=1450
03/10/2022 22:05:26 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=729
03/10/2022 22:05:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
03/10/2022 22:05:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=739
03/10/2022 22:05:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
03/10/2022 22:05:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
03/10/2022 22:05:39 - INFO - __main__ - Global step 1500 Train loss 0.05 Classification-F1 0.9372549019607843 on epoch=749
03/10/2022 22:05:39 - INFO - __main__ - Saving model with best Classification-F1: 0.8745098039215686 -> 0.9372549019607843 on epoch=749, global_step=1500
03/10/2022 22:05:42 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.04 on epoch=754
03/10/2022 22:05:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
03/10/2022 22:05:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
03/10/2022 22:05:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
03/10/2022 22:05:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
03/10/2022 22:05:56 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.9375 on epoch=774
03/10/2022 22:05:56 - INFO - __main__ - Saving model with best Classification-F1: 0.9372549019607843 -> 0.9375 on epoch=774, global_step=1550
03/10/2022 22:05:59 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=779
03/10/2022 22:06:01 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
03/10/2022 22:06:04 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
03/10/2022 22:06:07 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
03/10/2022 22:06:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
03/10/2022 22:06:12 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.9375 on epoch=799
03/10/2022 22:06:15 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
03/10/2022 22:06:18 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
03/10/2022 22:06:21 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
03/10/2022 22:06:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/10/2022 22:06:26 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
03/10/2022 22:06:28 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.9375 on epoch=824
03/10/2022 22:06:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/10/2022 22:06:34 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
03/10/2022 22:06:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
03/10/2022 22:06:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
03/10/2022 22:06:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/10/2022 22:06:44 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.906158357771261 on epoch=849
03/10/2022 22:06:47 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.06 on epoch=854
03/10/2022 22:06:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
03/10/2022 22:06:53 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
03/10/2022 22:06:56 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
03/10/2022 22:06:58 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
03/10/2022 22:07:00 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.9375 on epoch=874
03/10/2022 22:07:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.02 on epoch=879
03/10/2022 22:07:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/10/2022 22:07:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
03/10/2022 22:07:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/10/2022 22:07:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/10/2022 22:07:16 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=899
03/10/2022 22:07:16 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=899, global_step=1800
03/10/2022 22:07:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/10/2022 22:07:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/10/2022 22:07:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
03/10/2022 22:07:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
03/10/2022 22:07:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/10/2022 22:07:33 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.906158357771261 on epoch=924
03/10/2022 22:07:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/10/2022 22:07:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/10/2022 22:07:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/10/2022 22:07:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
03/10/2022 22:07:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/10/2022 22:07:50 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=949
03/10/2022 22:07:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/10/2022 22:07:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/10/2022 22:07:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
03/10/2022 22:08:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/10/2022 22:08:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/10/2022 22:08:06 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=974
03/10/2022 22:08:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
03/10/2022 22:08:12 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/10/2022 22:08:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
03/10/2022 22:08:18 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/10/2022 22:08:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
03/10/2022 22:08:22 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.906158357771261 on epoch=999
03/10/2022 22:08:22 - INFO - __main__ - save last model!
03/10/2022 22:08:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 22:08:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:08:22 - INFO - __main__ - Printing 3 examples
03/10/2022 22:08:22 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 22:08:22 - INFO - __main__ - ['negative']
03/10/2022 22:08:22 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 22:08:22 - INFO - __main__ - ['negative']
03/10/2022 22:08:22 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 22:08:22 - INFO - __main__ - ['negative']
03/10/2022 22:08:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:08:22 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:08:22 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:08:22 - INFO - __main__ - Printing 3 examples
03/10/2022 22:08:22 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:08:22 - INFO - __main__ - ['negative']
03/10/2022 22:08:22 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:08:22 - INFO - __main__ - ['negative']
03/10/2022 22:08:22 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:08:22 - INFO - __main__ - ['negative']
03/10/2022 22:08:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:08:22 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:08:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:08:22 - INFO - __main__ - Printing 3 examples
03/10/2022 22:08:22 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 22:08:22 - INFO - __main__ - ['negative']
03/10/2022 22:08:22 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 22:08:22 - INFO - __main__ - ['negative']
03/10/2022 22:08:22 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 22:08:22 - INFO - __main__ - ['negative']
03/10/2022 22:08:22 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:08:22 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:08:23 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:08:23 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:08:24 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:08:37 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:08:37 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:08:38 - INFO - __main__ - Starting training!
03/10/2022 22:09:16 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_13_0.5_8_predictions.txt
03/10/2022 22:09:16 - INFO - __main__ - Classification-F1 on test data: 0.9299
03/10/2022 22:09:16 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.5, bsz=8, dev_performance=0.9687194525904204, test_performance=0.929851565913473
03/10/2022 22:09:16 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.4, bsz=8 ...
03/10/2022 22:09:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:09:17 - INFO - __main__ - Printing 3 examples
03/10/2022 22:09:17 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 22:09:17 - INFO - __main__ - ['negative']
03/10/2022 22:09:17 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 22:09:17 - INFO - __main__ - ['negative']
03/10/2022 22:09:17 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 22:09:17 - INFO - __main__ - ['negative']
03/10/2022 22:09:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:09:17 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:09:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:09:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:09:17 - INFO - __main__ - Printing 3 examples
03/10/2022 22:09:17 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 22:09:17 - INFO - __main__ - ['negative']
03/10/2022 22:09:17 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 22:09:17 - INFO - __main__ - ['negative']
03/10/2022 22:09:17 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 22:09:17 - INFO - __main__ - ['negative']
03/10/2022 22:09:17 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:09:17 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:09:17 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:09:30 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:09:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:09:31 - INFO - __main__ - Starting training!
03/10/2022 22:09:34 - INFO - __main__ - Step 10 Global step 10 Train loss 1.14 on epoch=4
03/10/2022 22:09:37 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=9
03/10/2022 22:09:40 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=14
03/10/2022 22:09:43 - INFO - __main__ - Step 40 Global step 40 Train loss 0.43 on epoch=19
03/10/2022 22:09:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.53 on epoch=24
03/10/2022 22:09:47 - INFO - __main__ - Global step 50 Train loss 0.60 Classification-F1 0.3992490613266583 on epoch=24
03/10/2022 22:09:47 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3992490613266583 on epoch=24, global_step=50
03/10/2022 22:09:50 - INFO - __main__ - Step 60 Global step 60 Train loss 0.67 on epoch=29
03/10/2022 22:09:53 - INFO - __main__ - Step 70 Global step 70 Train loss 0.66 on epoch=34
03/10/2022 22:09:56 - INFO - __main__ - Step 80 Global step 80 Train loss 0.37 on epoch=39
03/10/2022 22:09:59 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=44
03/10/2022 22:10:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=49
03/10/2022 22:10:02 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.5134502923976608 on epoch=49
03/10/2022 22:10:02 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.5134502923976608 on epoch=49, global_step=100
03/10/2022 22:10:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
03/10/2022 22:10:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
03/10/2022 22:10:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
03/10/2022 22:10:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.48 on epoch=69
03/10/2022 22:10:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.68 on epoch=74
03/10/2022 22:10:17 - INFO - __main__ - Global step 150 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 22:10:20 - INFO - __main__ - Step 160 Global step 160 Train loss 0.59 on epoch=79
03/10/2022 22:10:23 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
03/10/2022 22:10:26 - INFO - __main__ - Step 180 Global step 180 Train loss 0.42 on epoch=89
03/10/2022 22:10:29 - INFO - __main__ - Step 190 Global step 190 Train loss 0.40 on epoch=94
03/10/2022 22:10:32 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
03/10/2022 22:10:32 - INFO - __main__ - Global step 200 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 22:10:35 - INFO - __main__ - Step 210 Global step 210 Train loss 0.45 on epoch=104
03/10/2022 22:10:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
03/10/2022 22:10:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
03/10/2022 22:10:44 - INFO - __main__ - Step 240 Global step 240 Train loss 0.41 on epoch=119
03/10/2022 22:10:47 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=124
03/10/2022 22:10:48 - INFO - __main__ - Global step 250 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=124
03/10/2022 22:10:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
03/10/2022 22:10:54 - INFO - __main__ - Step 270 Global step 270 Train loss 0.38 on epoch=134
03/10/2022 22:10:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.37 on epoch=139
03/10/2022 22:11:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
03/10/2022 22:11:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.41 on epoch=149
03/10/2022 22:11:03 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=149
03/10/2022 22:11:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.32 on epoch=154
03/10/2022 22:11:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=159
03/10/2022 22:11:12 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=164
03/10/2022 22:11:15 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=169
03/10/2022 22:11:18 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
03/10/2022 22:11:18 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=174
03/10/2022 22:11:21 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=179
03/10/2022 22:11:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.43 on epoch=184
03/10/2022 22:11:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=189
03/10/2022 22:11:30 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=194
03/10/2022 22:11:33 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=199
03/10/2022 22:11:33 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=199
03/10/2022 22:11:36 - INFO - __main__ - Step 410 Global step 410 Train loss 0.41 on epoch=204
03/10/2022 22:11:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=209
03/10/2022 22:11:42 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=214
03/10/2022 22:11:45 - INFO - __main__ - Step 440 Global step 440 Train loss 0.40 on epoch=219
03/10/2022 22:11:48 - INFO - __main__ - Step 450 Global step 450 Train loss 0.37 on epoch=224
03/10/2022 22:11:49 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.6101882613510521 on epoch=224
03/10/2022 22:11:49 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.6101882613510521 on epoch=224, global_step=450
03/10/2022 22:11:52 - INFO - __main__ - Step 460 Global step 460 Train loss 0.37 on epoch=229
03/10/2022 22:11:55 - INFO - __main__ - Step 470 Global step 470 Train loss 0.42 on epoch=234
03/10/2022 22:11:58 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=239
03/10/2022 22:12:00 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=244
03/10/2022 22:12:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=249
03/10/2022 22:12:04 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=249
03/10/2022 22:12:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.33 on epoch=254
03/10/2022 22:12:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.37 on epoch=259
03/10/2022 22:12:13 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=264
03/10/2022 22:12:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=269
03/10/2022 22:12:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=274
03/10/2022 22:12:19 - INFO - __main__ - Global step 550 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=274
03/10/2022 22:12:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=279
03/10/2022 22:12:25 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=284
03/10/2022 22:12:28 - INFO - __main__ - Step 580 Global step 580 Train loss 0.37 on epoch=289
03/10/2022 22:12:31 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=294
03/10/2022 22:12:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.38 on epoch=299
03/10/2022 22:12:34 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=299
03/10/2022 22:12:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=304
03/10/2022 22:12:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.34 on epoch=309
03/10/2022 22:12:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=314
03/10/2022 22:12:46 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=319
03/10/2022 22:12:49 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=324
03/10/2022 22:12:50 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.6113360323886641 on epoch=324
03/10/2022 22:12:50 - INFO - __main__ - Saving model with best Classification-F1: 0.6101882613510521 -> 0.6113360323886641 on epoch=324, global_step=650
03/10/2022 22:12:53 - INFO - __main__ - Step 660 Global step 660 Train loss 0.35 on epoch=329
03/10/2022 22:12:55 - INFO - __main__ - Step 670 Global step 670 Train loss 0.33 on epoch=334
03/10/2022 22:12:58 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=339
03/10/2022 22:13:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.35 on epoch=344
03/10/2022 22:13:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.34 on epoch=349
03/10/2022 22:13:05 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.6761133603238867 on epoch=349
03/10/2022 22:13:05 - INFO - __main__ - Saving model with best Classification-F1: 0.6113360323886641 -> 0.6761133603238867 on epoch=349, global_step=700
03/10/2022 22:13:08 - INFO - __main__ - Step 710 Global step 710 Train loss 0.33 on epoch=354
03/10/2022 22:13:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.34 on epoch=359
03/10/2022 22:13:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.34 on epoch=364
03/10/2022 22:13:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=369
03/10/2022 22:13:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=374
03/10/2022 22:13:20 - INFO - __main__ - Global step 750 Train loss 0.34 Classification-F1 0.5134502923976608 on epoch=374
03/10/2022 22:13:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=379
03/10/2022 22:13:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=384
03/10/2022 22:13:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
03/10/2022 22:13:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=394
03/10/2022 22:13:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=399
03/10/2022 22:13:35 - INFO - __main__ - Global step 800 Train loss 0.35 Classification-F1 0.3992490613266583 on epoch=399
03/10/2022 22:13:38 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=404
03/10/2022 22:13:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.31 on epoch=409
03/10/2022 22:13:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=414
03/10/2022 22:13:47 - INFO - __main__ - Step 840 Global step 840 Train loss 0.32 on epoch=419
03/10/2022 22:13:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=424
03/10/2022 22:13:50 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.7408906882591093 on epoch=424
03/10/2022 22:13:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6761133603238867 -> 0.7408906882591093 on epoch=424, global_step=850
03/10/2022 22:13:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=429
03/10/2022 22:13:56 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=434
03/10/2022 22:13:59 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=439
03/10/2022 22:14:02 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
03/10/2022 22:14:05 - INFO - __main__ - Step 900 Global step 900 Train loss 0.35 on epoch=449
03/10/2022 22:14:06 - INFO - __main__ - Global step 900 Train loss 0.34 Classification-F1 0.5636363636363637 on epoch=449
03/10/2022 22:14:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.44 on epoch=454
03/10/2022 22:14:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/10/2022 22:14:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.38 on epoch=464
03/10/2022 22:14:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=469
03/10/2022 22:14:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.37 on epoch=474
03/10/2022 22:14:21 - INFO - __main__ - Global step 950 Train loss 0.40 Classification-F1 0.5134502923976608 on epoch=474
03/10/2022 22:14:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.61 on epoch=479
03/10/2022 22:14:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.39 on epoch=484
03/10/2022 22:14:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=489
03/10/2022 22:14:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=494
03/10/2022 22:14:36 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
03/10/2022 22:14:36 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=499
03/10/2022 22:14:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.50 on epoch=504
03/10/2022 22:14:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.38 on epoch=509
03/10/2022 22:14:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=514
03/10/2022 22:14:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.67 on epoch=519
03/10/2022 22:14:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=524
03/10/2022 22:14:51 - INFO - __main__ - Global step 1050 Train loss 0.47 Classification-F1 0.4589371980676329 on epoch=524
03/10/2022 22:14:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=529
03/10/2022 22:14:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=534
03/10/2022 22:15:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=539
03/10/2022 22:15:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=544
03/10/2022 22:15:06 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.33 on epoch=549
03/10/2022 22:15:06 - INFO - __main__ - Global step 1100 Train loss 0.33 Classification-F1 0.5134502923976608 on epoch=549
03/10/2022 22:15:10 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=554
03/10/2022 22:15:12 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
03/10/2022 22:15:15 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=564
03/10/2022 22:15:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.40 on epoch=569
03/10/2022 22:15:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.36 on epoch=574
03/10/2022 22:15:22 - INFO - __main__ - Global step 1150 Train loss 0.36 Classification-F1 0.5134502923976608 on epoch=574
03/10/2022 22:15:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=579
03/10/2022 22:15:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=584
03/10/2022 22:15:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.35 on epoch=589
03/10/2022 22:15:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.32 on epoch=594
03/10/2022 22:15:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.35 on epoch=599
03/10/2022 22:15:37 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.5636363636363637 on epoch=599
03/10/2022 22:15:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=604
03/10/2022 22:15:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.36 on epoch=609
03/10/2022 22:15:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=614
03/10/2022 22:15:48 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.33 on epoch=619
03/10/2022 22:15:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.30 on epoch=624
03/10/2022 22:15:52 - INFO - __main__ - Global step 1250 Train loss 0.33 Classification-F1 0.6101882613510521 on epoch=624
03/10/2022 22:15:55 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.35 on epoch=629
03/10/2022 22:15:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=634
03/10/2022 22:16:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=639
03/10/2022 22:16:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.42 on epoch=644
03/10/2022 22:16:06 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.56 on epoch=649
03/10/2022 22:16:07 - INFO - __main__ - Global step 1300 Train loss 0.40 Classification-F1 0.7408906882591093 on epoch=649
03/10/2022 22:16:10 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=654
03/10/2022 22:16:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=659
03/10/2022 22:16:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.35 on epoch=664
03/10/2022 22:16:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.34 on epoch=669
03/10/2022 22:16:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=674
03/10/2022 22:16:22 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.7408906882591093 on epoch=674
03/10/2022 22:16:25 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.38 on epoch=679
03/10/2022 22:16:28 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=684
03/10/2022 22:16:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=689
03/10/2022 22:16:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.39 on epoch=694
03/10/2022 22:16:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=699
03/10/2022 22:16:37 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.8095238095238095 on epoch=699
03/10/2022 22:16:37 - INFO - __main__ - Saving model with best Classification-F1: 0.7408906882591093 -> 0.8095238095238095 on epoch=699, global_step=1400
03/10/2022 22:16:40 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=704
03/10/2022 22:16:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.32 on epoch=709
03/10/2022 22:16:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.32 on epoch=714
03/10/2022 22:16:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.33 on epoch=719
03/10/2022 22:16:51 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.30 on epoch=724
03/10/2022 22:16:52 - INFO - __main__ - Global step 1450 Train loss 0.31 Classification-F1 0.805668016194332 on epoch=724
03/10/2022 22:16:55 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.33 on epoch=729
03/10/2022 22:16:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.31 on epoch=734
03/10/2022 22:17:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.30 on epoch=739
03/10/2022 22:17:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.37 on epoch=744
03/10/2022 22:17:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=749
03/10/2022 22:17:07 - INFO - __main__ - Global step 1500 Train loss 0.32 Classification-F1 0.7333333333333334 on epoch=749
03/10/2022 22:17:10 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.35 on epoch=754
03/10/2022 22:17:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.33 on epoch=759
03/10/2022 22:17:16 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.35 on epoch=764
03/10/2022 22:17:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.32 on epoch=769
03/10/2022 22:17:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
03/10/2022 22:17:22 - INFO - __main__ - Global step 1550 Train loss 0.33 Classification-F1 0.6945917285259808 on epoch=774
03/10/2022 22:17:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.32 on epoch=779
03/10/2022 22:17:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.31 on epoch=784
03/10/2022 22:17:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.37 on epoch=789
03/10/2022 22:17:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.33 on epoch=794
03/10/2022 22:17:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=799
03/10/2022 22:17:37 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.6536796536796536 on epoch=799
03/10/2022 22:17:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.33 on epoch=804
03/10/2022 22:17:43 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.30 on epoch=809
03/10/2022 22:17:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.33 on epoch=814
03/10/2022 22:17:49 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.35 on epoch=819
03/10/2022 22:17:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=824
03/10/2022 22:17:52 - INFO - __main__ - Global step 1650 Train loss 0.32 Classification-F1 0.6101882613510521 on epoch=824
03/10/2022 22:17:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.33 on epoch=829
03/10/2022 22:17:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.32 on epoch=834
03/10/2022 22:18:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=839
03/10/2022 22:18:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=844
03/10/2022 22:18:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
03/10/2022 22:18:07 - INFO - __main__ - Global step 1700 Train loss 0.31 Classification-F1 0.6945917285259808 on epoch=849
03/10/2022 22:18:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.32 on epoch=854
03/10/2022 22:18:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=859
03/10/2022 22:18:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
03/10/2022 22:18:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=869
03/10/2022 22:18:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.31 on epoch=874
03/10/2022 22:18:23 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.6536796536796536 on epoch=874
03/10/2022 22:18:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.30 on epoch=879
03/10/2022 22:18:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.30 on epoch=884
03/10/2022 22:18:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.29 on epoch=889
03/10/2022 22:18:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.30 on epoch=894
03/10/2022 22:18:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=899
03/10/2022 22:18:38 - INFO - __main__ - Global step 1800 Train loss 0.30 Classification-F1 0.7333333333333334 on epoch=899
03/10/2022 22:18:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.33 on epoch=904
03/10/2022 22:18:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.29 on epoch=909
03/10/2022 22:18:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.31 on epoch=914
03/10/2022 22:18:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.30 on epoch=919
03/10/2022 22:18:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.30 on epoch=924
03/10/2022 22:18:53 - INFO - __main__ - Global step 1850 Train loss 0.31 Classification-F1 0.6536796536796536 on epoch=924
03/10/2022 22:18:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=929
03/10/2022 22:18:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.29 on epoch=934
03/10/2022 22:19:02 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
03/10/2022 22:19:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=944
03/10/2022 22:19:08 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=949
03/10/2022 22:19:09 - INFO - __main__ - Global step 1900 Train loss 0.31 Classification-F1 0.7702564102564102 on epoch=949
03/10/2022 22:19:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.31 on epoch=954
03/10/2022 22:19:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
03/10/2022 22:19:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
03/10/2022 22:19:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.33 on epoch=969
03/10/2022 22:19:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.31 on epoch=974
03/10/2022 22:19:24 - INFO - __main__ - Global step 1950 Train loss 0.31 Classification-F1 0.7333333333333334 on epoch=974
03/10/2022 22:19:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=979
03/10/2022 22:19:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=984
03/10/2022 22:19:33 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
03/10/2022 22:19:36 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=994
03/10/2022 22:19:39 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.29 on epoch=999
03/10/2022 22:19:40 - INFO - __main__ - Global step 2000 Train loss 0.30 Classification-F1 0.7408906882591093 on epoch=999
03/10/2022 22:19:40 - INFO - __main__ - save last model!
03/10/2022 22:19:40 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 22:19:40 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:19:40 - INFO - __main__ - Printing 3 examples
03/10/2022 22:19:40 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:19:40 - INFO - __main__ - ['negative']
03/10/2022 22:19:40 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:19:40 - INFO - __main__ - ['negative']
03/10/2022 22:19:40 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:19:40 - INFO - __main__ - ['negative']
03/10/2022 22:19:40 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:19:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:19:40 - INFO - __main__ - Printing 3 examples
03/10/2022 22:19:40 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 22:19:40 - INFO - __main__ - ['negative']
03/10/2022 22:19:40 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 22:19:40 - INFO - __main__ - ['negative']
03/10/2022 22:19:40 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 22:19:40 - INFO - __main__ - ['negative']
03/10/2022 22:19:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:19:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:19:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:19:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:19:40 - INFO - __main__ - Printing 3 examples
03/10/2022 22:19:40 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 22:19:40 - INFO - __main__ - ['negative']
03/10/2022 22:19:40 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 22:19:40 - INFO - __main__ - ['negative']
03/10/2022 22:19:40 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 22:19:40 - INFO - __main__ - ['negative']
03/10/2022 22:19:40 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:19:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:19:40 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:19:40 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:19:41 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:19:54 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:19:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:19:55 - INFO - __main__ - Starting training!
03/10/2022 22:20:03 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_13_0.4_8_predictions.txt
03/10/2022 22:20:03 - INFO - __main__ - Classification-F1 on test data: 0.7570
03/10/2022 22:20:04 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.4, bsz=8, dev_performance=0.8095238095238095, test_performance=0.757040400562839
03/10/2022 22:20:04 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.3, bsz=8 ...
03/10/2022 22:20:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:20:05 - INFO - __main__ - Printing 3 examples
03/10/2022 22:20:05 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 22:20:05 - INFO - __main__ - ['negative']
03/10/2022 22:20:05 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 22:20:05 - INFO - __main__ - ['negative']
03/10/2022 22:20:05 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 22:20:05 - INFO - __main__ - ['negative']
03/10/2022 22:20:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:20:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:20:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:20:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:20:05 - INFO - __main__ - Printing 3 examples
03/10/2022 22:20:05 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 22:20:05 - INFO - __main__ - ['negative']
03/10/2022 22:20:05 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 22:20:05 - INFO - __main__ - ['negative']
03/10/2022 22:20:05 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 22:20:05 - INFO - __main__ - ['negative']
03/10/2022 22:20:05 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:20:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:20:05 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:20:18 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:20:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:20:18 - INFO - __main__ - Starting training!
03/10/2022 22:20:25 - INFO - __main__ - Step 10 Global step 10 Train loss 1.26 on epoch=4
03/10/2022 22:20:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=9
03/10/2022 22:20:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.41 on epoch=14
03/10/2022 22:20:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=19
03/10/2022 22:20:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=24
03/10/2022 22:20:37 - INFO - __main__ - Global step 50 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 22:20:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 22:20:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.39 on epoch=29
03/10/2022 22:20:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=34
03/10/2022 22:20:45 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=39
03/10/2022 22:20:48 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
03/10/2022 22:20:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.37 on epoch=49
03/10/2022 22:20:52 - INFO - __main__ - Global step 100 Train loss 0.38 Classification-F1 0.6389743589743591 on epoch=49
03/10/2022 22:20:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.6389743589743591 on epoch=49, global_step=100
03/10/2022 22:20:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
03/10/2022 22:20:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=59
03/10/2022 22:21:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=64
03/10/2022 22:21:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.34 on epoch=69
03/10/2022 22:21:06 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=74
03/10/2022 22:21:07 - INFO - __main__ - Global step 150 Train loss 0.36 Classification-F1 0.6875 on epoch=74
03/10/2022 22:21:07 - INFO - __main__ - Saving model with best Classification-F1: 0.6389743589743591 -> 0.6875 on epoch=74, global_step=150
03/10/2022 22:21:09 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
03/10/2022 22:21:12 - INFO - __main__ - Step 170 Global step 170 Train loss 0.38 on epoch=84
03/10/2022 22:21:15 - INFO - __main__ - Step 180 Global step 180 Train loss 0.34 on epoch=89
03/10/2022 22:21:18 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=94
03/10/2022 22:21:21 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
03/10/2022 22:21:22 - INFO - __main__ - Global step 200 Train loss 0.37 Classification-F1 0.5134502923976608 on epoch=99
03/10/2022 22:21:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.36 on epoch=104
03/10/2022 22:21:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=109
03/10/2022 22:21:30 - INFO - __main__ - Step 230 Global step 230 Train loss 0.34 on epoch=114
03/10/2022 22:21:33 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=119
03/10/2022 22:21:36 - INFO - __main__ - Step 250 Global step 250 Train loss 0.32 on epoch=124
03/10/2022 22:21:36 - INFO - __main__ - Global step 250 Train loss 0.33 Classification-F1 0.5636363636363637 on epoch=124
03/10/2022 22:21:39 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=129
03/10/2022 22:21:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=134
03/10/2022 22:21:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=139
03/10/2022 22:21:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
03/10/2022 22:21:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.30 on epoch=149
03/10/2022 22:21:51 - INFO - __main__ - Global step 300 Train loss 0.32 Classification-F1 0.805668016194332 on epoch=149
03/10/2022 22:21:51 - INFO - __main__ - Saving model with best Classification-F1: 0.6875 -> 0.805668016194332 on epoch=149, global_step=300
03/10/2022 22:21:54 - INFO - __main__ - Step 310 Global step 310 Train loss 0.30 on epoch=154
03/10/2022 22:21:57 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=159
03/10/2022 22:22:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
03/10/2022 22:22:03 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=169
03/10/2022 22:22:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=174
03/10/2022 22:22:06 - INFO - __main__ - Global step 350 Train loss 0.31 Classification-F1 0.8745098039215686 on epoch=174
03/10/2022 22:22:06 - INFO - __main__ - Saving model with best Classification-F1: 0.805668016194332 -> 0.8745098039215686 on epoch=174, global_step=350
03/10/2022 22:22:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
03/10/2022 22:22:12 - INFO - __main__ - Step 370 Global step 370 Train loss 0.30 on epoch=184
03/10/2022 22:22:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
03/10/2022 22:22:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=194
03/10/2022 22:22:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
03/10/2022 22:22:21 - INFO - __main__ - Global step 400 Train loss 0.28 Classification-F1 0.906158357771261 on epoch=199
03/10/2022 22:22:21 - INFO - __main__ - Saving model with best Classification-F1: 0.8745098039215686 -> 0.906158357771261 on epoch=199, global_step=400
03/10/2022 22:22:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
03/10/2022 22:22:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
03/10/2022 22:22:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
03/10/2022 22:22:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
03/10/2022 22:22:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.20 on epoch=224
03/10/2022 22:22:36 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.906158357771261 on epoch=224
03/10/2022 22:22:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=229
03/10/2022 22:22:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=234
03/10/2022 22:22:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=239
03/10/2022 22:22:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.31 on epoch=244
03/10/2022 22:22:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=249
03/10/2022 22:22:51 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.7490196078431373 on epoch=249
03/10/2022 22:22:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=254
03/10/2022 22:22:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
03/10/2022 22:23:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
03/10/2022 22:23:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
03/10/2022 22:23:05 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=274
03/10/2022 22:23:06 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.8423645320197044 on epoch=274
03/10/2022 22:23:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.15 on epoch=279
03/10/2022 22:23:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.16 on epoch=284
03/10/2022 22:23:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
03/10/2022 22:23:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
03/10/2022 22:23:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=299
03/10/2022 22:23:22 - INFO - __main__ - Global step 600 Train loss 0.15 Classification-F1 0.8423645320197044 on epoch=299
03/10/2022 22:23:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
03/10/2022 22:23:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.14 on epoch=309
03/10/2022 22:23:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
03/10/2022 22:23:33 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
03/10/2022 22:23:36 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
03/10/2022 22:23:37 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.8095238095238095 on epoch=324
03/10/2022 22:23:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.12 on epoch=329
03/10/2022 22:23:42 - INFO - __main__ - Step 670 Global step 670 Train loss 0.11 on epoch=334
03/10/2022 22:23:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.10 on epoch=339
03/10/2022 22:23:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
03/10/2022 22:23:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
03/10/2022 22:23:52 - INFO - __main__ - Global step 700 Train loss 0.11 Classification-F1 0.8435972629521017 on epoch=349
03/10/2022 22:23:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.09 on epoch=354
03/10/2022 22:23:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
03/10/2022 22:24:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
03/10/2022 22:24:04 - INFO - __main__ - Step 740 Global step 740 Train loss 0.08 on epoch=369
03/10/2022 22:24:07 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=374
03/10/2022 22:24:12 - INFO - __main__ - Global step 750 Train loss 0.07 Classification-F1 0.8435972629521017 on epoch=374
03/10/2022 22:24:15 - INFO - __main__ - Step 760 Global step 760 Train loss 0.05 on epoch=379
03/10/2022 22:24:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.08 on epoch=384
03/10/2022 22:24:21 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
03/10/2022 22:24:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
03/10/2022 22:24:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
03/10/2022 22:24:28 - INFO - __main__ - Global step 800 Train loss 0.05 Classification-F1 0.8745098039215686 on epoch=399
03/10/2022 22:24:31 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
03/10/2022 22:24:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=409
03/10/2022 22:24:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.04 on epoch=414
03/10/2022 22:24:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.05 on epoch=419
03/10/2022 22:24:42 - INFO - __main__ - Step 850 Global step 850 Train loss 0.02 on epoch=424
03/10/2022 22:24:44 - INFO - __main__ - Global step 850 Train loss 0.04 Classification-F1 0.8745098039215686 on epoch=424
03/10/2022 22:24:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.02 on epoch=429
03/10/2022 22:24:49 - INFO - __main__ - Step 870 Global step 870 Train loss 0.03 on epoch=434
03/10/2022 22:24:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
03/10/2022 22:24:55 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
03/10/2022 22:24:58 - INFO - __main__ - Step 900 Global step 900 Train loss 0.02 on epoch=449
03/10/2022 22:24:59 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.906158357771261 on epoch=449
03/10/2022 22:25:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
03/10/2022 22:25:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/10/2022 22:25:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
03/10/2022 22:25:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
03/10/2022 22:25:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
03/10/2022 22:25:14 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.906158357771261 on epoch=474
03/10/2022 22:25:17 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/10/2022 22:25:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/10/2022 22:25:23 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
03/10/2022 22:25:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/10/2022 22:25:29 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/10/2022 22:25:29 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.906158357771261 on epoch=499
03/10/2022 22:25:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.02 on epoch=504
03/10/2022 22:25:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/10/2022 22:25:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
03/10/2022 22:25:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/10/2022 22:25:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/10/2022 22:25:45 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.8745098039215686 on epoch=524
03/10/2022 22:25:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/10/2022 22:25:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/10/2022 22:25:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/10/2022 22:25:56 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/10/2022 22:25:59 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/10/2022 22:26:00 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.906158357771261 on epoch=549
03/10/2022 22:26:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/10/2022 22:26:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/10/2022 22:26:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/10/2022 22:26:12 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/10/2022 22:26:15 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/10/2022 22:26:16 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.875 on epoch=574
03/10/2022 22:26:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/10/2022 22:26:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
03/10/2022 22:26:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/10/2022 22:26:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
03/10/2022 22:26:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/10/2022 22:26:31 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.9054187192118226 on epoch=599
03/10/2022 22:26:34 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/10/2022 22:26:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/10/2022 22:26:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/10/2022 22:26:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/10/2022 22:26:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/10/2022 22:26:47 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.8745098039215686 on epoch=624
03/10/2022 22:26:49 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/10/2022 22:26:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/10/2022 22:26:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/10/2022 22:26:58 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/10/2022 22:27:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/10/2022 22:27:02 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9054187192118226 on epoch=649
03/10/2022 22:27:05 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/10/2022 22:27:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/10/2022 22:27:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
03/10/2022 22:27:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
03/10/2022 22:27:17 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/10/2022 22:27:18 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.906158357771261 on epoch=674
03/10/2022 22:27:21 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/10/2022 22:27:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/10/2022 22:27:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/10/2022 22:27:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/10/2022 22:27:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/10/2022 22:27:34 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.875 on epoch=699
03/10/2022 22:27:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/10/2022 22:27:40 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/10/2022 22:27:42 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/10/2022 22:27:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/10/2022 22:27:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/10/2022 22:27:49 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.873015873015873 on epoch=724
03/10/2022 22:27:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/10/2022 22:27:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/10/2022 22:27:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/10/2022 22:28:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/10/2022 22:28:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
03/10/2022 22:28:05 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=749
03/10/2022 22:28:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/10/2022 22:28:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/10/2022 22:28:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/10/2022 22:28:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/10/2022 22:28:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/10/2022 22:28:21 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=774
03/10/2022 22:28:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/10/2022 22:28:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/10/2022 22:28:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/10/2022 22:28:32 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/10/2022 22:28:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/10/2022 22:28:36 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=799
03/10/2022 22:28:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/10/2022 22:28:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/10/2022 22:28:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/10/2022 22:28:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
03/10/2022 22:28:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/10/2022 22:28:52 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.8745098039215686 on epoch=824
03/10/2022 22:28:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/10/2022 22:28:58 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/10/2022 22:29:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/10/2022 22:29:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/10/2022 22:29:06 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/10/2022 22:29:08 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.906158357771261 on epoch=849
03/10/2022 22:29:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/10/2022 22:29:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/10/2022 22:29:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/10/2022 22:29:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/10/2022 22:29:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/10/2022 22:29:23 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.875 on epoch=874
03/10/2022 22:29:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/10/2022 22:29:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/10/2022 22:29:32 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/10/2022 22:29:35 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/10/2022 22:29:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/10/2022 22:29:39 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.8435972629521017 on epoch=899
03/10/2022 22:29:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/10/2022 22:29:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/10/2022 22:29:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/10/2022 22:29:50 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/10/2022 22:29:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/10/2022 22:29:56 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=924
03/10/2022 22:29:59 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/10/2022 22:30:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/10/2022 22:30:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/10/2022 22:30:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/10/2022 22:30:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/10/2022 22:30:13 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=949
03/10/2022 22:30:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/10/2022 22:30:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/10/2022 22:30:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/10/2022 22:30:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/10/2022 22:30:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/10/2022 22:30:29 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.906158357771261 on epoch=974
03/10/2022 22:30:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/10/2022 22:30:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/10/2022 22:30:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/10/2022 22:30:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/10/2022 22:30:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/10/2022 22:30:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:30:45 - INFO - __main__ - Printing 3 examples
03/10/2022 22:30:45 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 22:30:45 - INFO - __main__ - ['negative']
03/10/2022 22:30:45 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 22:30:45 - INFO - __main__ - ['negative']
03/10/2022 22:30:45 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 22:30:45 - INFO - __main__ - ['negative']
03/10/2022 22:30:45 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:30:45 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:30:45 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:30:45 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:30:45 - INFO - __main__ - Printing 3 examples
03/10/2022 22:30:45 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 22:30:45 - INFO - __main__ - ['negative']
03/10/2022 22:30:45 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 22:30:45 - INFO - __main__ - ['negative']
03/10/2022 22:30:45 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 22:30:45 - INFO - __main__ - ['negative']
03/10/2022 22:30:45 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:30:45 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:30:45 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:30:45 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.906158357771261 on epoch=999
03/10/2022 22:30:45 - INFO - __main__ - save last model!
03/10/2022 22:30:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 22:30:45 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:30:45 - INFO - __main__ - Printing 3 examples
03/10/2022 22:30:45 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:30:45 - INFO - __main__ - ['negative']
03/10/2022 22:30:45 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:30:45 - INFO - __main__ - ['negative']
03/10/2022 22:30:45 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:30:45 - INFO - __main__ - ['negative']
03/10/2022 22:30:45 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:30:46 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:30:47 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:30:59 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:31:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:31:00 - INFO - __main__ - Starting training!
03/10/2022 22:31:33 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_13_0.3_8_predictions.txt
03/10/2022 22:31:33 - INFO - __main__ - Classification-F1 on test data: 0.9228
03/10/2022 22:31:34 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.3, bsz=8, dev_performance=0.906158357771261, test_performance=0.9227992007210755
03/10/2022 22:31:34 - INFO - __main__ - Running ... prefix=amazon_polarity_16_13, lr=0.2, bsz=8 ...
03/10/2022 22:31:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:31:35 - INFO - __main__ - Printing 3 examples
03/10/2022 22:31:35 - INFO - __main__ -  [amazon_polarity] title: Not good [SEP] content: Did not like the book really boring might be a classic .but I'm to good for this horrid book sorry
03/10/2022 22:31:35 - INFO - __main__ - ['negative']
03/10/2022 22:31:35 - INFO - __main__ -  [amazon_polarity] title: Not Worth It, Get the Neat Instead! [SEP] content: I got the diaper genie before my daughter was born. We used it for a while, but it was very hard (not to mention unpleasent) to push the diapers down. It really took some force. The bag constantly ripped. I always had to seperately bag messy diapers because when they got pushed down, some always "escaped". I finally got so annoyed with it I didn't use it any more. We got the Neat system by Safety First at my daughters christening and it is wonderful! Not nearly as much effort to get the diapers down, much nicer looking, and much better than the genie. Save your money, go with the Neat instead!
03/10/2022 22:31:35 - INFO - __main__ - ['negative']
03/10/2022 22:31:35 - INFO - __main__ -  [amazon_polarity] title: a waste of money [SEP] content: I bought this product due to the sloping seats in my Dodge Caravan and thought that it was going to be very useful. Boy, was I wrong. When I tried to put this under my infant carseat it brought the carseat so far away from the back of the seat that I was unable to safely buckle it. There was no way to get a tight fit for the carseat using the 1 inch rule. (the carseat should not move more than 1 inch in any direction) I again tried to use the leveler when we converted to a convertible carseat,and yet again no luck. I recommend the poll noddles, they are much easier to use and a fraction of the cost.
03/10/2022 22:31:35 - INFO - __main__ - ['negative']
03/10/2022 22:31:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:31:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:31:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:31:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:31:35 - INFO - __main__ - Printing 3 examples
03/10/2022 22:31:35 - INFO - __main__ -  [amazon_polarity] title: sold out [SEP] content: i think keane sold out. their first cd was amazing and this one sort of fizzled and died. if they could reconnect with whatever it was that made their first one so great, they'd be back in business.
03/10/2022 22:31:35 - INFO - __main__ - ['negative']
03/10/2022 22:31:35 - INFO - __main__ -  [amazon_polarity] title: NO SUPPORT - STAY AWAY [SEP] content: I ordered tapered proxabrush refills. I received wide instead. I called the company left messages and no one returned the call. I did not pursue it further as it was not worth my time for a $4 item
03/10/2022 22:31:35 - INFO - __main__ - ['negative']
03/10/2022 22:31:35 - INFO - __main__ -  [amazon_polarity] title: Sorry. This one just doesn't make it. [SEP] content: I was real disappointed when I saw this movie. It wasn't as good as Look Who's Talking, the first episode, and it was worse than Look Who's Talking Two. It was a waste of time.
03/10/2022 22:31:35 - INFO - __main__ - ['negative']
03/10/2022 22:31:35 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:31:35 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:31:35 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:31:48 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:31:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:31:49 - INFO - __main__ - Starting training!
03/10/2022 22:31:55 - INFO - __main__ - Step 10 Global step 10 Train loss 1.46 on epoch=4
03/10/2022 22:31:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.58 on epoch=9
03/10/2022 22:32:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=14
03/10/2022 22:32:03 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=19
03/10/2022 22:32:06 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=24
03/10/2022 22:32:07 - INFO - __main__ - Global step 50 Train loss 0.69 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 22:32:07 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 22:32:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=29
03/10/2022 22:32:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.35 on epoch=34
03/10/2022 22:32:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=39
03/10/2022 22:32:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=44
03/10/2022 22:32:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=49
03/10/2022 22:32:22 - INFO - __main__ - Global step 100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 22:32:25 - INFO - __main__ - Step 110 Global step 110 Train loss 0.36 on epoch=54
03/10/2022 22:32:28 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=59
03/10/2022 22:32:31 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=64
03/10/2022 22:32:34 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=69
03/10/2022 22:32:36 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
03/10/2022 22:32:37 - INFO - __main__ - Global step 150 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 22:32:40 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
03/10/2022 22:32:43 - INFO - __main__ - Step 170 Global step 170 Train loss 0.48 on epoch=84
03/10/2022 22:32:46 - INFO - __main__ - Step 180 Global step 180 Train loss 0.35 on epoch=89
03/10/2022 22:32:49 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
03/10/2022 22:32:52 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
03/10/2022 22:32:52 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3992490613266583 on epoch=99
03/10/2022 22:32:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=99, global_step=200
03/10/2022 22:32:55 - INFO - __main__ - Step 210 Global step 210 Train loss 0.35 on epoch=104
03/10/2022 22:32:58 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=109
03/10/2022 22:33:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
03/10/2022 22:33:04 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=119
03/10/2022 22:33:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.34 on epoch=124
03/10/2022 22:33:07 - INFO - __main__ - Global step 250 Train loss 0.35 Classification-F1 0.716256157635468 on epoch=124
03/10/2022 22:33:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.716256157635468 on epoch=124, global_step=250
03/10/2022 22:33:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=129
03/10/2022 22:33:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.33 on epoch=134
03/10/2022 22:33:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=139
03/10/2022 22:33:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
03/10/2022 22:33:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
03/10/2022 22:33:22 - INFO - __main__ - Global step 300 Train loss 0.32 Classification-F1 0.6945917285259808 on epoch=149
03/10/2022 22:33:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.31 on epoch=154
03/10/2022 22:33:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.31 on epoch=159
03/10/2022 22:33:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
03/10/2022 22:33:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.32 on epoch=169
03/10/2022 22:33:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=174
03/10/2022 22:33:37 - INFO - __main__ - Global step 350 Train loss 0.31 Classification-F1 0.8745098039215686 on epoch=174
03/10/2022 22:33:37 - INFO - __main__ - Saving model with best Classification-F1: 0.716256157635468 -> 0.8745098039215686 on epoch=174, global_step=350
03/10/2022 22:33:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
03/10/2022 22:33:43 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
03/10/2022 22:33:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=189
03/10/2022 22:33:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
03/10/2022 22:33:52 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/10/2022 22:33:52 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.906158357771261 on epoch=199
03/10/2022 22:33:52 - INFO - __main__ - Saving model with best Classification-F1: 0.8745098039215686 -> 0.906158357771261 on epoch=199, global_step=400
03/10/2022 22:33:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/10/2022 22:33:58 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
03/10/2022 22:34:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
03/10/2022 22:34:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
03/10/2022 22:34:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
03/10/2022 22:34:07 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.8423645320197044 on epoch=224
03/10/2022 22:34:10 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=229
03/10/2022 22:34:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
03/10/2022 22:34:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=239
03/10/2022 22:34:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
03/10/2022 22:34:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.17 on epoch=249
03/10/2022 22:34:22 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.906158357771261 on epoch=249
03/10/2022 22:34:25 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=254
03/10/2022 22:34:28 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
03/10/2022 22:34:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.16 on epoch=264
03/10/2022 22:34:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
03/10/2022 22:34:37 - INFO - __main__ - Step 550 Global step 550 Train loss 0.13 on epoch=274
03/10/2022 22:34:38 - INFO - __main__ - Global step 550 Train loss 0.16 Classification-F1 0.8095238095238095 on epoch=274
03/10/2022 22:34:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=279
03/10/2022 22:34:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.15 on epoch=284
03/10/2022 22:34:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.16 on epoch=289
03/10/2022 22:34:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.11 on epoch=294
03/10/2022 22:34:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.15 on epoch=299
03/10/2022 22:34:54 - INFO - __main__ - Global step 600 Train loss 0.14 Classification-F1 0.8423645320197044 on epoch=299
03/10/2022 22:34:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.11 on epoch=304
03/10/2022 22:35:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.10 on epoch=309
03/10/2022 22:35:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.11 on epoch=314
03/10/2022 22:35:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.10 on epoch=319
03/10/2022 22:35:08 - INFO - __main__ - Step 650 Global step 650 Train loss 0.10 on epoch=324
03/10/2022 22:35:10 - INFO - __main__ - Global step 650 Train loss 0.10 Classification-F1 0.8095238095238095 on epoch=324
03/10/2022 22:35:12 - INFO - __main__ - Step 660 Global step 660 Train loss 0.10 on epoch=329
03/10/2022 22:35:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
03/10/2022 22:35:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.06 on epoch=339
03/10/2022 22:35:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.11 on epoch=344
03/10/2022 22:35:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.06 on epoch=349
03/10/2022 22:35:26 - INFO - __main__ - Global step 700 Train loss 0.08 Classification-F1 0.8435972629521017 on epoch=349
03/10/2022 22:35:29 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
03/10/2022 22:35:32 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
03/10/2022 22:35:35 - INFO - __main__ - Step 730 Global step 730 Train loss 0.05 on epoch=364
03/10/2022 22:35:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
03/10/2022 22:35:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
03/10/2022 22:35:42 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.875 on epoch=374
03/10/2022 22:35:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
03/10/2022 22:35:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
03/10/2022 22:35:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
03/10/2022 22:35:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.03 on epoch=394
03/10/2022 22:35:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.03 on epoch=399
03/10/2022 22:35:58 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.9054187192118226 on epoch=399
03/10/2022 22:36:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
03/10/2022 22:36:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
03/10/2022 22:36:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
03/10/2022 22:36:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/10/2022 22:36:13 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
03/10/2022 22:36:16 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.8745098039215686 on epoch=424
03/10/2022 22:36:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
03/10/2022 22:36:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/10/2022 22:36:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/10/2022 22:36:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
03/10/2022 22:36:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/10/2022 22:36:33 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.8745098039215686 on epoch=449
03/10/2022 22:36:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.03 on epoch=454
03/10/2022 22:36:39 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/10/2022 22:36:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.02 on epoch=464
03/10/2022 22:36:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/10/2022 22:36:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/10/2022 22:36:50 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.8745098039215686 on epoch=474
03/10/2022 22:36:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/10/2022 22:36:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.02 on epoch=484
03/10/2022 22:36:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.01 on epoch=489
03/10/2022 22:37:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
03/10/2022 22:37:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/10/2022 22:37:06 - INFO - __main__ - Global step 1000 Train loss 0.02 Classification-F1 0.8435972629521017 on epoch=499
03/10/2022 22:37:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/10/2022 22:37:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/10/2022 22:37:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/10/2022 22:37:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/10/2022 22:37:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/10/2022 22:37:23 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.8125 on epoch=524
03/10/2022 22:37:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/10/2022 22:37:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
03/10/2022 22:37:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/10/2022 22:37:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/10/2022 22:37:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/10/2022 22:37:39 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.8745098039215686 on epoch=549
03/10/2022 22:37:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/10/2022 22:37:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/10/2022 22:37:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/10/2022 22:37:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/10/2022 22:37:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
03/10/2022 22:37:58 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.8745098039215686 on epoch=574
03/10/2022 22:38:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/10/2022 22:38:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/10/2022 22:38:07 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/10/2022 22:38:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/10/2022 22:38:12 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/10/2022 22:38:13 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.8423645320197044 on epoch=599
03/10/2022 22:38:16 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/10/2022 22:38:19 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/10/2022 22:38:22 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/10/2022 22:38:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/10/2022 22:38:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/10/2022 22:38:29 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.906158357771261 on epoch=624
03/10/2022 22:38:32 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/10/2022 22:38:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/10/2022 22:38:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/10/2022 22:38:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/10/2022 22:38:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/10/2022 22:38:44 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.8745098039215686 on epoch=649
03/10/2022 22:38:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/10/2022 22:38:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/10/2022 22:38:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
03/10/2022 22:38:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/10/2022 22:38:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/10/2022 22:39:00 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.8423645320197044 on epoch=674
03/10/2022 22:39:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/10/2022 22:39:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/10/2022 22:39:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/10/2022 22:39:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
03/10/2022 22:39:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/10/2022 22:39:15 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.8745098039215686 on epoch=699
03/10/2022 22:39:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/10/2022 22:39:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/10/2022 22:39:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/10/2022 22:39:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/10/2022 22:39:29 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/10/2022 22:39:30 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=724
03/10/2022 22:39:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/10/2022 22:39:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/10/2022 22:39:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/10/2022 22:39:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/10/2022 22:39:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
03/10/2022 22:39:46 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=749
03/10/2022 22:39:49 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
03/10/2022 22:39:51 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/10/2022 22:39:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
03/10/2022 22:39:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/10/2022 22:40:00 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/10/2022 22:40:01 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.8745098039215686 on epoch=774
03/10/2022 22:40:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
03/10/2022 22:40:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/10/2022 22:40:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/10/2022 22:40:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/10/2022 22:40:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/10/2022 22:40:17 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.875 on epoch=799
03/10/2022 22:40:20 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/10/2022 22:40:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/10/2022 22:40:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/10/2022 22:40:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
03/10/2022 22:40:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/10/2022 22:40:32 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=824
03/10/2022 22:40:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/10/2022 22:40:38 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/10/2022 22:40:41 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/10/2022 22:40:44 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/10/2022 22:40:47 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=849
03/10/2022 22:40:48 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.8423645320197044 on epoch=849
03/10/2022 22:40:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/10/2022 22:40:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/10/2022 22:40:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/10/2022 22:40:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/10/2022 22:41:02 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/10/2022 22:41:03 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.8423645320197044 on epoch=874
03/10/2022 22:41:06 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/10/2022 22:41:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/10/2022 22:41:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/10/2022 22:41:15 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/10/2022 22:41:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/10/2022 22:41:19 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.8423645320197044 on epoch=899
03/10/2022 22:41:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/10/2022 22:41:25 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/10/2022 22:41:27 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/10/2022 22:41:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/10/2022 22:41:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/10/2022 22:41:34 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=924
03/10/2022 22:41:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.02 on epoch=929
03/10/2022 22:41:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/10/2022 22:41:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/10/2022 22:41:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/10/2022 22:41:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/10/2022 22:41:49 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.8423645320197044 on epoch=949
03/10/2022 22:41:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/10/2022 22:41:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/10/2022 22:41:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
03/10/2022 22:42:01 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/10/2022 22:42:04 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/10/2022 22:42:05 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=974
03/10/2022 22:42:08 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/10/2022 22:42:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/10/2022 22:42:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/10/2022 22:42:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/10/2022 22:42:19 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
03/10/2022 22:42:20 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.8745098039215686 on epoch=999
03/10/2022 22:42:20 - INFO - __main__ - save last model!
03/10/2022 22:42:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 22:42:20 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:42:20 - INFO - __main__ - Printing 3 examples
03/10/2022 22:42:20 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:42:20 - INFO - __main__ - ['negative']
03/10/2022 22:42:20 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:42:20 - INFO - __main__ - ['negative']
03/10/2022 22:42:20 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:42:20 - INFO - __main__ - ['negative']
03/10/2022 22:42:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:42:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:42:21 - INFO - __main__ - Printing 3 examples
03/10/2022 22:42:21 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 22:42:21 - INFO - __main__ - ['positive']
03/10/2022 22:42:21 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 22:42:21 - INFO - __main__ - ['positive']
03/10/2022 22:42:21 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 22:42:21 - INFO - __main__ - ['positive']
03/10/2022 22:42:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:42:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:42:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:42:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:42:21 - INFO - __main__ - Printing 3 examples
03/10/2022 22:42:21 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 22:42:21 - INFO - __main__ - ['positive']
03/10/2022 22:42:21 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 22:42:21 - INFO - __main__ - ['positive']
03/10/2022 22:42:21 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 22:42:21 - INFO - __main__ - ['positive']
03/10/2022 22:42:21 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:42:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:42:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:42:21 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:42:22 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:42:35 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:42:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:42:36 - INFO - __main__ - Starting training!
03/10/2022 22:42:50 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_13_0.2_8_predictions.txt
03/10/2022 22:42:50 - INFO - __main__ - Classification-F1 on test data: 0.9208
03/10/2022 22:42:52 - INFO - __main__ - prefix=amazon_polarity_16_13, lr=0.2, bsz=8, dev_performance=0.906158357771261, test_performance=0.9208251026517578
03/10/2022 22:42:52 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.5, bsz=8 ...
03/10/2022 22:42:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:42:53 - INFO - __main__ - Printing 3 examples
03/10/2022 22:42:53 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 22:42:53 - INFO - __main__ - ['positive']
03/10/2022 22:42:53 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 22:42:53 - INFO - __main__ - ['positive']
03/10/2022 22:42:53 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 22:42:53 - INFO - __main__ - ['positive']
03/10/2022 22:42:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:42:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:42:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:42:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:42:53 - INFO - __main__ - Printing 3 examples
03/10/2022 22:42:53 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 22:42:53 - INFO - __main__ - ['positive']
03/10/2022 22:42:53 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 22:42:53 - INFO - __main__ - ['positive']
03/10/2022 22:42:53 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 22:42:53 - INFO - __main__ - ['positive']
03/10/2022 22:42:53 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:42:53 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:42:53 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:43:06 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:43:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:43:07 - INFO - __main__ - Starting training!
03/10/2022 22:43:10 - INFO - __main__ - Step 10 Global step 10 Train loss 1.16 on epoch=4
03/10/2022 22:43:13 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=9
03/10/2022 22:43:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.45 on epoch=14
03/10/2022 22:43:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=19
03/10/2022 22:43:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.39 on epoch=24
03/10/2022 22:43:22 - INFO - __main__ - Global step 50 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 22:43:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 22:43:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=29
03/10/2022 22:43:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
03/10/2022 22:43:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.41 on epoch=39
03/10/2022 22:43:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.43 on epoch=44
03/10/2022 22:43:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=49
03/10/2022 22:43:37 - INFO - __main__ - Global step 100 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 22:43:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
03/10/2022 22:43:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.38 on epoch=59
03/10/2022 22:43:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=64
03/10/2022 22:43:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
03/10/2022 22:43:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=74
03/10/2022 22:43:52 - INFO - __main__ - Global step 150 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 22:43:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.41 on epoch=79
03/10/2022 22:43:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=84
03/10/2022 22:44:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=89
03/10/2022 22:44:03 - INFO - __main__ - Step 190 Global step 190 Train loss 0.38 on epoch=94
03/10/2022 22:44:06 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
03/10/2022 22:44:07 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 22:44:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
03/10/2022 22:44:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.39 on epoch=109
03/10/2022 22:44:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=114
03/10/2022 22:44:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.38 on epoch=119
03/10/2022 22:44:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.39 on epoch=124
03/10/2022 22:44:22 - INFO - __main__ - Global step 250 Train loss 0.39 Classification-F1 0.4666666666666667 on epoch=124
03/10/2022 22:44:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4666666666666667 on epoch=124, global_step=250
03/10/2022 22:44:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
03/10/2022 22:44:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
03/10/2022 22:44:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=139
03/10/2022 22:44:33 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=144
03/10/2022 22:44:36 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=149
03/10/2022 22:44:36 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.6190476190476191 on epoch=149
03/10/2022 22:44:37 - INFO - __main__ - Saving model with best Classification-F1: 0.4666666666666667 -> 0.6190476190476191 on epoch=149, global_step=300
03/10/2022 22:44:39 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=154
03/10/2022 22:44:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=159
03/10/2022 22:44:45 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
03/10/2022 22:44:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=169
03/10/2022 22:44:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
03/10/2022 22:44:51 - INFO - __main__ - Global step 350 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=174
03/10/2022 22:44:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=179
03/10/2022 22:44:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=184
03/10/2022 22:45:00 - INFO - __main__ - Step 380 Global step 380 Train loss 0.39 on epoch=189
03/10/2022 22:45:03 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=194
03/10/2022 22:45:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.38 on epoch=199
03/10/2022 22:45:06 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.7793103448275862 on epoch=199
03/10/2022 22:45:06 - INFO - __main__ - Saving model with best Classification-F1: 0.6190476190476191 -> 0.7793103448275862 on epoch=199, global_step=400
03/10/2022 22:45:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=204
03/10/2022 22:45:12 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=209
03/10/2022 22:45:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.33 on epoch=214
03/10/2022 22:45:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=219
03/10/2022 22:45:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=224
03/10/2022 22:45:21 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.6862745098039216 on epoch=224
03/10/2022 22:45:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
03/10/2022 22:45:27 - INFO - __main__ - Step 470 Global step 470 Train loss 0.38 on epoch=234
03/10/2022 22:45:30 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=239
03/10/2022 22:45:32 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=244
03/10/2022 22:45:35 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=249
03/10/2022 22:45:36 - INFO - __main__ - Global step 500 Train loss 0.37 Classification-F1 0.6476476476476476 on epoch=249
03/10/2022 22:45:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=254
03/10/2022 22:45:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
03/10/2022 22:45:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.35 on epoch=264
03/10/2022 22:45:47 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=269
03/10/2022 22:45:50 - INFO - __main__ - Step 550 Global step 550 Train loss 0.35 on epoch=274
03/10/2022 22:45:51 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.746031746031746 on epoch=274
03/10/2022 22:45:54 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=279
03/10/2022 22:45:56 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=284
03/10/2022 22:45:59 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=289
03/10/2022 22:46:02 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=294
03/10/2022 22:46:05 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=299
03/10/2022 22:46:06 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.8125 on epoch=299
03/10/2022 22:46:06 - INFO - __main__ - Saving model with best Classification-F1: 0.7793103448275862 -> 0.8125 on epoch=299, global_step=600
03/10/2022 22:46:08 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=304
03/10/2022 22:46:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.33 on epoch=309
03/10/2022 22:46:14 - INFO - __main__ - Step 630 Global step 630 Train loss 0.29 on epoch=314
03/10/2022 22:46:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=319
03/10/2022 22:46:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=324
03/10/2022 22:46:20 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.716256157635468 on epoch=324
03/10/2022 22:46:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.31 on epoch=329
03/10/2022 22:46:26 - INFO - __main__ - Step 670 Global step 670 Train loss 0.35 on epoch=334
03/10/2022 22:46:29 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=339
03/10/2022 22:46:32 - INFO - __main__ - Step 690 Global step 690 Train loss 0.34 on epoch=344
03/10/2022 22:46:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=349
03/10/2022 22:46:35 - INFO - __main__ - Global step 700 Train loss 0.34 Classification-F1 0.7490196078431373 on epoch=349
03/10/2022 22:46:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=354
03/10/2022 22:46:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=359
03/10/2022 22:46:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=364
03/10/2022 22:46:47 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=369
03/10/2022 22:46:50 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=374
03/10/2022 22:46:50 - INFO - __main__ - Global step 750 Train loss 0.30 Classification-F1 0.7408906882591093 on epoch=374
03/10/2022 22:46:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.28 on epoch=379
03/10/2022 22:46:56 - INFO - __main__ - Step 770 Global step 770 Train loss 0.27 on epoch=384
03/10/2022 22:46:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=389
03/10/2022 22:47:02 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=394
03/10/2022 22:47:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=399
03/10/2022 22:47:05 - INFO - __main__ - Global step 800 Train loss 0.28 Classification-F1 0.7793103448275862 on epoch=399
03/10/2022 22:47:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=404
03/10/2022 22:47:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
03/10/2022 22:47:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=414
03/10/2022 22:47:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=419
03/10/2022 22:47:19 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=424
03/10/2022 22:47:20 - INFO - __main__ - Global step 850 Train loss 0.26 Classification-F1 0.7810361681329424 on epoch=424
03/10/2022 22:47:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=429
03/10/2022 22:47:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=434
03/10/2022 22:47:29 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=439
03/10/2022 22:47:31 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
03/10/2022 22:47:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=449
03/10/2022 22:47:35 - INFO - __main__ - Global step 900 Train loss 0.25 Classification-F1 0.8095238095238095 on epoch=449
03/10/2022 22:47:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=454
03/10/2022 22:47:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=459
03/10/2022 22:47:44 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=464
03/10/2022 22:47:46 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=469
03/10/2022 22:47:49 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=474
03/10/2022 22:47:50 - INFO - __main__ - Global step 950 Train loss 0.17 Classification-F1 0.746031746031746 on epoch=474
03/10/2022 22:47:53 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=479
03/10/2022 22:47:56 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=484
03/10/2022 22:47:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.11 on epoch=489
03/10/2022 22:48:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=494
03/10/2022 22:48:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=499
03/10/2022 22:48:06 - INFO - __main__ - Global step 1000 Train loss 0.13 Classification-F1 0.8125 on epoch=499
03/10/2022 22:48:08 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.12 on epoch=504
03/10/2022 22:48:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=509
03/10/2022 22:48:14 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.09 on epoch=514
03/10/2022 22:48:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.13 on epoch=519
03/10/2022 22:48:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=524
03/10/2022 22:48:21 - INFO - __main__ - Global step 1050 Train loss 0.10 Classification-F1 0.6825396825396826 on epoch=524
03/10/2022 22:48:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=529
03/10/2022 22:48:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=534
03/10/2022 22:48:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
03/10/2022 22:48:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
03/10/2022 22:48:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
03/10/2022 22:48:36 - INFO - __main__ - Global step 1100 Train loss 0.06 Classification-F1 0.7810361681329424 on epoch=549
03/10/2022 22:48:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
03/10/2022 22:48:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
03/10/2022 22:48:45 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
03/10/2022 22:48:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=569
03/10/2022 22:48:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.03 on epoch=574
03/10/2022 22:48:51 - INFO - __main__ - Global step 1150 Train loss 0.03 Classification-F1 0.8095238095238095 on epoch=574
03/10/2022 22:48:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
03/10/2022 22:48:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
03/10/2022 22:49:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
03/10/2022 22:49:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
03/10/2022 22:49:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
03/10/2022 22:49:06 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.7793103448275862 on epoch=599
03/10/2022 22:49:09 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
03/10/2022 22:49:12 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.02 on epoch=609
03/10/2022 22:49:15 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
03/10/2022 22:49:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/10/2022 22:49:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/10/2022 22:49:23 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.8435972629521017 on epoch=624
03/10/2022 22:49:23 - INFO - __main__ - Saving model with best Classification-F1: 0.8125 -> 0.8435972629521017 on epoch=624, global_step=1250
03/10/2022 22:49:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/10/2022 22:49:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
03/10/2022 22:49:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/10/2022 22:49:34 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
03/10/2022 22:49:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/10/2022 22:49:40 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.875 on epoch=649
03/10/2022 22:49:40 - INFO - __main__ - Saving model with best Classification-F1: 0.8435972629521017 -> 0.875 on epoch=649, global_step=1300
03/10/2022 22:49:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/10/2022 22:49:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/10/2022 22:49:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
03/10/2022 22:49:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/10/2022 22:49:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/10/2022 22:49:56 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.8435972629521017 on epoch=674
03/10/2022 22:49:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/10/2022 22:50:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/10/2022 22:50:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/10/2022 22:50:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/10/2022 22:50:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/10/2022 22:50:13 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.8117647058823529 on epoch=699
03/10/2022 22:50:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/10/2022 22:50:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.05 on epoch=709
03/10/2022 22:50:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/10/2022 22:50:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/10/2022 22:50:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/10/2022 22:50:29 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.8435972629521017 on epoch=724
03/10/2022 22:50:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/10/2022 22:50:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/10/2022 22:50:37 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/10/2022 22:50:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/10/2022 22:50:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/10/2022 22:50:44 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.8435972629521017 on epoch=749
03/10/2022 22:50:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/10/2022 22:50:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/10/2022 22:50:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/10/2022 22:50:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/10/2022 22:50:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/10/2022 22:50:59 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.8435972629521017 on epoch=774
03/10/2022 22:51:02 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/10/2022 22:51:05 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
03/10/2022 22:51:08 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/10/2022 22:51:11 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=794
03/10/2022 22:51:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
03/10/2022 22:51:15 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.8117647058823529 on epoch=799
03/10/2022 22:51:17 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/10/2022 22:51:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/10/2022 22:51:23 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/10/2022 22:51:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/10/2022 22:51:29 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/10/2022 22:51:30 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.8435972629521017 on epoch=824
03/10/2022 22:51:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/10/2022 22:51:36 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/10/2022 22:51:38 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
03/10/2022 22:51:41 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/10/2022 22:51:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/10/2022 22:51:45 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.8435972629521017 on epoch=849
03/10/2022 22:51:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
03/10/2022 22:51:51 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/10/2022 22:51:54 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/10/2022 22:51:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
03/10/2022 22:52:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/10/2022 22:52:01 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.875 on epoch=874
03/10/2022 22:52:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/10/2022 22:52:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/10/2022 22:52:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/10/2022 22:52:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/10/2022 22:52:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/10/2022 22:52:16 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.8435972629521017 on epoch=899
03/10/2022 22:52:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.10 on epoch=904
03/10/2022 22:52:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/10/2022 22:52:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/10/2022 22:52:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/10/2022 22:52:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/10/2022 22:52:32 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.8435972629521017 on epoch=924
03/10/2022 22:52:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/10/2022 22:52:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/10/2022 22:52:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/10/2022 22:52:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/10/2022 22:52:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/10/2022 22:52:47 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.875 on epoch=949
03/10/2022 22:52:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/10/2022 22:52:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/10/2022 22:52:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/10/2022 22:52:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/10/2022 22:53:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/10/2022 22:53:04 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.8435972629521017 on epoch=974
03/10/2022 22:53:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/10/2022 22:53:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/10/2022 22:53:12 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/10/2022 22:53:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/10/2022 22:53:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/10/2022 22:53:20 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.875 on epoch=999
03/10/2022 22:53:20 - INFO - __main__ - save last model!
03/10/2022 22:53:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 22:53:20 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 22:53:20 - INFO - __main__ - Printing 3 examples
03/10/2022 22:53:20 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 22:53:20 - INFO - __main__ - ['negative']
03/10/2022 22:53:20 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 22:53:20 - INFO - __main__ - ['negative']
03/10/2022 22:53:20 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 22:53:20 - INFO - __main__ - ['negative']
03/10/2022 22:53:20 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:53:20 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:53:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:53:21 - INFO - __main__ - Printing 3 examples
03/10/2022 22:53:21 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 22:53:21 - INFO - __main__ - ['positive']
03/10/2022 22:53:21 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 22:53:21 - INFO - __main__ - ['positive']
03/10/2022 22:53:21 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 22:53:21 - INFO - __main__ - ['positive']
03/10/2022 22:53:21 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 22:53:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:53:21 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:53:21 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:53:21 - INFO - __main__ - Printing 3 examples
03/10/2022 22:53:21 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 22:53:21 - INFO - __main__ - ['positive']
03/10/2022 22:53:21 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 22:53:21 - INFO - __main__ - ['positive']
03/10/2022 22:53:21 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 22:53:21 - INFO - __main__ - ['positive']
03/10/2022 22:53:21 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:53:21 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:53:21 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 22:53:21 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:53:35 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:53:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:53:36 - INFO - __main__ - Starting training!
03/10/2022 22:54:03 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_21_0.5_8_predictions.txt
03/10/2022 22:54:03 - INFO - __main__ - Classification-F1 on test data: 0.8409
03/10/2022 22:54:04 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.5, bsz=8, dev_performance=0.875, test_performance=0.8408840044392363
03/10/2022 22:54:04 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.4, bsz=8 ...
03/10/2022 22:54:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:54:05 - INFO - __main__ - Printing 3 examples
03/10/2022 22:54:05 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 22:54:05 - INFO - __main__ - ['positive']
03/10/2022 22:54:05 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 22:54:05 - INFO - __main__ - ['positive']
03/10/2022 22:54:05 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 22:54:05 - INFO - __main__ - ['positive']
03/10/2022 22:54:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 22:54:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:54:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 22:54:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 22:54:05 - INFO - __main__ - Printing 3 examples
03/10/2022 22:54:05 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 22:54:05 - INFO - __main__ - ['positive']
03/10/2022 22:54:05 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 22:54:05 - INFO - __main__ - ['positive']
03/10/2022 22:54:05 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 22:54:05 - INFO - __main__ - ['positive']
03/10/2022 22:54:05 - INFO - __main__ - Tokenizing Input ...
03/10/2022 22:54:05 - INFO - __main__ - Tokenizing Output ...
03/10/2022 22:54:05 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 22:54:18 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 22:54:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 22:54:19 - INFO - __main__ - Starting training!
03/10/2022 22:54:25 - INFO - __main__ - Step 10 Global step 10 Train loss 1.12 on epoch=4
03/10/2022 22:54:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=9
03/10/2022 22:54:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.42 on epoch=14
03/10/2022 22:54:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.42 on epoch=19
03/10/2022 22:54:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=24
03/10/2022 22:54:37 - INFO - __main__ - Global step 50 Train loss 0.58 Classification-F1 0.3992490613266583 on epoch=24
03/10/2022 22:54:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3992490613266583 on epoch=24, global_step=50
03/10/2022 22:54:40 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=29
03/10/2022 22:54:43 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
03/10/2022 22:54:46 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
03/10/2022 22:54:49 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=44
03/10/2022 22:54:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.40 on epoch=49
03/10/2022 22:54:52 - INFO - __main__ - Global step 100 Train loss 0.41 Classification-F1 0.6267232237539766 on epoch=49
03/10/2022 22:54:52 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.6267232237539766 on epoch=49, global_step=100
03/10/2022 22:54:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.36 on epoch=54
03/10/2022 22:54:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=59
03/10/2022 22:55:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=64
03/10/2022 22:55:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=69
03/10/2022 22:55:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
03/10/2022 22:55:07 - INFO - __main__ - Global step 150 Train loss 0.36 Classification-F1 0.5333333333333333 on epoch=74
03/10/2022 22:55:10 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=79
03/10/2022 22:55:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=84
03/10/2022 22:55:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
03/10/2022 22:55:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
03/10/2022 22:55:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=99
03/10/2022 22:55:23 - INFO - __main__ - Global step 200 Train loss 0.35 Classification-F1 0.7408906882591093 on epoch=99
03/10/2022 22:55:23 - INFO - __main__ - Saving model with best Classification-F1: 0.6267232237539766 -> 0.7408906882591093 on epoch=99, global_step=200
03/10/2022 22:55:25 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
03/10/2022 22:55:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=109
03/10/2022 22:55:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.33 on epoch=114
03/10/2022 22:55:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
03/10/2022 22:55:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
03/10/2022 22:55:38 - INFO - __main__ - Global step 250 Train loss 0.32 Classification-F1 0.7490196078431373 on epoch=124
03/10/2022 22:55:38 - INFO - __main__ - Saving model with best Classification-F1: 0.7408906882591093 -> 0.7490196078431373 on epoch=124, global_step=250
03/10/2022 22:55:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.34 on epoch=129
03/10/2022 22:55:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.31 on epoch=134
03/10/2022 22:55:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.30 on epoch=139
03/10/2022 22:55:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
03/10/2022 22:55:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
03/10/2022 22:55:53 - INFO - __main__ - Global step 300 Train loss 0.31 Classification-F1 0.8423645320197044 on epoch=149
03/10/2022 22:55:53 - INFO - __main__ - Saving model with best Classification-F1: 0.7490196078431373 -> 0.8423645320197044 on epoch=149, global_step=300
03/10/2022 22:55:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
03/10/2022 22:55:59 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
03/10/2022 22:56:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.21 on epoch=164
03/10/2022 22:56:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=169
03/10/2022 22:56:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.28 on epoch=174
03/10/2022 22:56:09 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.9372549019607843 on epoch=174
03/10/2022 22:56:09 - INFO - __main__ - Saving model with best Classification-F1: 0.8423645320197044 -> 0.9372549019607843 on epoch=174, global_step=350
03/10/2022 22:56:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.17 on epoch=179
03/10/2022 22:56:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.14 on epoch=184
03/10/2022 22:56:18 - INFO - __main__ - Step 380 Global step 380 Train loss 0.16 on epoch=189
03/10/2022 22:56:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.13 on epoch=194
03/10/2022 22:56:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.10 on epoch=199
03/10/2022 22:56:25 - INFO - __main__ - Global step 400 Train loss 0.14 Classification-F1 0.9372549019607843 on epoch=199
03/10/2022 22:56:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.11 on epoch=204
03/10/2022 22:56:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.07 on epoch=209
03/10/2022 22:56:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.07 on epoch=214
03/10/2022 22:56:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
03/10/2022 22:56:40 - INFO - __main__ - Step 450 Global step 450 Train loss 0.07 on epoch=224
03/10/2022 22:56:42 - INFO - __main__ - Global step 450 Train loss 0.08 Classification-F1 0.9687194525904204 on epoch=224
03/10/2022 22:56:42 - INFO - __main__ - Saving model with best Classification-F1: 0.9372549019607843 -> 0.9687194525904204 on epoch=224, global_step=450
03/10/2022 22:56:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.06 on epoch=229
03/10/2022 22:56:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.07 on epoch=234
03/10/2022 22:56:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.07 on epoch=239
03/10/2022 22:56:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
03/10/2022 22:56:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
03/10/2022 22:56:58 - INFO - __main__ - Global step 500 Train loss 0.07 Classification-F1 0.9687194525904204 on epoch=249
03/10/2022 22:57:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.04 on epoch=254
03/10/2022 22:57:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.02 on epoch=259
03/10/2022 22:57:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.03 on epoch=264
03/10/2022 22:57:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.03 on epoch=269
03/10/2022 22:57:12 - INFO - __main__ - Step 550 Global step 550 Train loss 0.10 on epoch=274
03/10/2022 22:57:14 - INFO - __main__ - Global step 550 Train loss 0.04 Classification-F1 0.9372549019607843 on epoch=274
03/10/2022 22:57:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
03/10/2022 22:57:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.03 on epoch=284
03/10/2022 22:57:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.04 on epoch=289
03/10/2022 22:57:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.03 on epoch=294
03/10/2022 22:57:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.02 on epoch=299
03/10/2022 22:57:31 - INFO - __main__ - Global step 600 Train loss 0.03 Classification-F1 0.9372549019607843 on epoch=299
03/10/2022 22:57:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.02 on epoch=304
03/10/2022 22:57:37 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/10/2022 22:57:40 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/10/2022 22:57:43 - INFO - __main__ - Step 640 Global step 640 Train loss 0.03 on epoch=319
03/10/2022 22:57:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.02 on epoch=324
03/10/2022 22:57:47 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=324
03/10/2022 22:57:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.02 on epoch=329
03/10/2022 22:57:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.02 on epoch=334
03/10/2022 22:57:56 - INFO - __main__ - Step 680 Global step 680 Train loss 0.04 on epoch=339
03/10/2022 22:57:59 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/10/2022 22:58:02 - INFO - __main__ - Step 700 Global step 700 Train loss 0.03 on epoch=349
03/10/2022 22:58:03 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=349
03/10/2022 22:58:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/10/2022 22:58:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/10/2022 22:58:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/10/2022 22:58:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/10/2022 22:58:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.04 on epoch=374
03/10/2022 22:58:20 - INFO - __main__ - Global step 750 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=374
03/10/2022 22:58:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.00 on epoch=379
03/10/2022 22:58:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/10/2022 22:58:29 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
03/10/2022 22:58:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/10/2022 22:58:35 - INFO - __main__ - Step 800 Global step 800 Train loss 0.02 on epoch=399
03/10/2022 22:58:37 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=399
03/10/2022 22:58:40 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
03/10/2022 22:58:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=409
03/10/2022 22:58:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
03/10/2022 22:58:49 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/10/2022 22:58:51 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/10/2022 22:58:55 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.9372549019607843 on epoch=424
03/10/2022 22:58:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/10/2022 22:59:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/10/2022 22:59:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/10/2022 22:59:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/10/2022 22:59:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/10/2022 22:59:12 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=449
03/10/2022 22:59:15 - INFO - __main__ - Step 910 Global step 910 Train loss 0.02 on epoch=454
03/10/2022 22:59:18 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/10/2022 22:59:20 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/10/2022 22:59:23 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/10/2022 22:59:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/10/2022 22:59:29 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=474
03/10/2022 22:59:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/10/2022 22:59:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/10/2022 22:59:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
03/10/2022 22:59:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/10/2022 22:59:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/10/2022 22:59:45 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=499
03/10/2022 22:59:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/10/2022 22:59:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/10/2022 22:59:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/10/2022 22:59:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/10/2022 22:59:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/10/2022 23:00:01 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=524
03/10/2022 23:00:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/10/2022 23:00:07 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/10/2022 23:00:10 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/10/2022 23:00:13 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/10/2022 23:00:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/10/2022 23:00:18 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=549
03/10/2022 23:00:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/10/2022 23:00:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/10/2022 23:00:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/10/2022 23:00:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/10/2022 23:00:32 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/10/2022 23:00:34 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=574
03/10/2022 23:00:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/10/2022 23:00:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/10/2022 23:00:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.02 on epoch=589
03/10/2022 23:00:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.03 on epoch=594
03/10/2022 23:00:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/10/2022 23:00:50 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=599
03/10/2022 23:00:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.02 on epoch=604
03/10/2022 23:00:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/10/2022 23:00:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/10/2022 23:01:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/10/2022 23:01:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/10/2022 23:01:07 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=624
03/10/2022 23:01:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/10/2022 23:01:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
03/10/2022 23:01:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/10/2022 23:01:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/10/2022 23:01:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/10/2022 23:01:26 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=649
03/10/2022 23:01:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/10/2022 23:01:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/10/2022 23:01:35 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/10/2022 23:01:37 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/10/2022 23:01:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/10/2022 23:01:42 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/10/2022 23:01:45 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/10/2022 23:01:48 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/10/2022 23:01:51 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/10/2022 23:01:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/10/2022 23:01:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/10/2022 23:01:59 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/10/2022 23:02:02 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/10/2022 23:02:04 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/10/2022 23:02:07 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/10/2022 23:02:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/10/2022 23:02:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/10/2022 23:02:18 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/10/2022 23:02:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/10/2022 23:02:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/10/2022 23:02:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
03/10/2022 23:02:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
03/10/2022 23:02:32 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/10/2022 23:02:35 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=749
03/10/2022 23:02:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/10/2022 23:02:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/10/2022 23:02:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/10/2022 23:02:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/10/2022 23:02:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/10/2022 23:02:52 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=774
03/10/2022 23:02:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/10/2022 23:02:58 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/10/2022 23:03:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/10/2022 23:03:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
03/10/2022 23:03:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/10/2022 23:03:08 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=799
03/10/2022 23:03:11 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/10/2022 23:03:14 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/10/2022 23:03:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/10/2022 23:03:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/10/2022 23:03:23 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/10/2022 23:03:25 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/10/2022 23:03:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/10/2022 23:03:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
03/10/2022 23:03:33 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.04 on epoch=839
03/10/2022 23:03:36 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/10/2022 23:03:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/10/2022 23:03:42 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=849
03/10/2022 23:03:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/10/2022 23:03:48 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/10/2022 23:03:51 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/10/2022 23:03:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/10/2022 23:03:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/10/2022 23:03:59 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/10/2022 23:04:02 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/10/2022 23:04:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/10/2022 23:04:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/10/2022 23:04:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/10/2022 23:04:14 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/10/2022 23:04:17 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/10/2022 23:04:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/10/2022 23:04:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/10/2022 23:04:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/10/2022 23:04:29 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/10/2022 23:04:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/10/2022 23:04:37 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/10/2022 23:04:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/10/2022 23:04:42 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/10/2022 23:04:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/10/2022 23:04:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/10/2022 23:04:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/10/2022 23:04:55 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/10/2022 23:04:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/10/2022 23:05:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/10/2022 23:05:03 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/10/2022 23:05:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/10/2022 23:05:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/10/2022 23:05:14 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/10/2022 23:05:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/10/2022 23:05:19 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/10/2022 23:05:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/10/2022 23:05:25 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/10/2022 23:05:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/10/2022 23:05:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:05:29 - INFO - __main__ - Printing 3 examples
03/10/2022 23:05:29 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 23:05:29 - INFO - __main__ - ['positive']
03/10/2022 23:05:29 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 23:05:29 - INFO - __main__ - ['positive']
03/10/2022 23:05:29 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 23:05:29 - INFO - __main__ - ['positive']
03/10/2022 23:05:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:05:29 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:05:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:05:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:05:29 - INFO - __main__ - Printing 3 examples
03/10/2022 23:05:29 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 23:05:29 - INFO - __main__ - ['positive']
03/10/2022 23:05:29 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 23:05:29 - INFO - __main__ - ['positive']
03/10/2022 23:05:29 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 23:05:29 - INFO - __main__ - ['positive']
03/10/2022 23:05:29 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:05:29 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:05:29 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:05:31 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=999
03/10/2022 23:05:31 - INFO - __main__ - save last model!
03/10/2022 23:05:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 23:05:31 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 23:05:31 - INFO - __main__ - Printing 3 examples
03/10/2022 23:05:31 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 23:05:31 - INFO - __main__ - ['negative']
03/10/2022 23:05:31 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 23:05:31 - INFO - __main__ - ['negative']
03/10/2022 23:05:31 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 23:05:31 - INFO - __main__ - ['negative']
03/10/2022 23:05:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:05:32 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:05:33 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 23:05:43 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:05:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:05:44 - INFO - __main__ - Starting training!
03/10/2022 23:07:27 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_21_0.4_8_predictions.txt
03/10/2022 23:07:27 - INFO - __main__ - Classification-F1 on test data: 0.9349
03/10/2022 23:07:27 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.4, bsz=8, dev_performance=0.9687194525904204, test_performance=0.934868107918535
03/10/2022 23:07:27 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.3, bsz=8 ...
03/10/2022 23:07:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:07:28 - INFO - __main__ - Printing 3 examples
03/10/2022 23:07:28 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 23:07:28 - INFO - __main__ - ['positive']
03/10/2022 23:07:28 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 23:07:28 - INFO - __main__ - ['positive']
03/10/2022 23:07:28 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 23:07:28 - INFO - __main__ - ['positive']
03/10/2022 23:07:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:07:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:07:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:07:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:07:28 - INFO - __main__ - Printing 3 examples
03/10/2022 23:07:28 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 23:07:28 - INFO - __main__ - ['positive']
03/10/2022 23:07:28 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 23:07:28 - INFO - __main__ - ['positive']
03/10/2022 23:07:28 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 23:07:28 - INFO - __main__ - ['positive']
03/10/2022 23:07:28 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:07:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:07:28 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:07:42 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:07:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:07:42 - INFO - __main__ - Starting training!
03/10/2022 23:07:46 - INFO - __main__ - Step 10 Global step 10 Train loss 1.27 on epoch=4
03/10/2022 23:07:49 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=9
03/10/2022 23:07:52 - INFO - __main__ - Step 30 Global step 30 Train loss 0.42 on epoch=14
03/10/2022 23:07:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=19
03/10/2022 23:07:57 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=24
03/10/2022 23:07:58 - INFO - __main__ - Global step 50 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 23:07:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 23:08:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.38 on epoch=29
03/10/2022 23:08:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=34
03/10/2022 23:08:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
03/10/2022 23:08:10 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
03/10/2022 23:08:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=49
03/10/2022 23:08:13 - INFO - __main__ - Global step 100 Train loss 0.40 Classification-F1 0.5636363636363637 on epoch=49
03/10/2022 23:08:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5636363636363637 on epoch=49, global_step=100
03/10/2022 23:08:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
03/10/2022 23:08:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=59
03/10/2022 23:08:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
03/10/2022 23:08:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=69
03/10/2022 23:08:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.40 on epoch=74
03/10/2022 23:08:28 - INFO - __main__ - Global step 150 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 23:08:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=79
03/10/2022 23:08:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.34 on epoch=84
03/10/2022 23:08:37 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
03/10/2022 23:08:40 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
03/10/2022 23:08:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=99
03/10/2022 23:08:43 - INFO - __main__ - Global step 200 Train loss 0.37 Classification-F1 0.6761133603238867 on epoch=99
03/10/2022 23:08:43 - INFO - __main__ - Saving model with best Classification-F1: 0.5636363636363637 -> 0.6761133603238867 on epoch=99, global_step=200
03/10/2022 23:08:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
03/10/2022 23:08:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
03/10/2022 23:08:52 - INFO - __main__ - Step 230 Global step 230 Train loss 0.30 on epoch=114
03/10/2022 23:08:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
03/10/2022 23:08:58 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
03/10/2022 23:08:58 - INFO - __main__ - Global step 250 Train loss 0.31 Classification-F1 0.6190476190476191 on epoch=124
03/10/2022 23:09:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=129
03/10/2022 23:09:04 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=134
03/10/2022 23:09:07 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
03/10/2022 23:09:10 - INFO - __main__ - Step 290 Global step 290 Train loss 0.33 on epoch=144
03/10/2022 23:09:13 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=149
03/10/2022 23:09:13 - INFO - __main__ - Global step 300 Train loss 0.30 Classification-F1 0.746031746031746 on epoch=149
03/10/2022 23:09:13 - INFO - __main__ - Saving model with best Classification-F1: 0.6761133603238867 -> 0.746031746031746 on epoch=149, global_step=300
03/10/2022 23:09:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
03/10/2022 23:09:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.30 on epoch=159
03/10/2022 23:09:22 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
03/10/2022 23:09:25 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/10/2022 23:09:28 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
03/10/2022 23:09:28 - INFO - __main__ - Global step 350 Train loss 0.27 Classification-F1 0.8745098039215686 on epoch=174
03/10/2022 23:09:29 - INFO - __main__ - Saving model with best Classification-F1: 0.746031746031746 -> 0.8745098039215686 on epoch=174, global_step=350
03/10/2022 23:09:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/10/2022 23:09:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.18 on epoch=184
03/10/2022 23:09:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=189
03/10/2022 23:09:40 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=194
03/10/2022 23:09:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.15 on epoch=199
03/10/2022 23:09:44 - INFO - __main__ - Global step 400 Train loss 0.21 Classification-F1 0.9372549019607843 on epoch=199
03/10/2022 23:09:44 - INFO - __main__ - Saving model with best Classification-F1: 0.8745098039215686 -> 0.9372549019607843 on epoch=199, global_step=400
03/10/2022 23:09:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=204
03/10/2022 23:09:50 - INFO - __main__ - Step 420 Global step 420 Train loss 0.08 on epoch=209
03/10/2022 23:09:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.08 on epoch=214
03/10/2022 23:09:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.07 on epoch=219
03/10/2022 23:09:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.06 on epoch=224
03/10/2022 23:10:00 - INFO - __main__ - Global step 450 Train loss 0.10 Classification-F1 0.9687194525904204 on epoch=224
03/10/2022 23:10:00 - INFO - __main__ - Saving model with best Classification-F1: 0.9372549019607843 -> 0.9687194525904204 on epoch=224, global_step=450
03/10/2022 23:10:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.03 on epoch=229
03/10/2022 23:10:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.05 on epoch=234
03/10/2022 23:10:09 - INFO - __main__ - Step 480 Global step 480 Train loss 0.03 on epoch=239
03/10/2022 23:10:12 - INFO - __main__ - Step 490 Global step 490 Train loss 0.04 on epoch=244
03/10/2022 23:10:15 - INFO - __main__ - Step 500 Global step 500 Train loss 0.03 on epoch=249
03/10/2022 23:10:17 - INFO - __main__ - Global step 500 Train loss 0.04 Classification-F1 0.9687194525904204 on epoch=249
03/10/2022 23:10:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.02 on epoch=254
03/10/2022 23:10:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.04 on epoch=259
03/10/2022 23:10:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.02 on epoch=264
03/10/2022 23:10:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.02 on epoch=269
03/10/2022 23:10:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.06 on epoch=274
03/10/2022 23:10:33 - INFO - __main__ - Global step 550 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=274
03/10/2022 23:10:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.04 on epoch=279
03/10/2022 23:10:39 - INFO - __main__ - Step 570 Global step 570 Train loss 0.01 on epoch=284
03/10/2022 23:10:42 - INFO - __main__ - Step 580 Global step 580 Train loss 0.01 on epoch=289
03/10/2022 23:10:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/10/2022 23:10:48 - INFO - __main__ - Step 600 Global step 600 Train loss 0.03 on epoch=299
03/10/2022 23:10:50 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=299
03/10/2022 23:10:53 - INFO - __main__ - Step 610 Global step 610 Train loss 0.03 on epoch=304
03/10/2022 23:10:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.01 on epoch=309
03/10/2022 23:10:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.04 on epoch=314
03/10/2022 23:11:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/10/2022 23:11:04 - INFO - __main__ - Step 650 Global step 650 Train loss 0.01 on epoch=324
03/10/2022 23:11:06 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=324
03/10/2022 23:11:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.00 on epoch=329
03/10/2022 23:11:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/10/2022 23:11:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/10/2022 23:11:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.01 on epoch=344
03/10/2022 23:11:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.02 on epoch=349
03/10/2022 23:11:23 - INFO - __main__ - Global step 700 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=349
03/10/2022 23:11:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.01 on epoch=354
03/10/2022 23:11:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/10/2022 23:11:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/10/2022 23:11:34 - INFO - __main__ - Step 740 Global step 740 Train loss 0.00 on epoch=369
03/10/2022 23:11:37 - INFO - __main__ - Step 750 Global step 750 Train loss 0.00 on epoch=374
03/10/2022 23:11:40 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=374
03/10/2022 23:11:42 - INFO - __main__ - Step 760 Global step 760 Train loss 0.01 on epoch=379
03/10/2022 23:11:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.00 on epoch=384
03/10/2022 23:11:48 - INFO - __main__ - Step 780 Global step 780 Train loss 0.01 on epoch=389
03/10/2022 23:11:51 - INFO - __main__ - Step 790 Global step 790 Train loss 0.01 on epoch=394
03/10/2022 23:11:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/10/2022 23:11:57 - INFO - __main__ - Global step 800 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=399
03/10/2022 23:11:59 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/10/2022 23:12:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.01 on epoch=409
03/10/2022 23:12:05 - INFO - __main__ - Step 830 Global step 830 Train loss 0.00 on epoch=414
03/10/2022 23:12:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.02 on epoch=419
03/10/2022 23:12:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.04 on epoch=424
03/10/2022 23:12:13 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.9372549019607843 on epoch=424
03/10/2022 23:12:16 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/10/2022 23:12:19 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/10/2022 23:12:22 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/10/2022 23:12:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/10/2022 23:12:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/10/2022 23:12:30 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/10/2022 23:12:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/10/2022 23:12:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/10/2022 23:12:38 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/10/2022 23:12:41 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/10/2022 23:12:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/10/2022 23:12:46 - INFO - __main__ - Global step 950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=474
03/10/2022 23:12:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/10/2022 23:12:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/10/2022 23:12:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/10/2022 23:12:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/10/2022 23:13:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/10/2022 23:13:03 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=499
03/10/2022 23:13:05 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/10/2022 23:13:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/10/2022 23:13:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/10/2022 23:13:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/10/2022 23:13:17 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/10/2022 23:13:19 - INFO - __main__ - Global step 1050 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=524
03/10/2022 23:13:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/10/2022 23:13:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/10/2022 23:13:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/10/2022 23:13:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/10/2022 23:13:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/10/2022 23:13:36 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=549
03/10/2022 23:13:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.06 on epoch=554
03/10/2022 23:13:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/10/2022 23:13:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/10/2022 23:13:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/10/2022 23:13:50 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/10/2022 23:13:52 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=574
03/10/2022 23:13:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/10/2022 23:13:57 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/10/2022 23:14:00 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/10/2022 23:14:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/10/2022 23:14:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/10/2022 23:14:08 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/10/2022 23:14:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/10/2022 23:14:13 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/10/2022 23:14:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/10/2022 23:14:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/10/2022 23:14:22 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/10/2022 23:14:24 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/10/2022 23:14:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/10/2022 23:14:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/10/2022 23:14:32 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/10/2022 23:14:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/10/2022 23:14:38 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/10/2022 23:14:40 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=649
03/10/2022 23:14:42 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/10/2022 23:14:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/10/2022 23:14:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/10/2022 23:14:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/10/2022 23:14:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/10/2022 23:14:56 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/10/2022 23:14:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/10/2022 23:15:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/10/2022 23:15:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/10/2022 23:15:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/10/2022 23:15:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/10/2022 23:15:12 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/10/2022 23:15:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/10/2022 23:15:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/10/2022 23:15:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/10/2022 23:15:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/10/2022 23:15:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/10/2022 23:15:28 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/10/2022 23:15:31 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/10/2022 23:15:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/10/2022 23:15:36 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/10/2022 23:15:39 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/10/2022 23:15:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/10/2022 23:15:44 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/10/2022 23:15:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/10/2022 23:15:50 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/10/2022 23:15:53 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/10/2022 23:15:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/10/2022 23:15:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/10/2022 23:16:01 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/10/2022 23:16:04 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/10/2022 23:16:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/10/2022 23:16:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/10/2022 23:16:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/10/2022 23:16:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/10/2022 23:16:18 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/10/2022 23:16:21 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/10/2022 23:16:23 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/10/2022 23:16:26 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=814
03/10/2022 23:16:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/10/2022 23:16:32 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/10/2022 23:16:34 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=824
03/10/2022 23:16:37 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/10/2022 23:16:39 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/10/2022 23:16:42 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/10/2022 23:16:45 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/10/2022 23:16:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/10/2022 23:16:50 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=849
03/10/2022 23:16:53 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/10/2022 23:16:56 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/10/2022 23:16:58 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/10/2022 23:17:01 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/10/2022 23:17:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/10/2022 23:17:06 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/10/2022 23:17:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/10/2022 23:17:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/10/2022 23:17:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/10/2022 23:17:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/10/2022 23:17:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/10/2022 23:17:22 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/10/2022 23:17:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/10/2022 23:17:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/10/2022 23:17:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/10/2022 23:17:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/10/2022 23:17:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/10/2022 23:17:38 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/10/2022 23:17:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/10/2022 23:17:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/10/2022 23:17:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/10/2022 23:17:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/10/2022 23:17:52 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/10/2022 23:17:54 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9372549019607843 on epoch=949
03/10/2022 23:17:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/10/2022 23:18:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/10/2022 23:18:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/10/2022 23:18:05 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/10/2022 23:18:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/10/2022 23:18:10 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/10/2022 23:18:13 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/10/2022 23:18:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/10/2022 23:18:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/10/2022 23:18:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/10/2022 23:18:24 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/10/2022 23:18:25 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/10/2022 23:18:25 - INFO - __main__ - save last model!
03/10/2022 23:18:25 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 23:18:25 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 23:18:25 - INFO - __main__ - Printing 3 examples
03/10/2022 23:18:25 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 23:18:25 - INFO - __main__ - ['negative']
03/10/2022 23:18:25 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 23:18:25 - INFO - __main__ - ['negative']
03/10/2022 23:18:25 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 23:18:25 - INFO - __main__ - ['negative']
03/10/2022 23:18:25 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:18:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:18:25 - INFO - __main__ - Printing 3 examples
03/10/2022 23:18:25 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 23:18:25 - INFO - __main__ - ['positive']
03/10/2022 23:18:25 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 23:18:25 - INFO - __main__ - ['positive']
03/10/2022 23:18:25 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 23:18:25 - INFO - __main__ - ['positive']
03/10/2022 23:18:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:18:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:18:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:18:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:18:25 - INFO - __main__ - Printing 3 examples
03/10/2022 23:18:25 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 23:18:25 - INFO - __main__ - ['positive']
03/10/2022 23:18:25 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 23:18:25 - INFO - __main__ - ['positive']
03/10/2022 23:18:25 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 23:18:25 - INFO - __main__ - ['positive']
03/10/2022 23:18:25 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:18:25 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:18:26 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:18:26 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:18:27 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 23:18:39 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:18:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:18:40 - INFO - __main__ - Starting training!
03/10/2022 23:19:08 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_21_0.3_8_predictions.txt
03/10/2022 23:19:08 - INFO - __main__ - Classification-F1 on test data: 0.9349
03/10/2022 23:19:09 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.3, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9349202773397411
03/10/2022 23:19:09 - INFO - __main__ - Running ... prefix=amazon_polarity_16_21, lr=0.2, bsz=8 ...
03/10/2022 23:19:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:19:09 - INFO - __main__ - Printing 3 examples
03/10/2022 23:19:09 - INFO - __main__ -  [amazon_polarity] title: *SWEET* [SEP] content: This book is a must have for BSB and Kevin fans!! It is so good! My friends mom even found it very interesting!!!!!
03/10/2022 23:19:09 - INFO - __main__ - ['positive']
03/10/2022 23:19:09 - INFO - __main__ -  [amazon_polarity] title: Great Book about Civil Affairs [SEP] content: This is as good a Civil Affairs book as is available out there. It covers a Reserve CA unit in Iraq in 2004 and provides good insight to what Civil Affairs does, but has some grammatical errors and editing issues. Good book, overall, but I'd like to see an active unit chronicled both in and out of the Global War on Terror.
03/10/2022 23:19:09 - INFO - __main__ - ['positive']
03/10/2022 23:19:09 - INFO - __main__ -  [amazon_polarity] title: Smart and gorgeous [SEP] content: This is really a gorgeous watch.The numbers are clear on the white base, and the stain less steel band is dual shaded, which adds to its beauty.Received in very good condition.I bought it for 34 bucks from Amazon.It could be worn on all kinds of occasions and officially as well, displays date too.I bought it as a gift for some one, the price is also good.
03/10/2022 23:19:09 - INFO - __main__ - ['positive']
03/10/2022 23:19:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:19:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:19:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:19:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:19:09 - INFO - __main__ - Printing 3 examples
03/10/2022 23:19:09 - INFO - __main__ -  [amazon_polarity] title: Works Great [SEP] content: This is a much better alternative to the suction cup mount that came with my GPS. Just place it on your dash and it stays in place perfectly.
03/10/2022 23:19:09 - INFO - __main__ - ['positive']
03/10/2022 23:19:09 - INFO - __main__ -  [amazon_polarity] title: So Funny [SEP] content: Chris Tucker at his best, no question. One of my favorite movies of all time. Recomend to anyone who has a sense of humor
03/10/2022 23:19:09 - INFO - __main__ - ['positive']
03/10/2022 23:19:09 - INFO - __main__ -  [amazon_polarity] title: mind blowing! [SEP] content: I love the way Ellen Hopkins portrays the story through prose form. I caught myself reading "Crank" at lightning fast speeds because of how capturing her writing is. With writing in prose forms it offered me a new way of reading a book and having to learn how to read a book written this way. It was refreshing and daring. "Crank" is a detailed book about life, love, friends, family and drugs. You won't be disappointed.
03/10/2022 23:19:09 - INFO - __main__ - ['positive']
03/10/2022 23:19:09 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:19:09 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:19:10 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:19:23 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:19:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:19:24 - INFO - __main__ - Starting training!
03/10/2022 23:19:27 - INFO - __main__ - Step 10 Global step 10 Train loss 1.71 on epoch=4
03/10/2022 23:19:30 - INFO - __main__ - Step 20 Global step 20 Train loss 0.69 on epoch=9
03/10/2022 23:19:33 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=14
03/10/2022 23:19:36 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=19
03/10/2022 23:19:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=24
03/10/2022 23:19:39 - INFO - __main__ - Global step 50 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 23:19:39 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 23:19:42 - INFO - __main__ - Step 60 Global step 60 Train loss 0.47 on epoch=29
03/10/2022 23:19:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=34
03/10/2022 23:19:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.43 on epoch=39
03/10/2022 23:19:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=44
03/10/2022 23:19:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=49
03/10/2022 23:19:54 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 23:19:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.43 on epoch=54
03/10/2022 23:20:00 - INFO - __main__ - Step 120 Global step 120 Train loss 0.35 on epoch=59
03/10/2022 23:20:03 - INFO - __main__ - Step 130 Global step 130 Train loss 0.44 on epoch=64
03/10/2022 23:20:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.43 on epoch=69
03/10/2022 23:20:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=74
03/10/2022 23:20:09 - INFO - __main__ - Global step 150 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=74
03/10/2022 23:20:12 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
03/10/2022 23:20:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
03/10/2022 23:20:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
03/10/2022 23:20:20 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=94
03/10/2022 23:20:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
03/10/2022 23:20:24 - INFO - __main__ - Global step 200 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 23:20:27 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=104
03/10/2022 23:20:30 - INFO - __main__ - Step 220 Global step 220 Train loss 0.36 on epoch=109
03/10/2022 23:20:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.33 on epoch=114
03/10/2022 23:20:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
03/10/2022 23:20:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=124
03/10/2022 23:20:39 - INFO - __main__ - Global step 250 Train loss 0.36 Classification-F1 0.6536796536796536 on epoch=124
03/10/2022 23:20:39 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.6536796536796536 on epoch=124, global_step=250
03/10/2022 23:20:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
03/10/2022 23:20:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.36 on epoch=134
03/10/2022 23:20:47 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=139
03/10/2022 23:20:50 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
03/10/2022 23:20:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=149
03/10/2022 23:20:54 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.5933528836754642 on epoch=149
03/10/2022 23:20:57 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=154
03/10/2022 23:21:00 - INFO - __main__ - Step 320 Global step 320 Train loss 0.34 on epoch=159
03/10/2022 23:21:02 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
03/10/2022 23:21:05 - INFO - __main__ - Step 340 Global step 340 Train loss 0.34 on epoch=169
03/10/2022 23:21:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=174
03/10/2022 23:21:09 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.6532019704433498 on epoch=174
03/10/2022 23:21:12 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=179
03/10/2022 23:21:14 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=184
03/10/2022 23:21:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=189
03/10/2022 23:21:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
03/10/2022 23:21:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.41 on epoch=199
03/10/2022 23:21:24 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.6862745098039216 on epoch=199
03/10/2022 23:21:24 - INFO - __main__ - Saving model with best Classification-F1: 0.6536796536796536 -> 0.6862745098039216 on epoch=199, global_step=400
03/10/2022 23:21:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.50 on epoch=204
03/10/2022 23:21:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.35 on epoch=209
03/10/2022 23:21:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=214
03/10/2022 23:21:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=219
03/10/2022 23:21:38 - INFO - __main__ - Step 450 Global step 450 Train loss 0.34 on epoch=224
03/10/2022 23:21:39 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.625 on epoch=224
03/10/2022 23:21:41 - INFO - __main__ - Step 460 Global step 460 Train loss 0.32 on epoch=229
03/10/2022 23:21:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=234
03/10/2022 23:21:47 - INFO - __main__ - Step 480 Global step 480 Train loss 0.35 on epoch=239
03/10/2022 23:21:50 - INFO - __main__ - Step 490 Global step 490 Train loss 0.33 on epoch=244
03/10/2022 23:21:53 - INFO - __main__ - Step 500 Global step 500 Train loss 0.30 on epoch=249
03/10/2022 23:21:53 - INFO - __main__ - Global step 500 Train loss 0.33 Classification-F1 0.6101882613510521 on epoch=249
03/10/2022 23:21:56 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=254
03/10/2022 23:21:59 - INFO - __main__ - Step 520 Global step 520 Train loss 0.33 on epoch=259
03/10/2022 23:22:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=264
03/10/2022 23:22:05 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=269
03/10/2022 23:22:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=274
03/10/2022 23:22:08 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.7408906882591093 on epoch=274
03/10/2022 23:22:08 - INFO - __main__ - Saving model with best Classification-F1: 0.6862745098039216 -> 0.7408906882591093 on epoch=274, global_step=550
03/10/2022 23:22:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.34 on epoch=279
03/10/2022 23:22:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=284
03/10/2022 23:22:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.35 on epoch=289
03/10/2022 23:22:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.42 on epoch=294
03/10/2022 23:22:23 - INFO - __main__ - Step 600 Global step 600 Train loss 0.30 on epoch=299
03/10/2022 23:22:24 - INFO - __main__ - Global step 600 Train loss 0.35 Classification-F1 0.6666666666666667 on epoch=299
03/10/2022 23:22:26 - INFO - __main__ - Step 610 Global step 610 Train loss 0.33 on epoch=304
03/10/2022 23:22:29 - INFO - __main__ - Step 620 Global step 620 Train loss 0.29 on epoch=309
03/10/2022 23:22:32 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=314
03/10/2022 23:22:35 - INFO - __main__ - Step 640 Global step 640 Train loss 0.33 on epoch=319
03/10/2022 23:22:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=324
03/10/2022 23:22:39 - INFO - __main__ - Global step 650 Train loss 0.33 Classification-F1 0.716256157635468 on epoch=324
03/10/2022 23:22:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=329
03/10/2022 23:22:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.29 on epoch=334
03/10/2022 23:22:47 - INFO - __main__ - Step 680 Global step 680 Train loss 0.33 on epoch=339
03/10/2022 23:22:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=344
03/10/2022 23:22:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.33 on epoch=349
03/10/2022 23:22:54 - INFO - __main__ - Global step 700 Train loss 0.32 Classification-F1 0.716256157635468 on epoch=349
03/10/2022 23:22:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.31 on epoch=354
03/10/2022 23:23:00 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=359
03/10/2022 23:23:03 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=364
03/10/2022 23:23:05 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=369
03/10/2022 23:23:08 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=374
03/10/2022 23:23:09 - INFO - __main__ - Global step 750 Train loss 0.31 Classification-F1 0.7490196078431373 on epoch=374
03/10/2022 23:23:09 - INFO - __main__ - Saving model with best Classification-F1: 0.7408906882591093 -> 0.7490196078431373 on epoch=374, global_step=750
03/10/2022 23:23:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.34 on epoch=379
03/10/2022 23:23:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=384
03/10/2022 23:23:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
03/10/2022 23:23:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.32 on epoch=394
03/10/2022 23:23:24 - INFO - __main__ - Step 800 Global step 800 Train loss 0.30 on epoch=399
03/10/2022 23:23:24 - INFO - __main__ - Global step 800 Train loss 0.32 Classification-F1 0.7793103448275862 on epoch=399
03/10/2022 23:23:24 - INFO - __main__ - Saving model with best Classification-F1: 0.7490196078431373 -> 0.7793103448275862 on epoch=399, global_step=800
03/10/2022 23:23:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
03/10/2022 23:23:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.32 on epoch=409
03/10/2022 23:23:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.29 on epoch=414
03/10/2022 23:23:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.27 on epoch=419
03/10/2022 23:23:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=424
03/10/2022 23:23:39 - INFO - __main__ - Global step 850 Train loss 0.28 Classification-F1 0.7793103448275862 on epoch=424
03/10/2022 23:23:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=429
03/10/2022 23:23:45 - INFO - __main__ - Step 870 Global step 870 Train loss 0.32 on epoch=434
03/10/2022 23:23:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=439
03/10/2022 23:23:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=444
03/10/2022 23:23:54 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=449
03/10/2022 23:23:55 - INFO - __main__ - Global step 900 Train loss 0.26 Classification-F1 0.9372549019607843 on epoch=449
03/10/2022 23:23:55 - INFO - __main__ - Saving model with best Classification-F1: 0.7793103448275862 -> 0.9372549019607843 on epoch=449, global_step=900
03/10/2022 23:23:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=454
03/10/2022 23:24:01 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
03/10/2022 23:24:03 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/10/2022 23:24:06 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=469
03/10/2022 23:24:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
03/10/2022 23:24:10 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.9687194525904204 on epoch=474
03/10/2022 23:24:10 - INFO - __main__ - Saving model with best Classification-F1: 0.9372549019607843 -> 0.9687194525904204 on epoch=474, global_step=950
03/10/2022 23:24:13 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=479
03/10/2022 23:24:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=484
03/10/2022 23:24:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.15 on epoch=489
03/10/2022 23:24:22 - INFO - __main__ - Step 990 Global step 990 Train loss 0.14 on epoch=494
03/10/2022 23:24:24 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
03/10/2022 23:24:25 - INFO - __main__ - Global step 1000 Train loss 0.16 Classification-F1 0.9687194525904204 on epoch=499
03/10/2022 23:24:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=504
03/10/2022 23:24:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=509
03/10/2022 23:24:34 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.11 on epoch=514
03/10/2022 23:24:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=519
03/10/2022 23:24:40 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=524
03/10/2022 23:24:40 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.9687194525904204 on epoch=524
03/10/2022 23:24:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.07 on epoch=529
03/10/2022 23:24:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=534
03/10/2022 23:24:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=539
03/10/2022 23:24:52 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=544
03/10/2022 23:24:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=549
03/10/2022 23:24:55 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.9687194525904204 on epoch=549
03/10/2022 23:24:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.09 on epoch=554
03/10/2022 23:25:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=559
03/10/2022 23:25:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
03/10/2022 23:25:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=569
03/10/2022 23:25:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
03/10/2022 23:25:11 - INFO - __main__ - Global step 1150 Train loss 0.08 Classification-F1 0.9687194525904204 on epoch=574
03/10/2022 23:25:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
03/10/2022 23:25:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
03/10/2022 23:25:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
03/10/2022 23:25:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
03/10/2022 23:25:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.05 on epoch=599
03/10/2022 23:25:26 - INFO - __main__ - Global step 1200 Train loss 0.05 Classification-F1 0.9687194525904204 on epoch=599
03/10/2022 23:25:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
03/10/2022 23:25:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=609
03/10/2022 23:25:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
03/10/2022 23:25:37 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
03/10/2022 23:25:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
03/10/2022 23:25:41 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.9687194525904204 on epoch=624
03/10/2022 23:25:44 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
03/10/2022 23:25:47 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.07 on epoch=634
03/10/2022 23:25:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
03/10/2022 23:25:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
03/10/2022 23:25:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
03/10/2022 23:25:58 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.9687194525904204 on epoch=649
03/10/2022 23:26:01 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
03/10/2022 23:26:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
03/10/2022 23:26:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
03/10/2022 23:26:09 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.02 on epoch=669
03/10/2022 23:26:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.03 on epoch=674
03/10/2022 23:26:15 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=674
03/10/2022 23:26:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=679
03/10/2022 23:26:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.03 on epoch=684
03/10/2022 23:26:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
03/10/2022 23:26:26 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
03/10/2022 23:26:29 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
03/10/2022 23:26:31 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=699
03/10/2022 23:26:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/10/2022 23:26:37 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
03/10/2022 23:26:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
03/10/2022 23:26:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/10/2022 23:26:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
03/10/2022 23:26:47 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=724
03/10/2022 23:26:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
03/10/2022 23:26:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
03/10/2022 23:26:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
03/10/2022 23:26:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=744
03/10/2022 23:27:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.04 on epoch=749
03/10/2022 23:27:04 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.9687194525904204 on epoch=749
03/10/2022 23:27:07 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/10/2022 23:27:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
03/10/2022 23:27:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
03/10/2022 23:27:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
03/10/2022 23:27:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
03/10/2022 23:27:20 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=774
03/10/2022 23:27:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
03/10/2022 23:27:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
03/10/2022 23:27:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
03/10/2022 23:27:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
03/10/2022 23:27:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/10/2022 23:27:37 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=799
03/10/2022 23:27:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/10/2022 23:27:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
03/10/2022 23:27:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/10/2022 23:27:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/10/2022 23:27:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/10/2022 23:27:53 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=824
03/10/2022 23:27:56 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/10/2022 23:27:59 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/10/2022 23:28:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/10/2022 23:28:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/10/2022 23:28:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=849
03/10/2022 23:28:09 - INFO - __main__ - Global step 1700 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=849
03/10/2022 23:28:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/10/2022 23:28:15 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/10/2022 23:28:18 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/10/2022 23:28:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
03/10/2022 23:28:24 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/10/2022 23:28:25 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=874
03/10/2022 23:28:28 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/10/2022 23:28:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/10/2022 23:28:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
03/10/2022 23:28:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/10/2022 23:28:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/10/2022 23:28:41 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=899
03/10/2022 23:28:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/10/2022 23:28:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/10/2022 23:28:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
03/10/2022 23:28:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/10/2022 23:28:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/10/2022 23:28:57 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=924
03/10/2022 23:29:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/10/2022 23:29:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/10/2022 23:29:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/10/2022 23:29:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
03/10/2022 23:29:12 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
03/10/2022 23:29:14 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=949
03/10/2022 23:29:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/10/2022 23:29:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/10/2022 23:29:23 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
03/10/2022 23:29:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/10/2022 23:29:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/10/2022 23:29:31 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=974
03/10/2022 23:29:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/10/2022 23:29:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/10/2022 23:29:40 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.03 on epoch=989
03/10/2022 23:29:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
03/10/2022 23:29:46 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/10/2022 23:29:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:29:47 - INFO - __main__ - Printing 3 examples
03/10/2022 23:29:47 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 23:29:47 - INFO - __main__ - ['negative']
03/10/2022 23:29:47 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 23:29:47 - INFO - __main__ - ['negative']
03/10/2022 23:29:47 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 23:29:47 - INFO - __main__ - ['negative']
03/10/2022 23:29:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:29:47 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:29:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:29:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:29:47 - INFO - __main__ - Printing 3 examples
03/10/2022 23:29:47 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 23:29:47 - INFO - __main__ - ['negative']
03/10/2022 23:29:47 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 23:29:47 - INFO - __main__ - ['negative']
03/10/2022 23:29:47 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 23:29:47 - INFO - __main__ - ['negative']
03/10/2022 23:29:47 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:29:47 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:29:47 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:29:48 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=999
03/10/2022 23:29:48 - INFO - __main__ - save last model!
03/10/2022 23:29:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 23:29:48 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 23:29:48 - INFO - __main__ - Printing 3 examples
03/10/2022 23:29:48 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 23:29:48 - INFO - __main__ - ['negative']
03/10/2022 23:29:48 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 23:29:48 - INFO - __main__ - ['negative']
03/10/2022 23:29:48 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 23:29:48 - INFO - __main__ - ['negative']
03/10/2022 23:29:48 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:29:48 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:29:49 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 23:30:01 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:30:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:30:02 - INFO - __main__ - Starting training!
03/10/2022 23:31:01 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_21_0.2_8_predictions.txt
03/10/2022 23:31:01 - INFO - __main__ - Classification-F1 on test data: 0.9460
03/10/2022 23:31:01 - INFO - __main__ - prefix=amazon_polarity_16_21, lr=0.2, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9459634713066033
03/10/2022 23:31:01 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.5, bsz=8 ...
03/10/2022 23:31:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:31:02 - INFO - __main__ - Printing 3 examples
03/10/2022 23:31:02 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 23:31:02 - INFO - __main__ - ['negative']
03/10/2022 23:31:02 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 23:31:02 - INFO - __main__ - ['negative']
03/10/2022 23:31:02 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 23:31:02 - INFO - __main__ - ['negative']
03/10/2022 23:31:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:31:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:31:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:31:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:31:02 - INFO - __main__ - Printing 3 examples
03/10/2022 23:31:02 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 23:31:02 - INFO - __main__ - ['negative']
03/10/2022 23:31:02 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 23:31:02 - INFO - __main__ - ['negative']
03/10/2022 23:31:02 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 23:31:02 - INFO - __main__ - ['negative']
03/10/2022 23:31:02 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:31:02 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:31:02 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:31:15 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:31:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:31:16 - INFO - __main__ - Starting training!
03/10/2022 23:31:22 - INFO - __main__ - Step 10 Global step 10 Train loss 1.00 on epoch=4
03/10/2022 23:31:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.45 on epoch=9
03/10/2022 23:31:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=14
03/10/2022 23:31:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.40 on epoch=19
03/10/2022 23:31:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=24
03/10/2022 23:31:35 - INFO - __main__ - Global step 50 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 23:31:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 23:31:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=29
03/10/2022 23:31:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
03/10/2022 23:31:44 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=39
03/10/2022 23:31:47 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=44
03/10/2022 23:31:51 - INFO - __main__ - Step 100 Global step 100 Train loss 0.37 on epoch=49
03/10/2022 23:31:51 - INFO - __main__ - Global step 100 Train loss 0.38 Classification-F1 0.4554554554554554 on epoch=49
03/10/2022 23:31:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4554554554554554 on epoch=49, global_step=100
03/10/2022 23:31:54 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
03/10/2022 23:31:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=59
03/10/2022 23:32:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=64
03/10/2022 23:32:04 - INFO - __main__ - Step 140 Global step 140 Train loss 0.38 on epoch=69
03/10/2022 23:32:07 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
03/10/2022 23:32:07 - INFO - __main__ - Global step 150 Train loss 0.37 Classification-F1 0.5555555555555556 on epoch=74
03/10/2022 23:32:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.5555555555555556 on epoch=74, global_step=150
03/10/2022 23:32:10 - INFO - __main__ - Step 160 Global step 160 Train loss 1.28 on epoch=79
03/10/2022 23:32:13 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=84
03/10/2022 23:32:16 - INFO - __main__ - Step 180 Global step 180 Train loss 0.43 on epoch=89
03/10/2022 23:32:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.46 on epoch=94
03/10/2022 23:32:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
03/10/2022 23:32:23 - INFO - __main__ - Global step 200 Train loss 0.68 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 23:32:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.37 on epoch=104
03/10/2022 23:32:29 - INFO - __main__ - Step 220 Global step 220 Train loss 0.50 on epoch=109
03/10/2022 23:32:32 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
03/10/2022 23:32:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.42 on epoch=119
03/10/2022 23:32:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
03/10/2022 23:32:39 - INFO - __main__ - Global step 250 Train loss 0.42 Classification-F1 0.5588547189819725 on epoch=124
03/10/2022 23:32:39 - INFO - __main__ - Saving model with best Classification-F1: 0.5555555555555556 -> 0.5588547189819725 on epoch=124, global_step=250
03/10/2022 23:32:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=129
03/10/2022 23:32:45 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=134
03/10/2022 23:32:48 - INFO - __main__ - Step 280 Global step 280 Train loss 0.36 on epoch=139
03/10/2022 23:32:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=144
03/10/2022 23:32:54 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=149
03/10/2022 23:32:55 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.5933528836754642 on epoch=149
03/10/2022 23:32:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.5933528836754642 on epoch=149, global_step=300
03/10/2022 23:32:58 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=154
03/10/2022 23:33:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=159
03/10/2022 23:33:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.44 on epoch=164
03/10/2022 23:33:07 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=169
03/10/2022 23:33:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=174
03/10/2022 23:33:10 - INFO - __main__ - Global step 350 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=174
03/10/2022 23:33:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=179
03/10/2022 23:33:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=184
03/10/2022 23:33:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.38 on epoch=189
03/10/2022 23:33:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.37 on epoch=194
03/10/2022 23:33:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.55 on epoch=199
03/10/2022 23:33:26 - INFO - __main__ - Global step 400 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=199
03/10/2022 23:33:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=204
03/10/2022 23:33:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.42 on epoch=209
03/10/2022 23:33:36 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=214
03/10/2022 23:33:39 - INFO - __main__ - Step 440 Global step 440 Train loss 0.38 on epoch=219
03/10/2022 23:33:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.38 on epoch=224
03/10/2022 23:33:42 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=224
03/10/2022 23:33:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
03/10/2022 23:33:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.40 on epoch=234
03/10/2022 23:33:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=239
03/10/2022 23:33:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.40 on epoch=244
03/10/2022 23:33:57 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=249
03/10/2022 23:33:58 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=249
03/10/2022 23:34:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.38 on epoch=254
03/10/2022 23:34:04 - INFO - __main__ - Step 520 Global step 520 Train loss 0.39 on epoch=259
03/10/2022 23:34:07 - INFO - __main__ - Step 530 Global step 530 Train loss 0.39 on epoch=264
03/10/2022 23:34:10 - INFO - __main__ - Step 540 Global step 540 Train loss 0.38 on epoch=269
03/10/2022 23:34:13 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=274
03/10/2022 23:34:14 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.49090909090909085 on epoch=274
03/10/2022 23:34:17 - INFO - __main__ - Step 560 Global step 560 Train loss 0.39 on epoch=279
03/10/2022 23:34:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=284
03/10/2022 23:34:23 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=289
03/10/2022 23:34:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.37 on epoch=294
03/10/2022 23:34:29 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=299
03/10/2022 23:34:30 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=299
03/10/2022 23:34:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.38 on epoch=304
03/10/2022 23:34:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=309
03/10/2022 23:34:39 - INFO - __main__ - Step 630 Global step 630 Train loss 0.35 on epoch=314
03/10/2022 23:34:42 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=319
03/10/2022 23:34:45 - INFO - __main__ - Step 650 Global step 650 Train loss 0.34 on epoch=324
03/10/2022 23:34:45 - INFO - __main__ - Global step 650 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=324
03/10/2022 23:34:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=329
03/10/2022 23:34:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=334
03/10/2022 23:34:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.35 on epoch=339
03/10/2022 23:34:58 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=344
03/10/2022 23:35:01 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=349
03/10/2022 23:35:01 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=349
03/10/2022 23:35:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.47 on epoch=354
03/10/2022 23:35:07 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=359
03/10/2022 23:35:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=364
03/10/2022 23:35:13 - INFO - __main__ - Step 740 Global step 740 Train loss 0.34 on epoch=369
03/10/2022 23:35:17 - INFO - __main__ - Step 750 Global step 750 Train loss 0.40 on epoch=374
03/10/2022 23:35:17 - INFO - __main__ - Global step 750 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=374
03/10/2022 23:35:20 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=379
03/10/2022 23:35:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.37 on epoch=384
03/10/2022 23:35:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.41 on epoch=389
03/10/2022 23:35:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=394
03/10/2022 23:35:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=399
03/10/2022 23:35:33 - INFO - __main__ - Global step 800 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=399
03/10/2022 23:35:36 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=404
03/10/2022 23:35:39 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=409
03/10/2022 23:35:42 - INFO - __main__ - Step 830 Global step 830 Train loss 0.36 on epoch=414
03/10/2022 23:35:45 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=419
03/10/2022 23:35:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=424
03/10/2022 23:35:49 - INFO - __main__ - Global step 850 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=424
03/10/2022 23:35:52 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=429
03/10/2022 23:35:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.36 on epoch=434
03/10/2022 23:35:58 - INFO - __main__ - Step 880 Global step 880 Train loss 0.38 on epoch=439
03/10/2022 23:36:01 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
03/10/2022 23:36:04 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=449
03/10/2022 23:36:04 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.5151515151515151 on epoch=449
03/10/2022 23:36:07 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=454
03/10/2022 23:36:10 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=459
03/10/2022 23:36:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.39 on epoch=464
03/10/2022 23:36:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
03/10/2022 23:36:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.34 on epoch=474
03/10/2022 23:36:20 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=474
03/10/2022 23:36:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.38 on epoch=479
03/10/2022 23:36:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=484
03/10/2022 23:36:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=489
03/10/2022 23:36:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.37 on epoch=494
03/10/2022 23:36:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
03/10/2022 23:36:36 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=499
03/10/2022 23:36:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.38 on epoch=504
03/10/2022 23:36:42 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.41 on epoch=509
03/10/2022 23:36:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=514
03/10/2022 23:36:48 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.34 on epoch=519
03/10/2022 23:36:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=524
03/10/2022 23:36:52 - INFO - __main__ - Global step 1050 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=524
03/10/2022 23:36:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.37 on epoch=529
03/10/2022 23:36:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.40 on epoch=534
03/10/2022 23:37:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=539
03/10/2022 23:37:04 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=544
03/10/2022 23:37:07 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=549
03/10/2022 23:37:07 - INFO - __main__ - Global step 1100 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=549
03/10/2022 23:37:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.38 on epoch=554
03/10/2022 23:37:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.35 on epoch=559
03/10/2022 23:37:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=564
03/10/2022 23:37:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.39 on epoch=569
03/10/2022 23:37:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=574
03/10/2022 23:37:23 - INFO - __main__ - Global step 1150 Train loss 0.37 Classification-F1 0.6536796536796536 on epoch=574
03/10/2022 23:37:23 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6536796536796536 on epoch=574, global_step=1150
03/10/2022 23:37:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.35 on epoch=579
03/10/2022 23:37:29 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.38 on epoch=584
03/10/2022 23:37:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.37 on epoch=589
03/10/2022 23:37:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=594
03/10/2022 23:37:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.37 on epoch=599
03/10/2022 23:37:39 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.7810361681329424 on epoch=599
03/10/2022 23:37:39 - INFO - __main__ - Saving model with best Classification-F1: 0.6536796536796536 -> 0.7810361681329424 on epoch=599, global_step=1200
03/10/2022 23:37:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.39 on epoch=604
03/10/2022 23:37:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=609
03/10/2022 23:37:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.36 on epoch=614
03/10/2022 23:37:51 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.39 on epoch=619
03/10/2022 23:37:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=624
03/10/2022 23:37:55 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.75 on epoch=624
03/10/2022 23:37:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.40 on epoch=629
03/10/2022 23:38:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.34 on epoch=634
03/10/2022 23:38:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=639
03/10/2022 23:38:07 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.36 on epoch=644
03/10/2022 23:38:10 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.39 on epoch=649
03/10/2022 23:38:11 - INFO - __main__ - Global step 1300 Train loss 0.37 Classification-F1 0.4589371980676329 on epoch=649
03/10/2022 23:38:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.34 on epoch=654
03/10/2022 23:38:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.38 on epoch=659
03/10/2022 23:38:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=664
03/10/2022 23:38:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=669
03/10/2022 23:38:26 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=674
03/10/2022 23:38:26 - INFO - __main__ - Global step 1350 Train loss 0.37 Classification-F1 0.5134502923976608 on epoch=674
03/10/2022 23:38:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=679
03/10/2022 23:38:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=684
03/10/2022 23:38:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.38 on epoch=689
03/10/2022 23:38:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
03/10/2022 23:38:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.37 on epoch=699
03/10/2022 23:38:42 - INFO - __main__ - Global step 1400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=699
03/10/2022 23:38:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.35 on epoch=704
03/10/2022 23:38:48 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.37 on epoch=709
03/10/2022 23:38:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.37 on epoch=714
03/10/2022 23:38:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.38 on epoch=719
03/10/2022 23:38:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.40 on epoch=724
03/10/2022 23:38:58 - INFO - __main__ - Global step 1450 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=724
03/10/2022 23:39:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.34 on epoch=729
03/10/2022 23:39:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
03/10/2022 23:39:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.35 on epoch=739
03/10/2022 23:39:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.36 on epoch=744
03/10/2022 23:39:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=749
03/10/2022 23:39:14 - INFO - __main__ - Global step 1500 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=749
03/10/2022 23:39:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.39 on epoch=754
03/10/2022 23:39:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.36 on epoch=759
03/10/2022 23:39:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=764
03/10/2022 23:39:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.34 on epoch=769
03/10/2022 23:39:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=774
03/10/2022 23:39:30 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=774
03/10/2022 23:39:33 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.34 on epoch=779
03/10/2022 23:39:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.35 on epoch=784
03/10/2022 23:39:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
03/10/2022 23:39:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
03/10/2022 23:39:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=799
03/10/2022 23:39:45 - INFO - __main__ - Global step 1600 Train loss 0.35 Classification-F1 0.4589371980676329 on epoch=799
03/10/2022 23:39:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=804
03/10/2022 23:39:52 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=809
03/10/2022 23:39:55 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.37 on epoch=814
03/10/2022 23:39:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=819
03/10/2022 23:40:01 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.33 on epoch=824
03/10/2022 23:40:01 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.49090909090909085 on epoch=824
03/10/2022 23:40:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=829
03/10/2022 23:40:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.31 on epoch=834
03/10/2022 23:40:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.33 on epoch=839
03/10/2022 23:40:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=844
03/10/2022 23:40:17 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=849
03/10/2022 23:40:18 - INFO - __main__ - Global step 1700 Train loss 0.35 Classification-F1 0.4666666666666667 on epoch=849
03/10/2022 23:40:21 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.36 on epoch=854
03/10/2022 23:40:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.36 on epoch=859
03/10/2022 23:40:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
03/10/2022 23:40:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.35 on epoch=869
03/10/2022 23:40:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.33 on epoch=874
03/10/2022 23:40:34 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.5835835835835835 on epoch=874
03/10/2022 23:40:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=879
03/10/2022 23:40:40 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.31 on epoch=884
03/10/2022 23:40:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.33 on epoch=889
03/10/2022 23:40:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=894
03/10/2022 23:40:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.34 on epoch=899
03/10/2022 23:40:50 - INFO - __main__ - Global step 1800 Train loss 0.33 Classification-F1 0.4181818181818182 on epoch=899
03/10/2022 23:40:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=904
03/10/2022 23:40:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.37 on epoch=909
03/10/2022 23:40:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=914
03/10/2022 23:41:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=919
03/10/2022 23:41:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.32 on epoch=924
03/10/2022 23:41:06 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.4589371980676329 on epoch=924
03/10/2022 23:41:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.33 on epoch=929
03/10/2022 23:41:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
03/10/2022 23:41:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.37 on epoch=939
03/10/2022 23:41:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=944
03/10/2022 23:41:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.33 on epoch=949
03/10/2022 23:41:22 - INFO - __main__ - Global step 1900 Train loss 0.34 Classification-F1 0.6267232237539766 on epoch=949
03/10/2022 23:41:25 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.46 on epoch=954
03/10/2022 23:41:28 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
03/10/2022 23:41:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
03/10/2022 23:41:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.36 on epoch=969
03/10/2022 23:41:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=974
03/10/2022 23:41:38 - INFO - __main__ - Global step 1950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=974
03/10/2022 23:41:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.35 on epoch=979
03/10/2022 23:41:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.29 on epoch=984
03/10/2022 23:41:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=989
03/10/2022 23:41:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=994
03/10/2022 23:41:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
03/10/2022 23:41:54 - INFO - __main__ - Global step 2000 Train loss 0.33 Classification-F1 0.716256157635468 on epoch=999
03/10/2022 23:41:54 - INFO - __main__ - save last model!
03/10/2022 23:41:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 23:41:54 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 23:41:54 - INFO - __main__ - Printing 3 examples
03/10/2022 23:41:54 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 23:41:54 - INFO - __main__ - ['negative']
03/10/2022 23:41:54 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 23:41:54 - INFO - __main__ - ['negative']
03/10/2022 23:41:54 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 23:41:54 - INFO - __main__ - ['negative']
03/10/2022 23:41:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:41:55 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:41:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:41:55 - INFO - __main__ - Printing 3 examples
03/10/2022 23:41:55 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 23:41:55 - INFO - __main__ - ['negative']
03/10/2022 23:41:55 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 23:41:55 - INFO - __main__ - ['negative']
03/10/2022 23:41:55 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 23:41:55 - INFO - __main__ - ['negative']
03/10/2022 23:41:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:41:55 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:41:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:41:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:41:55 - INFO - __main__ - Printing 3 examples
03/10/2022 23:41:55 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 23:41:55 - INFO - __main__ - ['negative']
03/10/2022 23:41:55 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 23:41:55 - INFO - __main__ - ['negative']
03/10/2022 23:41:55 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 23:41:55 - INFO - __main__ - ['negative']
03/10/2022 23:41:55 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:41:55 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:41:55 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:41:56 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 23:42:09 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:42:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:42:09 - INFO - __main__ - Starting training!
03/10/2022 23:42:13 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_42_0.5_8_predictions.txt
03/10/2022 23:42:13 - INFO - __main__ - Classification-F1 on test data: 0.7390
03/10/2022 23:42:14 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.5, bsz=8, dev_performance=0.7810361681329424, test_performance=0.7390246777861634
03/10/2022 23:42:14 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.4, bsz=8 ...
03/10/2022 23:42:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:42:15 - INFO - __main__ - Printing 3 examples
03/10/2022 23:42:15 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 23:42:15 - INFO - __main__ - ['negative']
03/10/2022 23:42:15 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 23:42:15 - INFO - __main__ - ['negative']
03/10/2022 23:42:15 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 23:42:15 - INFO - __main__ - ['negative']
03/10/2022 23:42:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:42:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:42:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:42:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:42:15 - INFO - __main__ - Printing 3 examples
03/10/2022 23:42:15 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 23:42:15 - INFO - __main__ - ['negative']
03/10/2022 23:42:15 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 23:42:15 - INFO - __main__ - ['negative']
03/10/2022 23:42:15 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 23:42:15 - INFO - __main__ - ['negative']
03/10/2022 23:42:15 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:42:15 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:42:15 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:42:28 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:42:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:42:29 - INFO - __main__ - Starting training!
03/10/2022 23:42:33 - INFO - __main__ - Step 10 Global step 10 Train loss 1.14 on epoch=4
03/10/2022 23:42:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.47 on epoch=9
03/10/2022 23:42:39 - INFO - __main__ - Step 30 Global step 30 Train loss 0.44 on epoch=14
03/10/2022 23:42:42 - INFO - __main__ - Step 40 Global step 40 Train loss 0.39 on epoch=19
03/10/2022 23:42:45 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=24
03/10/2022 23:42:46 - INFO - __main__ - Global step 50 Train loss 0.57 Classification-F1 0.46843853820598 on epoch=24
03/10/2022 23:42:46 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.46843853820598 on epoch=24, global_step=50
03/10/2022 23:42:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.37 on epoch=29
03/10/2022 23:42:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
03/10/2022 23:42:55 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=39
03/10/2022 23:42:58 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
03/10/2022 23:43:01 - INFO - __main__ - Step 100 Global step 100 Train loss 0.44 on epoch=49
03/10/2022 23:43:02 - INFO - __main__ - Global step 100 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 23:43:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
03/10/2022 23:43:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
03/10/2022 23:43:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.36 on epoch=64
03/10/2022 23:43:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
03/10/2022 23:43:17 - INFO - __main__ - Step 150 Global step 150 Train loss 0.41 on epoch=74
03/10/2022 23:43:18 - INFO - __main__ - Global step 150 Train loss 0.37 Classification-F1 0.4589371980676329 on epoch=74
03/10/2022 23:43:21 - INFO - __main__ - Step 160 Global step 160 Train loss 0.39 on epoch=79
03/10/2022 23:43:24 - INFO - __main__ - Step 170 Global step 170 Train loss 0.43 on epoch=84
03/10/2022 23:43:27 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=89
03/10/2022 23:43:30 - INFO - __main__ - Step 190 Global step 190 Train loss 0.59 on epoch=94
03/10/2022 23:43:33 - INFO - __main__ - Step 200 Global step 200 Train loss 0.62 on epoch=99
03/10/2022 23:43:34 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.5151515151515151 on epoch=99
03/10/2022 23:43:34 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5151515151515151 on epoch=99, global_step=200
03/10/2022 23:43:37 - INFO - __main__ - Step 210 Global step 210 Train loss 0.58 on epoch=104
03/10/2022 23:43:40 - INFO - __main__ - Step 220 Global step 220 Train loss 0.40 on epoch=109
03/10/2022 23:43:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.58 on epoch=114
03/10/2022 23:43:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.48 on epoch=119
03/10/2022 23:43:49 - INFO - __main__ - Step 250 Global step 250 Train loss 0.48 on epoch=124
03/10/2022 23:43:50 - INFO - __main__ - Global step 250 Train loss 0.50 Classification-F1 0.5270935960591133 on epoch=124
03/10/2022 23:43:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5270935960591133 on epoch=124, global_step=250
03/10/2022 23:43:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
03/10/2022 23:43:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
03/10/2022 23:43:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.40 on epoch=139
03/10/2022 23:44:02 - INFO - __main__ - Step 290 Global step 290 Train loss 0.38 on epoch=144
03/10/2022 23:44:05 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
03/10/2022 23:44:06 - INFO - __main__ - Global step 300 Train loss 0.38 Classification-F1 0.4920634920634921 on epoch=149
03/10/2022 23:44:09 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=154
03/10/2022 23:44:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.39 on epoch=159
03/10/2022 23:44:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.39 on epoch=164
03/10/2022 23:44:18 - INFO - __main__ - Step 340 Global step 340 Train loss 0.38 on epoch=169
03/10/2022 23:44:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.39 on epoch=174
03/10/2022 23:44:22 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.4920634920634921 on epoch=174
03/10/2022 23:44:25 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=179
03/10/2022 23:44:28 - INFO - __main__ - Step 370 Global step 370 Train loss 0.39 on epoch=184
03/10/2022 23:44:31 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=189
03/10/2022 23:44:34 - INFO - __main__ - Step 390 Global step 390 Train loss 0.38 on epoch=194
03/10/2022 23:44:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.35 on epoch=199
03/10/2022 23:44:38 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.5076923076923077 on epoch=199
03/10/2022 23:44:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=204
03/10/2022 23:44:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=209
03/10/2022 23:44:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.42 on epoch=214
03/10/2022 23:44:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
03/10/2022 23:44:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=224
03/10/2022 23:44:54 - INFO - __main__ - Global step 450 Train loss 0.38 Classification-F1 0.5588547189819725 on epoch=224
03/10/2022 23:44:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5270935960591133 -> 0.5588547189819725 on epoch=224, global_step=450
03/10/2022 23:44:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.40 on epoch=229
03/10/2022 23:45:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.33 on epoch=234
03/10/2022 23:45:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=239
03/10/2022 23:45:06 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=244
03/10/2022 23:45:09 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=249
03/10/2022 23:45:10 - INFO - __main__ - Global step 500 Train loss 0.38 Classification-F1 0.539313399778516 on epoch=249
03/10/2022 23:45:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.34 on epoch=254
03/10/2022 23:45:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
03/10/2022 23:45:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.36 on epoch=264
03/10/2022 23:45:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.36 on epoch=269
03/10/2022 23:45:25 - INFO - __main__ - Step 550 Global step 550 Train loss 0.37 on epoch=274
03/10/2022 23:45:26 - INFO - __main__ - Global step 550 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=274
03/10/2022 23:45:29 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=279
03/10/2022 23:45:32 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=284
03/10/2022 23:45:35 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=289
03/10/2022 23:45:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.40 on epoch=294
03/10/2022 23:45:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=299
03/10/2022 23:45:41 - INFO - __main__ - Global step 600 Train loss 0.38 Classification-F1 0.5270935960591133 on epoch=299
03/10/2022 23:45:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=304
03/10/2022 23:45:47 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=309
03/10/2022 23:45:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.34 on epoch=314
03/10/2022 23:45:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=319
03/10/2022 23:45:57 - INFO - __main__ - Step 650 Global step 650 Train loss 0.40 on epoch=324
03/10/2022 23:45:57 - INFO - __main__ - Global step 650 Train loss 0.37 Classification-F1 0.5607843137254902 on epoch=324
03/10/2022 23:45:57 - INFO - __main__ - Saving model with best Classification-F1: 0.5588547189819725 -> 0.5607843137254902 on epoch=324, global_step=650
03/10/2022 23:46:00 - INFO - __main__ - Step 660 Global step 660 Train loss 0.34 on epoch=329
03/10/2022 23:46:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=334
03/10/2022 23:46:06 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=339
03/10/2022 23:46:09 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=344
03/10/2022 23:46:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=349
03/10/2022 23:46:13 - INFO - __main__ - Global step 700 Train loss 0.38 Classification-F1 0.39756367663344405 on epoch=349
03/10/2022 23:46:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.41 on epoch=354
03/10/2022 23:46:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.37 on epoch=359
03/10/2022 23:46:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.39 on epoch=364
03/10/2022 23:46:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=369
03/10/2022 23:46:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=374
03/10/2022 23:46:29 - INFO - __main__ - Global step 750 Train loss 0.38 Classification-F1 0.39999999999999997 on epoch=374
03/10/2022 23:46:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=379
03/10/2022 23:46:35 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=384
03/10/2022 23:46:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.37 on epoch=389
03/10/2022 23:46:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=394
03/10/2022 23:46:44 - INFO - __main__ - Step 800 Global step 800 Train loss 0.39 on epoch=399
03/10/2022 23:46:44 - INFO - __main__ - Global step 800 Train loss 0.37 Classification-F1 0.39756367663344405 on epoch=399
03/10/2022 23:46:47 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=404
03/10/2022 23:46:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=409
03/10/2022 23:46:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=414
03/10/2022 23:46:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=419
03/10/2022 23:46:59 - INFO - __main__ - Step 850 Global step 850 Train loss 0.35 on epoch=424
03/10/2022 23:47:00 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.37662337662337664 on epoch=424
03/10/2022 23:47:03 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=429
03/10/2022 23:47:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=434
03/10/2022 23:47:09 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=439
03/10/2022 23:47:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.35 on epoch=444
03/10/2022 23:47:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=449
03/10/2022 23:47:16 - INFO - __main__ - Global step 900 Train loss 0.36 Classification-F1 0.5933528836754642 on epoch=449
03/10/2022 23:47:16 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5933528836754642 on epoch=449, global_step=900
03/10/2022 23:47:19 - INFO - __main__ - Step 910 Global step 910 Train loss 0.39 on epoch=454
03/10/2022 23:47:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/10/2022 23:47:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.37 on epoch=464
03/10/2022 23:47:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.46 on epoch=469
03/10/2022 23:47:31 - INFO - __main__ - Step 950 Global step 950 Train loss 0.57 on epoch=474
03/10/2022 23:47:31 - INFO - __main__ - Global step 950 Train loss 0.43 Classification-F1 0.5270935960591133 on epoch=474
03/10/2022 23:47:35 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=479
03/10/2022 23:47:38 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=484
03/10/2022 23:47:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.48 on epoch=489
03/10/2022 23:47:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.38 on epoch=494
03/10/2022 23:47:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.36 on epoch=499
03/10/2022 23:47:47 - INFO - __main__ - Global step 1000 Train loss 0.38 Classification-F1 0.5636363636363637 on epoch=499
03/10/2022 23:47:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=504
03/10/2022 23:47:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.36 on epoch=509
03/10/2022 23:47:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=514
03/10/2022 23:47:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.39 on epoch=519
03/10/2022 23:48:02 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.37 on epoch=524
03/10/2022 23:48:03 - INFO - __main__ - Global step 1050 Train loss 0.36 Classification-F1 0.4920634920634921 on epoch=524
03/10/2022 23:48:06 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=529
03/10/2022 23:48:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.37 on epoch=534
03/10/2022 23:48:12 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.35 on epoch=539
03/10/2022 23:48:15 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=544
03/10/2022 23:48:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.36 on epoch=549
03/10/2022 23:48:19 - INFO - __main__ - Global step 1100 Train loss 0.37 Classification-F1 0.4589371980676329 on epoch=549
03/10/2022 23:48:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.36 on epoch=554
03/10/2022 23:48:25 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=559
03/10/2022 23:48:28 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.41 on epoch=564
03/10/2022 23:48:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.48 on epoch=569
03/10/2022 23:48:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.37 on epoch=574
03/10/2022 23:48:34 - INFO - __main__ - Global step 1150 Train loss 0.40 Classification-F1 0.41700404858299595 on epoch=574
03/10/2022 23:48:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.33 on epoch=579
03/10/2022 23:48:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
03/10/2022 23:48:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=589
03/10/2022 23:48:47 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=594
03/10/2022 23:48:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=599
03/10/2022 23:48:50 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.4231177094379639 on epoch=599
03/10/2022 23:48:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.40 on epoch=604
03/10/2022 23:48:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.40 on epoch=609
03/10/2022 23:48:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=614
03/10/2022 23:49:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=619
03/10/2022 23:49:05 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.38 on epoch=624
03/10/2022 23:49:06 - INFO - __main__ - Global step 1250 Train loss 0.38 Classification-F1 0.5933528836754642 on epoch=624
03/10/2022 23:49:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=629
03/10/2022 23:49:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.39 on epoch=634
03/10/2022 23:49:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=639
03/10/2022 23:49:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.40 on epoch=644
03/10/2022 23:49:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.41 on epoch=649
03/10/2022 23:49:22 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.5134502923976608 on epoch=649
03/10/2022 23:49:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.39 on epoch=654
03/10/2022 23:49:28 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.41 on epoch=659
03/10/2022 23:49:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.37 on epoch=664
03/10/2022 23:49:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.38 on epoch=669
03/10/2022 23:49:37 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.33 on epoch=674
03/10/2022 23:49:37 - INFO - __main__ - Global step 1350 Train loss 0.38 Classification-F1 0.5 on epoch=674
03/10/2022 23:49:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.36 on epoch=679
03/10/2022 23:49:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=684
03/10/2022 23:49:47 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.37 on epoch=689
03/10/2022 23:49:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.34 on epoch=694
03/10/2022 23:49:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.33 on epoch=699
03/10/2022 23:49:53 - INFO - __main__ - Global step 1400 Train loss 0.36 Classification-F1 0.37662337662337664 on epoch=699
03/10/2022 23:49:57 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.37 on epoch=704
03/10/2022 23:50:00 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=709
03/10/2022 23:50:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=714
03/10/2022 23:50:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=719
03/10/2022 23:50:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.34 on epoch=724
03/10/2022 23:50:10 - INFO - __main__ - Global step 1450 Train loss 0.36 Classification-F1 0.39756367663344405 on epoch=724
03/10/2022 23:50:13 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=729
03/10/2022 23:50:16 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.37 on epoch=734
03/10/2022 23:50:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
03/10/2022 23:50:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=744
03/10/2022 23:50:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.37 on epoch=749
03/10/2022 23:50:26 - INFO - __main__ - Global step 1500 Train loss 0.35 Classification-F1 0.4231177094379639 on epoch=749
03/10/2022 23:50:29 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.38 on epoch=754
03/10/2022 23:50:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.34 on epoch=759
03/10/2022 23:50:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.37 on epoch=764
03/10/2022 23:50:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.36 on epoch=769
03/10/2022 23:50:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.36 on epoch=774
03/10/2022 23:50:42 - INFO - __main__ - Global step 1550 Train loss 0.36 Classification-F1 0.39999999999999997 on epoch=774
03/10/2022 23:50:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=779
03/10/2022 23:50:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=784
03/10/2022 23:50:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
03/10/2022 23:50:54 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=794
03/10/2022 23:50:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.33 on epoch=799
03/10/2022 23:50:58 - INFO - __main__ - Global step 1600 Train loss 0.33 Classification-F1 0.37662337662337664 on epoch=799
03/10/2022 23:51:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.34 on epoch=804
03/10/2022 23:51:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.37 on epoch=809
03/10/2022 23:51:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=814
03/10/2022 23:51:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=819
03/10/2022 23:51:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.34 on epoch=824
03/10/2022 23:51:14 - INFO - __main__ - Global step 1650 Train loss 0.35 Classification-F1 0.37662337662337664 on epoch=824
03/10/2022 23:51:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.36 on epoch=829
03/10/2022 23:51:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=834
03/10/2022 23:51:23 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=839
03/10/2022 23:51:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.34 on epoch=844
03/10/2022 23:51:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.32 on epoch=849
03/10/2022 23:51:30 - INFO - __main__ - Global step 1700 Train loss 0.34 Classification-F1 0.37662337662337664 on epoch=849
03/10/2022 23:51:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.35 on epoch=854
03/10/2022 23:51:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=859
03/10/2022 23:51:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.38 on epoch=864
03/10/2022 23:51:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=869
03/10/2022 23:51:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
03/10/2022 23:51:46 - INFO - __main__ - Global step 1750 Train loss 0.36 Classification-F1 0.4181818181818182 on epoch=874
03/10/2022 23:51:49 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.35 on epoch=879
03/10/2022 23:51:52 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=884
03/10/2022 23:51:55 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=889
03/10/2022 23:51:58 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
03/10/2022 23:52:01 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=899
03/10/2022 23:52:02 - INFO - __main__ - Global step 1800 Train loss 0.34 Classification-F1 0.3552492046659597 on epoch=899
03/10/2022 23:52:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.32 on epoch=904
03/10/2022 23:52:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=909
03/10/2022 23:52:11 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=914
03/10/2022 23:52:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
03/10/2022 23:52:17 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.39 on epoch=924
03/10/2022 23:52:18 - INFO - __main__ - Global step 1850 Train loss 0.35 Classification-F1 0.37662337662337664 on epoch=924
03/10/2022 23:52:21 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.30 on epoch=929
03/10/2022 23:52:24 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.33 on epoch=934
03/10/2022 23:52:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.33 on epoch=939
03/10/2022 23:52:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.31 on epoch=944
03/10/2022 23:52:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
03/10/2022 23:52:34 - INFO - __main__ - Global step 1900 Train loss 0.33 Classification-F1 0.39756367663344405 on epoch=949
03/10/2022 23:52:37 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=954
03/10/2022 23:52:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.33 on epoch=959
03/10/2022 23:52:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
03/10/2022 23:52:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.40 on epoch=969
03/10/2022 23:52:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=974
03/10/2022 23:52:50 - INFO - __main__ - Global step 1950 Train loss 0.35 Classification-F1 0.4181818181818182 on epoch=974
03/10/2022 23:52:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.33 on epoch=979
03/10/2022 23:52:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.35 on epoch=984
03/10/2022 23:52:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=989
03/10/2022 23:53:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
03/10/2022 23:53:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
03/10/2022 23:53:06 - INFO - __main__ - Global step 2000 Train loss 0.34 Classification-F1 0.39756367663344405 on epoch=999
03/10/2022 23:53:06 - INFO - __main__ - save last model!
03/10/2022 23:53:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/10/2022 23:53:06 - INFO - __main__ - Start tokenizing ... 1000 instances
03/10/2022 23:53:06 - INFO - __main__ - Printing 3 examples
03/10/2022 23:53:06 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/10/2022 23:53:06 - INFO - __main__ - ['negative']
03/10/2022 23:53:06 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/10/2022 23:53:06 - INFO - __main__ - ['negative']
03/10/2022 23:53:06 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/10/2022 23:53:06 - INFO - __main__ - ['negative']
03/10/2022 23:53:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:53:07 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:53:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:53:07 - INFO - __main__ - Printing 3 examples
03/10/2022 23:53:07 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 23:53:07 - INFO - __main__ - ['negative']
03/10/2022 23:53:07 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 23:53:07 - INFO - __main__ - ['negative']
03/10/2022 23:53:07 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 23:53:07 - INFO - __main__ - ['negative']
03/10/2022 23:53:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/10/2022 23:53:07 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:53:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:53:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:53:07 - INFO - __main__ - Printing 3 examples
03/10/2022 23:53:07 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 23:53:07 - INFO - __main__ - ['negative']
03/10/2022 23:53:07 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 23:53:07 - INFO - __main__ - ['negative']
03/10/2022 23:53:07 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 23:53:07 - INFO - __main__ - ['negative']
03/10/2022 23:53:07 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:53:07 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:53:07 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:53:08 - INFO - __main__ - Loaded 1000 examples from test data
03/10/2022 23:53:21 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:53:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:53:22 - INFO - __main__ - Starting training!
03/10/2022 23:53:27 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_42_0.4_8_predictions.txt
03/10/2022 23:53:27 - INFO - __main__ - Classification-F1 on test data: 0.4239
03/10/2022 23:53:27 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.4, bsz=8, dev_performance=0.5933528836754642, test_performance=0.42392537936411023
03/10/2022 23:53:27 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.3, bsz=8 ...
03/10/2022 23:53:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:53:28 - INFO - __main__ - Printing 3 examples
03/10/2022 23:53:28 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/10/2022 23:53:28 - INFO - __main__ - ['negative']
03/10/2022 23:53:28 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/10/2022 23:53:28 - INFO - __main__ - ['negative']
03/10/2022 23:53:28 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/10/2022 23:53:28 - INFO - __main__ - ['negative']
03/10/2022 23:53:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/10/2022 23:53:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:53:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/10/2022 23:53:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/10/2022 23:53:28 - INFO - __main__ - Printing 3 examples
03/10/2022 23:53:28 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/10/2022 23:53:28 - INFO - __main__ - ['negative']
03/10/2022 23:53:28 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/10/2022 23:53:28 - INFO - __main__ - ['negative']
03/10/2022 23:53:28 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/10/2022 23:53:28 - INFO - __main__ - ['negative']
03/10/2022 23:53:28 - INFO - __main__ - Tokenizing Input ...
03/10/2022 23:53:28 - INFO - __main__ - Tokenizing Output ...
03/10/2022 23:53:28 - INFO - __main__ - Loaded 32 examples from dev data
03/10/2022 23:53:41 - INFO - __main__ - load prompt embedding from ckpt
03/10/2022 23:53:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/10/2022 23:53:42 - INFO - __main__ - Starting training!
03/10/2022 23:53:46 - INFO - __main__ - Step 10 Global step 10 Train loss 1.25 on epoch=4
03/10/2022 23:53:49 - INFO - __main__ - Step 20 Global step 20 Train loss 0.48 on epoch=9
03/10/2022 23:53:52 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=14
03/10/2022 23:53:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.42 on epoch=19
03/10/2022 23:53:58 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=24
03/10/2022 23:53:59 - INFO - __main__ - Global step 50 Train loss 0.62 Classification-F1 0.3333333333333333 on epoch=24
03/10/2022 23:53:59 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/10/2022 23:54:02 - INFO - __main__ - Step 60 Global step 60 Train loss 0.39 on epoch=29
03/10/2022 23:54:05 - INFO - __main__ - Step 70 Global step 70 Train loss 0.48 on epoch=34
03/10/2022 23:54:08 - INFO - __main__ - Step 80 Global step 80 Train loss 0.50 on epoch=39
03/10/2022 23:54:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.73 on epoch=44
03/10/2022 23:54:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=49
03/10/2022 23:54:15 - INFO - __main__ - Global step 100 Train loss 0.50 Classification-F1 0.3333333333333333 on epoch=49
03/10/2022 23:54:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
03/10/2022 23:54:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.47 on epoch=59
03/10/2022 23:54:24 - INFO - __main__ - Step 130 Global step 130 Train loss 0.56 on epoch=64
03/10/2022 23:54:27 - INFO - __main__ - Step 140 Global step 140 Train loss 0.91 on epoch=69
03/10/2022 23:54:31 - INFO - __main__ - Step 150 Global step 150 Train loss 0.87 on epoch=74
03/10/2022 23:54:31 - INFO - __main__ - Global step 150 Train loss 0.67 Classification-F1 0.4385964912280702 on epoch=74
03/10/2022 23:54:31 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4385964912280702 on epoch=74, global_step=150
03/10/2022 23:54:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.83 on epoch=79
03/10/2022 23:54:37 - INFO - __main__ - Step 170 Global step 170 Train loss 1.05 on epoch=84
03/10/2022 23:54:40 - INFO - __main__ - Step 180 Global step 180 Train loss 1.09 on epoch=89
03/10/2022 23:54:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.81 on epoch=94
03/10/2022 23:54:47 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=99
03/10/2022 23:54:47 - INFO - __main__ - Global step 200 Train loss 0.85 Classification-F1 0.3333333333333333 on epoch=99
03/10/2022 23:54:50 - INFO - __main__ - Step 210 Global step 210 Train loss 0.62 on epoch=104
03/10/2022 23:54:53 - INFO - __main__ - Step 220 Global step 220 Train loss 0.71 on epoch=109
03/10/2022 23:54:56 - INFO - __main__ - Step 230 Global step 230 Train loss 0.51 on epoch=114
03/10/2022 23:55:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.52 on epoch=119
03/10/2022 23:55:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.47 on epoch=124
03/10/2022 23:55:03 - INFO - __main__ - Global step 250 Train loss 0.56 Classification-F1 0.36374269005847953 on epoch=124
03/10/2022 23:55:06 - INFO - __main__ - Step 260 Global step 260 Train loss 0.55 on epoch=129
03/10/2022 23:55:09 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=134
03/10/2022 23:55:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.48 on epoch=139
03/10/2022 23:55:16 - INFO - __main__ - Step 290 Global step 290 Train loss 0.64 on epoch=144
03/10/2022 23:55:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.72 on epoch=149
03/10/2022 23:55:19 - INFO - __main__ - Global step 300 Train loss 0.57 Classification-F1 0.3333333333333333 on epoch=149
03/10/2022 23:55:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=154
03/10/2022 23:55:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.58 on epoch=159
03/10/2022 23:55:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.51 on epoch=164
03/10/2022 23:55:32 - INFO - __main__ - Step 340 Global step 340 Train loss 0.53 on epoch=169
03/10/2022 23:55:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=174
03/10/2022 23:55:35 - INFO - __main__ - Global step 350 Train loss 0.51 Classification-F1 0.4682306940371457 on epoch=174
03/10/2022 23:55:35 - INFO - __main__ - Saving model with best Classification-F1: 0.4385964912280702 -> 0.4682306940371457 on epoch=174, global_step=350
03/10/2022 23:55:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.46 on epoch=179
03/10/2022 23:55:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.52 on epoch=184
03/10/2022 23:55:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.49 on epoch=189
03/10/2022 23:55:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.65 on epoch=194
03/10/2022 23:55:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.74 on epoch=199
03/10/2022 23:55:52 - INFO - __main__ - Global step 400 Train loss 0.57 Classification-F1 0.4458874458874459 on epoch=199
03/10/2022 23:55:55 - INFO - __main__ - Step 410 Global step 410 Train loss 0.91 on epoch=204
03/10/2022 23:55:58 - INFO - __main__ - Step 420 Global step 420 Train loss 1.41 on epoch=209
03/10/2022 23:56:01 - INFO - __main__ - Step 430 Global step 430 Train loss 1.08 on epoch=214
03/10/2022 23:56:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.66 on epoch=219
03/10/2022 23:56:07 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=224
03/10/2022 23:56:08 - INFO - __main__ - Global step 450 Train loss 0.91 Classification-F1 0.3333333333333333 on epoch=224
03/10/2022 23:56:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.44 on epoch=229
03/10/2022 23:56:14 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=234
03/10/2022 23:56:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.45 on epoch=239
03/10/2022 23:56:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.45 on epoch=244
03/10/2022 23:56:23 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=249
03/10/2022 23:56:24 - INFO - __main__ - Global step 500 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=249
03/10/2022 23:56:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.53 on epoch=254
03/10/2022 23:56:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=259
03/10/2022 23:56:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=264
03/10/2022 23:56:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.44 on epoch=269
03/10/2022 23:56:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.45 on epoch=274
03/10/2022 23:56:40 - INFO - __main__ - Global step 550 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=274
03/10/2022 23:56:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.41 on epoch=279
03/10/2022 23:56:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.41 on epoch=284
03/10/2022 23:56:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.48 on epoch=289
03/10/2022 23:56:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=294
03/10/2022 23:56:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.45 on epoch=299
03/10/2022 23:56:56 - INFO - __main__ - Global step 600 Train loss 0.45 Classification-F1 0.3333333333333333 on epoch=299
03/10/2022 23:56:59 - INFO - __main__ - Step 610 Global step 610 Train loss 0.45 on epoch=304
03/10/2022 23:57:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.41 on epoch=309
03/10/2022 23:57:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.45 on epoch=314
03/10/2022 23:57:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.44 on epoch=319
03/10/2022 23:57:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.50 on epoch=324
03/10/2022 23:57:11 - INFO - __main__ - Global step 650 Train loss 0.45 Classification-F1 0.4385964912280702 on epoch=324
03/10/2022 23:57:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=329
03/10/2022 23:57:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.44 on epoch=334
03/10/2022 23:57:21 - INFO - __main__ - Step 680 Global step 680 Train loss 0.52 on epoch=339
03/10/2022 23:57:24 - INFO - __main__ - Step 690 Global step 690 Train loss 0.46 on epoch=344
03/10/2022 23:57:27 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=349
03/10/2022 23:57:27 - INFO - __main__ - Global step 700 Train loss 0.45 Classification-F1 0.4666666666666667 on epoch=349
03/10/2022 23:57:30 - INFO - __main__ - Step 710 Global step 710 Train loss 0.51 on epoch=354
03/10/2022 23:57:33 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=359
03/10/2022 23:57:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.43 on epoch=364
03/10/2022 23:57:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.48 on epoch=369
03/10/2022 23:57:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.48 on epoch=374
03/10/2022 23:57:43 - INFO - __main__ - Global step 750 Train loss 0.46 Classification-F1 0.3992490613266583 on epoch=374
03/10/2022 23:57:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.52 on epoch=379
03/10/2022 23:57:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=384
03/10/2022 23:57:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.38 on epoch=389
03/10/2022 23:57:55 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=394
03/10/2022 23:57:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.40 on epoch=399
03/10/2022 23:57:58 - INFO - __main__ - Global step 800 Train loss 0.46 Classification-F1 0.3333333333333333 on epoch=399
03/10/2022 23:58:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.41 on epoch=404
03/10/2022 23:58:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=409
03/10/2022 23:58:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.39 on epoch=414
03/10/2022 23:58:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=419
03/10/2022 23:58:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.42 on epoch=424
03/10/2022 23:58:14 - INFO - __main__ - Global step 850 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=424
03/10/2022 23:58:17 - INFO - __main__ - Step 860 Global step 860 Train loss 0.39 on epoch=429
03/10/2022 23:58:20 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=434
03/10/2022 23:58:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.42 on epoch=439
03/10/2022 23:58:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.40 on epoch=444
03/10/2022 23:58:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=449
03/10/2022 23:58:30 - INFO - __main__ - Global step 900 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=449
03/10/2022 23:58:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.37 on epoch=454
03/10/2022 23:58:36 - INFO - __main__ - Step 920 Global step 920 Train loss 0.41 on epoch=459
03/10/2022 23:58:39 - INFO - __main__ - Step 930 Global step 930 Train loss 0.45 on epoch=464
03/10/2022 23:58:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.41 on epoch=469
03/10/2022 23:58:45 - INFO - __main__ - Step 950 Global step 950 Train loss 0.44 on epoch=474
03/10/2022 23:58:45 - INFO - __main__ - Global step 950 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=474
03/10/2022 23:58:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.40 on epoch=479
03/10/2022 23:58:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.47 on epoch=484
03/10/2022 23:58:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.37 on epoch=489
03/10/2022 23:58:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.39 on epoch=494
03/10/2022 23:59:01 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.40 on epoch=499
03/10/2022 23:59:01 - INFO - __main__ - Global step 1000 Train loss 0.41 Classification-F1 0.3992490613266583 on epoch=499
03/10/2022 23:59:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
03/10/2022 23:59:07 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=509
03/10/2022 23:59:10 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.43 on epoch=514
03/10/2022 23:59:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.37 on epoch=519
03/10/2022 23:59:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.42 on epoch=524
03/10/2022 23:59:17 - INFO - __main__ - Global step 1050 Train loss 0.41 Classification-F1 0.3992490613266583 on epoch=524
03/10/2022 23:59:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.41 on epoch=529
03/10/2022 23:59:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.43 on epoch=534
03/10/2022 23:59:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=539
03/10/2022 23:59:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.68 on epoch=544
03/10/2022 23:59:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.66 on epoch=549
03/10/2022 23:59:32 - INFO - __main__ - Global step 1100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=549
03/10/2022 23:59:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.47 on epoch=554
03/10/2022 23:59:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.39 on epoch=559
03/10/2022 23:59:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=564
03/10/2022 23:59:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=569
03/10/2022 23:59:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
03/10/2022 23:59:48 - INFO - __main__ - Global step 1150 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=574
03/10/2022 23:59:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.41 on epoch=579
03/10/2022 23:59:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
03/10/2022 23:59:57 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
03/11/2022 00:00:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=594
03/11/2022 00:00:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.38 on epoch=599
03/11/2022 00:00:04 - INFO - __main__ - Global step 1200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=599
03/11/2022 00:00:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.46 on epoch=604
03/11/2022 00:00:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.37 on epoch=609
03/11/2022 00:00:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.41 on epoch=614
03/11/2022 00:00:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.43 on epoch=619
03/11/2022 00:00:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.41 on epoch=624
03/11/2022 00:00:19 - INFO - __main__ - Global step 1250 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=624
03/11/2022 00:00:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.38 on epoch=629
03/11/2022 00:00:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.45 on epoch=634
03/11/2022 00:00:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.38 on epoch=639
03/11/2022 00:00:32 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.45 on epoch=644
03/11/2022 00:00:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.37 on epoch=649
03/11/2022 00:00:35 - INFO - __main__ - Global step 1300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=649
03/11/2022 00:00:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.42 on epoch=654
03/11/2022 00:00:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.39 on epoch=659
03/11/2022 00:00:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.40 on epoch=664
03/11/2022 00:00:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.52 on epoch=669
03/11/2022 00:00:50 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.46 on epoch=674
03/11/2022 00:00:51 - INFO - __main__ - Global step 1350 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=674
03/11/2022 00:00:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=679
03/11/2022 00:00:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.47 on epoch=684
03/11/2022 00:01:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.43 on epoch=689
03/11/2022 00:01:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.36 on epoch=694
03/11/2022 00:01:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.39 on epoch=699
03/11/2022 00:01:07 - INFO - __main__ - Global step 1400 Train loss 0.41 Classification-F1 0.539313399778516 on epoch=699
03/11/2022 00:01:07 - INFO - __main__ - Saving model with best Classification-F1: 0.4682306940371457 -> 0.539313399778516 on epoch=699, global_step=1400
03/11/2022 00:01:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.38 on epoch=704
03/11/2022 00:01:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
03/11/2022 00:01:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=714
03/11/2022 00:01:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
03/11/2022 00:01:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.38 on epoch=724
03/11/2022 00:01:22 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.6862745098039216 on epoch=724
03/11/2022 00:01:22 - INFO - __main__ - Saving model with best Classification-F1: 0.539313399778516 -> 0.6862745098039216 on epoch=724, global_step=1450
03/11/2022 00:01:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.36 on epoch=729
03/11/2022 00:01:28 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=734
03/11/2022 00:01:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.39 on epoch=739
03/11/2022 00:01:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.41 on epoch=744
03/11/2022 00:01:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.36 on epoch=749
03/11/2022 00:01:38 - INFO - __main__ - Global step 1500 Train loss 0.39 Classification-F1 0.3816425120772947 on epoch=749
03/11/2022 00:01:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.37 on epoch=754
03/11/2022 00:01:44 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.47 on epoch=759
03/11/2022 00:01:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.39 on epoch=764
03/11/2022 00:01:50 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.41 on epoch=769
03/11/2022 00:01:53 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.38 on epoch=774
03/11/2022 00:01:54 - INFO - __main__ - Global step 1550 Train loss 0.40 Classification-F1 0.4385964912280702 on epoch=774
03/11/2022 00:01:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.35 on epoch=779
03/11/2022 00:02:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
03/11/2022 00:02:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=789
03/11/2022 00:02:06 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
03/11/2022 00:02:09 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.35 on epoch=799
03/11/2022 00:02:09 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.4385964912280702 on epoch=799
03/11/2022 00:02:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.41 on epoch=804
03/11/2022 00:02:16 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
03/11/2022 00:02:19 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.34 on epoch=814
03/11/2022 00:02:22 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.40 on epoch=819
03/11/2022 00:02:25 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.50 on epoch=824
03/11/2022 00:02:25 - INFO - __main__ - Global step 1650 Train loss 0.41 Classification-F1 0.5588547189819725 on epoch=824
03/11/2022 00:02:28 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.32 on epoch=829
03/11/2022 00:02:31 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.42 on epoch=834
03/11/2022 00:02:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.37 on epoch=839
03/11/2022 00:02:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.39 on epoch=844
03/11/2022 00:02:40 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=849
03/11/2022 00:02:41 - INFO - __main__ - Global step 1700 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=849
03/11/2022 00:02:44 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
03/11/2022 00:02:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=859
03/11/2022 00:02:50 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=864
03/11/2022 00:02:53 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
03/11/2022 00:02:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.46 on epoch=874
03/11/2022 00:02:57 - INFO - __main__ - Global step 1750 Train loss 0.41 Classification-F1 0.3816425120772947 on epoch=874
03/11/2022 00:03:00 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.39 on epoch=879
03/11/2022 00:03:03 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.39 on epoch=884
03/11/2022 00:03:06 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.39 on epoch=889
03/11/2022 00:03:09 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.41 on epoch=894
03/11/2022 00:03:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.32 on epoch=899
03/11/2022 00:03:12 - INFO - __main__ - Global step 1800 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=899
03/11/2022 00:03:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.36 on epoch=904
03/11/2022 00:03:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=909
03/11/2022 00:03:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.37 on epoch=914
03/11/2022 00:03:24 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
03/11/2022 00:03:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.40 on epoch=924
03/11/2022 00:03:28 - INFO - __main__ - Global step 1850 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=924
03/11/2022 00:03:31 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
03/11/2022 00:03:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.35 on epoch=934
03/11/2022 00:03:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.41 on epoch=939
03/11/2022 00:03:40 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
03/11/2022 00:03:43 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.38 on epoch=949
03/11/2022 00:03:44 - INFO - __main__ - Global step 1900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=949
03/11/2022 00:03:47 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=954
03/11/2022 00:03:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.40 on epoch=959
03/11/2022 00:03:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.40 on epoch=964
03/11/2022 00:03:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.42 on epoch=969
03/11/2022 00:03:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.35 on epoch=974
03/11/2022 00:03:59 - INFO - __main__ - Global step 1950 Train loss 0.38 Classification-F1 0.3191489361702127 on epoch=974
03/11/2022 00:04:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
03/11/2022 00:04:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.37 on epoch=984
03/11/2022 00:04:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.34 on epoch=989
03/11/2022 00:04:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.39 on epoch=994
03/11/2022 00:04:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.39 on epoch=999
03/11/2022 00:04:15 - INFO - __main__ - Global step 2000 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=999
03/11/2022 00:04:15 - INFO - __main__ - save last model!
03/11/2022 00:04:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 00:04:15 - INFO - __main__ - Start tokenizing ... 1000 instances
03/11/2022 00:04:15 - INFO - __main__ - Printing 3 examples
03/11/2022 00:04:15 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/11/2022 00:04:15 - INFO - __main__ - ['negative']
03/11/2022 00:04:15 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/11/2022 00:04:15 - INFO - __main__ - ['negative']
03/11/2022 00:04:15 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/11/2022 00:04:15 - INFO - __main__ - ['negative']
03/11/2022 00:04:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:04:16 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:04:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:04:16 - INFO - __main__ - Printing 3 examples
03/11/2022 00:04:16 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/11/2022 00:04:16 - INFO - __main__ - ['negative']
03/11/2022 00:04:16 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/11/2022 00:04:16 - INFO - __main__ - ['negative']
03/11/2022 00:04:16 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/11/2022 00:04:16 - INFO - __main__ - ['negative']
03/11/2022 00:04:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 00:04:16 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:04:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:04:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:04:16 - INFO - __main__ - Printing 3 examples
03/11/2022 00:04:16 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/11/2022 00:04:16 - INFO - __main__ - ['negative']
03/11/2022 00:04:16 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/11/2022 00:04:16 - INFO - __main__ - ['negative']
03/11/2022 00:04:16 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/11/2022 00:04:16 - INFO - __main__ - ['negative']
03/11/2022 00:04:16 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:04:16 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:04:16 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:04:17 - INFO - __main__ - Loaded 1000 examples from test data
03/11/2022 00:04:30 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:04:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:04:31 - INFO - __main__ - Starting training!
03/11/2022 00:04:35 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_42_0.3_8_predictions.txt
03/11/2022 00:04:35 - INFO - __main__ - Classification-F1 on test data: 0.4302
03/11/2022 00:04:35 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.3, bsz=8, dev_performance=0.6862745098039216, test_performance=0.4301568003990149
03/11/2022 00:04:35 - INFO - __main__ - Running ... prefix=amazon_polarity_16_42, lr=0.2, bsz=8 ...
03/11/2022 00:04:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:04:36 - INFO - __main__ - Printing 3 examples
03/11/2022 00:04:36 - INFO - __main__ -  [amazon_polarity] title: The special effects were cool... [SEP] content: And that was about it. The first hour was filled with talking about the research team that landed on the planet. When the aliens attacked, they did things that any idiot would never do and they just died. The only character that I actually liked was the one who had the weird eyes (the killer). The creators also didn't put into account that the aliens were hurt by light. Only once did the light bother them, and that one was already dead. The only reason that I gave this 2 stars was because the special effects saved it. The rest was stupid. My suggestion is to never see this movie...ever.
03/11/2022 00:04:36 - INFO - __main__ - ['negative']
03/11/2022 00:04:36 - INFO - __main__ -  [amazon_polarity] title: bad vacume advance [SEP] content: had the dissy in my car and had lots of trouble with it at idle played with the cam and dissy settings with no luck, finally pulled it out and sent it off to be tested and got a report back that it was pulling on 40 degrees of advance at idle.had it adjusted and re curved for my application and now the problem is solved, Not very happy for what is meant to be a quality American product.I recommend if you purchase one have it tested and set to suit your application other wise you may have no end of troubles.
03/11/2022 00:04:36 - INFO - __main__ - ['negative']
03/11/2022 00:04:36 - INFO - __main__ -  [amazon_polarity] title: Vulgar, distasteful, dumb [SEP] content: There's no doubt that Popa Chubby has a fair amount of talent, but "Booty & The Beast" suffers from guitar hero stylings and a lack of taste. The CD is just like Chubby's live shows-brash, outlandish, and just short of the type of thing that could give New York City a bad name in the blues forever. Avoid at all costs.
03/11/2022 00:04:36 - INFO - __main__ - ['negative']
03/11/2022 00:04:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:04:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:04:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:04:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:04:36 - INFO - __main__ - Printing 3 examples
03/11/2022 00:04:36 - INFO - __main__ -  [amazon_polarity] title: Fragrence Review [SEP] content: The product is fine. The scent a little weak. The bottle is not a shape that is easy to use. Possible a little pricey.
03/11/2022 00:04:36 - INFO - __main__ - ['negative']
03/11/2022 00:04:36 - INFO - __main__ -  [amazon_polarity] title: "Not in stock" [SEP] content: "Not in stock" is what I always eventually hear when trying to get this product, a-f-t-e-r, of course, I complete an order and have been waiting a week. This product is no longer being made! I put together a state of the art computer in 2004 using this leading edge ram, made by Kingston no less, and it is now no longer made!!! Spend half a grand on a computer and throw it away in 6 years!Beware, if you are lucky enough to get something at a premium from some collector then make sure you realize that it comes as a kit that contains (2) sticks of 1 GB ram. Not every vendor understands this and may only send you half of the kit (1 GB stick of ram).
03/11/2022 00:04:36 - INFO - __main__ - ['negative']
03/11/2022 00:04:36 - INFO - __main__ -  [amazon_polarity] title: easy set up [SEP] content: Nothing could be easier to set up than this remote and very easy to operate.Support is there with a phone call for missing codes, fully programmable and a learning remote as well. My main beef is this thing is a pig on batteries!Hyper sensitive for example while watching a movie the remote well turn on from the vibrations of the sub woofers and the remote is sitting on a pillow. If the remote is on a coffee table any one walks into the room the remote will turn on. While it is only on for 10 secs. it still is on and off constantly. Sent an e-mail to Universal Electronics on a way to reduce the sensitivity but received no responce, which I translate to meaning "not possible" Buyer beware and perhaps wait for the next model and perhaps this problem will be corrected.
03/11/2022 00:04:36 - INFO - __main__ - ['negative']
03/11/2022 00:04:36 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:04:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:04:36 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:04:49 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:04:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:04:50 - INFO - __main__ - Starting training!
03/11/2022 00:04:55 - INFO - __main__ - Step 10 Global step 10 Train loss 1.66 on epoch=4
03/11/2022 00:04:58 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=9
03/11/2022 00:05:01 - INFO - __main__ - Step 30 Global step 30 Train loss 0.54 on epoch=14
03/11/2022 00:05:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=19
03/11/2022 00:05:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.47 on epoch=24
03/11/2022 00:05:08 - INFO - __main__ - Global step 50 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 00:05:08 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 00:05:11 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=29
03/11/2022 00:05:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.46 on epoch=34
03/11/2022 00:05:17 - INFO - __main__ - Step 80 Global step 80 Train loss 0.41 on epoch=39
03/11/2022 00:05:20 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=44
03/11/2022 00:05:23 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=49
03/11/2022 00:05:24 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 00:05:27 - INFO - __main__ - Step 110 Global step 110 Train loss 0.40 on epoch=54
03/11/2022 00:05:30 - INFO - __main__ - Step 120 Global step 120 Train loss 0.39 on epoch=59
03/11/2022 00:05:33 - INFO - __main__ - Step 130 Global step 130 Train loss 0.40 on epoch=64
03/11/2022 00:05:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=69
03/11/2022 00:05:39 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
03/11/2022 00:05:40 - INFO - __main__ - Global step 150 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 00:05:43 - INFO - __main__ - Step 160 Global step 160 Train loss 0.38 on epoch=79
03/11/2022 00:05:46 - INFO - __main__ - Step 170 Global step 170 Train loss 0.39 on epoch=84
03/11/2022 00:05:49 - INFO - __main__ - Step 180 Global step 180 Train loss 0.37 on epoch=89
03/11/2022 00:05:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
03/11/2022 00:05:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.38 on epoch=99
03/11/2022 00:05:55 - INFO - __main__ - Global step 200 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 00:05:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
03/11/2022 00:06:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
03/11/2022 00:06:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.41 on epoch=114
03/11/2022 00:06:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.39 on epoch=119
03/11/2022 00:06:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
03/11/2022 00:06:11 - INFO - __main__ - Global step 250 Train loss 0.38 Classification-F1 0.7810361681329424 on epoch=124
03/11/2022 00:06:11 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.7810361681329424 on epoch=124, global_step=250
03/11/2022 00:06:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=129
03/11/2022 00:06:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.42 on epoch=134
03/11/2022 00:06:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
03/11/2022 00:06:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.37 on epoch=144
03/11/2022 00:06:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=149
03/11/2022 00:06:27 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.6559139784946237 on epoch=149
03/11/2022 00:06:30 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=154
03/11/2022 00:06:33 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=159
03/11/2022 00:06:36 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=164
03/11/2022 00:06:39 - INFO - __main__ - Step 340 Global step 340 Train loss 0.40 on epoch=169
03/11/2022 00:06:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.37 on epoch=174
03/11/2022 00:06:42 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.3992490613266583 on epoch=174
03/11/2022 00:06:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.36 on epoch=179
03/11/2022 00:06:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=184
03/11/2022 00:06:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=189
03/11/2022 00:06:55 - INFO - __main__ - Step 390 Global step 390 Train loss 0.35 on epoch=194
03/11/2022 00:06:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=199
03/11/2022 00:06:58 - INFO - __main__ - Global step 400 Train loss 0.35 Classification-F1 0.3992490613266583 on epoch=199
03/11/2022 00:07:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.35 on epoch=204
03/11/2022 00:07:04 - INFO - __main__ - Step 420 Global step 420 Train loss 0.37 on epoch=209
03/11/2022 00:07:07 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=214
03/11/2022 00:07:10 - INFO - __main__ - Step 440 Global step 440 Train loss 0.35 on epoch=219
03/11/2022 00:07:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.32 on epoch=224
03/11/2022 00:07:14 - INFO - __main__ - Global step 450 Train loss 0.35 Classification-F1 0.4385964912280702 on epoch=224
03/11/2022 00:07:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
03/11/2022 00:07:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=234
03/11/2022 00:07:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=239
03/11/2022 00:07:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=244
03/11/2022 00:07:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.36 on epoch=249
03/11/2022 00:07:30 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.4385964912280702 on epoch=249
03/11/2022 00:07:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
03/11/2022 00:07:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=259
03/11/2022 00:07:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.32 on epoch=264
03/11/2022 00:07:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.32 on epoch=269
03/11/2022 00:07:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.33 on epoch=274
03/11/2022 00:07:46 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.49090909090909085 on epoch=274
03/11/2022 00:07:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=279
03/11/2022 00:07:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.36 on epoch=284
03/11/2022 00:07:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=289
03/11/2022 00:07:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.32 on epoch=294
03/11/2022 00:08:01 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=299
03/11/2022 00:08:02 - INFO - __main__ - Global step 600 Train loss 0.33 Classification-F1 0.6761133603238867 on epoch=299
03/11/2022 00:08:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=304
03/11/2022 00:08:08 - INFO - __main__ - Step 620 Global step 620 Train loss 0.35 on epoch=309
03/11/2022 00:08:11 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=314
03/11/2022 00:08:14 - INFO - __main__ - Step 640 Global step 640 Train loss 0.30 on epoch=319
03/11/2022 00:08:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=324
03/11/2022 00:08:17 - INFO - __main__ - Global step 650 Train loss 0.32 Classification-F1 0.5151515151515151 on epoch=324
03/11/2022 00:08:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.33 on epoch=329
03/11/2022 00:08:23 - INFO - __main__ - Step 670 Global step 670 Train loss 0.30 on epoch=334
03/11/2022 00:08:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=339
03/11/2022 00:08:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.32 on epoch=344
03/11/2022 00:08:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.29 on epoch=349
03/11/2022 00:08:33 - INFO - __main__ - Global step 700 Train loss 0.31 Classification-F1 0.4181818181818182 on epoch=349
03/11/2022 00:08:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.37 on epoch=354
03/11/2022 00:08:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.31 on epoch=359
03/11/2022 00:08:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=364
03/11/2022 00:08:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.32 on epoch=369
03/11/2022 00:08:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=374
03/11/2022 00:08:49 - INFO - __main__ - Global step 750 Train loss 0.32 Classification-F1 0.4385964912280702 on epoch=374
03/11/2022 00:08:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.31 on epoch=379
03/11/2022 00:08:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=384
03/11/2022 00:08:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.31 on epoch=389
03/11/2022 00:09:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.30 on epoch=394
03/11/2022 00:09:04 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=399
03/11/2022 00:09:05 - INFO - __main__ - Global step 800 Train loss 0.30 Classification-F1 0.7490196078431373 on epoch=399
03/11/2022 00:09:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.30 on epoch=404
03/11/2022 00:09:11 - INFO - __main__ - Step 820 Global step 820 Train loss 0.30 on epoch=409
03/11/2022 00:09:14 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=414
03/11/2022 00:09:17 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=419
03/11/2022 00:09:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.32 on epoch=424
03/11/2022 00:09:21 - INFO - __main__ - Global step 850 Train loss 0.29 Classification-F1 0.7046153846153846 on epoch=424
03/11/2022 00:09:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=429
03/11/2022 00:09:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.28 on epoch=434
03/11/2022 00:09:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=439
03/11/2022 00:09:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.29 on epoch=444
03/11/2022 00:09:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
03/11/2022 00:09:37 - INFO - __main__ - Global step 900 Train loss 0.28 Classification-F1 0.6476476476476476 on epoch=449
03/11/2022 00:09:40 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
03/11/2022 00:09:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.39 on epoch=459
03/11/2022 00:09:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.47 on epoch=464
03/11/2022 00:09:49 - INFO - __main__ - Step 940 Global step 940 Train loss 0.63 on epoch=469
03/11/2022 00:09:52 - INFO - __main__ - Step 950 Global step 950 Train loss 1.39 on epoch=474
03/11/2022 00:09:52 - INFO - __main__ - Global step 950 Train loss 0.62 Classification-F1 0.7184750733137829 on epoch=474
03/11/2022 00:09:55 - INFO - __main__ - Step 960 Global step 960 Train loss 1.26 on epoch=479
03/11/2022 00:09:58 - INFO - __main__ - Step 970 Global step 970 Train loss 2.53 on epoch=484
03/11/2022 00:10:02 - INFO - __main__ - Step 980 Global step 980 Train loss 1.27 on epoch=489
03/11/2022 00:10:05 - INFO - __main__ - Step 990 Global step 990 Train loss 3.19 on epoch=494
03/11/2022 00:10:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.33 on epoch=499
03/11/2022 00:10:08 - INFO - __main__ - Global step 1000 Train loss 1.92 Classification-F1 0.6862745098039216 on epoch=499
03/11/2022 00:10:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.83 on epoch=504
03/11/2022 00:10:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.26 on epoch=509
03/11/2022 00:10:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.97 on epoch=514
03/11/2022 00:10:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.59 on epoch=519
03/11/2022 00:10:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.85 on epoch=524
03/11/2022 00:10:24 - INFO - __main__ - Global step 1050 Train loss 1.10 Classification-F1 0.5134502923976608 on epoch=524
03/11/2022 00:10:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.90 on epoch=529
03/11/2022 00:10:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
03/11/2022 00:10:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.47 on epoch=539
03/11/2022 00:10:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.38 on epoch=544
03/11/2022 00:10:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=549
03/11/2022 00:10:40 - INFO - __main__ - Global step 1100 Train loss 0.55 Classification-F1 0.3992490613266583 on epoch=549
03/11/2022 00:10:43 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.33 on epoch=554
03/11/2022 00:10:46 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.48 on epoch=559
03/11/2022 00:10:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.70 on epoch=564
03/11/2022 00:10:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=569
03/11/2022 00:10:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=574
03/11/2022 00:10:55 - INFO - __main__ - Global step 1150 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=574
03/11/2022 00:10:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
03/11/2022 00:11:02 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.50 on epoch=584
03/11/2022 00:11:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
03/11/2022 00:11:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=594
03/11/2022 00:11:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.95 on epoch=599
03/11/2022 00:11:11 - INFO - __main__ - Global step 1200 Train loss 0.57 Classification-F1 0.4589371980676329 on epoch=599
03/11/2022 00:11:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.90 on epoch=604
03/11/2022 00:11:17 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.06 on epoch=609
03/11/2022 00:11:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.69 on epoch=614
03/11/2022 00:11:23 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.60 on epoch=619
03/11/2022 00:11:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.62 on epoch=624
03/11/2022 00:11:27 - INFO - __main__ - Global step 1250 Train loss 0.77 Classification-F1 0.3333333333333333 on epoch=624
03/11/2022 00:11:30 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.47 on epoch=629
03/11/2022 00:11:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.44 on epoch=634
03/11/2022 00:11:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.41 on epoch=639
03/11/2022 00:11:39 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=644
03/11/2022 00:11:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.57 on epoch=649
03/11/2022 00:11:43 - INFO - __main__ - Global step 1300 Train loss 0.48 Classification-F1 0.3333333333333333 on epoch=649
03/11/2022 00:11:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.49 on epoch=654
03/11/2022 00:11:49 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.57 on epoch=659
03/11/2022 00:11:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.59 on epoch=664
03/11/2022 00:11:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.55 on epoch=669
03/11/2022 00:11:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.78 on epoch=674
03/11/2022 00:11:59 - INFO - __main__ - Global step 1350 Train loss 0.60 Classification-F1 0.3992490613266583 on epoch=674
03/11/2022 00:12:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.53 on epoch=679
03/11/2022 00:12:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.38 on epoch=684
03/11/2022 00:12:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.45 on epoch=689
03/11/2022 00:12:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=694
03/11/2022 00:12:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.36 on epoch=699
03/11/2022 00:12:15 - INFO - __main__ - Global step 1400 Train loss 0.42 Classification-F1 0.3992490613266583 on epoch=699
03/11/2022 00:12:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=704
03/11/2022 00:12:21 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.41 on epoch=709
03/11/2022 00:12:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.38 on epoch=714
03/11/2022 00:12:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.44 on epoch=719
03/11/2022 00:12:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.31 on epoch=724
03/11/2022 00:12:31 - INFO - __main__ - Global step 1450 Train loss 0.39 Classification-F1 0.3992490613266583 on epoch=724
03/11/2022 00:12:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.42 on epoch=729
03/11/2022 00:12:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.42 on epoch=734
03/11/2022 00:12:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.32 on epoch=739
03/11/2022 00:12:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=744
03/11/2022 00:12:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.34 on epoch=749
03/11/2022 00:12:47 - INFO - __main__ - Global step 1500 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=749
03/11/2022 00:12:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
03/11/2022 00:12:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.35 on epoch=759
03/11/2022 00:12:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.32 on epoch=764
03/11/2022 00:12:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.35 on epoch=769
03/11/2022 00:13:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.32 on epoch=774
03/11/2022 00:13:03 - INFO - __main__ - Global step 1550 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=774
03/11/2022 00:13:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.37 on epoch=779
03/11/2022 00:13:09 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.36 on epoch=784
03/11/2022 00:13:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.36 on epoch=789
03/11/2022 00:13:15 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.34 on epoch=794
03/11/2022 00:13:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.38 on epoch=799
03/11/2022 00:13:19 - INFO - __main__ - Global step 1600 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=799
03/11/2022 00:13:22 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.37 on epoch=804
03/11/2022 00:13:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.35 on epoch=809
03/11/2022 00:13:28 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
03/11/2022 00:13:32 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.37 on epoch=819
03/11/2022 00:13:35 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.37 on epoch=824
03/11/2022 00:13:35 - INFO - __main__ - Global step 1650 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=824
03/11/2022 00:13:38 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.35 on epoch=829
03/11/2022 00:13:41 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
03/11/2022 00:13:45 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.34 on epoch=839
03/11/2022 00:13:48 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.32 on epoch=844
03/11/2022 00:13:51 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.34 on epoch=849
03/11/2022 00:13:51 - INFO - __main__ - Global step 1700 Train loss 0.33 Classification-F1 0.4589371980676329 on epoch=849
03/11/2022 00:13:54 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.33 on epoch=854
03/11/2022 00:13:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.32 on epoch=859
03/11/2022 00:14:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.35 on epoch=864
03/11/2022 00:14:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.34 on epoch=869
03/11/2022 00:14:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.38 on epoch=874
03/11/2022 00:14:07 - INFO - __main__ - Global step 1750 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=874
03/11/2022 00:14:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=879
03/11/2022 00:14:14 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.33 on epoch=884
03/11/2022 00:14:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
03/11/2022 00:14:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
03/11/2022 00:14:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.33 on epoch=899
03/11/2022 00:14:23 - INFO - __main__ - Global step 1800 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=899
03/11/2022 00:14:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.38 on epoch=904
03/11/2022 00:14:30 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.32 on epoch=909
03/11/2022 00:14:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.33 on epoch=914
03/11/2022 00:14:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.31 on epoch=919
03/11/2022 00:14:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
03/11/2022 00:14:39 - INFO - __main__ - Global step 1850 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=924
03/11/2022 00:14:43 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
03/11/2022 00:14:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=934
03/11/2022 00:14:49 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.36 on epoch=939
03/11/2022 00:14:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.33 on epoch=944
03/11/2022 00:14:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.31 on epoch=949
03/11/2022 00:14:55 - INFO - __main__ - Global step 1900 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=949
03/11/2022 00:14:58 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.33 on epoch=954
03/11/2022 00:15:01 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
03/11/2022 00:15:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.32 on epoch=964
03/11/2022 00:15:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.34 on epoch=969
03/11/2022 00:15:10 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=974
03/11/2022 00:15:11 - INFO - __main__ - Global step 1950 Train loss 0.34 Classification-F1 0.3992490613266583 on epoch=974
03/11/2022 00:15:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.36 on epoch=979
03/11/2022 00:15:17 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
03/11/2022 00:15:20 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.32 on epoch=989
03/11/2022 00:15:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
03/11/2022 00:15:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
03/11/2022 00:15:27 - INFO - __main__ - Global step 2000 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=999
03/11/2022 00:15:27 - INFO - __main__ - save last model!
03/11/2022 00:15:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 00:15:27 - INFO - __main__ - Start tokenizing ... 1000 instances
03/11/2022 00:15:27 - INFO - __main__ - Printing 3 examples
03/11/2022 00:15:27 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/11/2022 00:15:27 - INFO - __main__ - ['negative']
03/11/2022 00:15:27 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/11/2022 00:15:27 - INFO - __main__ - ['negative']
03/11/2022 00:15:27 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/11/2022 00:15:27 - INFO - __main__ - ['negative']
03/11/2022 00:15:27 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:15:27 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:15:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:15:27 - INFO - __main__ - Printing 3 examples
03/11/2022 00:15:27 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/11/2022 00:15:27 - INFO - __main__ - ['negative']
03/11/2022 00:15:27 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/11/2022 00:15:27 - INFO - __main__ - ['negative']
03/11/2022 00:15:27 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/11/2022 00:15:27 - INFO - __main__ - ['negative']
03/11/2022 00:15:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 00:15:27 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:15:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:15:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:15:28 - INFO - __main__ - Printing 3 examples
03/11/2022 00:15:28 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/11/2022 00:15:28 - INFO - __main__ - ['negative']
03/11/2022 00:15:28 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/11/2022 00:15:28 - INFO - __main__ - ['negative']
03/11/2022 00:15:28 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/11/2022 00:15:28 - INFO - __main__ - ['negative']
03/11/2022 00:15:28 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:15:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:15:28 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:15:28 - INFO - __main__ - Loaded 1000 examples from test data
03/11/2022 00:15:41 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:15:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:15:42 - INFO - __main__ - Starting training!
03/11/2022 00:15:46 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_42_0.2_8_predictions.txt
03/11/2022 00:15:46 - INFO - __main__ - Classification-F1 on test data: 0.5288
03/11/2022 00:15:47 - INFO - __main__ - prefix=amazon_polarity_16_42, lr=0.2, bsz=8, dev_performance=0.7810361681329424, test_performance=0.5288473893862167
03/11/2022 00:15:47 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.5, bsz=8 ...
03/11/2022 00:15:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:15:47 - INFO - __main__ - Printing 3 examples
03/11/2022 00:15:47 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/11/2022 00:15:47 - INFO - __main__ - ['negative']
03/11/2022 00:15:47 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/11/2022 00:15:47 - INFO - __main__ - ['negative']
03/11/2022 00:15:47 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/11/2022 00:15:47 - INFO - __main__ - ['negative']
03/11/2022 00:15:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:15:47 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:15:48 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:15:48 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:15:48 - INFO - __main__ - Printing 3 examples
03/11/2022 00:15:48 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/11/2022 00:15:48 - INFO - __main__ - ['negative']
03/11/2022 00:15:48 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/11/2022 00:15:48 - INFO - __main__ - ['negative']
03/11/2022 00:15:48 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/11/2022 00:15:48 - INFO - __main__ - ['negative']
03/11/2022 00:15:48 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:15:48 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:15:48 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:16:01 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:16:01 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:16:01 - INFO - __main__ - Starting training!
03/11/2022 00:16:05 - INFO - __main__ - Step 10 Global step 10 Train loss 1.22 on epoch=4
03/11/2022 00:16:08 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=9
03/11/2022 00:16:11 - INFO - __main__ - Step 30 Global step 30 Train loss 0.60 on epoch=14
03/11/2022 00:16:14 - INFO - __main__ - Step 40 Global step 40 Train loss 0.69 on epoch=19
03/11/2022 00:16:17 - INFO - __main__ - Step 50 Global step 50 Train loss 0.78 on epoch=24
03/11/2022 00:16:18 - INFO - __main__ - Global step 50 Train loss 0.76 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 00:16:18 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 00:16:21 - INFO - __main__ - Step 60 Global step 60 Train loss 0.64 on epoch=29
03/11/2022 00:16:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.49 on epoch=34
03/11/2022 00:16:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=39
03/11/2022 00:16:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=44
03/11/2022 00:16:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.50 on epoch=49
03/11/2022 00:16:33 - INFO - __main__ - Global step 100 Train loss 0.53 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 00:16:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.45 on epoch=54
03/11/2022 00:16:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=59
03/11/2022 00:16:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
03/11/2022 00:16:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.41 on epoch=69
03/11/2022 00:16:48 - INFO - __main__ - Step 150 Global step 150 Train loss 0.42 on epoch=74
03/11/2022 00:16:48 - INFO - __main__ - Global step 150 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 00:16:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
03/11/2022 00:16:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.42 on epoch=84
03/11/2022 00:16:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
03/11/2022 00:17:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=94
03/11/2022 00:17:03 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=99
03/11/2022 00:17:03 - INFO - __main__ - Global step 200 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 00:17:06 - INFO - __main__ - Step 210 Global step 210 Train loss 0.43 on epoch=104
03/11/2022 00:17:09 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
03/11/2022 00:17:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.38 on epoch=114
03/11/2022 00:17:15 - INFO - __main__ - Step 240 Global step 240 Train loss 0.44 on epoch=119
03/11/2022 00:17:18 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
03/11/2022 00:17:18 - INFO - __main__ - Global step 250 Train loss 0.41 Classification-F1 0.3992490613266583 on epoch=124
03/11/2022 00:17:18 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3992490613266583 on epoch=124, global_step=250
03/11/2022 00:17:21 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
03/11/2022 00:17:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.40 on epoch=134
03/11/2022 00:17:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
03/11/2022 00:17:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=144
03/11/2022 00:17:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.40 on epoch=149
03/11/2022 00:17:33 - INFO - __main__ - Global step 300 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 00:17:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.39 on epoch=154
03/11/2022 00:17:39 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=159
03/11/2022 00:17:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=164
03/11/2022 00:17:45 - INFO - __main__ - Step 340 Global step 340 Train loss 0.42 on epoch=169
03/11/2022 00:17:48 - INFO - __main__ - Step 350 Global step 350 Train loss 0.38 on epoch=174
03/11/2022 00:17:48 - INFO - __main__ - Global step 350 Train loss 0.39 Classification-F1 0.3992490613266583 on epoch=174
03/11/2022 00:17:51 - INFO - __main__ - Step 360 Global step 360 Train loss 0.38 on epoch=179
03/11/2022 00:17:54 - INFO - __main__ - Step 370 Global step 370 Train loss 0.34 on epoch=184
03/11/2022 00:17:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.36 on epoch=189
03/11/2022 00:18:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=194
03/11/2022 00:18:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
03/11/2022 00:18:03 - INFO - __main__ - Global step 400 Train loss 0.36 Classification-F1 0.3992490613266583 on epoch=199
03/11/2022 00:18:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=204
03/11/2022 00:18:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.38 on epoch=209
03/11/2022 00:18:12 - INFO - __main__ - Step 430 Global step 430 Train loss 0.35 on epoch=214
03/11/2022 00:18:15 - INFO - __main__ - Step 440 Global step 440 Train loss 0.41 on epoch=219
03/11/2022 00:18:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.52 on epoch=224
03/11/2022 00:18:18 - INFO - __main__ - Global step 450 Train loss 0.40 Classification-F1 0.6113360323886641 on epoch=224
03/11/2022 00:18:19 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.6113360323886641 on epoch=224, global_step=450
03/11/2022 00:18:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.47 on epoch=229
03/11/2022 00:18:24 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=234
03/11/2022 00:18:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.37 on epoch=239
03/11/2022 00:18:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.38 on epoch=244
03/11/2022 00:18:33 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=249
03/11/2022 00:18:34 - INFO - __main__ - Global step 500 Train loss 0.39 Classification-F1 0.5588547189819725 on epoch=249
03/11/2022 00:18:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.45 on epoch=254
03/11/2022 00:18:39 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=259
03/11/2022 00:18:42 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=264
03/11/2022 00:18:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.37 on epoch=269
03/11/2022 00:18:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.39 on epoch=274
03/11/2022 00:18:49 - INFO - __main__ - Global step 550 Train loss 0.40 Classification-F1 0.3816425120772947 on epoch=274
03/11/2022 00:18:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.36 on epoch=279
03/11/2022 00:18:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.37 on epoch=284
03/11/2022 00:18:57 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=289
03/11/2022 00:19:00 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=294
03/11/2022 00:19:03 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=299
03/11/2022 00:19:03 - INFO - __main__ - Global step 600 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=299
03/11/2022 00:19:06 - INFO - __main__ - Step 610 Global step 610 Train loss 0.42 on epoch=304
03/11/2022 00:19:09 - INFO - __main__ - Step 620 Global step 620 Train loss 0.39 on epoch=309
03/11/2022 00:19:12 - INFO - __main__ - Step 630 Global step 630 Train loss 0.38 on epoch=314
03/11/2022 00:19:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.36 on epoch=319
03/11/2022 00:19:18 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=324
03/11/2022 00:19:18 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=324
03/11/2022 00:19:21 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=329
03/11/2022 00:19:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.37 on epoch=334
03/11/2022 00:19:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=339
03/11/2022 00:19:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.37 on epoch=344
03/11/2022 00:19:33 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=349
03/11/2022 00:19:33 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=349
03/11/2022 00:19:36 - INFO - __main__ - Step 710 Global step 710 Train loss 0.36 on epoch=354
03/11/2022 00:19:39 - INFO - __main__ - Step 720 Global step 720 Train loss 0.40 on epoch=359
03/11/2022 00:19:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
03/11/2022 00:19:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.39 on epoch=369
03/11/2022 00:19:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.49 on epoch=374
03/11/2022 00:19:49 - INFO - __main__ - Global step 750 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=374
03/11/2022 00:19:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=379
03/11/2022 00:19:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=384
03/11/2022 00:19:57 - INFO - __main__ - Step 780 Global step 780 Train loss 0.42 on epoch=389
03/11/2022 00:20:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.45 on epoch=394
03/11/2022 00:20:03 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=399
03/11/2022 00:20:04 - INFO - __main__ - Global step 800 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=399
03/11/2022 00:20:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.59 on epoch=404
03/11/2022 00:20:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=409
03/11/2022 00:20:12 - INFO - __main__ - Step 830 Global step 830 Train loss 0.41 on epoch=414
03/11/2022 00:20:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=419
03/11/2022 00:20:18 - INFO - __main__ - Step 850 Global step 850 Train loss 0.43 on epoch=424
03/11/2022 00:20:19 - INFO - __main__ - Global step 850 Train loss 0.44 Classification-F1 0.3333333333333333 on epoch=424
03/11/2022 00:20:22 - INFO - __main__ - Step 860 Global step 860 Train loss 0.37 on epoch=429
03/11/2022 00:20:25 - INFO - __main__ - Step 870 Global step 870 Train loss 0.37 on epoch=434
03/11/2022 00:20:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.36 on epoch=439
03/11/2022 00:20:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.37 on epoch=444
03/11/2022 00:20:33 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=449
03/11/2022 00:20:34 - INFO - __main__ - Global step 900 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=449
03/11/2022 00:20:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=454
03/11/2022 00:20:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.38 on epoch=459
03/11/2022 00:20:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=464
03/11/2022 00:20:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=469
03/11/2022 00:20:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
03/11/2022 00:20:49 - INFO - __main__ - Global step 950 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=474
03/11/2022 00:20:52 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=479
03/11/2022 00:20:55 - INFO - __main__ - Step 970 Global step 970 Train loss 0.37 on epoch=484
03/11/2022 00:20:58 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=489
03/11/2022 00:21:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=494
03/11/2022 00:21:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=499
03/11/2022 00:21:04 - INFO - __main__ - Global step 1000 Train loss 0.37 Classification-F1 0.46843853820598 on epoch=499
03/11/2022 00:21:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.37 on epoch=504
03/11/2022 00:21:10 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.44 on epoch=509
03/11/2022 00:21:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.53 on epoch=514
03/11/2022 00:21:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.73 on epoch=519
03/11/2022 00:21:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.39 on epoch=524
03/11/2022 00:21:19 - INFO - __main__ - Global step 1050 Train loss 0.49 Classification-F1 0.3992490613266583 on epoch=524
03/11/2022 00:21:22 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.71 on epoch=529
03/11/2022 00:21:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.11 on epoch=534
03/11/2022 00:21:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.07 on epoch=539
03/11/2022 00:21:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.97 on epoch=544
03/11/2022 00:21:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.95 on epoch=549
03/11/2022 00:21:34 - INFO - __main__ - Global step 1100 Train loss 1.96 Classification-F1 0.716256157635468 on epoch=549
03/11/2022 00:21:34 - INFO - __main__ - Saving model with best Classification-F1: 0.6113360323886641 -> 0.716256157635468 on epoch=549, global_step=1100
03/11/2022 00:21:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.46 on epoch=554
03/11/2022 00:21:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 4.45 on epoch=559
03/11/2022 00:21:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.83 on epoch=564
03/11/2022 00:21:46 - INFO - __main__ - Step 1140 Global step 1140 Train loss 2.77 on epoch=569
03/11/2022 00:21:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.93 on epoch=574
03/11/2022 00:21:49 - INFO - __main__ - Global step 1150 Train loss 3.29 Classification-F1 0.6190476190476191 on epoch=574
03/11/2022 00:21:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.45 on epoch=579
03/11/2022 00:21:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.20 on epoch=584
03/11/2022 00:21:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.88 on epoch=589
03/11/2022 00:22:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.82 on epoch=594
03/11/2022 00:22:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.25 on epoch=599
03/11/2022 00:22:05 - INFO - __main__ - Global step 1200 Train loss 1.72 Classification-F1 0.3333333333333333 on epoch=599
03/11/2022 00:22:07 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.95 on epoch=604
03/11/2022 00:22:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.32 on epoch=609
03/11/2022 00:22:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.20 on epoch=614
03/11/2022 00:22:16 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.95 on epoch=619
03/11/2022 00:22:19 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.03 on epoch=624
03/11/2022 00:22:20 - INFO - __main__ - Global step 1250 Train loss 1.29 Classification-F1 0.7046153846153846 on epoch=624
03/11/2022 00:22:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.67 on epoch=629
03/11/2022 00:22:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.62 on epoch=634
03/11/2022 00:22:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.65 on epoch=639
03/11/2022 00:22:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.63 on epoch=644
03/11/2022 00:22:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.69 on epoch=649
03/11/2022 00:22:35 - INFO - __main__ - Global step 1300 Train loss 0.65 Classification-F1 0.6666666666666667 on epoch=649
03/11/2022 00:22:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.75 on epoch=654
03/11/2022 00:22:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.66 on epoch=659
03/11/2022 00:22:44 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.77 on epoch=664
03/11/2022 00:22:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.57 on epoch=669
03/11/2022 00:22:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.50 on epoch=674
03/11/2022 00:22:50 - INFO - __main__ - Global step 1350 Train loss 0.65 Classification-F1 0.7702564102564102 on epoch=674
03/11/2022 00:22:50 - INFO - __main__ - Saving model with best Classification-F1: 0.716256157635468 -> 0.7702564102564102 on epoch=674, global_step=1350
03/11/2022 00:22:53 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.60 on epoch=679
03/11/2022 00:22:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.57 on epoch=684
03/11/2022 00:22:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.53 on epoch=689
03/11/2022 00:23:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.55 on epoch=694
03/11/2022 00:23:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.77 on epoch=699
03/11/2022 00:23:05 - INFO - __main__ - Global step 1400 Train loss 0.61 Classification-F1 0.8435972629521017 on epoch=699
03/11/2022 00:23:05 - INFO - __main__ - Saving model with best Classification-F1: 0.7702564102564102 -> 0.8435972629521017 on epoch=699, global_step=1400
03/11/2022 00:23:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.84 on epoch=704
03/11/2022 00:23:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.91 on epoch=709
03/11/2022 00:23:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.74 on epoch=714
03/11/2022 00:23:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.81 on epoch=719
03/11/2022 00:23:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.05 on epoch=724
03/11/2022 00:23:21 - INFO - __main__ - Global step 1450 Train loss 0.87 Classification-F1 0.805668016194332 on epoch=724
03/11/2022 00:23:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.09 on epoch=729
03/11/2022 00:23:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.85 on epoch=734
03/11/2022 00:23:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.63 on epoch=739
03/11/2022 00:23:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.62 on epoch=744
03/11/2022 00:23:35 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.75 on epoch=749
03/11/2022 00:23:36 - INFO - __main__ - Global step 1500 Train loss 0.99 Classification-F1 0.6267232237539766 on epoch=749
03/11/2022 00:23:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.64 on epoch=754
03/11/2022 00:23:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.56 on epoch=759
03/11/2022 00:23:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.08 on epoch=764
03/11/2022 00:23:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.79 on epoch=769
03/11/2022 00:23:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.84 on epoch=774
03/11/2022 00:23:51 - INFO - __main__ - Global step 1550 Train loss 0.78 Classification-F1 0.3333333333333333 on epoch=774
03/11/2022 00:23:54 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.93 on epoch=779
03/11/2022 00:23:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.95 on epoch=784
03/11/2022 00:24:00 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.90 on epoch=789
03/11/2022 00:24:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.61 on epoch=794
03/11/2022 00:24:05 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.63 on epoch=799
03/11/2022 00:24:06 - INFO - __main__ - Global step 1600 Train loss 0.80 Classification-F1 0.3333333333333333 on epoch=799
03/11/2022 00:24:09 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.64 on epoch=804
03/11/2022 00:24:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.78 on epoch=809
03/11/2022 00:24:15 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.66 on epoch=814
03/11/2022 00:24:18 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.65 on epoch=819
03/11/2022 00:24:21 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.53 on epoch=824
03/11/2022 00:24:21 - INFO - __main__ - Global step 1650 Train loss 0.66 Classification-F1 0.3992490613266583 on epoch=824
03/11/2022 00:24:24 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.61 on epoch=829
03/11/2022 00:24:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.64 on epoch=834
03/11/2022 00:24:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.67 on epoch=839
03/11/2022 00:24:33 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.56 on epoch=844
03/11/2022 00:24:36 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=849
03/11/2022 00:24:36 - INFO - __main__ - Global step 1700 Train loss 0.61 Classification-F1 0.3333333333333333 on epoch=849
03/11/2022 00:24:39 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.58 on epoch=854
03/11/2022 00:24:42 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.56 on epoch=859
03/11/2022 00:24:45 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.52 on epoch=864
03/11/2022 00:24:48 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.53 on epoch=869
03/11/2022 00:24:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.55 on epoch=874
03/11/2022 00:24:51 - INFO - __main__ - Global step 1750 Train loss 0.55 Classification-F1 0.3333333333333333 on epoch=874
03/11/2022 00:24:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.55 on epoch=879
03/11/2022 00:24:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.53 on epoch=884
03/11/2022 00:25:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.52 on epoch=889
03/11/2022 00:25:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.44 on epoch=894
03/11/2022 00:25:06 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.52 on epoch=899
03/11/2022 00:25:07 - INFO - __main__ - Global step 1800 Train loss 0.51 Classification-F1 0.3333333333333333 on epoch=899
03/11/2022 00:25:09 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.54 on epoch=904
03/11/2022 00:25:12 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.56 on epoch=909
03/11/2022 00:25:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.54 on epoch=914
03/11/2022 00:25:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.51 on epoch=919
03/11/2022 00:25:21 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.44 on epoch=924
03/11/2022 00:25:22 - INFO - __main__ - Global step 1850 Train loss 0.52 Classification-F1 0.4589371980676329 on epoch=924
03/11/2022 00:25:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.52 on epoch=929
03/11/2022 00:25:28 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.54 on epoch=934
03/11/2022 00:25:30 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.54 on epoch=939
03/11/2022 00:25:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.54 on epoch=944
03/11/2022 00:25:36 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.56 on epoch=949
03/11/2022 00:25:37 - INFO - __main__ - Global step 1900 Train loss 0.54 Classification-F1 0.3333333333333333 on epoch=949
03/11/2022 00:25:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.59 on epoch=954
03/11/2022 00:25:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.54 on epoch=959
03/11/2022 00:25:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.53 on epoch=964
03/11/2022 00:25:49 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.53 on epoch=969
03/11/2022 00:25:51 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.66 on epoch=974
03/11/2022 00:25:52 - INFO - __main__ - Global step 1950 Train loss 0.57 Classification-F1 0.5134502923976608 on epoch=974
03/11/2022 00:25:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.61 on epoch=979
03/11/2022 00:25:58 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.52 on epoch=984
03/11/2022 00:26:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.51 on epoch=989
03/11/2022 00:26:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.56 on epoch=994
03/11/2022 00:26:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.64 on epoch=999
03/11/2022 00:26:07 - INFO - __main__ - Global step 2000 Train loss 0.57 Classification-F1 0.7117117117117117 on epoch=999
03/11/2022 00:26:07 - INFO - __main__ - save last model!
03/11/2022 00:26:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 00:26:07 - INFO - __main__ - Start tokenizing ... 1000 instances
03/11/2022 00:26:07 - INFO - __main__ - Printing 3 examples
03/11/2022 00:26:07 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/11/2022 00:26:07 - INFO - __main__ - ['negative']
03/11/2022 00:26:07 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/11/2022 00:26:07 - INFO - __main__ - ['negative']
03/11/2022 00:26:07 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/11/2022 00:26:07 - INFO - __main__ - ['negative']
03/11/2022 00:26:07 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:26:08 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:26:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:26:08 - INFO - __main__ - Printing 3 examples
03/11/2022 00:26:08 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/11/2022 00:26:08 - INFO - __main__ - ['negative']
03/11/2022 00:26:08 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/11/2022 00:26:08 - INFO - __main__ - ['negative']
03/11/2022 00:26:08 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/11/2022 00:26:08 - INFO - __main__ - ['negative']
03/11/2022 00:26:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 00:26:08 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:26:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:26:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:26:08 - INFO - __main__ - Printing 3 examples
03/11/2022 00:26:08 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/11/2022 00:26:08 - INFO - __main__ - ['negative']
03/11/2022 00:26:08 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/11/2022 00:26:08 - INFO - __main__ - ['negative']
03/11/2022 00:26:08 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/11/2022 00:26:08 - INFO - __main__ - ['negative']
03/11/2022 00:26:08 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:26:08 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:26:08 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:26:09 - INFO - __main__ - Loaded 1000 examples from test data
03/11/2022 00:26:21 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:26:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:26:22 - INFO - __main__ - Starting training!
03/11/2022 00:26:27 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_87_0.5_8_predictions.txt
03/11/2022 00:26:27 - INFO - __main__ - Classification-F1 on test data: 0.5982
03/11/2022 00:26:27 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.5, bsz=8, dev_performance=0.8435972629521017, test_performance=0.5981975689901085
03/11/2022 00:26:27 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.4, bsz=8 ...
03/11/2022 00:26:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:26:28 - INFO - __main__ - Printing 3 examples
03/11/2022 00:26:28 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/11/2022 00:26:28 - INFO - __main__ - ['negative']
03/11/2022 00:26:28 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/11/2022 00:26:28 - INFO - __main__ - ['negative']
03/11/2022 00:26:28 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/11/2022 00:26:28 - INFO - __main__ - ['negative']
03/11/2022 00:26:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:26:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:26:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:26:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:26:28 - INFO - __main__ - Printing 3 examples
03/11/2022 00:26:28 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/11/2022 00:26:28 - INFO - __main__ - ['negative']
03/11/2022 00:26:28 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/11/2022 00:26:28 - INFO - __main__ - ['negative']
03/11/2022 00:26:28 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/11/2022 00:26:28 - INFO - __main__ - ['negative']
03/11/2022 00:26:28 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:26:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:26:28 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:26:41 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:26:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:26:42 - INFO - __main__ - Starting training!
03/11/2022 00:26:46 - INFO - __main__ - Step 10 Global step 10 Train loss 1.12 on epoch=4
03/11/2022 00:26:49 - INFO - __main__ - Step 20 Global step 20 Train loss 0.49 on epoch=9
03/11/2022 00:26:52 - INFO - __main__ - Step 30 Global step 30 Train loss 0.44 on epoch=14
03/11/2022 00:26:54 - INFO - __main__ - Step 40 Global step 40 Train loss 0.42 on epoch=19
03/11/2022 00:26:57 - INFO - __main__ - Step 50 Global step 50 Train loss 0.34 on epoch=24
03/11/2022 00:26:58 - INFO - __main__ - Global step 50 Train loss 0.56 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 00:26:58 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 00:27:01 - INFO - __main__ - Step 60 Global step 60 Train loss 0.36 on epoch=29
03/11/2022 00:27:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.39 on epoch=34
03/11/2022 00:27:07 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=39
03/11/2022 00:27:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
03/11/2022 00:27:12 - INFO - __main__ - Step 100 Global step 100 Train loss 0.37 on epoch=49
03/11/2022 00:27:13 - INFO - __main__ - Global step 100 Train loss 0.38 Classification-F1 0.8435972629521017 on epoch=49
03/11/2022 00:27:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.8435972629521017 on epoch=49, global_step=100
03/11/2022 00:27:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
03/11/2022 00:27:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.40 on epoch=59
03/11/2022 00:27:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
03/11/2022 00:27:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=69
03/11/2022 00:27:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=74
03/11/2022 00:27:28 - INFO - __main__ - Global step 150 Train loss 0.36 Classification-F1 0.5134502923976608 on epoch=74
03/11/2022 00:27:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.98 on epoch=79
03/11/2022 00:27:34 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=84
03/11/2022 00:27:36 - INFO - __main__ - Step 180 Global step 180 Train loss 0.38 on epoch=89
03/11/2022 00:27:39 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
03/11/2022 00:27:42 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
03/11/2022 00:27:43 - INFO - __main__ - Global step 200 Train loss 0.51 Classification-F1 0.4589371980676329 on epoch=99
03/11/2022 00:27:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
03/11/2022 00:27:48 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
03/11/2022 00:27:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
03/11/2022 00:27:54 - INFO - __main__ - Step 240 Global step 240 Train loss 0.34 on epoch=119
03/11/2022 00:27:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.35 on epoch=124
03/11/2022 00:27:58 - INFO - __main__ - Global step 250 Train loss 0.35 Classification-F1 0.5844155844155844 on epoch=124
03/11/2022 00:28:01 - INFO - __main__ - Step 260 Global step 260 Train loss 0.39 on epoch=129
03/11/2022 00:28:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.60 on epoch=134
03/11/2022 00:28:06 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
03/11/2022 00:28:09 - INFO - __main__ - Step 290 Global step 290 Train loss 0.39 on epoch=144
03/11/2022 00:28:12 - INFO - __main__ - Step 300 Global step 300 Train loss 0.37 on epoch=149
03/11/2022 00:28:13 - INFO - __main__ - Global step 300 Train loss 0.42 Classification-F1 0.539313399778516 on epoch=149
03/11/2022 00:28:16 - INFO - __main__ - Step 310 Global step 310 Train loss 0.40 on epoch=154
03/11/2022 00:28:19 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=159
03/11/2022 00:28:21 - INFO - __main__ - Step 330 Global step 330 Train loss 0.38 on epoch=164
03/11/2022 00:28:24 - INFO - __main__ - Step 340 Global step 340 Train loss 0.30 on epoch=169
03/11/2022 00:28:27 - INFO - __main__ - Step 350 Global step 350 Train loss 0.34 on epoch=174
03/11/2022 00:28:28 - INFO - __main__ - Global step 350 Train loss 0.36 Classification-F1 0.539313399778516 on epoch=174
03/11/2022 00:28:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.40 on epoch=179
03/11/2022 00:28:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.37 on epoch=184
03/11/2022 00:28:37 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=189
03/11/2022 00:28:39 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=194
03/11/2022 00:28:42 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=199
03/11/2022 00:28:43 - INFO - __main__ - Global step 400 Train loss 0.38 Classification-F1 0.539313399778516 on epoch=199
03/11/2022 00:28:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=204
03/11/2022 00:28:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.36 on epoch=209
03/11/2022 00:28:52 - INFO - __main__ - Step 430 Global step 430 Train loss 0.36 on epoch=214
03/11/2022 00:28:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.32 on epoch=219
03/11/2022 00:28:57 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=224
03/11/2022 00:28:58 - INFO - __main__ - Global step 450 Train loss 0.34 Classification-F1 0.5844155844155844 on epoch=224
03/11/2022 00:29:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.34 on epoch=229
03/11/2022 00:29:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.34 on epoch=234
03/11/2022 00:29:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.34 on epoch=239
03/11/2022 00:29:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.34 on epoch=244
03/11/2022 00:29:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.39 on epoch=249
03/11/2022 00:29:13 - INFO - __main__ - Global step 500 Train loss 0.35 Classification-F1 0.539313399778516 on epoch=249
03/11/2022 00:29:16 - INFO - __main__ - Step 510 Global step 510 Train loss 0.29 on epoch=254
03/11/2022 00:29:19 - INFO - __main__ - Step 520 Global step 520 Train loss 0.34 on epoch=259
03/11/2022 00:29:22 - INFO - __main__ - Step 530 Global step 530 Train loss 0.34 on epoch=264
03/11/2022 00:29:25 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=269
03/11/2022 00:29:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.32 on epoch=274
03/11/2022 00:29:28 - INFO - __main__ - Global step 550 Train loss 0.33 Classification-F1 0.49090909090909085 on epoch=274
03/11/2022 00:29:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.35 on epoch=279
03/11/2022 00:29:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.33 on epoch=284
03/11/2022 00:29:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=289
03/11/2022 00:29:40 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=294
03/11/2022 00:29:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.35 on epoch=299
03/11/2022 00:29:43 - INFO - __main__ - Global step 600 Train loss 0.34 Classification-F1 0.49090909090909085 on epoch=299
03/11/2022 00:29:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.35 on epoch=304
03/11/2022 00:29:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.36 on epoch=309
03/11/2022 00:29:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.32 on epoch=314
03/11/2022 00:29:55 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=319
03/11/2022 00:29:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.35 on epoch=324
03/11/2022 00:29:58 - INFO - __main__ - Global step 650 Train loss 0.34 Classification-F1 0.6666666666666667 on epoch=324
03/11/2022 00:30:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.36 on epoch=329
03/11/2022 00:30:04 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=334
03/11/2022 00:30:07 - INFO - __main__ - Step 680 Global step 680 Train loss 0.37 on epoch=339
03/11/2022 00:30:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.33 on epoch=344
03/11/2022 00:30:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.31 on epoch=349
03/11/2022 00:30:13 - INFO - __main__ - Global step 700 Train loss 0.35 Classification-F1 0.6666666666666667 on epoch=349
03/11/2022 00:30:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=354
03/11/2022 00:30:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.33 on epoch=359
03/11/2022 00:30:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.32 on epoch=364
03/11/2022 00:30:25 - INFO - __main__ - Step 740 Global step 740 Train loss 0.33 on epoch=369
03/11/2022 00:30:28 - INFO - __main__ - Step 750 Global step 750 Train loss 0.32 on epoch=374
03/11/2022 00:30:28 - INFO - __main__ - Global step 750 Train loss 0.33 Classification-F1 0.5844155844155844 on epoch=374
03/11/2022 00:30:31 - INFO - __main__ - Step 760 Global step 760 Train loss 0.37 on epoch=379
03/11/2022 00:30:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.33 on epoch=384
03/11/2022 00:30:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
03/11/2022 00:30:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.34 on epoch=394
03/11/2022 00:30:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.31 on epoch=399
03/11/2022 00:30:44 - INFO - __main__ - Global step 800 Train loss 0.34 Classification-F1 0.6666666666666667 on epoch=399
03/11/2022 00:30:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.31 on epoch=404
03/11/2022 00:30:49 - INFO - __main__ - Step 820 Global step 820 Train loss 0.33 on epoch=409
03/11/2022 00:30:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=414
03/11/2022 00:30:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=419
03/11/2022 00:30:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.31 on epoch=424
03/11/2022 00:30:59 - INFO - __main__ - Global step 850 Train loss 0.32 Classification-F1 0.539313399778516 on epoch=424
03/11/2022 00:31:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=429
03/11/2022 00:31:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=434
03/11/2022 00:31:07 - INFO - __main__ - Step 880 Global step 880 Train loss 0.31 on epoch=439
03/11/2022 00:31:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.32 on epoch=444
03/11/2022 00:31:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.34 on epoch=449
03/11/2022 00:31:14 - INFO - __main__ - Global step 900 Train loss 0.31 Classification-F1 0.539313399778516 on epoch=449
03/11/2022 00:31:17 - INFO - __main__ - Step 910 Global step 910 Train loss 0.28 on epoch=454
03/11/2022 00:31:19 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=459
03/11/2022 00:31:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=464
03/11/2022 00:31:25 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=469
03/11/2022 00:31:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.28 on epoch=474
03/11/2022 00:31:29 - INFO - __main__ - Global step 950 Train loss 0.31 Classification-F1 0.49090909090909085 on epoch=474
03/11/2022 00:31:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.33 on epoch=479
03/11/2022 00:31:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.30 on epoch=484
03/11/2022 00:31:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.31 on epoch=489
03/11/2022 00:31:40 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=494
03/11/2022 00:31:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=499
03/11/2022 00:31:44 - INFO - __main__ - Global step 1000 Train loss 0.31 Classification-F1 0.5844155844155844 on epoch=499
03/11/2022 00:31:47 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.31 on epoch=504
03/11/2022 00:31:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=509
03/11/2022 00:31:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=514
03/11/2022 00:31:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=519
03/11/2022 00:31:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.31 on epoch=524
03/11/2022 00:31:59 - INFO - __main__ - Global step 1050 Train loss 0.31 Classification-F1 0.539313399778516 on epoch=524
03/11/2022 00:32:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=529
03/11/2022 00:32:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=534
03/11/2022 00:32:08 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.26 on epoch=539
03/11/2022 00:32:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.30 on epoch=544
03/11/2022 00:32:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=549
03/11/2022 00:32:14 - INFO - __main__ - Global step 1100 Train loss 0.28 Classification-F1 0.8423645320197044 on epoch=549
03/11/2022 00:32:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.28 on epoch=554
03/11/2022 00:32:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=559
03/11/2022 00:32:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.28 on epoch=564
03/11/2022 00:32:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=569
03/11/2022 00:32:29 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.30 on epoch=574
03/11/2022 00:32:29 - INFO - __main__ - Global step 1150 Train loss 0.28 Classification-F1 0.7046153846153846 on epoch=574
03/11/2022 00:32:32 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=579
03/11/2022 00:32:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.27 on epoch=584
03/11/2022 00:32:38 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=589
03/11/2022 00:32:41 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=594
03/11/2022 00:32:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=599
03/11/2022 00:32:44 - INFO - __main__ - Global step 1200 Train loss 0.26 Classification-F1 0.7757757757757757 on epoch=599
03/11/2022 00:32:47 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=604
03/11/2022 00:32:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=609
03/11/2022 00:32:53 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=614
03/11/2022 00:32:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.27 on epoch=619
03/11/2022 00:32:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=624
03/11/2022 00:32:59 - INFO - __main__ - Global step 1250 Train loss 0.25 Classification-F1 0.6666666666666667 on epoch=624
03/11/2022 00:33:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=629
03/11/2022 00:33:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=634
03/11/2022 00:33:08 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=639
03/11/2022 00:33:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=644
03/11/2022 00:33:14 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.27 on epoch=649
03/11/2022 00:33:14 - INFO - __main__ - Global step 1300 Train loss 0.26 Classification-F1 0.8095238095238095 on epoch=649
03/11/2022 00:33:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=654
03/11/2022 00:33:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=659
03/11/2022 00:33:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=664
03/11/2022 00:33:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=669
03/11/2022 00:33:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=674
03/11/2022 00:33:30 - INFO - __main__ - Global step 1350 Train loss 0.22 Classification-F1 0.539313399778516 on epoch=674
03/11/2022 00:33:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=679
03/11/2022 00:33:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=684
03/11/2022 00:33:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=689
03/11/2022 00:33:41 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=694
03/11/2022 00:33:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=699
03/11/2022 00:33:45 - INFO - __main__ - Global step 1400 Train loss 0.22 Classification-F1 0.8745098039215686 on epoch=699
03/11/2022 00:33:45 - INFO - __main__ - Saving model with best Classification-F1: 0.8435972629521017 -> 0.8745098039215686 on epoch=699, global_step=1400
03/11/2022 00:33:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.21 on epoch=704
03/11/2022 00:33:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=709
03/11/2022 00:33:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=714
03/11/2022 00:33:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=719
03/11/2022 00:33:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=724
03/11/2022 00:34:01 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.8745098039215686 on epoch=724
03/11/2022 00:34:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=729
03/11/2022 00:34:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=734
03/11/2022 00:34:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=739
03/11/2022 00:34:12 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=744
03/11/2022 00:34:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=749
03/11/2022 00:34:17 - INFO - __main__ - Global step 1500 Train loss 0.16 Classification-F1 0.8745098039215686 on epoch=749
03/11/2022 00:34:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=754
03/11/2022 00:34:23 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=759
03/11/2022 00:34:25 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=764
03/11/2022 00:34:28 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=769
03/11/2022 00:34:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.11 on epoch=774
03/11/2022 00:34:33 - INFO - __main__ - Global step 1550 Train loss 0.14 Classification-F1 0.8745098039215686 on epoch=774
03/11/2022 00:34:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
03/11/2022 00:34:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=784
03/11/2022 00:34:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=789
03/11/2022 00:34:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.11 on epoch=794
03/11/2022 00:34:47 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=799
03/11/2022 00:34:49 - INFO - __main__ - Global step 1600 Train loss 0.12 Classification-F1 0.9375 on epoch=799
03/11/2022 00:34:49 - INFO - __main__ - Saving model with best Classification-F1: 0.8745098039215686 -> 0.9375 on epoch=799, global_step=1600
03/11/2022 00:34:51 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=804
03/11/2022 00:34:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=809
03/11/2022 00:34:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.11 on epoch=814
03/11/2022 00:35:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.11 on epoch=819
03/11/2022 00:35:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=824
03/11/2022 00:35:04 - INFO - __main__ - Global step 1650 Train loss 0.11 Classification-F1 0.906158357771261 on epoch=824
03/11/2022 00:35:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.05 on epoch=829
03/11/2022 00:35:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=834
03/11/2022 00:35:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=839
03/11/2022 00:35:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=844
03/11/2022 00:35:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=849
03/11/2022 00:35:20 - INFO - __main__ - Global step 1700 Train loss 0.09 Classification-F1 0.9375 on epoch=849
03/11/2022 00:35:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=854
03/11/2022 00:35:25 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=859
03/11/2022 00:35:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=864
03/11/2022 00:35:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=869
03/11/2022 00:35:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.08 on epoch=874
03/11/2022 00:35:35 - INFO - __main__ - Global step 1750 Train loss 0.08 Classification-F1 0.9375 on epoch=874
03/11/2022 00:35:38 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=879
03/11/2022 00:35:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=884
03/11/2022 00:35:44 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=889
03/11/2022 00:35:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.09 on epoch=894
03/11/2022 00:35:49 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.05 on epoch=899
03/11/2022 00:35:51 - INFO - __main__ - Global step 1800 Train loss 0.07 Classification-F1 0.9375 on epoch=899
03/11/2022 00:35:54 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.08 on epoch=904
03/11/2022 00:35:56 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.04 on epoch=909
03/11/2022 00:35:59 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=914
03/11/2022 00:36:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=919
03/11/2022 00:36:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=924
03/11/2022 00:36:07 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.9375 on epoch=924
03/11/2022 00:36:10 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=929
03/11/2022 00:36:12 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
03/11/2022 00:36:15 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=939
03/11/2022 00:36:18 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
03/11/2022 00:36:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.05 on epoch=949
03/11/2022 00:36:23 - INFO - __main__ - Global step 1900 Train loss 0.05 Classification-F1 0.9375 on epoch=949
03/11/2022 00:36:26 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=954
03/11/2022 00:36:29 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
03/11/2022 00:36:31 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
03/11/2022 00:36:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
03/11/2022 00:36:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.06 on epoch=974
03/11/2022 00:36:39 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.9375 on epoch=974
03/11/2022 00:36:42 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
03/11/2022 00:36:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.04 on epoch=984
03/11/2022 00:36:47 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
03/11/2022 00:36:50 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
03/11/2022 00:36:53 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
03/11/2022 00:36:54 - INFO - __main__ - Global step 2000 Train loss 0.03 Classification-F1 0.9375 on epoch=999
03/11/2022 00:36:54 - INFO - __main__ - save last model!
03/11/2022 00:36:54 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 00:36:54 - INFO - __main__ - Start tokenizing ... 1000 instances
03/11/2022 00:36:54 - INFO - __main__ - Printing 3 examples
03/11/2022 00:36:54 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/11/2022 00:36:54 - INFO - __main__ - ['negative']
03/11/2022 00:36:54 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/11/2022 00:36:54 - INFO - __main__ - ['negative']
03/11/2022 00:36:54 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/11/2022 00:36:54 - INFO - __main__ - ['negative']
03/11/2022 00:36:54 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:36:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:36:54 - INFO - __main__ - Printing 3 examples
03/11/2022 00:36:54 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/11/2022 00:36:54 - INFO - __main__ - ['negative']
03/11/2022 00:36:54 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/11/2022 00:36:54 - INFO - __main__ - ['negative']
03/11/2022 00:36:54 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/11/2022 00:36:54 - INFO - __main__ - ['negative']
03/11/2022 00:36:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 00:36:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:36:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:36:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:36:54 - INFO - __main__ - Printing 3 examples
03/11/2022 00:36:54 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/11/2022 00:36:54 - INFO - __main__ - ['negative']
03/11/2022 00:36:54 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/11/2022 00:36:54 - INFO - __main__ - ['negative']
03/11/2022 00:36:54 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/11/2022 00:36:54 - INFO - __main__ - ['negative']
03/11/2022 00:36:54 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:36:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:36:54 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:36:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:36:56 - INFO - __main__ - Loaded 1000 examples from test data
03/11/2022 00:37:09 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:37:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:37:10 - INFO - __main__ - Starting training!
03/11/2022 00:37:50 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_87_0.4_8_predictions.txt
03/11/2022 00:37:50 - INFO - __main__ - Classification-F1 on test data: 0.9250
03/11/2022 00:37:51 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.4, bsz=8, dev_performance=0.9375, test_performance=0.9249981249531238
03/11/2022 00:37:51 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.3, bsz=8 ...
03/11/2022 00:37:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:37:52 - INFO - __main__ - Printing 3 examples
03/11/2022 00:37:52 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/11/2022 00:37:52 - INFO - __main__ - ['negative']
03/11/2022 00:37:52 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/11/2022 00:37:52 - INFO - __main__ - ['negative']
03/11/2022 00:37:52 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/11/2022 00:37:52 - INFO - __main__ - ['negative']
03/11/2022 00:37:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:37:52 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:37:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:37:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:37:52 - INFO - __main__ - Printing 3 examples
03/11/2022 00:37:52 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/11/2022 00:37:52 - INFO - __main__ - ['negative']
03/11/2022 00:37:52 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/11/2022 00:37:52 - INFO - __main__ - ['negative']
03/11/2022 00:37:52 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/11/2022 00:37:52 - INFO - __main__ - ['negative']
03/11/2022 00:37:52 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:37:52 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:37:52 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:38:06 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:38:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:38:06 - INFO - __main__ - Starting training!
03/11/2022 00:38:10 - INFO - __main__ - Step 10 Global step 10 Train loss 1.40 on epoch=4
03/11/2022 00:38:13 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=9
03/11/2022 00:38:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=14
03/11/2022 00:38:19 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=19
03/11/2022 00:38:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.38 on epoch=24
03/11/2022 00:38:22 - INFO - __main__ - Global step 50 Train loss 0.64 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 00:38:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 00:38:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.40 on epoch=29
03/11/2022 00:38:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
03/11/2022 00:38:31 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=39
03/11/2022 00:38:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
03/11/2022 00:38:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.39 on epoch=49
03/11/2022 00:38:37 - INFO - __main__ - Global step 100 Train loss 0.39 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 00:38:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
03/11/2022 00:38:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=59
03/11/2022 00:38:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
03/11/2022 00:38:49 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
03/11/2022 00:38:52 - INFO - __main__ - Step 150 Global step 150 Train loss 0.36 on epoch=74
03/11/2022 00:38:52 - INFO - __main__ - Global step 150 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 00:38:55 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
03/11/2022 00:38:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
03/11/2022 00:39:01 - INFO - __main__ - Step 180 Global step 180 Train loss 0.37 on epoch=89
03/11/2022 00:39:04 - INFO - __main__ - Step 190 Global step 190 Train loss 0.34 on epoch=94
03/11/2022 00:39:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.39 on epoch=99
03/11/2022 00:39:07 - INFO - __main__ - Global step 200 Train loss 0.38 Classification-F1 0.5901477832512315 on epoch=99
03/11/2022 00:39:07 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5901477832512315 on epoch=99, global_step=200
03/11/2022 00:39:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
03/11/2022 00:39:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.34 on epoch=109
03/11/2022 00:39:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.37 on epoch=114
03/11/2022 00:39:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
03/11/2022 00:39:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=124
03/11/2022 00:39:22 - INFO - __main__ - Global step 250 Train loss 0.35 Classification-F1 0.3992490613266583 on epoch=124
03/11/2022 00:39:25 - INFO - __main__ - Step 260 Global step 260 Train loss 0.38 on epoch=129
03/11/2022 00:39:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
03/11/2022 00:39:31 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
03/11/2022 00:39:34 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
03/11/2022 00:39:37 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=149
03/11/2022 00:39:37 - INFO - __main__ - Global step 300 Train loss 0.37 Classification-F1 0.4589371980676329 on epoch=149
03/11/2022 00:39:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=154
03/11/2022 00:39:43 - INFO - __main__ - Step 320 Global step 320 Train loss 0.35 on epoch=159
03/11/2022 00:39:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=164
03/11/2022 00:39:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=169
03/11/2022 00:39:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.34 on epoch=174
03/11/2022 00:39:52 - INFO - __main__ - Global step 350 Train loss 0.35 Classification-F1 0.3816425120772947 on epoch=174
03/11/2022 00:39:55 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=179
03/11/2022 00:39:58 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=184
03/11/2022 00:40:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
03/11/2022 00:40:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.30 on epoch=194
03/11/2022 00:40:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
03/11/2022 00:40:07 - INFO - __main__ - Global step 400 Train loss 0.31 Classification-F1 0.8423645320197044 on epoch=199
03/11/2022 00:40:07 - INFO - __main__ - Saving model with best Classification-F1: 0.5901477832512315 -> 0.8423645320197044 on epoch=199, global_step=400
03/11/2022 00:40:10 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=204
03/11/2022 00:40:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
03/11/2022 00:40:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
03/11/2022 00:40:19 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
03/11/2022 00:40:22 - INFO - __main__ - Step 450 Global step 450 Train loss 0.17 on epoch=224
03/11/2022 00:40:23 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.9375 on epoch=224
03/11/2022 00:40:23 - INFO - __main__ - Saving model with best Classification-F1: 0.8423645320197044 -> 0.9375 on epoch=224, global_step=450
03/11/2022 00:40:26 - INFO - __main__ - Step 460 Global step 460 Train loss 0.13 on epoch=229
03/11/2022 00:40:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.10 on epoch=234
03/11/2022 00:40:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.11 on epoch=239
03/11/2022 00:40:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.08 on epoch=244
03/11/2022 00:40:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.06 on epoch=249
03/11/2022 00:40:38 - INFO - __main__ - Global step 500 Train loss 0.10 Classification-F1 0.9372549019607843 on epoch=249
03/11/2022 00:40:41 - INFO - __main__ - Step 510 Global step 510 Train loss 0.06 on epoch=254
03/11/2022 00:40:44 - INFO - __main__ - Step 520 Global step 520 Train loss 0.05 on epoch=259
03/11/2022 00:40:46 - INFO - __main__ - Step 530 Global step 530 Train loss 0.06 on epoch=264
03/11/2022 00:40:49 - INFO - __main__ - Step 540 Global step 540 Train loss 0.05 on epoch=269
03/11/2022 00:40:52 - INFO - __main__ - Step 550 Global step 550 Train loss 0.03 on epoch=274
03/11/2022 00:40:53 - INFO - __main__ - Global step 550 Train loss 0.05 Classification-F1 0.9687194525904204 on epoch=274
03/11/2022 00:40:53 - INFO - __main__ - Saving model with best Classification-F1: 0.9375 -> 0.9687194525904204 on epoch=274, global_step=550
03/11/2022 00:40:56 - INFO - __main__ - Step 560 Global step 560 Train loss 0.02 on epoch=279
03/11/2022 00:40:59 - INFO - __main__ - Step 570 Global step 570 Train loss 0.02 on epoch=284
03/11/2022 00:41:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.05 on epoch=289
03/11/2022 00:41:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.01 on epoch=294
03/11/2022 00:41:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.01 on epoch=299
03/11/2022 00:41:10 - INFO - __main__ - Global step 600 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=299
03/11/2022 00:41:13 - INFO - __main__ - Step 610 Global step 610 Train loss 0.01 on epoch=304
03/11/2022 00:41:16 - INFO - __main__ - Step 620 Global step 620 Train loss 0.02 on epoch=309
03/11/2022 00:41:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.02 on epoch=314
03/11/2022 00:41:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.01 on epoch=319
03/11/2022 00:41:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.03 on epoch=324
03/11/2022 00:41:27 - INFO - __main__ - Global step 650 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=324
03/11/2022 00:41:30 - INFO - __main__ - Step 660 Global step 660 Train loss 0.01 on epoch=329
03/11/2022 00:41:33 - INFO - __main__ - Step 670 Global step 670 Train loss 0.01 on epoch=334
03/11/2022 00:41:36 - INFO - __main__ - Step 680 Global step 680 Train loss 0.01 on epoch=339
03/11/2022 00:41:38 - INFO - __main__ - Step 690 Global step 690 Train loss 0.06 on epoch=344
03/11/2022 00:41:41 - INFO - __main__ - Step 700 Global step 700 Train loss 0.00 on epoch=349
03/11/2022 00:41:47 - INFO - __main__ - Global step 700 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=349
03/11/2022 00:41:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.00 on epoch=354
03/11/2022 00:41:53 - INFO - __main__ - Step 720 Global step 720 Train loss 0.01 on epoch=359
03/11/2022 00:41:56 - INFO - __main__ - Step 730 Global step 730 Train loss 0.01 on epoch=364
03/11/2022 00:41:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/11/2022 00:42:02 - INFO - __main__ - Step 750 Global step 750 Train loss 0.01 on epoch=374
03/11/2022 00:42:05 - INFO - __main__ - Global step 750 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=374
03/11/2022 00:42:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
03/11/2022 00:42:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.02 on epoch=384
03/11/2022 00:42:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.00 on epoch=389
03/11/2022 00:42:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.00 on epoch=394
03/11/2022 00:42:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.01 on epoch=399
03/11/2022 00:42:24 - INFO - __main__ - Global step 800 Train loss 0.02 Classification-F1 0.9687194525904204 on epoch=399
03/11/2022 00:42:27 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/11/2022 00:42:30 - INFO - __main__ - Step 820 Global step 820 Train loss 0.00 on epoch=409
03/11/2022 00:42:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.01 on epoch=414
03/11/2022 00:42:36 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/11/2022 00:42:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/11/2022 00:42:41 - INFO - __main__ - Global step 850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=424
03/11/2022 00:42:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/11/2022 00:42:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.00 on epoch=434
03/11/2022 00:42:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/11/2022 00:42:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/11/2022 00:42:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/11/2022 00:43:01 - INFO - __main__ - Global step 900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=449
03/11/2022 00:43:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/11/2022 00:43:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/11/2022 00:43:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/11/2022 00:43:12 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/11/2022 00:43:15 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/11/2022 00:43:18 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=474
03/11/2022 00:43:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/11/2022 00:43:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/11/2022 00:43:26 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/11/2022 00:43:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/11/2022 00:43:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/11/2022 00:43:34 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=499
03/11/2022 00:43:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/11/2022 00:43:40 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/11/2022 00:43:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
03/11/2022 00:43:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/11/2022 00:43:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/11/2022 00:43:51 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=524
03/11/2022 00:43:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/11/2022 00:43:57 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/11/2022 00:44:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/11/2022 00:44:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
03/11/2022 00:44:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/11/2022 00:44:12 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=549
03/11/2022 00:44:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/11/2022 00:44:18 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/11/2022 00:44:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/11/2022 00:44:23 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/11/2022 00:44:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/11/2022 00:44:32 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=574
03/11/2022 00:44:35 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/11/2022 00:44:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/11/2022 00:44:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/11/2022 00:44:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/11/2022 00:44:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/11/2022 00:44:49 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=599
03/11/2022 00:44:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/11/2022 00:44:55 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/11/2022 00:44:57 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/11/2022 00:45:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/11/2022 00:45:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/11/2022 00:45:05 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=624
03/11/2022 00:45:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/11/2022 00:45:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 00:45:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/11/2022 00:45:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/11/2022 00:45:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/11/2022 00:45:22 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=649
03/11/2022 00:45:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/11/2022 00:45:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/11/2022 00:45:30 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/11/2022 00:45:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 00:45:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 00:45:39 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=674
03/11/2022 00:45:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 00:45:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 00:45:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/11/2022 00:45:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 00:45:54 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
03/11/2022 00:45:56 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=699
03/11/2022 00:45:58 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/11/2022 00:46:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 00:46:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 00:46:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/11/2022 00:46:10 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/11/2022 00:46:11 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=724
03/11/2022 00:46:14 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 00:46:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 00:46:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 00:46:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 00:46:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 00:46:27 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=749
03/11/2022 00:46:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/11/2022 00:46:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 00:46:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 00:46:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 00:46:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 00:46:44 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=774
03/11/2022 00:46:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 00:46:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 00:46:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 00:46:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 00:46:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 00:46:59 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=799
03/11/2022 00:47:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 00:47:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 00:47:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 00:47:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 00:47:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 00:47:15 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=824
03/11/2022 00:47:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 00:47:21 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
03/11/2022 00:47:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 00:47:27 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 00:47:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 00:47:32 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.9687194525904204 on epoch=849
03/11/2022 00:47:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 00:47:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 00:47:40 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 00:47:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 00:47:46 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 00:47:48 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=874
03/11/2022 00:47:51 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 00:47:53 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 00:47:56 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 00:47:59 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 00:48:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 00:48:04 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=899
03/11/2022 00:48:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 00:48:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 00:48:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 00:48:15 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 00:48:18 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 00:48:19 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=924
03/11/2022 00:48:22 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 00:48:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 00:48:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 00:48:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 00:48:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 00:48:35 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=949
03/11/2022 00:48:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 00:48:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 00:48:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 00:48:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 00:48:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 00:48:50 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=974
03/11/2022 00:48:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 00:48:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 00:48:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 00:49:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 00:49:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 00:49:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:49:06 - INFO - __main__ - Printing 3 examples
03/11/2022 00:49:06 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/11/2022 00:49:06 - INFO - __main__ - ['negative']
03/11/2022 00:49:06 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/11/2022 00:49:06 - INFO - __main__ - ['negative']
03/11/2022 00:49:06 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/11/2022 00:49:06 - INFO - __main__ - ['negative']
03/11/2022 00:49:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 00:49:06 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:49:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:49:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:49:06 - INFO - __main__ - Printing 3 examples
03/11/2022 00:49:06 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/11/2022 00:49:06 - INFO - __main__ - ['negative']
03/11/2022 00:49:06 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/11/2022 00:49:06 - INFO - __main__ - ['negative']
03/11/2022 00:49:06 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/11/2022 00:49:06 - INFO - __main__ - ['negative']
03/11/2022 00:49:06 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:49:06 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:49:06 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:49:06 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.9687194525904204 on epoch=999
03/11/2022 00:49:06 - INFO - __main__ - save last model!
03/11/2022 00:49:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 00:49:06 - INFO - __main__ - Start tokenizing ... 1000 instances
03/11/2022 00:49:06 - INFO - __main__ - Printing 3 examples
03/11/2022 00:49:06 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/11/2022 00:49:06 - INFO - __main__ - ['negative']
03/11/2022 00:49:06 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/11/2022 00:49:06 - INFO - __main__ - ['negative']
03/11/2022 00:49:06 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/11/2022 00:49:06 - INFO - __main__ - ['negative']
03/11/2022 00:49:06 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:49:07 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:49:08 - INFO - __main__ - Loaded 1000 examples from test data
03/11/2022 00:49:20 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:49:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:49:21 - INFO - __main__ - Starting training!
03/11/2022 00:50:07 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_87_0.3_8_predictions.txt
03/11/2022 00:50:07 - INFO - __main__ - Classification-F1 on test data: 0.9269
03/11/2022 00:50:08 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.3, bsz=8, dev_performance=0.9687194525904204, test_performance=0.9268770803721055
03/11/2022 00:50:08 - INFO - __main__ - Running ... prefix=amazon_polarity_16_87, lr=0.2, bsz=8 ...
03/11/2022 00:50:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:50:08 - INFO - __main__ - Printing 3 examples
03/11/2022 00:50:08 - INFO - __main__ -  [amazon_polarity] title: Feminist propaganda that goes nowhere [SEP] content: Once again, too bad zero ratings aren't allowed.This movie is just another piece of feminist propaganda from the late 1970s that regurgitates the usual anti-male spiel and has nothing of any substance to offer. The fact that the "heroine" sort of wanders off the screen at the end is most telling. She has her "individuality" and that's about it. George Harrison's song "I Me Mine" should have been playing in the background at that point.Blecch.
03/11/2022 00:50:08 - INFO - __main__ - ['negative']
03/11/2022 00:50:08 - INFO - __main__ -  [amazon_polarity] title: Corelle Apricot Grove 16 pc Set [SEP] content: This was a Christmas gift. We are very displeased with this set, almost returned it. The dinner and desert plates are Corelle Apricot Grove, the coffee mugs are STONEWARE and the bowls are Corelle, but of a totally different pattern. Yes, the product description does say that, but I'm used to searching for a product and getting an exact match. I could of, and SHOULD HAVE gone to WalMart and purchased itthere for $10 less.
03/11/2022 00:50:08 - INFO - __main__ - ['negative']
03/11/2022 00:50:08 - INFO - __main__ -  [amazon_polarity] title: good for kids, I guess [SEP] content: The music is way too "synthy" for me. In many cases the "band" sounds like a synthesizer and a drum box. Many of the lyrics are inane. Lou's voice seems ok. My 8 year old likes it a lot.The songs have a certain pop catchiness to them, but there just isn't much depth there. I have a hard time believing that this is the king of mambo. I certeinly wish that I was as studly as he is.
03/11/2022 00:50:08 - INFO - __main__ - ['negative']
03/11/2022 00:50:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 00:50:08 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:50:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 00:50:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 00:50:09 - INFO - __main__ - Printing 3 examples
03/11/2022 00:50:09 - INFO - __main__ -  [amazon_polarity] title: MicroSuede Down Throw Brown [SEP] content: Unfortunately, an alternative to the product I ordered was sent to me. Alternative was not acceptable. Am presently working with merchant regarding refund. Spokesperson from merchant has been very prompt and courteous.
03/11/2022 00:50:09 - INFO - __main__ - ['negative']
03/11/2022 00:50:09 - INFO - __main__ -  [amazon_polarity] title: an unsatisfactory purchase [SEP] content: This tension rod is thin and very weak. It could not hold itself up. Sadly, I will be returning it.
03/11/2022 00:50:09 - INFO - __main__ - ['negative']
03/11/2022 00:50:09 - INFO - __main__ -  [amazon_polarity] title: What a waste of a cd [SEP] content: If you buy this cd, you like to throw away your cash. Anyone who is a fan already, has all of these songs and they sounded ten times better in a studio than they do onstage. If you haven't been a Chicks fan before, this one certainly won't convert you. Save the cash for their next album. Maybe it will be worth buying. This one is a stinker.
03/11/2022 00:50:09 - INFO - __main__ - ['negative']
03/11/2022 00:50:09 - INFO - __main__ - Tokenizing Input ...
03/11/2022 00:50:09 - INFO - __main__ - Tokenizing Output ...
03/11/2022 00:50:09 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 00:50:22 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 00:50:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 00:50:23 - INFO - __main__ - Starting training!
03/11/2022 00:50:26 - INFO - __main__ - Step 10 Global step 10 Train loss 1.53 on epoch=4
03/11/2022 00:50:29 - INFO - __main__ - Step 20 Global step 20 Train loss 0.60 on epoch=9
03/11/2022 00:50:32 - INFO - __main__ - Step 30 Global step 30 Train loss 0.49 on epoch=14
03/11/2022 00:50:35 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=19
03/11/2022 00:50:38 - INFO - __main__ - Step 50 Global step 50 Train loss 0.38 on epoch=24
03/11/2022 00:50:38 - INFO - __main__ - Global step 50 Train loss 0.68 Classification-F1 0.7408906882591093 on epoch=24
03/11/2022 00:50:38 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.7408906882591093 on epoch=24, global_step=50
03/11/2022 00:50:41 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=29
03/11/2022 00:50:44 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=34
03/11/2022 00:50:47 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=39
03/11/2022 00:50:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
03/11/2022 00:50:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.45 on epoch=49
03/11/2022 00:50:53 - INFO - __main__ - Global step 100 Train loss 0.42 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 00:50:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.42 on epoch=54
03/11/2022 00:50:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
03/11/2022 00:51:02 - INFO - __main__ - Step 130 Global step 130 Train loss 0.38 on epoch=64
03/11/2022 00:51:05 - INFO - __main__ - Step 140 Global step 140 Train loss 0.36 on epoch=69
03/11/2022 00:51:08 - INFO - __main__ - Step 150 Global step 150 Train loss 0.37 on epoch=74
03/11/2022 00:51:08 - INFO - __main__ - Global step 150 Train loss 0.39 Classification-F1 0.3992490613266583 on epoch=74
03/11/2022 00:51:11 - INFO - __main__ - Step 160 Global step 160 Train loss 0.40 on epoch=79
03/11/2022 00:51:14 - INFO - __main__ - Step 170 Global step 170 Train loss 0.33 on epoch=84
03/11/2022 00:51:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.70 on epoch=89
03/11/2022 00:51:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.56 on epoch=94
03/11/2022 00:51:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.37 on epoch=99
03/11/2022 00:51:23 - INFO - __main__ - Global step 200 Train loss 0.47 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 00:51:26 - INFO - __main__ - Step 210 Global step 210 Train loss 0.38 on epoch=104
03/11/2022 00:51:28 - INFO - __main__ - Step 220 Global step 220 Train loss 0.38 on epoch=109
03/11/2022 00:51:31 - INFO - __main__ - Step 230 Global step 230 Train loss 0.35 on epoch=114
03/11/2022 00:51:34 - INFO - __main__ - Step 240 Global step 240 Train loss 0.35 on epoch=119
03/11/2022 00:51:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.41 on epoch=124
03/11/2022 00:51:37 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 00:51:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.35 on epoch=129
03/11/2022 00:51:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
03/11/2022 00:51:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.35 on epoch=139
03/11/2022 00:51:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.59 on epoch=144
03/11/2022 00:51:52 - INFO - __main__ - Step 300 Global step 300 Train loss 0.39 on epoch=149
03/11/2022 00:51:52 - INFO - __main__ - Global step 300 Train loss 0.41 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 00:51:55 - INFO - __main__ - Step 310 Global step 310 Train loss 0.47 on epoch=154
03/11/2022 00:51:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=159
03/11/2022 00:52:01 - INFO - __main__ - Step 330 Global step 330 Train loss 0.35 on epoch=164
03/11/2022 00:52:04 - INFO - __main__ - Step 340 Global step 340 Train loss 0.36 on epoch=169
03/11/2022 00:52:06 - INFO - __main__ - Step 350 Global step 350 Train loss 0.34 on epoch=174
03/11/2022 00:52:07 - INFO - __main__ - Global step 350 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 00:52:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.37 on epoch=179
03/11/2022 00:52:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.38 on epoch=184
03/11/2022 00:52:16 - INFO - __main__ - Step 380 Global step 380 Train loss 0.37 on epoch=189
03/11/2022 00:52:18 - INFO - __main__ - Step 390 Global step 390 Train loss 0.36 on epoch=194
03/11/2022 00:52:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.37 on epoch=199
03/11/2022 00:52:22 - INFO - __main__ - Global step 400 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=199
03/11/2022 00:52:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.37 on epoch=204
03/11/2022 00:52:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=209
03/11/2022 00:52:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=214
03/11/2022 00:52:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.34 on epoch=219
03/11/2022 00:52:36 - INFO - __main__ - Step 450 Global step 450 Train loss 0.35 on epoch=224
03/11/2022 00:52:37 - INFO - __main__ - Global step 450 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=224
03/11/2022 00:52:39 - INFO - __main__ - Step 460 Global step 460 Train loss 0.36 on epoch=229
03/11/2022 00:52:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.37 on epoch=234
03/11/2022 00:52:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.36 on epoch=239
03/11/2022 00:52:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.37 on epoch=244
03/11/2022 00:52:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.37 on epoch=249
03/11/2022 00:52:51 - INFO - __main__ - Global step 500 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=249
03/11/2022 00:52:54 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=254
03/11/2022 00:52:57 - INFO - __main__ - Step 520 Global step 520 Train loss 0.38 on epoch=259
03/11/2022 00:53:00 - INFO - __main__ - Step 530 Global step 530 Train loss 0.38 on epoch=264
03/11/2022 00:53:03 - INFO - __main__ - Step 540 Global step 540 Train loss 0.34 on epoch=269
03/11/2022 00:53:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.40 on epoch=274
03/11/2022 00:53:06 - INFO - __main__ - Global step 550 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=274
03/11/2022 00:53:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.37 on epoch=279
03/11/2022 00:53:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.38 on epoch=284
03/11/2022 00:53:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.33 on epoch=289
03/11/2022 00:53:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.35 on epoch=294
03/11/2022 00:53:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.40 on epoch=299
03/11/2022 00:53:21 - INFO - __main__ - Global step 600 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=299
03/11/2022 00:53:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=304
03/11/2022 00:53:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.38 on epoch=309
03/11/2022 00:53:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.33 on epoch=314
03/11/2022 00:53:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.42 on epoch=319
03/11/2022 00:53:35 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=324
03/11/2022 00:53:36 - INFO - __main__ - Global step 650 Train loss 0.38 Classification-F1 0.3333333333333333 on epoch=324
03/11/2022 00:53:39 - INFO - __main__ - Step 660 Global step 660 Train loss 0.38 on epoch=329
03/11/2022 00:53:41 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=334
03/11/2022 00:53:44 - INFO - __main__ - Step 680 Global step 680 Train loss 0.34 on epoch=339
03/11/2022 00:53:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.40 on epoch=344
03/11/2022 00:53:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.36 on epoch=349
03/11/2022 00:53:50 - INFO - __main__ - Global step 700 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=349
03/11/2022 00:53:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=354
03/11/2022 00:53:56 - INFO - __main__ - Step 720 Global step 720 Train loss 0.32 on epoch=359
03/11/2022 00:53:59 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
03/11/2022 00:54:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=369
03/11/2022 00:54:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.33 on epoch=374
03/11/2022 00:54:05 - INFO - __main__ - Global step 750 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=374
03/11/2022 00:54:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=379
03/11/2022 00:54:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.34 on epoch=384
03/11/2022 00:54:14 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=389
03/11/2022 00:54:17 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=394
03/11/2022 00:54:19 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=399
03/11/2022 00:54:20 - INFO - __main__ - Global step 800 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=399
03/11/2022 00:54:23 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=404
03/11/2022 00:54:26 - INFO - __main__ - Step 820 Global step 820 Train loss 0.35 on epoch=409
03/11/2022 00:54:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=414
03/11/2022 00:54:31 - INFO - __main__ - Step 840 Global step 840 Train loss 0.37 on epoch=419
03/11/2022 00:54:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.36 on epoch=424
03/11/2022 00:54:35 - INFO - __main__ - Global step 850 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=424
03/11/2022 00:54:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=429
03/11/2022 00:54:41 - INFO - __main__ - Step 870 Global step 870 Train loss 0.34 on epoch=434
03/11/2022 00:54:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=439
03/11/2022 00:54:46 - INFO - __main__ - Step 890 Global step 890 Train loss 0.36 on epoch=444
03/11/2022 00:54:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.36 on epoch=449
03/11/2022 00:54:50 - INFO - __main__ - Global step 900 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=449
03/11/2022 00:54:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=454
03/11/2022 00:54:55 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=459
03/11/2022 00:54:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.36 on epoch=464
03/11/2022 00:55:01 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
03/11/2022 00:55:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=474
03/11/2022 00:55:04 - INFO - __main__ - Global step 950 Train loss 0.35 Classification-F1 0.3992490613266583 on epoch=474
03/11/2022 00:55:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=479
03/11/2022 00:55:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=484
03/11/2022 00:55:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.35 on epoch=489
03/11/2022 00:55:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=494
03/11/2022 00:55:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.38 on epoch=499
03/11/2022 00:55:19 - INFO - __main__ - Global step 1000 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=499
03/11/2022 00:55:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=504
03/11/2022 00:55:25 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.34 on epoch=509
03/11/2022 00:55:28 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=514
03/11/2022 00:55:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=519
03/11/2022 00:55:33 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.36 on epoch=524
03/11/2022 00:55:34 - INFO - __main__ - Global step 1050 Train loss 0.35 Classification-F1 0.3992490613266583 on epoch=524
03/11/2022 00:55:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.36 on epoch=529
03/11/2022 00:55:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.34 on epoch=534
03/11/2022 00:55:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.34 on epoch=539
03/11/2022 00:55:45 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=544
03/11/2022 00:55:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.35 on epoch=549
03/11/2022 00:55:49 - INFO - __main__ - Global step 1100 Train loss 0.35 Classification-F1 0.3992490613266583 on epoch=549
03/11/2022 00:55:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=554
03/11/2022 00:55:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.36 on epoch=559
03/11/2022 00:55:57 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.36 on epoch=564
03/11/2022 00:56:00 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.34 on epoch=569
03/11/2022 00:56:03 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.34 on epoch=574
03/11/2022 00:56:03 - INFO - __main__ - Global step 1150 Train loss 0.35 Classification-F1 0.6666666666666667 on epoch=574
03/11/2022 00:56:06 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.39 on epoch=579
03/11/2022 00:56:09 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.35 on epoch=584
03/11/2022 00:56:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.36 on epoch=589
03/11/2022 00:56:15 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.36 on epoch=594
03/11/2022 00:56:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.36 on epoch=599
03/11/2022 00:56:18 - INFO - __main__ - Global step 1200 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=599
03/11/2022 00:56:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.37 on epoch=604
03/11/2022 00:56:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.35 on epoch=609
03/11/2022 00:56:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=614
03/11/2022 00:56:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=619
03/11/2022 00:56:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=624
03/11/2022 00:56:33 - INFO - __main__ - Global step 1250 Train loss 0.35 Classification-F1 0.4589371980676329 on epoch=624
03/11/2022 00:56:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=629
03/11/2022 00:56:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.31 on epoch=634
03/11/2022 00:56:42 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=639
03/11/2022 00:56:44 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=644
03/11/2022 00:56:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=649
03/11/2022 00:56:48 - INFO - __main__ - Global step 1300 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=649
03/11/2022 00:56:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.38 on epoch=654
03/11/2022 00:56:54 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.33 on epoch=659
03/11/2022 00:56:56 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
03/11/2022 00:56:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.35 on epoch=669
03/11/2022 00:57:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.36 on epoch=674
03/11/2022 00:57:03 - INFO - __main__ - Global step 1350 Train loss 0.35 Classification-F1 0.3992490613266583 on epoch=674
03/11/2022 00:57:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.34 on epoch=679
03/11/2022 00:57:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.34 on epoch=684
03/11/2022 00:57:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.35 on epoch=689
03/11/2022 00:57:14 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.35 on epoch=694
03/11/2022 00:57:17 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.32 on epoch=699
03/11/2022 00:57:17 - INFO - __main__ - Global step 1400 Train loss 0.34 Classification-F1 0.3992490613266583 on epoch=699
03/11/2022 00:57:20 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.33 on epoch=704
03/11/2022 00:57:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.33 on epoch=709
03/11/2022 00:57:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=714
03/11/2022 00:57:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=719
03/11/2022 00:57:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.36 on epoch=724
03/11/2022 00:57:32 - INFO - __main__ - Global step 1450 Train loss 0.33 Classification-F1 0.6101882613510521 on epoch=724
03/11/2022 00:57:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.35 on epoch=729
03/11/2022 00:57:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.35 on epoch=734
03/11/2022 00:57:41 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.33 on epoch=739
03/11/2022 00:57:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.33 on epoch=744
03/11/2022 00:57:46 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.31 on epoch=749
03/11/2022 00:57:47 - INFO - __main__ - Global step 1500 Train loss 0.33 Classification-F1 0.4589371980676329 on epoch=749
03/11/2022 00:57:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.34 on epoch=754
03/11/2022 00:57:53 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.30 on epoch=759
03/11/2022 00:57:55 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.30 on epoch=764
03/11/2022 00:57:58 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.30 on epoch=769
03/11/2022 00:58:01 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.30 on epoch=774
03/11/2022 00:58:02 - INFO - __main__ - Global step 1550 Train loss 0.31 Classification-F1 0.6862745098039216 on epoch=774
03/11/2022 00:58:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.31 on epoch=779
03/11/2022 00:58:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.33 on epoch=784
03/11/2022 00:58:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.34 on epoch=789
03/11/2022 00:58:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.32 on epoch=794
03/11/2022 00:58:16 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.30 on epoch=799
03/11/2022 00:58:17 - INFO - __main__ - Global step 1600 Train loss 0.32 Classification-F1 0.6532019704433498 on epoch=799
03/11/2022 00:58:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.29 on epoch=804
03/11/2022 00:58:22 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.27 on epoch=809
03/11/2022 00:58:25 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.32 on epoch=814
03/11/2022 00:58:28 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=819
03/11/2022 00:58:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.32 on epoch=824
03/11/2022 00:58:31 - INFO - __main__ - Global step 1650 Train loss 0.30 Classification-F1 0.6825396825396826 on epoch=824
03/11/2022 00:58:34 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.30 on epoch=829
03/11/2022 00:58:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.29 on epoch=834
03/11/2022 00:58:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.31 on epoch=839
03/11/2022 00:58:43 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.30 on epoch=844
03/11/2022 00:58:45 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.30 on epoch=849
03/11/2022 00:58:46 - INFO - __main__ - Global step 1700 Train loss 0.30 Classification-F1 0.7184750733137829 on epoch=849
03/11/2022 00:58:49 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.29 on epoch=854
03/11/2022 00:58:52 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=859
03/11/2022 00:58:55 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=864
03/11/2022 00:58:57 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=869
03/11/2022 00:59:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.32 on epoch=874
03/11/2022 00:59:01 - INFO - __main__ - Global step 1750 Train loss 0.27 Classification-F1 0.5901477832512315 on epoch=874
03/11/2022 00:59:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.28 on epoch=879
03/11/2022 00:59:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.27 on epoch=884
03/11/2022 00:59:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.27 on epoch=889
03/11/2022 00:59:12 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=894
03/11/2022 00:59:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=899
03/11/2022 00:59:16 - INFO - __main__ - Global step 1800 Train loss 0.26 Classification-F1 0.6190476190476191 on epoch=899
03/11/2022 00:59:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=904
03/11/2022 00:59:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=909
03/11/2022 00:59:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=914
03/11/2022 00:59:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.29 on epoch=919
03/11/2022 00:59:30 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=924
03/11/2022 00:59:30 - INFO - __main__ - Global step 1850 Train loss 0.27 Classification-F1 0.4554554554554554 on epoch=924
03/11/2022 00:59:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=929
03/11/2022 00:59:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=934
03/11/2022 00:59:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=939
03/11/2022 00:59:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.26 on epoch=944
03/11/2022 00:59:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.20 on epoch=949
03/11/2022 00:59:45 - INFO - __main__ - Global step 1900 Train loss 0.24 Classification-F1 0.5555555555555556 on epoch=949
03/11/2022 00:59:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=954
03/11/2022 00:59:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=959
03/11/2022 00:59:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=964
03/11/2022 00:59:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=969
03/11/2022 00:59:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=974
03/11/2022 01:00:00 - INFO - __main__ - Global step 1950 Train loss 0.23 Classification-F1 0.6113360323886641 on epoch=974
03/11/2022 01:00:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=979
03/11/2022 01:00:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
03/11/2022 01:00:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.19 on epoch=989
03/11/2022 01:00:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=994
03/11/2022 01:00:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=999
03/11/2022 01:00:15 - INFO - __main__ - Global step 2000 Train loss 0.20 Classification-F1 0.6190476190476191 on epoch=999
03/11/2022 01:00:15 - INFO - __main__ - save last model!
03/11/2022 01:00:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 01:00:15 - INFO - __main__ - Start tokenizing ... 1000 instances
03/11/2022 01:00:15 - INFO - __main__ - Printing 3 examples
03/11/2022 01:00:15 - INFO - __main__ -  [amazon_polarity] title: Incomplete and poorly organized [SEP] content: I am currently taking a classroom course to get the MCSE, and the Global Knowledge Certification Press MCSE study guides are the books provided by the school. We have two certs left to go, but my class has already convinced the administration to drop these books. Yes, they are that bad. Explanations that assume you already understand the topic, or worse yet assume the topic is self-explanatory are the norm in these books. Labs leave out critical steps that make doing any part of the exercise impossible. All in all, this entire series of MCSE books is not worth your hard earned money. If you are seriously studying for your MCSE, stay as far away from this series as you can.
03/11/2022 01:00:15 - INFO - __main__ - ['negative']
03/11/2022 01:00:15 - INFO - __main__ -  [amazon_polarity] title: it became too "old" [SEP] content: For me it sounds like bad Mercyful Fate. If u really like good prog check out latest works of Fates. Pleasant Shade is much much more creative and inreresting than this.
03/11/2022 01:00:15 - INFO - __main__ - ['negative']
03/11/2022 01:00:15 - INFO - __main__ -  [amazon_polarity] title: DVD won't work, it says wrong universal code & won't play [SEP] content: I got this 4 my daughter 4 X-mas. It's the only movie she ask for, but when she tried to watch it, it wouldn't work. There is a message that comes up saying wrong universal code. What does this mean, never seen this before. So basically I wasted my money & now need to purchase a new one.
03/11/2022 01:00:15 - INFO - __main__ - ['negative']
03/11/2022 01:00:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 01:00:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:00:16 - INFO - __main__ - Loaded 1000 examples from test data
03/11/2022 01:00:38 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-amazon_polarity/amazon_polarity_16_87_0.2_8_predictions.txt
03/11/2022 01:00:38 - INFO - __main__ - Classification-F1 on test data: 0.6458
03/11/2022 01:00:38 - INFO - __main__ - prefix=amazon_polarity_16_87, lr=0.2, bsz=8, dev_performance=0.7408906882591093, test_performance=0.6458143681642934
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003509521484375 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15609", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 13614, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15610", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 13614, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 13614, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15702): No such process
Task: blimp-sentential_negation_npi_licensor_present, Checkpoint: models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_fomaml_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24568
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_sh20h7gr/none_b_lqgc5t
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24568
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_sh20h7gr/none_b_lqgc5t/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_sh20h7gr/none_b_lqgc5t/attempt_0/1/error.json
03/11/2022 01:00:44 - INFO - __main__ - Namespace(task_dir='data/blimp-sentential_negation_npi_licensor_present/', task_name='blimp-sentential_negation_npi_licensor_present', identifier='T5-large-fomaml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/11/2022 01:00:44 - INFO - __main__ - models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present
Output directory () already exists and is not empty.
03/11/2022 01:00:44 - INFO - __main__ - Namespace(task_dir='data/blimp-sentential_negation_npi_licensor_present/', task_name='blimp-sentential_negation_npi_licensor_present', identifier='T5-large-fomaml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/11/2022 01:00:44 - INFO - __main__ - models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present
03/11/2022 01:00:45 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/11/2022 01:00:45 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/11/2022 01:00:45 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/11/2022 01:00:45 - INFO - __main__ - args.device: cuda:0
03/11/2022 01:00:45 - INFO - __main__ - Using 2 gpus
03/11/2022 01:00:45 - INFO - __main__ - Fine-tuning the following samples: ['blimp-sentential_negation_npi_licensor_present_16_100', 'blimp-sentential_negation_npi_licensor_present_16_13', 'blimp-sentential_negation_npi_licensor_present_16_21', 'blimp-sentential_negation_npi_licensor_present_16_42', 'blimp-sentential_negation_npi_licensor_present_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/11/2022 01:00:45 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/11/2022 01:00:45 - INFO - __main__ - args.device: cuda:1
03/11/2022 01:00:45 - INFO - __main__ - Using 2 gpus
03/11/2022 01:00:45 - INFO - __main__ - Fine-tuning the following samples: ['blimp-sentential_negation_npi_licensor_present_16_100', 'blimp-sentential_negation_npi_licensor_present_16_13', 'blimp-sentential_negation_npi_licensor_present_16_21', 'blimp-sentential_negation_npi_licensor_present_16_42', 'blimp-sentential_negation_npi_licensor_present_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/11/2022 01:00:54 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.5, bsz=8 ...
03/11/2022 01:00:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:00:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:00:55 - INFO - __main__ - Printing 3 examples
03/11/2022 01:00:55 - INFO - __main__ - Printing 3 examples
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:00:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 01:00:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:00:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:00:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:00:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:00:55 - INFO - __main__ - Printing 3 examples
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:00:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:00:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:00:55 - INFO - __main__ - Printing 3 examples
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/11/2022 01:00:55 - INFO - __main__ - ['sentence 1']
03/11/2022 01:00:55 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:00:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:00:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:00:55 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:00:55 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:01:10 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:01:10 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:01:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:01:11 - INFO - __main__ - Starting training!
03/11/2022 01:01:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:01:15 - INFO - __main__ - Starting training!
03/11/2022 01:01:18 - INFO - __main__ - Step 10 Global step 10 Train loss 0.97 on epoch=4
03/11/2022 01:01:20 - INFO - __main__ - Step 20 Global step 20 Train loss 0.43 on epoch=9
03/11/2022 01:01:23 - INFO - __main__ - Step 30 Global step 30 Train loss 0.35 on epoch=14
03/11/2022 01:01:25 - INFO - __main__ - Step 40 Global step 40 Train loss 0.32 on epoch=19
03/11/2022 01:01:27 - INFO - __main__ - Step 50 Global step 50 Train loss 0.32 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/11/2022 01:01:28 - INFO - __main__ - Global step 50 Train loss 0.48 ACC 0.5 on epoch=24
03/11/2022 01:01:28 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 01:01:30 - INFO - __main__ - Step 60 Global step 60 Train loss 0.25 on epoch=29
03/11/2022 01:01:32 - INFO - __main__ - Step 70 Global step 70 Train loss 0.32 on epoch=34
03/11/2022 01:01:34 - INFO - __main__ - Step 80 Global step 80 Train loss 0.27 on epoch=39
03/11/2022 01:01:37 - INFO - __main__ - Step 90 Global step 90 Train loss 0.26 on epoch=44
03/11/2022 01:01:39 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
03/11/2022 01:01:39 - INFO - __main__ - Global step 100 Train loss 0.28 ACC 0.5 on epoch=49
03/11/2022 01:01:42 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=54
03/11/2022 01:01:44 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=59
03/11/2022 01:01:46 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=64
03/11/2022 01:01:48 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=69
03/11/2022 01:01:51 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
03/11/2022 01:01:51 - INFO - __main__ - Global step 150 Train loss 0.27 ACC 0.5 on epoch=74
03/11/2022 01:01:53 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=79
03/11/2022 01:01:56 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=84
03/11/2022 01:01:58 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
03/11/2022 01:02:00 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
03/11/2022 01:02:02 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/11/2022 01:02:03 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.5 on epoch=99
03/11/2022 01:02:05 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 01:02:07 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=109
03/11/2022 01:02:10 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=114
03/11/2022 01:02:12 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=119
03/11/2022 01:02:14 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=124
03/11/2022 01:02:15 - INFO - __main__ - Global step 250 Train loss 0.26 ACC 0.5 on epoch=124
03/11/2022 01:02:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.49 on epoch=129
03/11/2022 01:02:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
03/11/2022 01:02:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.29 on epoch=139
03/11/2022 01:02:23 - INFO - __main__ - Step 290 Global step 290 Train loss 0.53 on epoch=144
03/11/2022 01:02:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.60 on epoch=149
03/11/2022 01:02:26 - INFO - __main__ - Global step 300 Train loss 0.43 ACC 0.5 on epoch=149
03/11/2022 01:02:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=154
03/11/2022 01:02:31 - INFO - __main__ - Step 320 Global step 320 Train loss 0.37 on epoch=159
03/11/2022 01:02:33 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
03/11/2022 01:02:35 - INFO - __main__ - Step 340 Global step 340 Train loss 0.31 on epoch=169
03/11/2022 01:02:37 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
03/11/2022 01:02:38 - INFO - __main__ - Global step 350 Train loss 0.33 ACC 0.5625 on epoch=174
03/11/2022 01:02:38 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=174, global_step=350
03/11/2022 01:02:40 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=179
03/11/2022 01:02:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.32 on epoch=184
03/11/2022 01:02:45 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=189
03/11/2022 01:02:47 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=194
03/11/2022 01:02:49 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
03/11/2022 01:02:50 - INFO - __main__ - Global step 400 Train loss 0.28 ACC 0.5 on epoch=199
03/11/2022 01:02:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.30 on epoch=204
03/11/2022 01:02:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.30 on epoch=209
03/11/2022 01:02:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
03/11/2022 01:02:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
03/11/2022 01:03:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
03/11/2022 01:03:01 - INFO - __main__ - Global step 450 Train loss 0.27 ACC 0.53125 on epoch=224
03/11/2022 01:03:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
03/11/2022 01:03:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=234
03/11/2022 01:03:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 01:03:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
03/11/2022 01:03:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
03/11/2022 01:03:13 - INFO - __main__ - Global step 500 Train loss 0.26 ACC 0.53125 on epoch=249
03/11/2022 01:03:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
03/11/2022 01:03:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
03/11/2022 01:03:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
03/11/2022 01:03:21 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=269
03/11/2022 01:03:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.27 on epoch=274
03/11/2022 01:03:24 - INFO - __main__ - Global step 550 Train loss 0.26 ACC 0.5625 on epoch=274
03/11/2022 01:03:26 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/11/2022 01:03:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
03/11/2022 01:03:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
03/11/2022 01:03:33 - INFO - __main__ - Step 590 Global step 590 Train loss 0.27 on epoch=294
03/11/2022 01:03:35 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
03/11/2022 01:03:36 - INFO - __main__ - Global step 600 Train loss 0.26 ACC 0.5 on epoch=299
03/11/2022 01:03:38 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
03/11/2022 01:03:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=309
03/11/2022 01:03:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=314
03/11/2022 01:03:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
03/11/2022 01:03:47 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
03/11/2022 01:03:47 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.5 on epoch=324
03/11/2022 01:03:50 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
03/11/2022 01:03:52 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=334
03/11/2022 01:03:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=339
03/11/2022 01:03:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
03/11/2022 01:03:59 - INFO - __main__ - Step 700 Global step 700 Train loss 0.27 on epoch=349
03/11/2022 01:03:59 - INFO - __main__ - Global step 700 Train loss 0.26 ACC 0.5 on epoch=349
03/11/2022 01:04:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=354
03/11/2022 01:04:04 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=359
03/11/2022 01:04:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=364
03/11/2022 01:04:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
03/11/2022 01:04:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=374
03/11/2022 01:04:11 - INFO - __main__ - Global step 750 Train loss 0.25 ACC 0.5 on epoch=374
03/11/2022 01:04:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=379
03/11/2022 01:04:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
03/11/2022 01:04:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=389
03/11/2022 01:04:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=394
03/11/2022 01:04:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=399
03/11/2022 01:04:23 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.59375 on epoch=399
03/11/2022 01:04:23 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=399, global_step=800
03/11/2022 01:04:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
03/11/2022 01:04:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=409
03/11/2022 01:04:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=414
03/11/2022 01:04:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=419
03/11/2022 01:04:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
03/11/2022 01:04:34 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.59375 on epoch=424
03/11/2022 01:04:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=429
03/11/2022 01:04:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
03/11/2022 01:04:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=439
03/11/2022 01:04:43 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=444
03/11/2022 01:04:45 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=449
03/11/2022 01:04:46 - INFO - __main__ - Global step 900 Train loss 0.25 ACC 0.71875 on epoch=449
03/11/2022 01:04:46 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.71875 on epoch=449, global_step=900
03/11/2022 01:04:48 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
03/11/2022 01:04:50 - INFO - __main__ - Step 920 Global step 920 Train loss 0.27 on epoch=459
03/11/2022 01:04:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=464
03/11/2022 01:04:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=469
03/11/2022 01:04:57 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=474
03/11/2022 01:04:58 - INFO - __main__ - Global step 950 Train loss 0.25 ACC 0.5625 on epoch=474
03/11/2022 01:05:00 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=479
03/11/2022 01:05:02 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=484
03/11/2022 01:05:04 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=489
03/11/2022 01:05:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=494
03/11/2022 01:05:09 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.26 on epoch=499
03/11/2022 01:05:09 - INFO - __main__ - Global step 1000 Train loss 0.26 ACC 0.5 on epoch=499
03/11/2022 01:05:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.28 on epoch=504
03/11/2022 01:05:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.27 on epoch=509
03/11/2022 01:05:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=514
03/11/2022 01:05:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=519
03/11/2022 01:05:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=524
03/11/2022 01:05:21 - INFO - __main__ - Global step 1050 Train loss 0.26 ACC 0.5 on epoch=524
03/11/2022 01:05:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.26 on epoch=529
03/11/2022 01:05:26 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=534
03/11/2022 01:05:28 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=539
03/11/2022 01:05:30 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=544
03/11/2022 01:05:32 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=549
03/11/2022 01:05:33 - INFO - __main__ - Global step 1100 Train loss 0.25 ACC 0.53125 on epoch=549
03/11/2022 01:05:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=554
03/11/2022 01:05:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=559
03/11/2022 01:05:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=564
03/11/2022 01:05:42 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=569
03/11/2022 01:05:44 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=574
03/11/2022 01:05:44 - INFO - __main__ - Global step 1150 Train loss 0.24 ACC 0.5 on epoch=574
03/11/2022 01:05:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=579
03/11/2022 01:05:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=584
03/11/2022 01:05:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=589
03/11/2022 01:05:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=594
03/11/2022 01:05:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.25 on epoch=599
03/11/2022 01:05:56 - INFO - __main__ - Global step 1200 Train loss 0.25 ACC 0.59375 on epoch=599
03/11/2022 01:05:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
03/11/2022 01:06:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=609
03/11/2022 01:06:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=614
03/11/2022 01:06:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=619
03/11/2022 01:06:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=624
03/11/2022 01:06:08 - INFO - __main__ - Global step 1250 Train loss 0.26 ACC 0.59375 on epoch=624
03/11/2022 01:06:10 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=629
03/11/2022 01:06:12 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=634
03/11/2022 01:06:14 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=639
03/11/2022 01:06:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=644
03/11/2022 01:06:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=649
03/11/2022 01:06:19 - INFO - __main__ - Global step 1300 Train loss 0.25 ACC 0.5625 on epoch=649
03/11/2022 01:06:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=654
03/11/2022 01:06:24 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=659
03/11/2022 01:06:26 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=664
03/11/2022 01:06:28 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=669
03/11/2022 01:06:31 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
03/11/2022 01:06:31 - INFO - __main__ - Global step 1350 Train loss 0.24 ACC 0.5625 on epoch=674
03/11/2022 01:06:33 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=679
03/11/2022 01:06:36 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=684
03/11/2022 01:06:38 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=689
03/11/2022 01:06:40 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=694
03/11/2022 01:06:42 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=699
03/11/2022 01:06:43 - INFO - __main__ - Global step 1400 Train loss 0.23 ACC 0.625 on epoch=699
03/11/2022 01:06:45 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=704
03/11/2022 01:06:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=709
03/11/2022 01:06:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=714
03/11/2022 01:06:52 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=719
03/11/2022 01:06:54 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=724
03/11/2022 01:06:54 - INFO - __main__ - Global step 1450 Train loss 0.24 ACC 0.53125 on epoch=724
03/11/2022 01:06:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.25 on epoch=729
03/11/2022 01:06:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=734
03/11/2022 01:07:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=739
03/11/2022 01:07:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=744
03/11/2022 01:07:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.25 on epoch=749
03/11/2022 01:07:06 - INFO - __main__ - Global step 1500 Train loss 0.25 ACC 0.59375 on epoch=749
03/11/2022 01:07:08 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=754
03/11/2022 01:07:10 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=759
03/11/2022 01:07:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=764
03/11/2022 01:07:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=769
03/11/2022 01:07:17 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=774
03/11/2022 01:07:18 - INFO - __main__ - Global step 1550 Train loss 0.24 ACC 0.53125 on epoch=774
03/11/2022 01:07:20 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=779
03/11/2022 01:07:22 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=784
03/11/2022 01:07:24 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=789
03/11/2022 01:07:26 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.22 on epoch=794
03/11/2022 01:07:29 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=799
03/11/2022 01:07:29 - INFO - __main__ - Global step 1600 Train loss 0.22 ACC 0.59375 on epoch=799
03/11/2022 01:07:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=804
03/11/2022 01:07:34 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=809
03/11/2022 01:07:36 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=814
03/11/2022 01:07:38 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.60 on epoch=819
03/11/2022 01:07:40 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.86 on epoch=824
03/11/2022 01:07:41 - INFO - __main__ - Global step 1650 Train loss 0.44 ACC 0.5 on epoch=824
03/11/2022 01:07:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.09 on epoch=829
03/11/2022 01:07:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.36 on epoch=834
03/11/2022 01:07:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.44 on epoch=839
03/11/2022 01:07:50 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.73 on epoch=844
03/11/2022 01:07:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.79 on epoch=849
03/11/2022 01:07:53 - INFO - __main__ - Global step 1700 Train loss 0.68 ACC 0.53125 on epoch=849
03/11/2022 01:07:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.80 on epoch=854
03/11/2022 01:07:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.57 on epoch=859
03/11/2022 01:07:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.33 on epoch=864
03/11/2022 01:08:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=869
03/11/2022 01:08:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
03/11/2022 01:08:04 - INFO - __main__ - Global step 1750 Train loss 0.49 ACC 0.53125 on epoch=874
03/11/2022 01:08:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.30 on epoch=879
03/11/2022 01:08:09 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
03/11/2022 01:08:11 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.30 on epoch=889
03/11/2022 01:08:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.33 on epoch=894
03/11/2022 01:08:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.31 on epoch=899
03/11/2022 01:08:16 - INFO - __main__ - Global step 1800 Train loss 0.32 ACC 0.5 on epoch=899
03/11/2022 01:08:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.30 on epoch=904
03/11/2022 01:08:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.31 on epoch=909
03/11/2022 01:08:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.26 on epoch=914
03/11/2022 01:08:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.28 on epoch=919
03/11/2022 01:08:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=924
03/11/2022 01:08:28 - INFO - __main__ - Global step 1850 Train loss 0.28 ACC 0.5 on epoch=924
03/11/2022 01:08:30 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.28 on epoch=929
03/11/2022 01:08:32 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=934
03/11/2022 01:08:34 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.28 on epoch=939
03/11/2022 01:08:37 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.35 on epoch=944
03/11/2022 01:08:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=949
03/11/2022 01:08:39 - INFO - __main__ - Global step 1900 Train loss 0.31 ACC 0.5 on epoch=949
03/11/2022 01:08:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.29 on epoch=954
03/11/2022 01:08:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.32 on epoch=959
03/11/2022 01:08:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
03/11/2022 01:08:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
03/11/2022 01:08:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.38 on epoch=974
03/11/2022 01:08:51 - INFO - __main__ - Global step 1950 Train loss 0.34 ACC 0.5 on epoch=974
03/11/2022 01:08:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.44 on epoch=979
03/11/2022 01:08:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
03/11/2022 01:08:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
03/11/2022 01:09:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=994
03/11/2022 01:09:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.30 on epoch=999
03/11/2022 01:09:03 - INFO - __main__ - Global step 2000 Train loss 0.34 ACC 0.5 on epoch=999
03/11/2022 01:09:03 - INFO - __main__ - save last model!
03/11/2022 01:09:03 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 01:09:03 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 01:09:03 - INFO - __main__ - Printing 3 examples
03/11/2022 01:09:03 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 01:09:03 - INFO - __main__ - ['sentence 2']
03/11/2022 01:09:03 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 01:09:03 - INFO - __main__ - ['sentence 2']
03/11/2022 01:09:03 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 01:09:03 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:03 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:09:03 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:09:03 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 01:09:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:09:03 - INFO - __main__ - Printing 3 examples
03/11/2022 01:09:03 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/11/2022 01:09:03 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:03 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/11/2022 01:09:03 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:03 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:09:03 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 01:09:03 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:09:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:09:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:09:03 - INFO - __main__ - Printing 3 examples
03/11/2022 01:09:03 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/11/2022 01:09:03 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:03 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/11/2022 01:09:03 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:03 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/11/2022 01:09:03 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:03 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:09:03 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:09:03 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:09:06 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_100_0.5_8_predictions.txt
03/11/2022 01:09:06 - INFO - __main__ - ACC on test data: 0.5050
03/11/2022 01:09:07 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.5, bsz=8, dev_performance=0.71875, test_performance=0.505
03/11/2022 01:09:07 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.4, bsz=8 ...
03/11/2022 01:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:09:07 - INFO - __main__ - Printing 3 examples
03/11/2022 01:09:07 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/11/2022 01:09:07 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:07 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/11/2022 01:09:07 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:07 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:09:07 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 01:09:07 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:09:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:09:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:09:08 - INFO - __main__ - Printing 3 examples
03/11/2022 01:09:08 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/11/2022 01:09:08 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:08 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/11/2022 01:09:08 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:08 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/11/2022 01:09:08 - INFO - __main__ - ['sentence 1']
03/11/2022 01:09:08 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:09:08 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:09:08 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:09:15 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:09:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:09:16 - INFO - __main__ - Starting training!
03/11/2022 01:09:19 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:09:20 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:09:20 - INFO - __main__ - Starting training!
03/11/2022 01:09:24 - INFO - __main__ - Step 10 Global step 10 Train loss 1.17 on epoch=4
03/11/2022 01:09:26 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=9
03/11/2022 01:09:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.34 on epoch=14
03/11/2022 01:09:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.33 on epoch=19
03/11/2022 01:09:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=24
03/11/2022 01:09:33 - INFO - __main__ - Global step 50 Train loss 0.52 ACC 0.5 on epoch=24
03/11/2022 01:09:33 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 01:09:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=29
03/11/2022 01:09:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.28 on epoch=34
03/11/2022 01:09:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.27 on epoch=39
03/11/2022 01:09:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=44
03/11/2022 01:09:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=49
03/11/2022 01:09:45 - INFO - __main__ - Global step 100 Train loss 0.27 ACC 0.5 on epoch=49
03/11/2022 01:09:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=54
03/11/2022 01:09:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=59
03/11/2022 01:09:52 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
03/11/2022 01:09:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.24 on epoch=69
03/11/2022 01:09:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=74
03/11/2022 01:09:57 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.5625 on epoch=74
03/11/2022 01:09:57 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=74, global_step=150
03/11/2022 01:09:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
03/11/2022 01:10:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=84
03/11/2022 01:10:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
03/11/2022 01:10:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
03/11/2022 01:10:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/11/2022 01:10:08 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.46875 on epoch=99
03/11/2022 01:10:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 01:10:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 01:10:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
03/11/2022 01:10:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
03/11/2022 01:10:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
03/11/2022 01:10:20 - INFO - __main__ - Global step 250 Train loss 0.26 ACC 0.5 on epoch=124
03/11/2022 01:10:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
03/11/2022 01:10:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=134
03/11/2022 01:10:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 01:10:29 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
03/11/2022 01:10:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
03/11/2022 01:10:32 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.4375 on epoch=149
03/11/2022 01:10:34 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 01:10:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 01:10:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
03/11/2022 01:10:41 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 01:10:43 - INFO - __main__ - Step 350 Global step 350 Train loss 0.31 on epoch=174
03/11/2022 01:10:43 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.65625 on epoch=174
03/11/2022 01:10:43 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=174, global_step=350
03/11/2022 01:10:46 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
03/11/2022 01:10:48 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=184
03/11/2022 01:10:50 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
03/11/2022 01:10:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 01:10:55 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 01:10:55 - INFO - __main__ - Global step 400 Train loss 0.27 ACC 0.5 on epoch=199
03/11/2022 01:10:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
03/11/2022 01:10:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
03/11/2022 01:11:02 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
03/11/2022 01:11:04 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
03/11/2022 01:11:06 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
03/11/2022 01:11:07 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.5 on epoch=224
03/11/2022 01:11:09 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 01:11:11 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 01:11:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=239
03/11/2022 01:11:16 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=244
03/11/2022 01:11:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
03/11/2022 01:11:18 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5 on epoch=249
03/11/2022 01:11:21 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 01:11:23 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
03/11/2022 01:11:25 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
03/11/2022 01:11:27 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
03/11/2022 01:11:30 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
03/11/2022 01:11:30 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.5 on epoch=274
03/11/2022 01:11:32 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/11/2022 01:11:34 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=284
03/11/2022 01:11:37 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/11/2022 01:11:39 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=294
03/11/2022 01:11:41 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
03/11/2022 01:11:42 - INFO - __main__ - Global step 600 Train loss 0.24 ACC 0.5 on epoch=299
03/11/2022 01:11:44 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=304
03/11/2022 01:11:46 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
03/11/2022 01:11:48 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=314
03/11/2022 01:11:51 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
03/11/2022 01:11:53 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
03/11/2022 01:11:53 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.5 on epoch=324
03/11/2022 01:11:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=329
03/11/2022 01:11:58 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=334
03/11/2022 01:12:00 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=339
03/11/2022 01:12:02 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
03/11/2022 01:12:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=349
03/11/2022 01:12:05 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.5 on epoch=349
03/11/2022 01:12:07 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=354
03/11/2022 01:12:09 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
03/11/2022 01:12:12 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
03/11/2022 01:12:14 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=369
03/11/2022 01:12:16 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 01:12:16 - INFO - __main__ - Global step 750 Train loss 0.24 ACC 0.5 on epoch=374
03/11/2022 01:12:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
03/11/2022 01:12:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
03/11/2022 01:12:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
03/11/2022 01:12:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=394
03/11/2022 01:12:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=399
03/11/2022 01:12:28 - INFO - __main__ - Global step 800 Train loss 0.24 ACC 0.5 on epoch=399
03/11/2022 01:12:30 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=404
03/11/2022 01:12:33 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
03/11/2022 01:12:35 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
03/11/2022 01:12:37 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/11/2022 01:12:39 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=424
03/11/2022 01:12:40 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.5 on epoch=424
03/11/2022 01:12:42 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=429
03/11/2022 01:12:44 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=434
03/11/2022 01:12:46 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=439
03/11/2022 01:12:49 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=444
03/11/2022 01:12:51 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=449
03/11/2022 01:12:51 - INFO - __main__ - Global step 900 Train loss 0.24 ACC 0.5 on epoch=449
03/11/2022 01:12:54 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=454
03/11/2022 01:12:56 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
03/11/2022 01:12:58 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/11/2022 01:13:00 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=469
03/11/2022 01:13:02 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=474
03/11/2022 01:13:03 - INFO - __main__ - Global step 950 Train loss 0.24 ACC 0.5 on epoch=474
03/11/2022 01:13:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=479
03/11/2022 01:13:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=484
03/11/2022 01:13:10 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=489
03/11/2022 01:13:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=494
03/11/2022 01:13:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=499
03/11/2022 01:13:15 - INFO - __main__ - Global step 1000 Train loss 0.24 ACC 0.5 on epoch=499
03/11/2022 01:13:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.26 on epoch=504
03/11/2022 01:13:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.22 on epoch=509
03/11/2022 01:13:21 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=514
03/11/2022 01:13:23 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=519
03/11/2022 01:13:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=524
03/11/2022 01:13:26 - INFO - __main__ - Global step 1050 Train loss 0.23 ACC 0.5 on epoch=524
03/11/2022 01:13:28 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=529
03/11/2022 01:13:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=534
03/11/2022 01:13:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=539
03/11/2022 01:13:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/11/2022 01:13:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=549
03/11/2022 01:13:38 - INFO - __main__ - Global step 1100 Train loss 0.23 ACC 0.5 on epoch=549
03/11/2022 01:13:40 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=554
03/11/2022 01:13:42 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=559
03/11/2022 01:13:44 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=564
03/11/2022 01:13:47 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.22 on epoch=569
03/11/2022 01:13:49 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=574
03/11/2022 01:13:49 - INFO - __main__ - Global step 1150 Train loss 0.24 ACC 0.5 on epoch=574
03/11/2022 01:13:52 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
03/11/2022 01:13:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=584
03/11/2022 01:13:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.27 on epoch=589
03/11/2022 01:13:59 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=594
03/11/2022 01:14:01 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=599
03/11/2022 01:14:01 - INFO - __main__ - Global step 1200 Train loss 0.24 ACC 0.5 on epoch=599
03/11/2022 01:14:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
03/11/2022 01:14:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=609
03/11/2022 01:14:08 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=614
03/11/2022 01:14:10 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=619
03/11/2022 01:14:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/11/2022 01:14:13 - INFO - __main__ - Global step 1250 Train loss 0.24 ACC 0.5 on epoch=624
03/11/2022 01:14:15 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=629
03/11/2022 01:14:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=634
03/11/2022 01:14:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.40 on epoch=639
03/11/2022 01:14:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.44 on epoch=644
03/11/2022 01:14:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.50 on epoch=649
03/11/2022 01:14:25 - INFO - __main__ - Global step 1300 Train loss 0.37 ACC 0.46875 on epoch=649
03/11/2022 01:14:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=654
03/11/2022 01:14:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.32 on epoch=659
03/11/2022 01:14:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.29 on epoch=664
03/11/2022 01:14:34 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=669
03/11/2022 01:14:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
03/11/2022 01:14:37 - INFO - __main__ - Global step 1350 Train loss 0.29 ACC 0.59375 on epoch=674
03/11/2022 01:14:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=679
03/11/2022 01:14:41 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=684
03/11/2022 01:14:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=689
03/11/2022 01:14:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.21 on epoch=694
03/11/2022 01:14:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=699
03/11/2022 01:14:48 - INFO - __main__ - Global step 1400 Train loss 0.24 ACC 0.5625 on epoch=699
03/11/2022 01:14:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=704
03/11/2022 01:14:53 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=709
03/11/2022 01:14:55 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
03/11/2022 01:14:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=719
03/11/2022 01:15:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=724
03/11/2022 01:15:00 - INFO - __main__ - Global step 1450 Train loss 0.24 ACC 0.625 on epoch=724
03/11/2022 01:15:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=729
03/11/2022 01:15:05 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=734
03/11/2022 01:15:07 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=739
03/11/2022 01:15:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=744
03/11/2022 01:15:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=749
03/11/2022 01:15:12 - INFO - __main__ - Global step 1500 Train loss 0.24 ACC 0.5625 on epoch=749
03/11/2022 01:15:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=754
03/11/2022 01:15:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=759
03/11/2022 01:15:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=764
03/11/2022 01:15:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=769
03/11/2022 01:15:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
03/11/2022 01:15:24 - INFO - __main__ - Global step 1550 Train loss 0.25 ACC 0.5 on epoch=774
03/11/2022 01:15:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=779
03/11/2022 01:15:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.24 on epoch=784
03/11/2022 01:15:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=789
03/11/2022 01:15:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.35 on epoch=794
03/11/2022 01:15:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=799
03/11/2022 01:15:36 - INFO - __main__ - Global step 1600 Train loss 0.32 ACC 0.5 on epoch=799
03/11/2022 01:15:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.36 on epoch=804
03/11/2022 01:15:41 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.41 on epoch=809
03/11/2022 01:15:43 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.68 on epoch=814
03/11/2022 01:15:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
03/11/2022 01:15:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.28 on epoch=824
03/11/2022 01:15:48 - INFO - __main__ - Global step 1650 Train loss 0.43 ACC 0.5 on epoch=824
03/11/2022 01:15:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=829
03/11/2022 01:15:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
03/11/2022 01:15:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.27 on epoch=839
03/11/2022 01:15:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.27 on epoch=844
03/11/2022 01:15:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.27 on epoch=849
03/11/2022 01:16:00 - INFO - __main__ - Global step 1700 Train loss 0.27 ACC 0.5 on epoch=849
03/11/2022 01:16:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=854
03/11/2022 01:16:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.26 on epoch=859
03/11/2022 01:16:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
03/11/2022 01:16:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.26 on epoch=869
03/11/2022 01:16:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.26 on epoch=874
03/11/2022 01:16:11 - INFO - __main__ - Global step 1750 Train loss 0.26 ACC 0.5 on epoch=874
03/11/2022 01:16:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=879
03/11/2022 01:16:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.25 on epoch=884
03/11/2022 01:16:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.28 on epoch=889
03/11/2022 01:16:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.28 on epoch=894
03/11/2022 01:16:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.27 on epoch=899
03/11/2022 01:16:23 - INFO - __main__ - Global step 1800 Train loss 0.27 ACC 0.5 on epoch=899
03/11/2022 01:16:25 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.27 on epoch=904
03/11/2022 01:16:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.28 on epoch=909
03/11/2022 01:16:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.28 on epoch=914
03/11/2022 01:16:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=919
03/11/2022 01:16:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.27 on epoch=924
03/11/2022 01:16:35 - INFO - __main__ - Global step 1850 Train loss 0.27 ACC 0.5 on epoch=924
03/11/2022 01:16:37 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=929
03/11/2022 01:16:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=934
03/11/2022 01:16:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.27 on epoch=939
03/11/2022 01:16:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=944
03/11/2022 01:16:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=949
03/11/2022 01:16:47 - INFO - __main__ - Global step 1900 Train loss 0.26 ACC 0.5 on epoch=949
03/11/2022 01:16:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=954
03/11/2022 01:16:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.27 on epoch=959
03/11/2022 01:16:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=964
03/11/2022 01:16:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=969
03/11/2022 01:16:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.26 on epoch=974
03/11/2022 01:16:58 - INFO - __main__ - Global step 1950 Train loss 0.25 ACC 0.5 on epoch=974
03/11/2022 01:17:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.27 on epoch=979
03/11/2022 01:17:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.28 on epoch=984
03/11/2022 01:17:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.31 on epoch=989
03/11/2022 01:17:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.28 on epoch=994
03/11/2022 01:17:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.27 on epoch=999
03/11/2022 01:17:10 - INFO - __main__ - Global step 2000 Train loss 0.28 ACC 0.5 on epoch=999
03/11/2022 01:17:10 - INFO - __main__ - save last model!
03/11/2022 01:17:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 01:17:10 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 01:17:10 - INFO - __main__ - Printing 3 examples
03/11/2022 01:17:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 01:17:10 - INFO - __main__ - ['sentence 2']
03/11/2022 01:17:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 01:17:10 - INFO - __main__ - ['sentence 2']
03/11/2022 01:17:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 01:17:10 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:10 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:17:10 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:17:10 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 01:17:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:17:10 - INFO - __main__ - Printing 3 examples
03/11/2022 01:17:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/11/2022 01:17:11 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/11/2022 01:17:11 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:17:11 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 01:17:11 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:17:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:17:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:17:11 - INFO - __main__ - Printing 3 examples
03/11/2022 01:17:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/11/2022 01:17:11 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/11/2022 01:17:11 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/11/2022 01:17:11 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:11 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:17:11 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:17:11 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:17:14 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_100_0.4_8_predictions.txt
03/11/2022 01:17:14 - INFO - __main__ - ACC on test data: 0.4700
03/11/2022 01:17:14 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.4, bsz=8, dev_performance=0.65625, test_performance=0.47
03/11/2022 01:17:14 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.3, bsz=8 ...
03/11/2022 01:17:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:17:15 - INFO - __main__ - Printing 3 examples
03/11/2022 01:17:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/11/2022 01:17:15 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/11/2022 01:17:15 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:17:15 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 01:17:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:17:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:17:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:17:15 - INFO - __main__ - Printing 3 examples
03/11/2022 01:17:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/11/2022 01:17:15 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/11/2022 01:17:15 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/11/2022 01:17:15 - INFO - __main__ - ['sentence 1']
03/11/2022 01:17:15 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:17:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:17:15 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:17:23 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:17:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:17:23 - INFO - __main__ - Starting training!
03/11/2022 01:17:29 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:17:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:17:30 - INFO - __main__ - Starting training!
03/11/2022 01:17:32 - INFO - __main__ - Step 10 Global step 10 Train loss 1.33 on epoch=4
03/11/2022 01:17:35 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=9
03/11/2022 01:17:37 - INFO - __main__ - Step 30 Global step 30 Train loss 0.36 on epoch=14
03/11/2022 01:17:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.35 on epoch=19
03/11/2022 01:17:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=24
03/11/2022 01:17:42 - INFO - __main__ - Global step 50 Train loss 0.57 ACC 0.5 on epoch=24
03/11/2022 01:17:42 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 01:17:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.32 on epoch=29
03/11/2022 01:17:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=34
03/11/2022 01:17:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=39
03/11/2022 01:17:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=44
03/11/2022 01:17:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.28 on epoch=49
03/11/2022 01:17:54 - INFO - __main__ - Global step 100 Train loss 0.30 ACC 0.5 on epoch=49
03/11/2022 01:17:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
03/11/2022 01:17:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=59
03/11/2022 01:18:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.24 on epoch=64
03/11/2022 01:18:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
03/11/2022 01:18:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
03/11/2022 01:18:05 - INFO - __main__ - Global step 150 Train loss 0.27 ACC 0.5 on epoch=74
03/11/2022 01:18:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
03/11/2022 01:18:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
03/11/2022 01:18:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
03/11/2022 01:18:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
03/11/2022 01:18:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/11/2022 01:18:17 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.5 on epoch=99
03/11/2022 01:18:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
03/11/2022 01:18:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
03/11/2022 01:18:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
03/11/2022 01:18:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
03/11/2022 01:18:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=124
03/11/2022 01:18:28 - INFO - __main__ - Global step 250 Train loss 0.24 ACC 0.59375 on epoch=124
03/11/2022 01:18:29 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=124, global_step=250
03/11/2022 01:18:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
03/11/2022 01:18:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 01:18:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 01:18:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
03/11/2022 01:18:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
03/11/2022 01:18:40 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.5 on epoch=149
03/11/2022 01:18:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
03/11/2022 01:18:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
03/11/2022 01:18:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
03/11/2022 01:18:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
03/11/2022 01:18:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
03/11/2022 01:18:52 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.5 on epoch=174
03/11/2022 01:18:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 01:18:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 01:18:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 01:19:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=194
03/11/2022 01:19:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
03/11/2022 01:19:03 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.5 on epoch=199
03/11/2022 01:19:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=204
03/11/2022 01:19:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
03/11/2022 01:19:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
03/11/2022 01:19:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
03/11/2022 01:19:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
03/11/2022 01:19:15 - INFO - __main__ - Global step 450 Train loss 0.24 ACC 0.5 on epoch=224
03/11/2022 01:19:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
03/11/2022 01:19:19 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 01:19:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 01:19:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
03/11/2022 01:19:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
03/11/2022 01:19:27 - INFO - __main__ - Global step 500 Train loss 0.24 ACC 0.5 on epoch=249
03/11/2022 01:19:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=254
03/11/2022 01:19:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
03/11/2022 01:19:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=264
03/11/2022 01:19:35 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
03/11/2022 01:19:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
03/11/2022 01:19:38 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.5 on epoch=274
03/11/2022 01:19:40 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
03/11/2022 01:19:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=284
03/11/2022 01:19:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
03/11/2022 01:19:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
03/11/2022 01:19:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
03/11/2022 01:19:50 - INFO - __main__ - Global step 600 Train loss 0.24 ACC 0.5 on epoch=299
03/11/2022 01:19:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=304
03/11/2022 01:19:54 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
03/11/2022 01:19:56 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=314
03/11/2022 01:19:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=319
03/11/2022 01:20:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
03/11/2022 01:20:01 - INFO - __main__ - Global step 650 Train loss 0.23 ACC 0.5 on epoch=324
03/11/2022 01:20:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
03/11/2022 01:20:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=334
03/11/2022 01:20:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=339
03/11/2022 01:20:10 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=344
03/11/2022 01:20:12 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=349
03/11/2022 01:20:13 - INFO - __main__ - Global step 700 Train loss 0.24 ACC 0.5 on epoch=349
03/11/2022 01:20:15 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=354
03/11/2022 01:20:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=359
03/11/2022 01:20:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=364
03/11/2022 01:20:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
03/11/2022 01:20:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 01:20:25 - INFO - __main__ - Global step 750 Train loss 0.23 ACC 0.5 on epoch=374
03/11/2022 01:20:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
03/11/2022 01:20:29 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=384
03/11/2022 01:20:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
03/11/2022 01:20:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=394
03/11/2022 01:20:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=399
03/11/2022 01:20:37 - INFO - __main__ - Global step 800 Train loss 0.22 ACC 0.5 on epoch=399
03/11/2022 01:20:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=404
03/11/2022 01:20:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=409
03/11/2022 01:20:44 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=414
03/11/2022 01:20:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=419
03/11/2022 01:20:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=424
03/11/2022 01:20:49 - INFO - __main__ - Global step 850 Train loss 0.22 ACC 0.53125 on epoch=424
03/11/2022 01:20:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=429
03/11/2022 01:20:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=434
03/11/2022 01:20:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.21 on epoch=439
03/11/2022 01:20:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=444
03/11/2022 01:20:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=449
03/11/2022 01:21:00 - INFO - __main__ - Global step 900 Train loss 0.21 ACC 0.5625 on epoch=449
03/11/2022 01:21:03 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=454
03/11/2022 01:21:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=459
03/11/2022 01:21:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
03/11/2022 01:21:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=469
03/11/2022 01:21:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=474
03/11/2022 01:21:12 - INFO - __main__ - Global step 950 Train loss 0.20 ACC 0.59375 on epoch=474
03/11/2022 01:21:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=479
03/11/2022 01:21:17 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=484
03/11/2022 01:21:19 - INFO - __main__ - Step 980 Global step 980 Train loss 0.21 on epoch=489
03/11/2022 01:21:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.18 on epoch=494
03/11/2022 01:21:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
03/11/2022 01:21:24 - INFO - __main__ - Global step 1000 Train loss 0.18 ACC 0.53125 on epoch=499
03/11/2022 01:21:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.16 on epoch=504
03/11/2022 01:21:29 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.18 on epoch=509
03/11/2022 01:21:31 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=514
03/11/2022 01:21:33 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=519
03/11/2022 01:21:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=524
03/11/2022 01:21:36 - INFO - __main__ - Global step 1050 Train loss 0.17 ACC 0.59375 on epoch=524
03/11/2022 01:21:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=529
03/11/2022 01:21:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=534
03/11/2022 01:21:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=539
03/11/2022 01:21:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=544
03/11/2022 01:21:47 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=549
03/11/2022 01:21:47 - INFO - __main__ - Global step 1100 Train loss 0.18 ACC 0.5625 on epoch=549
03/11/2022 01:21:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.16 on epoch=554
03/11/2022 01:21:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=559
03/11/2022 01:21:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=564
03/11/2022 01:21:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=569
03/11/2022 01:21:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=574
03/11/2022 01:21:59 - INFO - __main__ - Global step 1150 Train loss 0.15 ACC 0.59375 on epoch=574
03/11/2022 01:22:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=579
03/11/2022 01:22:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.14 on epoch=584
03/11/2022 01:22:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.11 on epoch=589
03/11/2022 01:22:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=594
03/11/2022 01:22:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=599
03/11/2022 01:22:10 - INFO - __main__ - Global step 1200 Train loss 0.13 ACC 0.59375 on epoch=599
03/11/2022 01:22:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.12 on epoch=604
03/11/2022 01:22:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=609
03/11/2022 01:22:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.13 on epoch=614
03/11/2022 01:22:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=619
03/11/2022 01:22:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.12 on epoch=624
03/11/2022 01:22:22 - INFO - __main__ - Global step 1250 Train loss 0.12 ACC 0.46875 on epoch=624
03/11/2022 01:22:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=629
03/11/2022 01:22:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=634
03/11/2022 01:22:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.10 on epoch=639
03/11/2022 01:22:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=644
03/11/2022 01:22:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.10 on epoch=649
03/11/2022 01:22:34 - INFO - __main__ - Global step 1300 Train loss 0.10 ACC 0.5 on epoch=649
03/11/2022 01:22:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=654
03/11/2022 01:22:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=659
03/11/2022 01:22:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.09 on epoch=664
03/11/2022 01:22:42 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=669
03/11/2022 01:22:44 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=674
03/11/2022 01:22:45 - INFO - __main__ - Global step 1350 Train loss 0.09 ACC 0.46875 on epoch=674
03/11/2022 01:22:47 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=679
03/11/2022 01:22:49 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=684
03/11/2022 01:22:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.09 on epoch=689
03/11/2022 01:22:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
03/11/2022 01:22:56 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.09 on epoch=699
03/11/2022 01:22:57 - INFO - __main__ - Global step 1400 Train loss 0.08 ACC 0.4375 on epoch=699
03/11/2022 01:22:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=704
03/11/2022 01:23:01 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=709
03/11/2022 01:23:03 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
03/11/2022 01:23:05 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=719
03/11/2022 01:23:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.08 on epoch=724
03/11/2022 01:23:08 - INFO - __main__ - Global step 1450 Train loss 0.06 ACC 0.5 on epoch=724
03/11/2022 01:23:10 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
03/11/2022 01:23:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
03/11/2022 01:23:15 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.07 on epoch=739
03/11/2022 01:23:17 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=744
03/11/2022 01:23:19 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
03/11/2022 01:23:20 - INFO - __main__ - Global step 1500 Train loss 0.05 ACC 0.4375 on epoch=749
03/11/2022 01:23:22 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
03/11/2022 01:23:24 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
03/11/2022 01:23:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
03/11/2022 01:23:29 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.06 on epoch=769
03/11/2022 01:23:31 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.06 on epoch=774
03/11/2022 01:23:32 - INFO - __main__ - Global step 1550 Train loss 0.04 ACC 0.40625 on epoch=774
03/11/2022 01:23:34 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
03/11/2022 01:23:36 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
03/11/2022 01:23:39 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
03/11/2022 01:23:41 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
03/11/2022 01:23:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
03/11/2022 01:23:44 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.375 on epoch=799
03/11/2022 01:23:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.04 on epoch=804
03/11/2022 01:23:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
03/11/2022 01:23:51 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
03/11/2022 01:23:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.03 on epoch=819
03/11/2022 01:23:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
03/11/2022 01:23:56 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.5 on epoch=824
03/11/2022 01:23:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
03/11/2022 01:24:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
03/11/2022 01:24:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
03/11/2022 01:24:05 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/11/2022 01:24:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.03 on epoch=849
03/11/2022 01:24:07 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.53125 on epoch=849
03/11/2022 01:24:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.02 on epoch=854
03/11/2022 01:24:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/11/2022 01:24:14 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
03/11/2022 01:24:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
03/11/2022 01:24:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.02 on epoch=874
03/11/2022 01:24:19 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.5 on epoch=874
03/11/2022 01:24:22 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
03/11/2022 01:24:24 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/11/2022 01:24:26 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
03/11/2022 01:24:28 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
03/11/2022 01:24:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/11/2022 01:24:31 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.5 on epoch=899
03/11/2022 01:24:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
03/11/2022 01:24:36 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/11/2022 01:24:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
03/11/2022 01:24:40 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/11/2022 01:24:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/11/2022 01:24:43 - INFO - __main__ - Global step 1850 Train loss 0.02 ACC 0.46875 on epoch=924
03/11/2022 01:24:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.03 on epoch=929
03/11/2022 01:24:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
03/11/2022 01:24:50 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/11/2022 01:24:52 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 01:24:54 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/11/2022 01:24:55 - INFO - __main__ - Global step 1900 Train loss 0.02 ACC 0.46875 on epoch=949
03/11/2022 01:24:57 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 01:25:00 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
03/11/2022 01:25:02 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
03/11/2022 01:25:04 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 01:25:06 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
03/11/2022 01:25:07 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.40625 on epoch=974
03/11/2022 01:25:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 01:25:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/11/2022 01:25:14 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/11/2022 01:25:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
03/11/2022 01:25:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
03/11/2022 01:25:19 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.4375 on epoch=999
03/11/2022 01:25:19 - INFO - __main__ - save last model!
03/11/2022 01:25:19 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 01:25:19 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 01:25:19 - INFO - __main__ - Printing 3 examples
03/11/2022 01:25:19 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 01:25:19 - INFO - __main__ - ['sentence 2']
03/11/2022 01:25:19 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 01:25:19 - INFO - __main__ - ['sentence 2']
03/11/2022 01:25:19 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 01:25:19 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:19 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:25:19 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:25:19 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 01:25:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:25:20 - INFO - __main__ - Printing 3 examples
03/11/2022 01:25:20 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/11/2022 01:25:20 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:20 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/11/2022 01:25:20 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:20 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:25:20 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:20 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 01:25:20 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:25:20 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:25:20 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:25:20 - INFO - __main__ - Printing 3 examples
03/11/2022 01:25:20 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/11/2022 01:25:20 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:20 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/11/2022 01:25:20 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:20 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/11/2022 01:25:20 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:20 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:25:20 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:25:20 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:25:24 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_100_0.3_8_predictions.txt
03/11/2022 01:25:24 - INFO - __main__ - ACC on test data: 0.5150
03/11/2022 01:25:25 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.3, bsz=8, dev_performance=0.59375, test_performance=0.515
03/11/2022 01:25:25 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.2, bsz=8 ...
03/11/2022 01:25:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:25:25 - INFO - __main__ - Printing 3 examples
03/11/2022 01:25:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle should not ever plan to astound a lot of waitresses. [SEP] sentence 2: Rachelle should fortunately ever plan to astound a lot of waitresses.
03/11/2022 01:25:25 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Tanya has not ever begged every organization to lie. [SEP] sentence 2: Tanya has probably ever begged every organization to lie.
03/11/2022 01:25:25 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:25:25 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 01:25:25 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:25:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:25:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:25:25 - INFO - __main__ - Printing 3 examples
03/11/2022 01:25:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Those icicles had not ever frozen. [SEP] sentence 2: Those icicles had probably ever frozen.
03/11/2022 01:25:25 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: This association will not ever reveal who fell asleep. [SEP] sentence 2: This association will fortunately ever reveal who fell asleep.
03/11/2022 01:25:25 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:25 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Bill had not ever boasted about Lissa. [SEP] sentence 2: Bill had fortunately ever boasted about Lissa.
03/11/2022 01:25:25 - INFO - __main__ - ['sentence 1']
03/11/2022 01:25:25 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:25:25 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:25:25 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:25:33 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:25:33 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:25:33 - INFO - __main__ - Starting training!
03/11/2022 01:25:38 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:25:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:25:39 - INFO - __main__ - Starting training!
03/11/2022 01:25:42 - INFO - __main__ - Step 10 Global step 10 Train loss 1.45 on epoch=4
03/11/2022 01:25:44 - INFO - __main__ - Step 20 Global step 20 Train loss 0.63 on epoch=9
03/11/2022 01:25:46 - INFO - __main__ - Step 30 Global step 30 Train loss 0.46 on epoch=14
03/11/2022 01:25:48 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
03/11/2022 01:25:50 - INFO - __main__ - Step 50 Global step 50 Train loss 0.34 on epoch=24
03/11/2022 01:25:52 - INFO - __main__ - Global step 50 Train loss 0.65 ACC 0.5 on epoch=24
03/11/2022 01:25:52 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 01:25:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.31 on epoch=29
03/11/2022 01:25:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.34 on epoch=34
03/11/2022 01:25:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.34 on epoch=39
03/11/2022 01:26:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=44
03/11/2022 01:26:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=49
03/11/2022 01:26:03 - INFO - __main__ - Global step 100 Train loss 0.32 ACC 0.5 on epoch=49
03/11/2022 01:26:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=54
03/11/2022 01:26:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=59
03/11/2022 01:26:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.30 on epoch=64
03/11/2022 01:26:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
03/11/2022 01:26:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
03/11/2022 01:26:15 - INFO - __main__ - Global step 150 Train loss 0.30 ACC 0.46875 on epoch=74
03/11/2022 01:26:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
03/11/2022 01:26:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=84
03/11/2022 01:26:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
03/11/2022 01:26:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
03/11/2022 01:26:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/11/2022 01:26:26 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.5 on epoch=99
03/11/2022 01:26:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
03/11/2022 01:26:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 01:26:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
03/11/2022 01:26:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
03/11/2022 01:26:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
03/11/2022 01:26:38 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.5 on epoch=124
03/11/2022 01:26:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
03/11/2022 01:26:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
03/11/2022 01:26:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
03/11/2022 01:26:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
03/11/2022 01:26:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=149
03/11/2022 01:26:50 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.5 on epoch=149
03/11/2022 01:26:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=154
03/11/2022 01:26:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
03/11/2022 01:26:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
03/11/2022 01:26:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
03/11/2022 01:27:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
03/11/2022 01:27:01 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.5 on epoch=174
03/11/2022 01:27:03 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 01:27:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 01:27:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
03/11/2022 01:27:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 01:27:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
03/11/2022 01:27:13 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.5 on epoch=199
03/11/2022 01:27:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 01:27:17 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
03/11/2022 01:27:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
03/11/2022 01:27:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
03/11/2022 01:27:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
03/11/2022 01:27:25 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.5 on epoch=224
03/11/2022 01:27:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 01:27:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=234
03/11/2022 01:27:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
03/11/2022 01:27:33 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
03/11/2022 01:27:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
03/11/2022 01:27:36 - INFO - __main__ - Global step 500 Train loss 0.24 ACC 0.5 on epoch=249
03/11/2022 01:27:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 01:27:40 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
03/11/2022 01:27:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
03/11/2022 01:27:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
03/11/2022 01:27:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
03/11/2022 01:27:48 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.5 on epoch=274
03/11/2022 01:27:50 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
03/11/2022 01:27:52 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=284
03/11/2022 01:27:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/11/2022 01:27:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=294
03/11/2022 01:27:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
03/11/2022 01:27:59 - INFO - __main__ - Global step 600 Train loss 0.24 ACC 0.53125 on epoch=299
03/11/2022 01:27:59 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=299, global_step=600
03/11/2022 01:28:01 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=304
03/11/2022 01:28:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=309
03/11/2022 01:28:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
03/11/2022 01:28:08 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
03/11/2022 01:28:10 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
03/11/2022 01:28:11 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.46875 on epoch=324
03/11/2022 01:28:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=329
03/11/2022 01:28:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
03/11/2022 01:28:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
03/11/2022 01:28:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
03/11/2022 01:28:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
03/11/2022 01:28:22 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.53125 on epoch=349
03/11/2022 01:28:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=354
03/11/2022 01:28:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
03/11/2022 01:28:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
03/11/2022 01:28:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=369
03/11/2022 01:28:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 01:28:34 - INFO - __main__ - Global step 750 Train loss 0.23 ACC 0.59375 on epoch=374
03/11/2022 01:28:34 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=374, global_step=750
03/11/2022 01:28:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=379
03/11/2022 01:28:38 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
03/11/2022 01:28:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
03/11/2022 01:28:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=394
03/11/2022 01:28:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
03/11/2022 01:28:46 - INFO - __main__ - Global step 800 Train loss 0.24 ACC 0.5 on epoch=399
03/11/2022 01:28:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=404
03/11/2022 01:28:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=409
03/11/2022 01:28:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=414
03/11/2022 01:28:54 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=419
03/11/2022 01:28:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=424
03/11/2022 01:28:57 - INFO - __main__ - Global step 850 Train loss 0.24 ACC 0.625 on epoch=424
03/11/2022 01:28:57 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=424, global_step=850
03/11/2022 01:28:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=429
03/11/2022 01:29:02 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=434
03/11/2022 01:29:04 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
03/11/2022 01:29:06 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=444
03/11/2022 01:29:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=449
03/11/2022 01:29:09 - INFO - __main__ - Global step 900 Train loss 0.23 ACC 0.5625 on epoch=449
03/11/2022 01:29:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
03/11/2022 01:29:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=459
03/11/2022 01:29:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/11/2022 01:29:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=469
03/11/2022 01:29:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.21 on epoch=474
03/11/2022 01:29:20 - INFO - __main__ - Global step 950 Train loss 0.23 ACC 0.6875 on epoch=474
03/11/2022 01:29:21 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.6875 on epoch=474, global_step=950
03/11/2022 01:29:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=479
03/11/2022 01:29:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=484
03/11/2022 01:29:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=489
03/11/2022 01:29:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=494
03/11/2022 01:29:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=499
03/11/2022 01:29:32 - INFO - __main__ - Global step 1000 Train loss 0.24 ACC 0.5 on epoch=499
03/11/2022 01:29:34 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=504
03/11/2022 01:29:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=509
03/11/2022 01:29:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=514
03/11/2022 01:29:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=519
03/11/2022 01:29:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
03/11/2022 01:29:44 - INFO - __main__ - Global step 1050 Train loss 0.23 ACC 0.5625 on epoch=524
03/11/2022 01:29:46 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=529
03/11/2022 01:29:48 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=534
03/11/2022 01:29:50 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=539
03/11/2022 01:29:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
03/11/2022 01:29:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=549
03/11/2022 01:29:56 - INFO - __main__ - Global step 1100 Train loss 0.22 ACC 0.625 on epoch=549
03/11/2022 01:29:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=554
03/11/2022 01:30:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.22 on epoch=559
03/11/2022 01:30:02 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=564
03/11/2022 01:30:04 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=569
03/11/2022 01:30:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=574
03/11/2022 01:30:07 - INFO - __main__ - Global step 1150 Train loss 0.22 ACC 0.53125 on epoch=574
03/11/2022 01:30:09 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
03/11/2022 01:30:12 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=584
03/11/2022 01:30:14 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=589
03/11/2022 01:30:16 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/11/2022 01:30:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=599
03/11/2022 01:30:19 - INFO - __main__ - Global step 1200 Train loss 0.22 ACC 0.625 on epoch=599
03/11/2022 01:30:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
03/11/2022 01:30:23 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=609
03/11/2022 01:30:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=614
03/11/2022 01:30:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=619
03/11/2022 01:30:30 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=624
03/11/2022 01:30:31 - INFO - __main__ - Global step 1250 Train loss 0.21 ACC 0.5625 on epoch=624
03/11/2022 01:30:33 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=629
03/11/2022 01:30:35 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=634
03/11/2022 01:30:37 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=639
03/11/2022 01:30:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.19 on epoch=644
03/11/2022 01:30:42 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=649
03/11/2022 01:30:43 - INFO - __main__ - Global step 1300 Train loss 0.20 ACC 0.625 on epoch=649
03/11/2022 01:30:45 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=654
03/11/2022 01:30:47 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=659
03/11/2022 01:30:49 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=664
03/11/2022 01:30:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=669
03/11/2022 01:30:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=674
03/11/2022 01:30:54 - INFO - __main__ - Global step 1350 Train loss 0.19 ACC 0.625 on epoch=674
03/11/2022 01:30:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=679
03/11/2022 01:30:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=684
03/11/2022 01:31:01 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=689
03/11/2022 01:31:03 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=694
03/11/2022 01:31:05 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=699
03/11/2022 01:31:06 - INFO - __main__ - Global step 1400 Train loss 0.19 ACC 0.5625 on epoch=699
03/11/2022 01:31:08 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=704
03/11/2022 01:31:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=709
03/11/2022 01:31:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.16 on epoch=714
03/11/2022 01:31:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=719
03/11/2022 01:31:17 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.16 on epoch=724
03/11/2022 01:31:18 - INFO - __main__ - Global step 1450 Train loss 0.17 ACC 0.59375 on epoch=724
03/11/2022 01:31:20 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.17 on epoch=729
03/11/2022 01:31:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/11/2022 01:31:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.16 on epoch=739
03/11/2022 01:31:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=744
03/11/2022 01:31:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=749
03/11/2022 01:31:30 - INFO - __main__ - Global step 1500 Train loss 0.17 ACC 0.5625 on epoch=749
03/11/2022 01:31:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.19 on epoch=754
03/11/2022 01:31:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.14 on epoch=759
03/11/2022 01:31:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=764
03/11/2022 01:31:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=769
03/11/2022 01:31:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.15 on epoch=774
03/11/2022 01:31:41 - INFO - __main__ - Global step 1550 Train loss 0.15 ACC 0.53125 on epoch=774
03/11/2022 01:31:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.15 on epoch=779
03/11/2022 01:31:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=784
03/11/2022 01:31:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=789
03/11/2022 01:31:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=794
03/11/2022 01:31:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=799
03/11/2022 01:31:53 - INFO - __main__ - Global step 1600 Train loss 0.14 ACC 0.5625 on epoch=799
03/11/2022 01:31:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=804
03/11/2022 01:31:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.11 on epoch=809
03/11/2022 01:32:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=814
03/11/2022 01:32:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=819
03/11/2022 01:32:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=824
03/11/2022 01:32:05 - INFO - __main__ - Global step 1650 Train loss 0.13 ACC 0.5625 on epoch=824
03/11/2022 01:32:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=829
03/11/2022 01:32:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=834
03/11/2022 01:32:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=839
03/11/2022 01:32:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=844
03/11/2022 01:32:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=849
03/11/2022 01:32:16 - INFO - __main__ - Global step 1700 Train loss 0.11 ACC 0.53125 on epoch=849
03/11/2022 01:32:18 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=854
03/11/2022 01:32:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=859
03/11/2022 01:32:23 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=864
03/11/2022 01:32:25 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=869
03/11/2022 01:32:27 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=874
03/11/2022 01:32:28 - INFO - __main__ - Global step 1750 Train loss 0.12 ACC 0.53125 on epoch=874
03/11/2022 01:32:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=879
03/11/2022 01:32:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=884
03/11/2022 01:32:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=889
03/11/2022 01:32:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=894
03/11/2022 01:32:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=899
03/11/2022 01:32:40 - INFO - __main__ - Global step 1800 Train loss 0.11 ACC 0.53125 on epoch=899
03/11/2022 01:32:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=904
03/11/2022 01:32:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=909
03/11/2022 01:32:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=914
03/11/2022 01:32:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=919
03/11/2022 01:32:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/11/2022 01:32:52 - INFO - __main__ - Global step 1850 Train loss 0.09 ACC 0.5625 on epoch=924
03/11/2022 01:32:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=929
03/11/2022 01:32:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
03/11/2022 01:32:58 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=939
03/11/2022 01:33:00 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.10 on epoch=944
03/11/2022 01:33:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=949
03/11/2022 01:33:03 - INFO - __main__ - Global step 1900 Train loss 0.08 ACC 0.59375 on epoch=949
03/11/2022 01:33:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
03/11/2022 01:33:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
03/11/2022 01:33:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.10 on epoch=964
03/11/2022 01:33:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=969
03/11/2022 01:33:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=974
03/11/2022 01:33:15 - INFO - __main__ - Global step 1950 Train loss 0.08 ACC 0.5 on epoch=974
03/11/2022 01:33:17 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=979
03/11/2022 01:33:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=984
03/11/2022 01:33:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
03/11/2022 01:33:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=994
03/11/2022 01:33:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.04 on epoch=999
03/11/2022 01:33:27 - INFO - __main__ - Global step 2000 Train loss 0.06 ACC 0.46875 on epoch=999
03/11/2022 01:33:27 - INFO - __main__ - save last model!
03/11/2022 01:33:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 01:33:27 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 01:33:27 - INFO - __main__ - Printing 3 examples
03/11/2022 01:33:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 01:33:27 - INFO - __main__ - ['sentence 2']
03/11/2022 01:33:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 01:33:27 - INFO - __main__ - ['sentence 2']
03/11/2022 01:33:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 01:33:27 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:27 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:33:27 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:33:27 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 01:33:27 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:33:27 - INFO - __main__ - Printing 3 examples
03/11/2022 01:33:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/11/2022 01:33:27 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/11/2022 01:33:27 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:27 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/11/2022 01:33:27 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:27 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 01:33:27 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:33:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:33:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:33:28 - INFO - __main__ - Printing 3 examples
03/11/2022 01:33:28 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/11/2022 01:33:28 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:28 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:33:28 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:28 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/11/2022 01:33:28 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:28 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:33:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:33:28 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:33:32 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_100_0.2_8_predictions.txt
03/11/2022 01:33:32 - INFO - __main__ - ACC on test data: 0.5150
03/11/2022 01:33:32 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_100, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.515
03/11/2022 01:33:32 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.5, bsz=8 ...
03/11/2022 01:33:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:33:33 - INFO - __main__ - Printing 3 examples
03/11/2022 01:33:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/11/2022 01:33:33 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/11/2022 01:33:33 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/11/2022 01:33:33 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:33 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 01:33:33 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:33:33 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:33:33 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:33:33 - INFO - __main__ - Printing 3 examples
03/11/2022 01:33:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/11/2022 01:33:33 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:33:33 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/11/2022 01:33:33 - INFO - __main__ - ['sentence 1']
03/11/2022 01:33:33 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:33:33 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:33:33 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:33:42 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:33:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:33:43 - INFO - __main__ - Starting training!
03/11/2022 01:33:45 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:33:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:33:46 - INFO - __main__ - Starting training!
03/11/2022 01:33:51 - INFO - __main__ - Step 10 Global step 10 Train loss 1.16 on epoch=4
03/11/2022 01:33:53 - INFO - __main__ - Step 20 Global step 20 Train loss 0.55 on epoch=9
03/11/2022 01:33:55 - INFO - __main__ - Step 30 Global step 30 Train loss 0.39 on epoch=14
03/11/2022 01:33:57 - INFO - __main__ - Step 40 Global step 40 Train loss 0.33 on epoch=19
03/11/2022 01:33:59 - INFO - __main__ - Step 50 Global step 50 Train loss 0.37 on epoch=24
03/11/2022 01:34:00 - INFO - __main__ - Global step 50 Train loss 0.56 ACC 0.5625 on epoch=24
03/11/2022 01:34:00 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5625 on epoch=24, global_step=50
03/11/2022 01:34:02 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=29
03/11/2022 01:34:04 - INFO - __main__ - Step 70 Global step 70 Train loss 0.27 on epoch=34
03/11/2022 01:34:06 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=39
03/11/2022 01:34:09 - INFO - __main__ - Step 90 Global step 90 Train loss 0.29 on epoch=44
03/11/2022 01:34:11 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=49
03/11/2022 01:34:11 - INFO - __main__ - Global step 100 Train loss 0.28 ACC 0.5 on epoch=49
03/11/2022 01:34:13 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
03/11/2022 01:34:16 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
03/11/2022 01:34:18 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=64
03/11/2022 01:34:20 - INFO - __main__ - Step 140 Global step 140 Train loss 0.25 on epoch=69
03/11/2022 01:34:22 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
03/11/2022 01:34:23 - INFO - __main__ - Global step 150 Train loss 0.26 ACC 0.5 on epoch=74
03/11/2022 01:34:25 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
03/11/2022 01:34:27 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
03/11/2022 01:34:29 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=89
03/11/2022 01:34:31 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=94
03/11/2022 01:34:34 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
03/11/2022 01:34:34 - INFO - __main__ - Global step 200 Train loss 0.24 ACC 0.5 on epoch=99
03/11/2022 01:34:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 01:34:39 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=109
03/11/2022 01:34:41 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
03/11/2022 01:34:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
03/11/2022 01:34:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
03/11/2022 01:34:46 - INFO - __main__ - Global step 250 Train loss 0.25 ACC 0.53125 on epoch=124
03/11/2022 01:34:48 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
03/11/2022 01:34:50 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 01:34:52 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
03/11/2022 01:34:54 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
03/11/2022 01:34:56 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
03/11/2022 01:34:57 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.5 on epoch=149
03/11/2022 01:34:59 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 01:35:01 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
03/11/2022 01:35:04 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
03/11/2022 01:35:06 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 01:35:08 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
03/11/2022 01:35:08 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.5 on epoch=174
03/11/2022 01:35:10 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 01:35:13 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 01:35:15 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 01:35:17 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=194
03/11/2022 01:35:19 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 01:35:21 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.53125 on epoch=199
03/11/2022 01:35:23 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
03/11/2022 01:35:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
03/11/2022 01:35:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.21 on epoch=214
03/11/2022 01:35:30 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
03/11/2022 01:35:32 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
03/11/2022 01:35:34 - INFO - __main__ - Global step 450 Train loss 0.23 ACC 0.46875 on epoch=224
03/11/2022 01:35:36 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 01:35:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
03/11/2022 01:35:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=239
03/11/2022 01:35:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
03/11/2022 01:35:45 - INFO - __main__ - Step 500 Global step 500 Train loss 0.21 on epoch=249
03/11/2022 01:35:45 - INFO - __main__ - Global step 500 Train loss 0.22 ACC 0.53125 on epoch=249
03/11/2022 01:35:48 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=254
03/11/2022 01:35:50 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=259
03/11/2022 01:35:52 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
03/11/2022 01:35:54 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=269
03/11/2022 01:35:56 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=274
03/11/2022 01:35:57 - INFO - __main__ - Global step 550 Train loss 0.21 ACC 0.53125 on epoch=274
03/11/2022 01:35:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.18 on epoch=279
03/11/2022 01:36:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.17 on epoch=284
03/11/2022 01:36:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=289
03/11/2022 01:36:06 - INFO - __main__ - Step 590 Global step 590 Train loss 0.18 on epoch=294
03/11/2022 01:36:08 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=299
03/11/2022 01:36:09 - INFO - __main__ - Global step 600 Train loss 0.18 ACC 0.4375 on epoch=299
03/11/2022 01:36:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=304
03/11/2022 01:36:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=309
03/11/2022 01:36:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=314
03/11/2022 01:36:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.16 on epoch=319
03/11/2022 01:36:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=324
03/11/2022 01:36:20 - INFO - __main__ - Global step 650 Train loss 0.16 ACC 0.53125 on epoch=324
03/11/2022 01:36:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.16 on epoch=329
03/11/2022 01:36:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=334
03/11/2022 01:36:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.15 on epoch=339
03/11/2022 01:36:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.12 on epoch=344
03/11/2022 01:36:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.11 on epoch=349
03/11/2022 01:36:32 - INFO - __main__ - Global step 700 Train loss 0.14 ACC 0.46875 on epoch=349
03/11/2022 01:36:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.12 on epoch=354
03/11/2022 01:36:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
03/11/2022 01:36:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.12 on epoch=364
03/11/2022 01:36:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
03/11/2022 01:36:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=374
03/11/2022 01:36:44 - INFO - __main__ - Global step 750 Train loss 0.11 ACC 0.5 on epoch=374
03/11/2022 01:36:47 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
03/11/2022 01:36:49 - INFO - __main__ - Step 770 Global step 770 Train loss 0.07 on epoch=384
03/11/2022 01:36:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.07 on epoch=389
03/11/2022 01:36:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
03/11/2022 01:36:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
03/11/2022 01:36:56 - INFO - __main__ - Global step 800 Train loss 0.06 ACC 0.5 on epoch=399
03/11/2022 01:36:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
03/11/2022 01:37:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.06 on epoch=409
03/11/2022 01:37:03 - INFO - __main__ - Step 830 Global step 830 Train loss 0.09 on epoch=414
03/11/2022 01:37:05 - INFO - __main__ - Step 840 Global step 840 Train loss 0.06 on epoch=419
03/11/2022 01:37:07 - INFO - __main__ - Step 850 Global step 850 Train loss 0.07 on epoch=424
03/11/2022 01:37:08 - INFO - __main__ - Global step 850 Train loss 0.07 ACC 0.5 on epoch=424
03/11/2022 01:37:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
03/11/2022 01:37:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=434
03/11/2022 01:37:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
03/11/2022 01:37:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.04 on epoch=444
03/11/2022 01:37:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.07 on epoch=449
03/11/2022 01:37:19 - INFO - __main__ - Global step 900 Train loss 0.05 ACC 0.40625 on epoch=449
03/11/2022 01:37:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
03/11/2022 01:37:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.08 on epoch=459
03/11/2022 01:37:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
03/11/2022 01:37:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.05 on epoch=469
03/11/2022 01:37:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
03/11/2022 01:37:31 - INFO - __main__ - Global step 950 Train loss 0.05 ACC 0.46875 on epoch=474
03/11/2022 01:37:33 - INFO - __main__ - Step 960 Global step 960 Train loss 0.02 on epoch=479
03/11/2022 01:37:35 - INFO - __main__ - Step 970 Global step 970 Train loss 0.05 on epoch=484
03/11/2022 01:37:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
03/11/2022 01:37:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/11/2022 01:37:42 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.05 on epoch=499
03/11/2022 01:37:42 - INFO - __main__ - Global step 1000 Train loss 0.04 ACC 0.53125 on epoch=499
03/11/2022 01:37:45 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
03/11/2022 01:37:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
03/11/2022 01:37:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/11/2022 01:37:51 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.07 on epoch=519
03/11/2022 01:37:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
03/11/2022 01:37:54 - INFO - __main__ - Global step 1050 Train loss 0.04 ACC 0.5625 on epoch=524
03/11/2022 01:37:56 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/11/2022 01:37:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
03/11/2022 01:38:01 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/11/2022 01:38:03 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
03/11/2022 01:38:05 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/11/2022 01:38:06 - INFO - __main__ - Global step 1100 Train loss 0.02 ACC 0.53125 on epoch=549
03/11/2022 01:38:08 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
03/11/2022 01:38:10 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/11/2022 01:38:12 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/11/2022 01:38:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/11/2022 01:38:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/11/2022 01:38:17 - INFO - __main__ - Global step 1150 Train loss 0.02 ACC 0.5625 on epoch=574
03/11/2022 01:38:19 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/11/2022 01:38:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.02 on epoch=584
03/11/2022 01:38:24 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/11/2022 01:38:26 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/11/2022 01:38:28 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/11/2022 01:38:29 - INFO - __main__ - Global step 1200 Train loss 0.01 ACC 0.53125 on epoch=599
03/11/2022 01:38:31 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/11/2022 01:38:33 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/11/2022 01:38:35 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/11/2022 01:38:38 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
03/11/2022 01:38:40 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/11/2022 01:38:40 - INFO - __main__ - Global step 1250 Train loss 0.01 ACC 0.5 on epoch=624
03/11/2022 01:38:43 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
03/11/2022 01:38:45 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
03/11/2022 01:38:47 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/11/2022 01:38:49 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
03/11/2022 01:38:51 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/11/2022 01:38:52 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.46875 on epoch=649
03/11/2022 01:38:54 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/11/2022 01:38:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/11/2022 01:38:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
03/11/2022 01:39:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/11/2022 01:39:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 01:39:03 - INFO - __main__ - Global step 1350 Train loss 0.01 ACC 0.5 on epoch=674
03/11/2022 01:39:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/11/2022 01:39:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/11/2022 01:39:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/11/2022 01:39:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 01:39:14 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/11/2022 01:39:15 - INFO - __main__ - Global step 1400 Train loss 0.01 ACC 0.53125 on epoch=699
03/11/2022 01:39:17 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/11/2022 01:39:19 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/11/2022 01:39:22 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 01:39:24 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 01:39:26 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/11/2022 01:39:27 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 0.53125 on epoch=724
03/11/2022 01:39:29 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/11/2022 01:39:31 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 01:39:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 01:39:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 01:39:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 01:39:38 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 0.5 on epoch=749
03/11/2022 01:39:41 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/11/2022 01:39:43 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 01:39:45 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 01:39:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/11/2022 01:39:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 01:39:50 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 0.5 on epoch=774
03/11/2022 01:39:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 01:39:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
03/11/2022 01:39:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/11/2022 01:39:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 01:40:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 01:40:02 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 0.53125 on epoch=799
03/11/2022 01:40:04 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 01:40:06 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 01:40:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 01:40:11 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 01:40:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/11/2022 01:40:13 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.5 on epoch=824
03/11/2022 01:40:16 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 01:40:18 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 01:40:20 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 01:40:22 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 01:40:25 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 01:40:25 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 0.53125 on epoch=849
03/11/2022 01:40:27 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 01:40:30 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 01:40:32 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 01:40:34 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 01:40:36 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 01:40:37 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 0.53125 on epoch=874
03/11/2022 01:40:39 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 01:40:41 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 01:40:43 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 01:40:46 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 01:40:48 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 01:40:49 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 0.53125 on epoch=899
03/11/2022 01:40:51 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 01:40:53 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 01:40:55 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
03/11/2022 01:40:57 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 01:41:00 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 01:41:00 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 0.53125 on epoch=924
03/11/2022 01:41:03 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 01:41:05 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 01:41:07 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 01:41:09 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 01:41:11 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 01:41:12 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.53125 on epoch=949
03/11/2022 01:41:14 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 01:41:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/11/2022 01:41:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 01:41:21 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 01:41:23 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 01:41:23 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 0.5 on epoch=974
03/11/2022 01:41:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 01:41:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 01:41:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
03/11/2022 01:41:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 01:41:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 01:41:35 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.5 on epoch=999
03/11/2022 01:41:35 - INFO - __main__ - save last model!
03/11/2022 01:41:35 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 01:41:35 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 01:41:35 - INFO - __main__ - Printing 3 examples
03/11/2022 01:41:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 01:41:35 - INFO - __main__ - ['sentence 2']
03/11/2022 01:41:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 01:41:35 - INFO - __main__ - ['sentence 2']
03/11/2022 01:41:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 01:41:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:35 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:41:35 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:41:35 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 01:41:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:41:36 - INFO - __main__ - Printing 3 examples
03/11/2022 01:41:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/11/2022 01:41:36 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/11/2022 01:41:36 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/11/2022 01:41:36 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 01:41:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:41:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:41:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:41:36 - INFO - __main__ - Printing 3 examples
03/11/2022 01:41:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/11/2022 01:41:36 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:41:36 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/11/2022 01:41:36 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:36 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:41:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:41:36 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:41:39 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_13_0.5_8_predictions.txt
03/11/2022 01:41:39 - INFO - __main__ - ACC on test data: 0.4850
03/11/2022 01:41:39 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.5, bsz=8, dev_performance=0.5625, test_performance=0.485
03/11/2022 01:41:39 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.4, bsz=8 ...
03/11/2022 01:41:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:41:40 - INFO - __main__ - Printing 3 examples
03/11/2022 01:41:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/11/2022 01:41:40 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/11/2022 01:41:40 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/11/2022 01:41:40 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:40 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 01:41:40 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:41:40 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:41:40 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:41:40 - INFO - __main__ - Printing 3 examples
03/11/2022 01:41:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/11/2022 01:41:40 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:41:40 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:40 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/11/2022 01:41:40 - INFO - __main__ - ['sentence 1']
03/11/2022 01:41:40 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:41:40 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:41:40 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:41:50 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:41:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:41:51 - INFO - __main__ - Starting training!
03/11/2022 01:41:52 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:41:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:41:53 - INFO - __main__ - Starting training!
03/11/2022 01:41:56 - INFO - __main__ - Step 10 Global step 10 Train loss 1.11 on epoch=4
03/11/2022 01:41:58 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=9
03/11/2022 01:42:00 - INFO - __main__ - Step 30 Global step 30 Train loss 0.36 on epoch=14
03/11/2022 01:42:02 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
03/11/2022 01:42:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.33 on epoch=24
03/11/2022 01:42:05 - INFO - __main__ - Global step 50 Train loss 0.52 ACC 0.5 on epoch=24
03/11/2022 01:42:05 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 01:42:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=29
03/11/2022 01:42:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.31 on epoch=34
03/11/2022 01:42:12 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=39
03/11/2022 01:42:14 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=44
03/11/2022 01:42:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=49
03/11/2022 01:42:16 - INFO - __main__ - Global step 100 Train loss 0.31 ACC 0.5 on epoch=49
03/11/2022 01:42:19 - INFO - __main__ - Step 110 Global step 110 Train loss 0.39 on epoch=54
03/11/2022 01:42:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.42 on epoch=59
03/11/2022 01:42:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.34 on epoch=64
03/11/2022 01:42:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=69
03/11/2022 01:42:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=74
03/11/2022 01:42:28 - INFO - __main__ - Global step 150 Train loss 0.35 ACC 0.5 on epoch=74
03/11/2022 01:42:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
03/11/2022 01:42:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.31 on epoch=84
03/11/2022 01:42:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.32 on epoch=89
03/11/2022 01:42:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
03/11/2022 01:42:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/11/2022 01:42:39 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.5 on epoch=99
03/11/2022 01:42:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.30 on epoch=104
03/11/2022 01:42:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=109
03/11/2022 01:42:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
03/11/2022 01:42:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
03/11/2022 01:42:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
03/11/2022 01:42:50 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.5 on epoch=124
03/11/2022 01:42:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=129
03/11/2022 01:42:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
03/11/2022 01:42:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
03/11/2022 01:42:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
03/11/2022 01:43:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
03/11/2022 01:43:02 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5625 on epoch=149
03/11/2022 01:43:02 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=149, global_step=300
03/11/2022 01:43:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
03/11/2022 01:43:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 01:43:08 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=164
03/11/2022 01:43:10 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
03/11/2022 01:43:12 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
03/11/2022 01:43:13 - INFO - __main__ - Global step 350 Train loss 0.27 ACC 0.53125 on epoch=174
03/11/2022 01:43:15 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
03/11/2022 01:43:17 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
03/11/2022 01:43:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.30 on epoch=189
03/11/2022 01:43:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=194
03/11/2022 01:43:24 - INFO - __main__ - Step 400 Global step 400 Train loss 0.31 on epoch=199
03/11/2022 01:43:24 - INFO - __main__ - Global step 400 Train loss 0.30 ACC 0.4375 on epoch=199
03/11/2022 01:43:27 - INFO - __main__ - Step 410 Global step 410 Train loss 0.36 on epoch=204
03/11/2022 01:43:29 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=209
03/11/2022 01:43:31 - INFO - __main__ - Step 430 Global step 430 Train loss 0.34 on epoch=214
03/11/2022 01:43:33 - INFO - __main__ - Step 440 Global step 440 Train loss 0.36 on epoch=219
03/11/2022 01:43:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.30 on epoch=224
03/11/2022 01:43:36 - INFO - __main__ - Global step 450 Train loss 0.34 ACC 0.5 on epoch=224
03/11/2022 01:43:38 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
03/11/2022 01:43:40 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
03/11/2022 01:43:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=239
03/11/2022 01:43:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
03/11/2022 01:43:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
03/11/2022 01:43:47 - INFO - __main__ - Global step 500 Train loss 0.27 ACC 0.5 on epoch=249
03/11/2022 01:43:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=254
03/11/2022 01:43:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
03/11/2022 01:43:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
03/11/2022 01:43:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
03/11/2022 01:43:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
03/11/2022 01:43:58 - INFO - __main__ - Global step 550 Train loss 0.26 ACC 0.5 on epoch=274
03/11/2022 01:44:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=279
03/11/2022 01:44:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
03/11/2022 01:44:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
03/11/2022 01:44:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/11/2022 01:44:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
03/11/2022 01:44:10 - INFO - __main__ - Global step 600 Train loss 0.26 ACC 0.5 on epoch=299
03/11/2022 01:44:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
03/11/2022 01:44:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
03/11/2022 01:44:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=314
03/11/2022 01:44:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.34 on epoch=319
03/11/2022 01:44:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.33 on epoch=324
03/11/2022 01:44:21 - INFO - __main__ - Global step 650 Train loss 0.29 ACC 0.46875 on epoch=324
03/11/2022 01:44:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.28 on epoch=329
03/11/2022 01:44:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=334
03/11/2022 01:44:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.27 on epoch=339
03/11/2022 01:44:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.28 on epoch=344
03/11/2022 01:44:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
03/11/2022 01:44:32 - INFO - __main__ - Global step 700 Train loss 0.27 ACC 0.5 on epoch=349
03/11/2022 01:44:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=354
03/11/2022 01:44:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=359
03/11/2022 01:44:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.31 on epoch=364
03/11/2022 01:44:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
03/11/2022 01:44:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=374
03/11/2022 01:44:44 - INFO - __main__ - Global step 750 Train loss 0.28 ACC 0.46875 on epoch=374
03/11/2022 01:44:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=379
03/11/2022 01:44:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=384
03/11/2022 01:44:50 - INFO - __main__ - Step 780 Global step 780 Train loss 0.27 on epoch=389
03/11/2022 01:44:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=394
03/11/2022 01:44:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
03/11/2022 01:44:55 - INFO - __main__ - Global step 800 Train loss 0.26 ACC 0.34375 on epoch=399
03/11/2022 01:44:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.29 on epoch=404
03/11/2022 01:44:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
03/11/2022 01:45:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=414
03/11/2022 01:45:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.26 on epoch=419
03/11/2022 01:45:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=424
03/11/2022 01:45:06 - INFO - __main__ - Global step 850 Train loss 0.27 ACC 0.5 on epoch=424
03/11/2022 01:45:08 - INFO - __main__ - Step 860 Global step 860 Train loss 0.28 on epoch=429
03/11/2022 01:45:10 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
03/11/2022 01:45:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.28 on epoch=439
03/11/2022 01:45:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
03/11/2022 01:45:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=449
03/11/2022 01:45:18 - INFO - __main__ - Global step 900 Train loss 0.27 ACC 0.5 on epoch=449
03/11/2022 01:45:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=454
03/11/2022 01:45:22 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=459
03/11/2022 01:45:24 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/11/2022 01:45:26 - INFO - __main__ - Step 940 Global step 940 Train loss 0.26 on epoch=469
03/11/2022 01:45:28 - INFO - __main__ - Step 950 Global step 950 Train loss 0.27 on epoch=474
03/11/2022 01:45:29 - INFO - __main__ - Global step 950 Train loss 0.26 ACC 0.5 on epoch=474
03/11/2022 01:45:31 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=479
03/11/2022 01:45:33 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=484
03/11/2022 01:45:35 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=489
03/11/2022 01:45:38 - INFO - __main__ - Step 990 Global step 990 Train loss 0.27 on epoch=494
03/11/2022 01:45:40 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=499
03/11/2022 01:45:40 - INFO - __main__ - Global step 1000 Train loss 0.26 ACC 0.46875 on epoch=499
03/11/2022 01:45:42 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=504
03/11/2022 01:45:45 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.26 on epoch=509
03/11/2022 01:45:47 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=514
03/11/2022 01:45:49 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=519
03/11/2022 01:45:51 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=524
03/11/2022 01:45:51 - INFO - __main__ - Global step 1050 Train loss 0.26 ACC 0.5 on epoch=524
03/11/2022 01:45:54 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
03/11/2022 01:45:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.27 on epoch=534
03/11/2022 01:45:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=539
03/11/2022 01:46:00 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.26 on epoch=544
03/11/2022 01:46:02 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=549
03/11/2022 01:46:03 - INFO - __main__ - Global step 1100 Train loss 0.27 ACC 0.59375 on epoch=549
03/11/2022 01:46:03 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=549, global_step=1100
03/11/2022 01:46:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.26 on epoch=554
03/11/2022 01:46:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=559
03/11/2022 01:46:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.25 on epoch=564
03/11/2022 01:46:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=569
03/11/2022 01:46:14 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=574
03/11/2022 01:46:14 - INFO - __main__ - Global step 1150 Train loss 0.26 ACC 0.5 on epoch=574
03/11/2022 01:46:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.27 on epoch=579
03/11/2022 01:46:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=584
03/11/2022 01:46:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=589
03/11/2022 01:46:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.27 on epoch=594
03/11/2022 01:46:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=599
03/11/2022 01:46:25 - INFO - __main__ - Global step 1200 Train loss 0.26 ACC 0.5 on epoch=599
03/11/2022 01:46:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
03/11/2022 01:46:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.24 on epoch=609
03/11/2022 01:46:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=614
03/11/2022 01:46:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=619
03/11/2022 01:46:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=624
03/11/2022 01:46:37 - INFO - __main__ - Global step 1250 Train loss 0.25 ACC 0.40625 on epoch=624
03/11/2022 01:46:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=629
03/11/2022 01:46:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=634
03/11/2022 01:46:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=639
03/11/2022 01:46:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=644
03/11/2022 01:46:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/11/2022 01:46:48 - INFO - __main__ - Global step 1300 Train loss 0.26 ACC 0.46875 on epoch=649
03/11/2022 01:46:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=654
03/11/2022 01:46:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=659
03/11/2022 01:46:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=664
03/11/2022 01:46:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=669
03/11/2022 01:46:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
03/11/2022 01:47:00 - INFO - __main__ - Global step 1350 Train loss 0.26 ACC 0.5 on epoch=674
03/11/2022 01:47:02 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=679
03/11/2022 01:47:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=684
03/11/2022 01:47:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=689
03/11/2022 01:47:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=694
03/11/2022 01:47:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=699
03/11/2022 01:47:11 - INFO - __main__ - Global step 1400 Train loss 0.25 ACC 0.5 on epoch=699
03/11/2022 01:47:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=704
03/11/2022 01:47:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=709
03/11/2022 01:47:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=714
03/11/2022 01:47:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=719
03/11/2022 01:47:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=724
03/11/2022 01:47:22 - INFO - __main__ - Global step 1450 Train loss 0.25 ACC 0.5 on epoch=724
03/11/2022 01:47:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.26 on epoch=729
03/11/2022 01:47:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.26 on epoch=734
03/11/2022 01:47:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=739
03/11/2022 01:47:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=744
03/11/2022 01:47:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.27 on epoch=749
03/11/2022 01:47:34 - INFO - __main__ - Global step 1500 Train loss 0.26 ACC 0.5 on epoch=749
03/11/2022 01:47:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=754
03/11/2022 01:47:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=759
03/11/2022 01:47:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.25 on epoch=764
03/11/2022 01:47:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=769
03/11/2022 01:47:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
03/11/2022 01:47:46 - INFO - __main__ - Global step 1550 Train loss 0.25 ACC 0.5 on epoch=774
03/11/2022 01:47:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=779
03/11/2022 01:47:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=784
03/11/2022 01:47:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.27 on epoch=789
03/11/2022 01:47:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=794
03/11/2022 01:47:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
03/11/2022 01:47:58 - INFO - __main__ - Global step 1600 Train loss 0.25 ACC 0.5 on epoch=799
03/11/2022 01:48:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=804
03/11/2022 01:48:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.26 on epoch=809
03/11/2022 01:48:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=814
03/11/2022 01:48:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=819
03/11/2022 01:48:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=824
03/11/2022 01:48:10 - INFO - __main__ - Global step 1650 Train loss 0.25 ACC 0.5 on epoch=824
03/11/2022 01:48:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.26 on epoch=829
03/11/2022 01:48:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=834
03/11/2022 01:48:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=839
03/11/2022 01:48:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.25 on epoch=844
03/11/2022 01:48:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=849
03/11/2022 01:48:22 - INFO - __main__ - Global step 1700 Train loss 0.25 ACC 0.5 on epoch=849
03/11/2022 01:48:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.25 on epoch=854
03/11/2022 01:48:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=859
03/11/2022 01:48:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=864
03/11/2022 01:48:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
03/11/2022 01:48:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=874
03/11/2022 01:48:34 - INFO - __main__ - Global step 1750 Train loss 0.25 ACC 0.5 on epoch=874
03/11/2022 01:48:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=879
03/11/2022 01:48:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=884
03/11/2022 01:48:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=889
03/11/2022 01:48:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=894
03/11/2022 01:48:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=899
03/11/2022 01:48:46 - INFO - __main__ - Global step 1800 Train loss 0.25 ACC 0.5 on epoch=899
03/11/2022 01:48:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=904
03/11/2022 01:48:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=909
03/11/2022 01:48:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.24 on epoch=914
03/11/2022 01:48:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=919
03/11/2022 01:48:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.26 on epoch=924
03/11/2022 01:48:58 - INFO - __main__ - Global step 1850 Train loss 0.25 ACC 0.5 on epoch=924
03/11/2022 01:49:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=929
03/11/2022 01:49:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.25 on epoch=934
03/11/2022 01:49:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=939
03/11/2022 01:49:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=944
03/11/2022 01:49:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=949
03/11/2022 01:49:10 - INFO - __main__ - Global step 1900 Train loss 0.25 ACC 0.5 on epoch=949
03/11/2022 01:49:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.24 on epoch=954
03/11/2022 01:49:15 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.26 on epoch=959
03/11/2022 01:49:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=964
03/11/2022 01:49:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=969
03/11/2022 01:49:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=974
03/11/2022 01:49:22 - INFO - __main__ - Global step 1950 Train loss 0.24 ACC 0.5 on epoch=974
03/11/2022 01:49:24 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.26 on epoch=979
03/11/2022 01:49:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=984
03/11/2022 01:49:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=989
03/11/2022 01:49:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=994
03/11/2022 01:49:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.24 on epoch=999
03/11/2022 01:49:34 - INFO - __main__ - Global step 2000 Train loss 0.24 ACC 0.5 on epoch=999
03/11/2022 01:49:34 - INFO - __main__ - save last model!
03/11/2022 01:49:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 01:49:34 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 01:49:34 - INFO - __main__ - Printing 3 examples
03/11/2022 01:49:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 01:49:34 - INFO - __main__ - ['sentence 2']
03/11/2022 01:49:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 01:49:34 - INFO - __main__ - ['sentence 2']
03/11/2022 01:49:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 01:49:34 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:34 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:49:34 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:49:34 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 01:49:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:49:35 - INFO - __main__ - Printing 3 examples
03/11/2022 01:49:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/11/2022 01:49:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/11/2022 01:49:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/11/2022 01:49:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 01:49:35 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:49:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:49:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:49:35 - INFO - __main__ - Printing 3 examples
03/11/2022 01:49:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/11/2022 01:49:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:49:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/11/2022 01:49:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:35 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:49:35 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:49:35 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:49:38 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_13_0.4_8_predictions.txt
03/11/2022 01:49:38 - INFO - __main__ - ACC on test data: 0.5050
03/11/2022 01:49:38 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.4, bsz=8, dev_performance=0.59375, test_performance=0.505
03/11/2022 01:49:38 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.3, bsz=8 ...
03/11/2022 01:49:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:49:39 - INFO - __main__ - Printing 3 examples
03/11/2022 01:49:39 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/11/2022 01:49:39 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:39 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/11/2022 01:49:39 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:39 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/11/2022 01:49:39 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 01:49:39 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:49:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:49:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:49:39 - INFO - __main__ - Printing 3 examples
03/11/2022 01:49:39 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/11/2022 01:49:39 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:39 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:49:39 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:39 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/11/2022 01:49:39 - INFO - __main__ - ['sentence 1']
03/11/2022 01:49:39 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:49:39 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:49:39 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:49:48 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:49:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:49:48 - INFO - __main__ - Starting training!
03/11/2022 01:49:51 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:49:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:49:52 - INFO - __main__ - Starting training!
03/11/2022 01:49:55 - INFO - __main__ - Step 10 Global step 10 Train loss 1.28 on epoch=4
03/11/2022 01:49:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=9
03/11/2022 01:49:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=14
03/11/2022 01:50:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
03/11/2022 01:50:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.35 on epoch=24
03/11/2022 01:50:04 - INFO - __main__ - Global step 50 Train loss 0.58 ACC 0.5 on epoch=24
03/11/2022 01:50:04 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 01:50:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.32 on epoch=29
03/11/2022 01:50:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.33 on epoch=34
03/11/2022 01:50:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
03/11/2022 01:50:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=44
03/11/2022 01:50:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
03/11/2022 01:50:15 - INFO - __main__ - Global step 100 Train loss 0.30 ACC 0.5 on epoch=49
03/11/2022 01:50:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
03/11/2022 01:50:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.26 on epoch=59
03/11/2022 01:50:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
03/11/2022 01:50:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=69
03/11/2022 01:50:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
03/11/2022 01:50:26 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.46875 on epoch=74
03/11/2022 01:50:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
03/11/2022 01:50:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=84
03/11/2022 01:50:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
03/11/2022 01:50:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
03/11/2022 01:50:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/11/2022 01:50:37 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.53125 on epoch=99
03/11/2022 01:50:37 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=99, global_step=200
03/11/2022 01:50:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 01:50:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.25 on epoch=109
03/11/2022 01:50:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
03/11/2022 01:50:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
03/11/2022 01:50:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
03/11/2022 01:50:49 - INFO - __main__ - Global step 250 Train loss 0.25 ACC 0.5 on epoch=124
03/11/2022 01:50:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
03/11/2022 01:50:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
03/11/2022 01:50:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 01:50:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
03/11/2022 01:50:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=149
03/11/2022 01:51:00 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.5 on epoch=149
03/11/2022 01:51:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
03/11/2022 01:51:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 01:51:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
03/11/2022 01:51:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
03/11/2022 01:51:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
03/11/2022 01:51:11 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.53125 on epoch=174
03/11/2022 01:51:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
03/11/2022 01:51:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 01:51:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
03/11/2022 01:51:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 01:51:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 01:51:22 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.5 on epoch=199
03/11/2022 01:51:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 01:51:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
03/11/2022 01:51:29 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
03/11/2022 01:51:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
03/11/2022 01:51:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
03/11/2022 01:51:33 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.59375 on epoch=224
03/11/2022 01:51:33 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=224, global_step=450
03/11/2022 01:51:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 01:51:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
03/11/2022 01:51:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
03/11/2022 01:51:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
03/11/2022 01:51:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
03/11/2022 01:51:44 - INFO - __main__ - Global step 500 Train loss 0.24 ACC 0.5 on epoch=249
03/11/2022 01:51:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 01:51:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
03/11/2022 01:51:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
03/11/2022 01:51:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
03/11/2022 01:51:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
03/11/2022 01:51:56 - INFO - __main__ - Global step 550 Train loss 0.24 ACC 0.625 on epoch=274
03/11/2022 01:51:56 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=274, global_step=550
03/11/2022 01:51:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/11/2022 01:52:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=284
03/11/2022 01:52:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/11/2022 01:52:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
03/11/2022 01:52:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
03/11/2022 01:52:07 - INFO - __main__ - Global step 600 Train loss 0.24 ACC 0.5625 on epoch=299
03/11/2022 01:52:09 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=304
03/11/2022 01:52:11 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=309
03/11/2022 01:52:13 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
03/11/2022 01:52:15 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
03/11/2022 01:52:17 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
03/11/2022 01:52:18 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.53125 on epoch=324
03/11/2022 01:52:20 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=329
03/11/2022 01:52:22 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=334
03/11/2022 01:52:24 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=339
03/11/2022 01:52:27 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
03/11/2022 01:52:29 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
03/11/2022 01:52:29 - INFO - __main__ - Global step 700 Train loss 0.24 ACC 0.59375 on epoch=349
03/11/2022 01:52:31 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=354
03/11/2022 01:52:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
03/11/2022 01:52:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
03/11/2022 01:52:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
03/11/2022 01:52:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 01:52:41 - INFO - __main__ - Global step 750 Train loss 0.24 ACC 0.59375 on epoch=374
03/11/2022 01:52:43 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
03/11/2022 01:52:45 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=384
03/11/2022 01:52:47 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
03/11/2022 01:52:49 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=394
03/11/2022 01:52:51 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=399
03/11/2022 01:52:52 - INFO - __main__ - Global step 800 Train loss 0.23 ACC 0.53125 on epoch=399
03/11/2022 01:52:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.23 on epoch=404
03/11/2022 01:52:57 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=409
03/11/2022 01:52:59 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
03/11/2022 01:53:01 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=419
03/11/2022 01:53:03 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=424
03/11/2022 01:53:04 - INFO - __main__ - Global step 850 Train loss 0.23 ACC 0.625 on epoch=424
03/11/2022 01:53:06 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=429
03/11/2022 01:53:08 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=434
03/11/2022 01:53:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
03/11/2022 01:53:12 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
03/11/2022 01:53:15 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=449
03/11/2022 01:53:16 - INFO - __main__ - Global step 900 Train loss 0.23 ACC 0.59375 on epoch=449
03/11/2022 01:53:18 - INFO - __main__ - Step 910 Global step 910 Train loss 0.19 on epoch=454
03/11/2022 01:53:20 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=459
03/11/2022 01:53:22 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=464
03/11/2022 01:53:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=469
03/11/2022 01:53:26 - INFO - __main__ - Step 950 Global step 950 Train loss 0.22 on epoch=474
03/11/2022 01:53:28 - INFO - __main__ - Global step 950 Train loss 0.21 ACC 0.59375 on epoch=474
03/11/2022 01:53:30 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=479
03/11/2022 01:53:32 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=484
03/11/2022 01:53:34 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=489
03/11/2022 01:53:36 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=494
03/11/2022 01:53:38 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=499
03/11/2022 01:53:39 - INFO - __main__ - Global step 1000 Train loss 0.24 ACC 0.53125 on epoch=499
03/11/2022 01:53:41 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.22 on epoch=504
03/11/2022 01:53:43 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
03/11/2022 01:53:45 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=514
03/11/2022 01:53:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=519
03/11/2022 01:53:49 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=524
03/11/2022 01:53:50 - INFO - __main__ - Global step 1050 Train loss 0.21 ACC 0.5625 on epoch=524
03/11/2022 01:53:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=529
03/11/2022 01:53:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.21 on epoch=534
03/11/2022 01:53:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.20 on epoch=539
03/11/2022 01:53:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.20 on epoch=544
03/11/2022 01:54:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=549
03/11/2022 01:54:02 - INFO - __main__ - Global step 1100 Train loss 0.20 ACC 0.5625 on epoch=549
03/11/2022 01:54:05 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=554
03/11/2022 01:54:07 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.20 on epoch=559
03/11/2022 01:54:09 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=564
03/11/2022 01:54:11 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=569
03/11/2022 01:54:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=574
03/11/2022 01:54:14 - INFO - __main__ - Global step 1150 Train loss 0.20 ACC 0.59375 on epoch=574
03/11/2022 01:54:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.20 on epoch=579
03/11/2022 01:54:18 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.18 on epoch=584
03/11/2022 01:54:20 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=589
03/11/2022 01:54:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=594
03/11/2022 01:54:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=599
03/11/2022 01:54:25 - INFO - __main__ - Global step 1200 Train loss 0.20 ACC 0.5625 on epoch=599
03/11/2022 01:54:28 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.20 on epoch=604
03/11/2022 01:54:30 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=609
03/11/2022 01:54:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=614
03/11/2022 01:54:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.18 on epoch=619
03/11/2022 01:54:36 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=624
03/11/2022 01:54:37 - INFO - __main__ - Global step 1250 Train loss 0.20 ACC 0.5625 on epoch=624
03/11/2022 01:54:39 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=629
03/11/2022 01:54:41 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.19 on epoch=634
03/11/2022 01:54:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.17 on epoch=639
03/11/2022 01:54:46 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.18 on epoch=644
03/11/2022 01:54:48 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=649
03/11/2022 01:54:49 - INFO - __main__ - Global step 1300 Train loss 0.19 ACC 0.53125 on epoch=649
03/11/2022 01:54:51 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.18 on epoch=654
03/11/2022 01:54:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.21 on epoch=659
03/11/2022 01:54:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=664
03/11/2022 01:54:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=669
03/11/2022 01:54:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=674
03/11/2022 01:55:00 - INFO - __main__ - Global step 1350 Train loss 0.19 ACC 0.46875 on epoch=674
03/11/2022 01:55:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=679
03/11/2022 01:55:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=684
03/11/2022 01:55:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=689
03/11/2022 01:55:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.20 on epoch=694
03/11/2022 01:55:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=699
03/11/2022 01:55:12 - INFO - __main__ - Global step 1400 Train loss 0.23 ACC 0.53125 on epoch=699
03/11/2022 01:55:14 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=704
03/11/2022 01:55:16 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=709
03/11/2022 01:55:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.18 on epoch=714
03/11/2022 01:55:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=719
03/11/2022 01:55:22 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=724
03/11/2022 01:55:23 - INFO - __main__ - Global step 1450 Train loss 0.19 ACC 0.5 on epoch=724
03/11/2022 01:55:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
03/11/2022 01:55:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=734
03/11/2022 01:55:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.19 on epoch=739
03/11/2022 01:55:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=744
03/11/2022 01:55:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=749
03/11/2022 01:55:34 - INFO - __main__ - Global step 1500 Train loss 0.19 ACC 0.5 on epoch=749
03/11/2022 01:55:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=754
03/11/2022 01:55:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.20 on epoch=759
03/11/2022 01:55:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.18 on epoch=764
03/11/2022 01:55:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.18 on epoch=769
03/11/2022 01:55:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.20 on epoch=774
03/11/2022 01:55:46 - INFO - __main__ - Global step 1550 Train loss 0.20 ACC 0.53125 on epoch=774
03/11/2022 01:55:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.19 on epoch=779
03/11/2022 01:55:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=784
03/11/2022 01:55:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.20 on epoch=789
03/11/2022 01:55:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.17 on epoch=794
03/11/2022 01:55:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.18 on epoch=799
03/11/2022 01:55:57 - INFO - __main__ - Global step 1600 Train loss 0.19 ACC 0.53125 on epoch=799
03/11/2022 01:56:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=804
03/11/2022 01:56:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=809
03/11/2022 01:56:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.20 on epoch=814
03/11/2022 01:56:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=819
03/11/2022 01:56:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.19 on epoch=824
03/11/2022 01:56:09 - INFO - __main__ - Global step 1650 Train loss 0.20 ACC 0.5625 on epoch=824
03/11/2022 01:56:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.18 on epoch=829
03/11/2022 01:56:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.18 on epoch=834
03/11/2022 01:56:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.20 on epoch=839
03/11/2022 01:56:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.18 on epoch=844
03/11/2022 01:56:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.18 on epoch=849
03/11/2022 01:56:20 - INFO - __main__ - Global step 1700 Train loss 0.19 ACC 0.5625 on epoch=849
03/11/2022 01:56:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.17 on epoch=854
03/11/2022 01:56:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.15 on epoch=859
03/11/2022 01:56:27 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=864
03/11/2022 01:56:29 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.16 on epoch=869
03/11/2022 01:56:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=874
03/11/2022 01:56:32 - INFO - __main__ - Global step 1750 Train loss 0.17 ACC 0.5 on epoch=874
03/11/2022 01:56:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=879
03/11/2022 01:56:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.17 on epoch=884
03/11/2022 01:56:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.16 on epoch=889
03/11/2022 01:56:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.18 on epoch=894
03/11/2022 01:56:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.22 on epoch=899
03/11/2022 01:56:43 - INFO - __main__ - Global step 1800 Train loss 0.18 ACC 0.5 on epoch=899
03/11/2022 01:56:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.17 on epoch=904
03/11/2022 01:56:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=909
03/11/2022 01:56:49 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=914
03/11/2022 01:56:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.15 on epoch=919
03/11/2022 01:56:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=924
03/11/2022 01:56:54 - INFO - __main__ - Global step 1850 Train loss 0.17 ACC 0.5625 on epoch=924
03/11/2022 01:56:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=929
03/11/2022 01:56:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=934
03/11/2022 01:57:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.18 on epoch=939
03/11/2022 01:57:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.17 on epoch=944
03/11/2022 01:57:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=949
03/11/2022 01:57:06 - INFO - __main__ - Global step 1900 Train loss 0.19 ACC 0.5625 on epoch=949
03/11/2022 01:57:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.17 on epoch=954
03/11/2022 01:57:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=959
03/11/2022 01:57:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=964
03/11/2022 01:57:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.16 on epoch=969
03/11/2022 01:57:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=974
03/11/2022 01:57:17 - INFO - __main__ - Global step 1950 Train loss 0.17 ACC 0.53125 on epoch=974
03/11/2022 01:57:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=979
03/11/2022 01:57:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=984
03/11/2022 01:57:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.16 on epoch=989
03/11/2022 01:57:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=994
03/11/2022 01:57:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.16 on epoch=999
03/11/2022 01:57:29 - INFO - __main__ - Global step 2000 Train loss 0.17 ACC 0.5625 on epoch=999
03/11/2022 01:57:29 - INFO - __main__ - save last model!
03/11/2022 01:57:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 01:57:29 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 01:57:29 - INFO - __main__ - Printing 3 examples
03/11/2022 01:57:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 01:57:29 - INFO - __main__ - ['sentence 2']
03/11/2022 01:57:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 01:57:29 - INFO - __main__ - ['sentence 2']
03/11/2022 01:57:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 01:57:29 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:29 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:57:29 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:57:29 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 01:57:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:57:29 - INFO - __main__ - Printing 3 examples
03/11/2022 01:57:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/11/2022 01:57:29 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/11/2022 01:57:29 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/11/2022 01:57:29 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 01:57:29 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:57:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:57:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:57:29 - INFO - __main__ - Printing 3 examples
03/11/2022 01:57:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/11/2022 01:57:29 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:57:29 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/11/2022 01:57:29 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:29 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:57:29 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:57:29 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:57:34 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_13_0.3_8_predictions.txt
03/11/2022 01:57:34 - INFO - __main__ - ACC on test data: 0.5700
03/11/2022 01:57:34 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.3, bsz=8, dev_performance=0.625, test_performance=0.57
03/11/2022 01:57:34 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.2, bsz=8 ...
03/11/2022 01:57:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:57:35 - INFO - __main__ - Printing 3 examples
03/11/2022 01:57:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Amy will not ever alarm Sara. [SEP] sentence 2: Amy will probably ever alarm Sara.
03/11/2022 01:57:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Heather has not ever hoped to kiss Phillip. [SEP] sentence 2: Heather has really ever hoped to kiss Phillip.
03/11/2022 01:57:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Angela did not ever heal Becca. [SEP] sentence 2: Angela did probably ever heal Becca.
03/11/2022 01:57:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 01:57:35 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:57:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 01:57:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 01:57:35 - INFO - __main__ - Printing 3 examples
03/11/2022 01:57:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Nicole should not ever practice. [SEP] sentence 2: Nicole should probably ever practice.
03/11/2022 01:57:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Some guests did not ever embrace. [SEP] sentence 2: Some guests did fortunately ever embrace.
03/11/2022 01:57:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:35 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Anna did not ever worry Andrea. [SEP] sentence 2: Anna did really ever worry Andrea.
03/11/2022 01:57:35 - INFO - __main__ - ['sentence 1']
03/11/2022 01:57:35 - INFO - __main__ - Tokenizing Input ...
03/11/2022 01:57:35 - INFO - __main__ - Tokenizing Output ...
03/11/2022 01:57:35 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 01:57:43 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:57:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:57:44 - INFO - __main__ - Starting training!
03/11/2022 01:57:47 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 01:57:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 01:57:48 - INFO - __main__ - Starting training!
03/11/2022 01:57:53 - INFO - __main__ - Step 10 Global step 10 Train loss 1.43 on epoch=4
03/11/2022 01:57:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=9
03/11/2022 01:57:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.44 on epoch=14
03/11/2022 01:58:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=19
03/11/2022 01:58:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.38 on epoch=24
03/11/2022 01:58:03 - INFO - __main__ - Global step 50 Train loss 0.67 ACC 0.5 on epoch=24
03/11/2022 01:58:03 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 01:58:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.38 on epoch=29
03/11/2022 01:58:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=34
03/11/2022 01:58:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.33 on epoch=39
03/11/2022 01:58:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.29 on epoch=44
03/11/2022 01:58:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=49
03/11/2022 01:58:15 - INFO - __main__ - Global step 100 Train loss 0.33 ACC 0.5 on epoch=49
03/11/2022 01:58:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
03/11/2022 01:58:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=59
03/11/2022 01:58:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
03/11/2022 01:58:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=69
03/11/2022 01:58:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
03/11/2022 01:58:27 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.5 on epoch=74
03/11/2022 01:58:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
03/11/2022 01:58:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=84
03/11/2022 01:58:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
03/11/2022 01:58:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
03/11/2022 01:58:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
03/11/2022 01:58:39 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.5625 on epoch=99
03/11/2022 01:58:39 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=99, global_step=200
03/11/2022 01:58:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 01:58:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 01:58:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
03/11/2022 01:58:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
03/11/2022 01:58:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
03/11/2022 01:58:51 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.5 on epoch=124
03/11/2022 01:58:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
03/11/2022 01:58:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
03/11/2022 01:58:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
03/11/2022 01:59:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
03/11/2022 01:59:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
03/11/2022 01:59:02 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.5 on epoch=149
03/11/2022 01:59:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 01:59:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
03/11/2022 01:59:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=164
03/11/2022 01:59:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 01:59:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=174
03/11/2022 01:59:14 - INFO - __main__ - Global step 350 Train loss 0.24 ACC 0.5 on epoch=174
03/11/2022 01:59:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
03/11/2022 01:59:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.23 on epoch=184
03/11/2022 01:59:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 01:59:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.24 on epoch=194
03/11/2022 01:59:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
03/11/2022 01:59:26 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.5 on epoch=199
03/11/2022 01:59:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
03/11/2022 01:59:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
03/11/2022 01:59:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
03/11/2022 01:59:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
03/11/2022 01:59:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
03/11/2022 01:59:38 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.5 on epoch=224
03/11/2022 01:59:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
03/11/2022 01:59:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
03/11/2022 01:59:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 01:59:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
03/11/2022 01:59:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
03/11/2022 01:59:50 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5 on epoch=249
03/11/2022 01:59:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=254
03/11/2022 01:59:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
03/11/2022 01:59:57 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
03/11/2022 01:59:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
03/11/2022 02:00:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
03/11/2022 02:00:02 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.5 on epoch=274
03/11/2022 02:00:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/11/2022 02:00:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=284
03/11/2022 02:00:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
03/11/2022 02:00:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/11/2022 02:00:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
03/11/2022 02:00:13 - INFO - __main__ - Global step 600 Train loss 0.24 ACC 0.53125 on epoch=299
03/11/2022 02:00:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=304
03/11/2022 02:00:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=309
03/11/2022 02:00:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
03/11/2022 02:00:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
03/11/2022 02:00:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
03/11/2022 02:00:25 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.5 on epoch=324
03/11/2022 02:00:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
03/11/2022 02:00:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=334
03/11/2022 02:00:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=339
03/11/2022 02:00:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
03/11/2022 02:00:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
03/11/2022 02:00:37 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.5 on epoch=349
03/11/2022 02:00:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=354
03/11/2022 02:00:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
03/11/2022 02:00:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
03/11/2022 02:00:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=369
03/11/2022 02:00:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 02:00:49 - INFO - __main__ - Global step 750 Train loss 0.24 ACC 0.5 on epoch=374
03/11/2022 02:00:51 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=379
03/11/2022 02:00:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.26 on epoch=384
03/11/2022 02:00:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=389
03/11/2022 02:00:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
03/11/2022 02:01:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=399
03/11/2022 02:01:01 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.53125 on epoch=399
03/11/2022 02:01:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
03/11/2022 02:01:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
03/11/2022 02:01:08 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=414
03/11/2022 02:01:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/11/2022 02:01:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
03/11/2022 02:01:13 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.5 on epoch=424
03/11/2022 02:01:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
03/11/2022 02:01:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
03/11/2022 02:01:20 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=439
03/11/2022 02:01:22 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=444
03/11/2022 02:01:24 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=449
03/11/2022 02:01:25 - INFO - __main__ - Global step 900 Train loss 0.24 ACC 0.5 on epoch=449
03/11/2022 02:01:27 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=454
03/11/2022 02:01:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=459
03/11/2022 02:01:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/11/2022 02:01:34 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=469
03/11/2022 02:01:36 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=474
03/11/2022 02:01:36 - INFO - __main__ - Global step 950 Train loss 0.24 ACC 0.5 on epoch=474
03/11/2022 02:01:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=479
03/11/2022 02:01:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=484
03/11/2022 02:01:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.26 on epoch=489
03/11/2022 02:01:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=494
03/11/2022 02:01:48 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=499
03/11/2022 02:01:48 - INFO - __main__ - Global step 1000 Train loss 0.25 ACC 0.5 on epoch=499
03/11/2022 02:01:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=504
03/11/2022 02:01:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=509
03/11/2022 02:01:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=514
03/11/2022 02:01:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=519
03/11/2022 02:02:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=524
03/11/2022 02:02:00 - INFO - __main__ - Global step 1050 Train loss 0.25 ACC 0.5 on epoch=524
03/11/2022 02:02:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
03/11/2022 02:02:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=534
03/11/2022 02:02:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=539
03/11/2022 02:02:09 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/11/2022 02:02:11 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=549
03/11/2022 02:02:12 - INFO - __main__ - Global step 1100 Train loss 0.24 ACC 0.5 on epoch=549
03/11/2022 02:02:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=554
03/11/2022 02:02:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=559
03/11/2022 02:02:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=564
03/11/2022 02:02:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=569
03/11/2022 02:02:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.25 on epoch=574
03/11/2022 02:02:24 - INFO - __main__ - Global step 1150 Train loss 0.25 ACC 0.5 on epoch=574
03/11/2022 02:02:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
03/11/2022 02:02:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=584
03/11/2022 02:02:31 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=589
03/11/2022 02:02:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/11/2022 02:02:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=599
03/11/2022 02:02:36 - INFO - __main__ - Global step 1200 Train loss 0.24 ACC 0.5 on epoch=599
03/11/2022 02:02:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.23 on epoch=604
03/11/2022 02:02:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=609
03/11/2022 02:02:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=614
03/11/2022 02:02:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.23 on epoch=619
03/11/2022 02:02:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=624
03/11/2022 02:02:48 - INFO - __main__ - Global step 1250 Train loss 0.25 ACC 0.5 on epoch=624
03/11/2022 02:02:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=629
03/11/2022 02:02:52 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=634
03/11/2022 02:02:54 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=639
03/11/2022 02:02:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=644
03/11/2022 02:02:59 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=649
03/11/2022 02:02:59 - INFO - __main__ - Global step 1300 Train loss 0.23 ACC 0.46875 on epoch=649
03/11/2022 02:03:02 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=654
03/11/2022 02:03:04 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=659
03/11/2022 02:03:06 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.23 on epoch=664
03/11/2022 02:03:08 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
03/11/2022 02:03:11 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
03/11/2022 02:03:11 - INFO - __main__ - Global step 1350 Train loss 0.24 ACC 0.46875 on epoch=674
03/11/2022 02:03:14 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=679
03/11/2022 02:03:16 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=684
03/11/2022 02:03:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=689
03/11/2022 02:03:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.25 on epoch=694
03/11/2022 02:03:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=699
03/11/2022 02:03:23 - INFO - __main__ - Global step 1400 Train loss 0.24 ACC 0.53125 on epoch=699
03/11/2022 02:03:26 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=704
03/11/2022 02:03:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.27 on epoch=709
03/11/2022 02:03:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.26 on epoch=714
03/11/2022 02:03:32 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=719
03/11/2022 02:03:35 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=724
03/11/2022 02:03:35 - INFO - __main__ - Global step 1450 Train loss 0.24 ACC 0.5 on epoch=724
03/11/2022 02:03:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=729
03/11/2022 02:03:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=734
03/11/2022 02:03:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=739
03/11/2022 02:03:45 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.22 on epoch=744
03/11/2022 02:03:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=749
03/11/2022 02:03:48 - INFO - __main__ - Global step 1500 Train loss 0.23 ACC 0.53125 on epoch=749
03/11/2022 02:03:50 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=754
03/11/2022 02:03:52 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=759
03/11/2022 02:03:54 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=764
03/11/2022 02:03:56 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.25 on epoch=769
03/11/2022 02:03:58 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=774
03/11/2022 02:03:59 - INFO - __main__ - Global step 1550 Train loss 0.23 ACC 0.5625 on epoch=774
03/11/2022 02:04:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=779
03/11/2022 02:04:04 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.25 on epoch=784
03/11/2022 02:04:06 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=789
03/11/2022 02:04:08 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.23 on epoch=794
03/11/2022 02:04:10 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=799
03/11/2022 02:04:11 - INFO - __main__ - Global step 1600 Train loss 0.24 ACC 0.46875 on epoch=799
03/11/2022 02:04:13 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
03/11/2022 02:04:15 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=809
03/11/2022 02:04:17 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=814
03/11/2022 02:04:20 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=819
03/11/2022 02:04:22 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=824
03/11/2022 02:04:22 - INFO - __main__ - Global step 1650 Train loss 0.23 ACC 0.53125 on epoch=824
03/11/2022 02:04:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=829
03/11/2022 02:04:27 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=834
03/11/2022 02:04:29 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=839
03/11/2022 02:04:31 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=844
03/11/2022 02:04:34 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=849
03/11/2022 02:04:34 - INFO - __main__ - Global step 1700 Train loss 0.24 ACC 0.53125 on epoch=849
03/11/2022 02:04:36 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=854
03/11/2022 02:04:39 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
03/11/2022 02:04:41 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=864
03/11/2022 02:04:43 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.22 on epoch=869
03/11/2022 02:04:45 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=874
03/11/2022 02:04:46 - INFO - __main__ - Global step 1750 Train loss 0.24 ACC 0.53125 on epoch=874
03/11/2022 02:04:48 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=879
03/11/2022 02:04:50 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=884
03/11/2022 02:04:53 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=889
03/11/2022 02:04:55 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=894
03/11/2022 02:04:57 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.24 on epoch=899
03/11/2022 02:04:58 - INFO - __main__ - Global step 1800 Train loss 0.23 ACC 0.5625 on epoch=899
03/11/2022 02:05:01 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=904
03/11/2022 02:05:03 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=909
03/11/2022 02:05:05 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=914
03/11/2022 02:05:07 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=919
03/11/2022 02:05:09 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.23 on epoch=924
03/11/2022 02:05:10 - INFO - __main__ - Global step 1850 Train loss 0.21 ACC 0.53125 on epoch=924
03/11/2022 02:05:13 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=929
03/11/2022 02:05:15 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=934
03/11/2022 02:05:17 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=939
03/11/2022 02:05:19 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=944
03/11/2022 02:05:21 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=949
03/11/2022 02:05:22 - INFO - __main__ - Global step 1900 Train loss 0.21 ACC 0.59375 on epoch=949
03/11/2022 02:05:22 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=949, global_step=1900
03/11/2022 02:05:24 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/11/2022 02:05:27 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=959
03/11/2022 02:05:29 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=964
03/11/2022 02:05:31 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.24 on epoch=969
03/11/2022 02:05:33 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=974
03/11/2022 02:05:34 - INFO - __main__ - Global step 1950 Train loss 0.22 ACC 0.65625 on epoch=974
03/11/2022 02:05:34 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=974, global_step=1950
03/11/2022 02:05:37 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=979
03/11/2022 02:05:39 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.22 on epoch=984
03/11/2022 02:05:41 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=989
03/11/2022 02:05:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=994
03/11/2022 02:05:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=999
03/11/2022 02:05:46 - INFO - __main__ - Global step 2000 Train loss 0.21 ACC 0.59375 on epoch=999
03/11/2022 02:05:46 - INFO - __main__ - save last model!
03/11/2022 02:05:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 02:05:46 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 02:05:46 - INFO - __main__ - Printing 3 examples
03/11/2022 02:05:46 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 02:05:46 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:46 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 02:05:46 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:46 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 02:05:46 - INFO - __main__ - ['sentence 1']
03/11/2022 02:05:46 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:05:46 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:05:46 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 02:05:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:05:47 - INFO - __main__ - Printing 3 examples
03/11/2022 02:05:47 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/11/2022 02:05:47 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:47 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/11/2022 02:05:47 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:47 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/11/2022 02:05:47 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 02:05:47 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:05:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:05:47 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:05:47 - INFO - __main__ - Printing 3 examples
03/11/2022 02:05:47 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/11/2022 02:05:47 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:47 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/11/2022 02:05:47 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:47 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/11/2022 02:05:47 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:47 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:05:47 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:05:47 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:05:53 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_13_0.2_8_predictions.txt
03/11/2022 02:05:53 - INFO - __main__ - ACC on test data: 0.5450
03/11/2022 02:05:53 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_13, lr=0.2, bsz=8, dev_performance=0.65625, test_performance=0.545
03/11/2022 02:05:53 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.5, bsz=8 ...
03/11/2022 02:05:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:05:54 - INFO - __main__ - Printing 3 examples
03/11/2022 02:05:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/11/2022 02:05:54 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/11/2022 02:05:54 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/11/2022 02:05:54 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 02:05:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:05:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:05:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:05:54 - INFO - __main__ - Printing 3 examples
03/11/2022 02:05:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/11/2022 02:05:54 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/11/2022 02:05:54 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:54 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/11/2022 02:05:54 - INFO - __main__ - ['sentence 2']
03/11/2022 02:05:54 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:05:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:05:54 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:05:59 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:06:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:06:00 - INFO - __main__ - Starting training!
03/11/2022 02:06:06 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:06:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:06:07 - INFO - __main__ - Starting training!
03/11/2022 02:06:10 - INFO - __main__ - Step 10 Global step 10 Train loss 1.13 on epoch=4
03/11/2022 02:06:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.46 on epoch=9
03/11/2022 02:06:14 - INFO - __main__ - Step 30 Global step 30 Train loss 0.36 on epoch=14
03/11/2022 02:06:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.35 on epoch=19
03/11/2022 02:06:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.30 on epoch=24
03/11/2022 02:06:19 - INFO - __main__ - Global step 50 Train loss 0.52 ACC 0.5 on epoch=24
03/11/2022 02:06:19 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 02:06:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=29
03/11/2022 02:06:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=34
03/11/2022 02:06:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=39
03/11/2022 02:06:28 - INFO - __main__ - Step 90 Global step 90 Train loss 0.32 on epoch=44
03/11/2022 02:06:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=49
03/11/2022 02:06:31 - INFO - __main__ - Global step 100 Train loss 0.29 ACC 0.5 on epoch=49
03/11/2022 02:06:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
03/11/2022 02:06:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=59
03/11/2022 02:06:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.24 on epoch=64
03/11/2022 02:06:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.62 on epoch=69
03/11/2022 02:06:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.39 on epoch=74
03/11/2022 02:06:43 - INFO - __main__ - Global step 150 Train loss 0.36 ACC 0.5 on epoch=74
03/11/2022 02:06:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=79
03/11/2022 02:06:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=84
03/11/2022 02:06:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=89
03/11/2022 02:06:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.39 on epoch=94
03/11/2022 02:06:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=99
03/11/2022 02:06:54 - INFO - __main__ - Global step 200 Train loss 0.30 ACC 0.34375 on epoch=99
03/11/2022 02:06:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 02:06:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
03/11/2022 02:07:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
03/11/2022 02:07:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
03/11/2022 02:07:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
03/11/2022 02:07:06 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.5 on epoch=124
03/11/2022 02:07:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
03/11/2022 02:07:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
03/11/2022 02:07:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
03/11/2022 02:07:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
03/11/2022 02:07:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
03/11/2022 02:07:18 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.5 on epoch=149
03/11/2022 02:07:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
03/11/2022 02:07:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
03/11/2022 02:07:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
03/11/2022 02:07:26 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=169
03/11/2022 02:07:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
03/11/2022 02:07:29 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.5 on epoch=174
03/11/2022 02:07:31 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
03/11/2022 02:07:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
03/11/2022 02:07:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 02:07:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 02:07:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=199
03/11/2022 02:07:41 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.53125 on epoch=199
03/11/2022 02:07:41 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=199, global_step=400
03/11/2022 02:07:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
03/11/2022 02:07:45 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
03/11/2022 02:07:47 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
03/11/2022 02:07:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
03/11/2022 02:07:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
03/11/2022 02:07:52 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.5 on epoch=224
03/11/2022 02:07:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
03/11/2022 02:07:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
03/11/2022 02:07:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
03/11/2022 02:08:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
03/11/2022 02:08:03 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
03/11/2022 02:08:04 - INFO - __main__ - Global step 500 Train loss 0.26 ACC 0.53125 on epoch=249
03/11/2022 02:08:06 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 02:08:08 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
03/11/2022 02:08:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
03/11/2022 02:08:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
03/11/2022 02:08:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
03/11/2022 02:08:16 - INFO - __main__ - Global step 550 Train loss 0.26 ACC 0.5 on epoch=274
03/11/2022 02:08:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/11/2022 02:08:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=284
03/11/2022 02:08:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/11/2022 02:08:24 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/11/2022 02:08:26 - INFO - __main__ - Step 600 Global step 600 Train loss 0.29 on epoch=299
03/11/2022 02:08:27 - INFO - __main__ - Global step 600 Train loss 0.26 ACC 0.5 on epoch=299
03/11/2022 02:08:29 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=304
03/11/2022 02:08:31 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=309
03/11/2022 02:08:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
03/11/2022 02:08:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=319
03/11/2022 02:08:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
03/11/2022 02:08:38 - INFO - __main__ - Global step 650 Train loss 0.26 ACC 0.5 on epoch=324
03/11/2022 02:08:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=329
03/11/2022 02:08:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.28 on epoch=334
03/11/2022 02:08:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.27 on epoch=339
03/11/2022 02:08:47 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
03/11/2022 02:08:49 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
03/11/2022 02:08:50 - INFO - __main__ - Global step 700 Train loss 0.26 ACC 0.5 on epoch=349
03/11/2022 02:08:52 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=354
03/11/2022 02:08:54 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
03/11/2022 02:08:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=364
03/11/2022 02:08:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
03/11/2022 02:09:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.26 on epoch=374
03/11/2022 02:09:01 - INFO - __main__ - Global step 750 Train loss 0.25 ACC 0.46875 on epoch=374
03/11/2022 02:09:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=379
03/11/2022 02:09:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=384
03/11/2022 02:09:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=389
03/11/2022 02:09:10 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=394
03/11/2022 02:09:12 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
03/11/2022 02:09:13 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.5 on epoch=399
03/11/2022 02:09:15 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
03/11/2022 02:09:17 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
03/11/2022 02:09:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=414
03/11/2022 02:09:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=419
03/11/2022 02:09:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=424
03/11/2022 02:09:24 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.5 on epoch=424
03/11/2022 02:09:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=429
03/11/2022 02:09:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
03/11/2022 02:09:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=439
03/11/2022 02:09:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=444
03/11/2022 02:09:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=449
03/11/2022 02:09:36 - INFO - __main__ - Global step 900 Train loss 0.25 ACC 0.5 on epoch=449
03/11/2022 02:09:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
03/11/2022 02:09:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=459
03/11/2022 02:09:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=464
03/11/2022 02:09:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=469
03/11/2022 02:09:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=474
03/11/2022 02:09:47 - INFO - __main__ - Global step 950 Train loss 0.24 ACC 0.5 on epoch=474
03/11/2022 02:09:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=479
03/11/2022 02:09:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.27 on epoch=484
03/11/2022 02:09:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.27 on epoch=489
03/11/2022 02:09:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=494
03/11/2022 02:09:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.25 on epoch=499
03/11/2022 02:09:59 - INFO - __main__ - Global step 1000 Train loss 0.26 ACC 0.5 on epoch=499
03/11/2022 02:10:01 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=504
03/11/2022 02:10:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=509
03/11/2022 02:10:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=514
03/11/2022 02:10:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=519
03/11/2022 02:10:10 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=524
03/11/2022 02:10:11 - INFO - __main__ - Global step 1050 Train loss 0.24 ACC 0.5 on epoch=524
03/11/2022 02:10:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
03/11/2022 02:10:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=534
03/11/2022 02:10:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=539
03/11/2022 02:10:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=544
03/11/2022 02:10:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=549
03/11/2022 02:10:22 - INFO - __main__ - Global step 1100 Train loss 0.24 ACC 0.5 on epoch=549
03/11/2022 02:10:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=554
03/11/2022 02:10:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=559
03/11/2022 02:10:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=564
03/11/2022 02:10:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=569
03/11/2022 02:10:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.25 on epoch=574
03/11/2022 02:10:34 - INFO - __main__ - Global step 1150 Train loss 0.24 ACC 0.5 on epoch=574
03/11/2022 02:10:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
03/11/2022 02:10:38 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=584
03/11/2022 02:10:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=589
03/11/2022 02:10:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.22 on epoch=594
03/11/2022 02:10:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=599
03/11/2022 02:10:46 - INFO - __main__ - Global step 1200 Train loss 0.24 ACC 0.5 on epoch=599
03/11/2022 02:10:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=604
03/11/2022 02:10:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=609
03/11/2022 02:10:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=614
03/11/2022 02:10:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=619
03/11/2022 02:10:56 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=624
03/11/2022 02:10:57 - INFO - __main__ - Global step 1250 Train loss 0.25 ACC 0.5 on epoch=624
03/11/2022 02:10:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.25 on epoch=629
03/11/2022 02:11:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=634
03/11/2022 02:11:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=639
03/11/2022 02:11:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.24 on epoch=644
03/11/2022 02:11:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=649
03/11/2022 02:11:08 - INFO - __main__ - Global step 1300 Train loss 0.24 ACC 0.5 on epoch=649
03/11/2022 02:11:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=654
03/11/2022 02:11:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
03/11/2022 02:11:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=664
03/11/2022 02:11:17 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
03/11/2022 02:11:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.27 on epoch=674
03/11/2022 02:11:20 - INFO - __main__ - Global step 1350 Train loss 0.25 ACC 0.5 on epoch=674
03/11/2022 02:11:22 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=679
03/11/2022 02:11:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=684
03/11/2022 02:11:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=689
03/11/2022 02:11:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=694
03/11/2022 02:11:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=699
03/11/2022 02:11:32 - INFO - __main__ - Global step 1400 Train loss 0.24 ACC 0.5 on epoch=699
03/11/2022 02:11:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=704
03/11/2022 02:11:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=709
03/11/2022 02:11:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=714
03/11/2022 02:11:40 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=719
03/11/2022 02:11:42 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=724
03/11/2022 02:11:43 - INFO - __main__ - Global step 1450 Train loss 0.25 ACC 0.46875 on epoch=724
03/11/2022 02:11:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=729
03/11/2022 02:11:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.24 on epoch=734
03/11/2022 02:11:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=739
03/11/2022 02:11:52 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=744
03/11/2022 02:11:54 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=749
03/11/2022 02:11:54 - INFO - __main__ - Global step 1500 Train loss 0.24 ACC 0.46875 on epoch=749
03/11/2022 02:11:57 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=754
03/11/2022 02:11:59 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=759
03/11/2022 02:12:01 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=764
03/11/2022 02:12:03 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=769
03/11/2022 02:12:05 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=774
03/11/2022 02:12:06 - INFO - __main__ - Global step 1550 Train loss 0.23 ACC 0.4375 on epoch=774
03/11/2022 02:12:08 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=779
03/11/2022 02:12:10 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=784
03/11/2022 02:12:12 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=789
03/11/2022 02:12:14 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=794
03/11/2022 02:12:17 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
03/11/2022 02:12:17 - INFO - __main__ - Global step 1600 Train loss 0.24 ACC 0.46875 on epoch=799
03/11/2022 02:12:19 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
03/11/2022 02:12:21 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.22 on epoch=809
03/11/2022 02:12:24 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=814
03/11/2022 02:12:26 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=819
03/11/2022 02:12:28 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=824
03/11/2022 02:12:28 - INFO - __main__ - Global step 1650 Train loss 0.22 ACC 0.375 on epoch=824
03/11/2022 02:12:31 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=829
03/11/2022 02:12:33 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=834
03/11/2022 02:12:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=839
03/11/2022 02:12:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=844
03/11/2022 02:12:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.23 on epoch=849
03/11/2022 02:12:40 - INFO - __main__ - Global step 1700 Train loss 0.23 ACC 0.3125 on epoch=849
03/11/2022 02:12:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=854
03/11/2022 02:12:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.22 on epoch=859
03/11/2022 02:12:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=864
03/11/2022 02:12:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=869
03/11/2022 02:12:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.21 on epoch=874
03/11/2022 02:12:51 - INFO - __main__ - Global step 1750 Train loss 0.22 ACC 0.5 on epoch=874
03/11/2022 02:12:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=879
03/11/2022 02:12:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=884
03/11/2022 02:12:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=889
03/11/2022 02:13:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=894
03/11/2022 02:13:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=899
03/11/2022 02:13:03 - INFO - __main__ - Global step 1800 Train loss 0.23 ACC 0.5 on epoch=899
03/11/2022 02:13:05 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=904
03/11/2022 02:13:07 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=909
03/11/2022 02:13:09 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=914
03/11/2022 02:13:11 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=919
03/11/2022 02:13:13 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.21 on epoch=924
03/11/2022 02:13:14 - INFO - __main__ - Global step 1850 Train loss 0.22 ACC 0.46875 on epoch=924
03/11/2022 02:13:16 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=929
03/11/2022 02:13:18 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=934
03/11/2022 02:13:21 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.22 on epoch=939
03/11/2022 02:13:23 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=944
03/11/2022 02:13:25 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=949
03/11/2022 02:13:26 - INFO - __main__ - Global step 1900 Train loss 0.22 ACC 0.53125 on epoch=949
03/11/2022 02:13:28 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/11/2022 02:13:30 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=959
03/11/2022 02:13:32 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.21 on epoch=964
03/11/2022 02:13:34 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=969
03/11/2022 02:13:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.23 on epoch=974
03/11/2022 02:13:37 - INFO - __main__ - Global step 1950 Train loss 0.21 ACC 0.40625 on epoch=974
03/11/2022 02:13:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=979
03/11/2022 02:13:42 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.20 on epoch=984
03/11/2022 02:13:44 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.21 on epoch=989
03/11/2022 02:13:46 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=994
03/11/2022 02:13:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=999
03/11/2022 02:13:49 - INFO - __main__ - Global step 2000 Train loss 0.21 ACC 0.5625 on epoch=999
03/11/2022 02:13:49 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=999, global_step=2000
03/11/2022 02:13:49 - INFO - __main__ - save last model!
03/11/2022 02:13:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 02:13:49 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 02:13:49 - INFO - __main__ - Printing 3 examples
03/11/2022 02:13:49 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 02:13:49 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:49 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 02:13:49 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:49 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 02:13:49 - INFO - __main__ - ['sentence 1']
03/11/2022 02:13:49 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:13:49 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:13:49 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 02:13:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:13:50 - INFO - __main__ - Printing 3 examples
03/11/2022 02:13:50 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/11/2022 02:13:50 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:50 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/11/2022 02:13:50 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:50 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/11/2022 02:13:50 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:50 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 02:13:50 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:13:50 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:13:50 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:13:50 - INFO - __main__ - Printing 3 examples
03/11/2022 02:13:50 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/11/2022 02:13:50 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:50 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/11/2022 02:13:50 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:50 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/11/2022 02:13:50 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:50 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:13:50 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:13:50 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:13:57 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_21_0.5_8_predictions.txt
03/11/2022 02:13:57 - INFO - __main__ - ACC on test data: 0.4900
03/11/2022 02:13:57 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.5, bsz=8, dev_performance=0.5625, test_performance=0.49
03/11/2022 02:13:57 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.4, bsz=8 ...
03/11/2022 02:13:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:13:58 - INFO - __main__ - Printing 3 examples
03/11/2022 02:13:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/11/2022 02:13:58 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/11/2022 02:13:58 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/11/2022 02:13:58 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 02:13:58 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:13:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:13:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:13:58 - INFO - __main__ - Printing 3 examples
03/11/2022 02:13:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/11/2022 02:13:58 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/11/2022 02:13:58 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:58 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/11/2022 02:13:58 - INFO - __main__ - ['sentence 2']
03/11/2022 02:13:58 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:13:58 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:13:58 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:14:04 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:14:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:14:05 - INFO - __main__ - Starting training!
03/11/2022 02:14:12 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:14:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:14:13 - INFO - __main__ - Starting training!
03/11/2022 02:14:16 - INFO - __main__ - Step 10 Global step 10 Train loss 1.28 on epoch=4
03/11/2022 02:14:18 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=9
03/11/2022 02:14:20 - INFO - __main__ - Step 30 Global step 30 Train loss 0.39 on epoch=14
03/11/2022 02:14:22 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
03/11/2022 02:14:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.32 on epoch=24
03/11/2022 02:14:25 - INFO - __main__ - Global step 50 Train loss 0.58 ACC 0.5 on epoch=24
03/11/2022 02:14:25 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 02:14:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.30 on epoch=29
03/11/2022 02:14:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
03/11/2022 02:14:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.27 on epoch=39
03/11/2022 02:14:34 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=44
03/11/2022 02:14:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=49
03/11/2022 02:14:37 - INFO - __main__ - Global step 100 Train loss 0.29 ACC 0.5 on epoch=49
03/11/2022 02:14:39 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
03/11/2022 02:14:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
03/11/2022 02:14:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=64
03/11/2022 02:14:46 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=69
03/11/2022 02:14:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.29 on epoch=74
03/11/2022 02:14:49 - INFO - __main__ - Global step 150 Train loss 0.27 ACC 0.5 on epoch=74
03/11/2022 02:14:51 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
03/11/2022 02:14:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
03/11/2022 02:14:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.32 on epoch=89
03/11/2022 02:14:58 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
03/11/2022 02:15:00 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
03/11/2022 02:15:01 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.5 on epoch=99
03/11/2022 02:15:03 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 02:15:05 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
03/11/2022 02:15:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
03/11/2022 02:15:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
03/11/2022 02:15:12 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
03/11/2022 02:15:13 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.5 on epoch=124
03/11/2022 02:15:15 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
03/11/2022 02:15:17 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 02:15:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 02:15:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=144
03/11/2022 02:15:24 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
03/11/2022 02:15:25 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.53125 on epoch=149
03/11/2022 02:15:25 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=149, global_step=300
03/11/2022 02:15:27 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 02:15:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 02:15:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
03/11/2022 02:15:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
03/11/2022 02:15:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
03/11/2022 02:15:37 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.59375 on epoch=174
03/11/2022 02:15:37 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=174, global_step=350
03/11/2022 02:15:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=179
03/11/2022 02:15:41 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
03/11/2022 02:15:43 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
03/11/2022 02:15:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 02:15:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
03/11/2022 02:15:49 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.5 on epoch=199
03/11/2022 02:15:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 02:15:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
03/11/2022 02:15:55 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
03/11/2022 02:15:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=219
03/11/2022 02:16:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
03/11/2022 02:16:00 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.5 on epoch=224
03/11/2022 02:16:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
03/11/2022 02:16:05 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
03/11/2022 02:16:07 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 02:16:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
03/11/2022 02:16:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
03/11/2022 02:16:12 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5 on epoch=249
03/11/2022 02:16:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 02:16:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
03/11/2022 02:16:19 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
03/11/2022 02:16:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
03/11/2022 02:16:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
03/11/2022 02:16:24 - INFO - __main__ - Global step 550 Train loss 0.24 ACC 0.5 on epoch=274
03/11/2022 02:16:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=279
03/11/2022 02:16:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=284
03/11/2022 02:16:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.26 on epoch=289
03/11/2022 02:16:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/11/2022 02:16:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
03/11/2022 02:16:36 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.59375 on epoch=299
03/11/2022 02:16:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
03/11/2022 02:16:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=309
03/11/2022 02:16:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=314
03/11/2022 02:16:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
03/11/2022 02:16:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
03/11/2022 02:16:48 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.5 on epoch=324
03/11/2022 02:16:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=329
03/11/2022 02:16:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=334
03/11/2022 02:16:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=339
03/11/2022 02:16:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.26 on epoch=344
03/11/2022 02:17:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
03/11/2022 02:17:00 - INFO - __main__ - Global step 700 Train loss 0.24 ACC 0.5 on epoch=349
03/11/2022 02:17:02 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=354
03/11/2022 02:17:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
03/11/2022 02:17:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.26 on epoch=364
03/11/2022 02:17:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=369
03/11/2022 02:17:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 02:17:12 - INFO - __main__ - Global step 750 Train loss 0.25 ACC 0.65625 on epoch=374
03/11/2022 02:17:12 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=374, global_step=750
03/11/2022 02:17:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
03/11/2022 02:17:17 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
03/11/2022 02:17:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=389
03/11/2022 02:17:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=394
03/11/2022 02:17:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
03/11/2022 02:17:24 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.5 on epoch=399
03/11/2022 02:17:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
03/11/2022 02:17:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=409
03/11/2022 02:17:31 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=414
03/11/2022 02:17:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/11/2022 02:17:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=424
03/11/2022 02:17:36 - INFO - __main__ - Global step 850 Train loss 0.26 ACC 0.5 on epoch=424
03/11/2022 02:17:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
03/11/2022 02:17:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
03/11/2022 02:17:43 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=439
03/11/2022 02:17:45 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=444
03/11/2022 02:17:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.24 on epoch=449
03/11/2022 02:17:48 - INFO - __main__ - Global step 900 Train loss 0.24 ACC 0.5 on epoch=449
03/11/2022 02:17:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
03/11/2022 02:17:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=459
03/11/2022 02:17:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/11/2022 02:17:57 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=469
03/11/2022 02:17:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=474
03/11/2022 02:17:59 - INFO - __main__ - Global step 950 Train loss 0.24 ACC 0.5 on epoch=474
03/11/2022 02:18:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=479
03/11/2022 02:18:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=484
03/11/2022 02:18:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=489
03/11/2022 02:18:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=494
03/11/2022 02:18:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=499
03/11/2022 02:18:11 - INFO - __main__ - Global step 1000 Train loss 0.25 ACC 0.5 on epoch=499
03/11/2022 02:18:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=504
03/11/2022 02:18:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=509
03/11/2022 02:18:18 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=514
03/11/2022 02:18:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.25 on epoch=519
03/11/2022 02:18:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=524
03/11/2022 02:18:23 - INFO - __main__ - Global step 1050 Train loss 0.25 ACC 0.5 on epoch=524
03/11/2022 02:18:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=529
03/11/2022 02:18:28 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=534
03/11/2022 02:18:30 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=539
03/11/2022 02:18:32 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/11/2022 02:18:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=549
03/11/2022 02:18:35 - INFO - __main__ - Global step 1100 Train loss 0.24 ACC 0.5 on epoch=549
03/11/2022 02:18:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=554
03/11/2022 02:18:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.27 on epoch=559
03/11/2022 02:18:42 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.27 on epoch=564
03/11/2022 02:18:44 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=569
03/11/2022 02:18:46 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=574
03/11/2022 02:18:47 - INFO - __main__ - Global step 1150 Train loss 0.26 ACC 0.5 on epoch=574
03/11/2022 02:18:49 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.23 on epoch=579
03/11/2022 02:18:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=584
03/11/2022 02:18:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=589
03/11/2022 02:18:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=594
03/11/2022 02:18:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=599
03/11/2022 02:18:59 - INFO - __main__ - Global step 1200 Train loss 0.24 ACC 0.53125 on epoch=599
03/11/2022 02:19:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
03/11/2022 02:19:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.26 on epoch=609
03/11/2022 02:19:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=614
03/11/2022 02:19:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=619
03/11/2022 02:19:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/11/2022 02:19:11 - INFO - __main__ - Global step 1250 Train loss 0.24 ACC 0.5 on epoch=624
03/11/2022 02:19:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=629
03/11/2022 02:19:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.25 on epoch=634
03/11/2022 02:19:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.26 on epoch=639
03/11/2022 02:19:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=644
03/11/2022 02:19:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/11/2022 02:19:23 - INFO - __main__ - Global step 1300 Train loss 0.25 ACC 0.5 on epoch=649
03/11/2022 02:19:25 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=654
03/11/2022 02:19:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=659
03/11/2022 02:19:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=664
03/11/2022 02:19:32 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
03/11/2022 02:19:34 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
03/11/2022 02:19:35 - INFO - __main__ - Global step 1350 Train loss 0.25 ACC 0.5 on epoch=674
03/11/2022 02:19:37 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=679
03/11/2022 02:19:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=684
03/11/2022 02:19:42 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=689
03/11/2022 02:19:44 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=694
03/11/2022 02:19:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=699
03/11/2022 02:19:47 - INFO - __main__ - Global step 1400 Train loss 0.23 ACC 0.5 on epoch=699
03/11/2022 02:19:49 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=704
03/11/2022 02:19:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=709
03/11/2022 02:19:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=714
03/11/2022 02:19:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=719
03/11/2022 02:19:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.25 on epoch=724
03/11/2022 02:19:59 - INFO - __main__ - Global step 1450 Train loss 0.24 ACC 0.65625 on epoch=724
03/11/2022 02:20:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=729
03/11/2022 02:20:04 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=734
03/11/2022 02:20:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=739
03/11/2022 02:20:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.25 on epoch=744
03/11/2022 02:20:11 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=749
03/11/2022 02:20:11 - INFO - __main__ - Global step 1500 Train loss 0.25 ACC 0.5 on epoch=749
03/11/2022 02:20:14 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=754
03/11/2022 02:20:16 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.27 on epoch=759
03/11/2022 02:20:18 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.38 on epoch=764
03/11/2022 02:20:21 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=769
03/11/2022 02:20:23 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.33 on epoch=774
03/11/2022 02:20:23 - INFO - __main__ - Global step 1550 Train loss 0.30 ACC 0.5 on epoch=774
03/11/2022 02:20:26 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.33 on epoch=779
03/11/2022 02:20:28 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=784
03/11/2022 02:20:30 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=789
03/11/2022 02:20:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=794
03/11/2022 02:20:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.26 on epoch=799
03/11/2022 02:20:36 - INFO - __main__ - Global step 1600 Train loss 0.30 ACC 0.53125 on epoch=799
03/11/2022 02:20:38 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=804
03/11/2022 02:20:40 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.24 on epoch=809
03/11/2022 02:20:42 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.24 on epoch=814
03/11/2022 02:20:45 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.26 on epoch=819
03/11/2022 02:20:47 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=824
03/11/2022 02:20:48 - INFO - __main__ - Global step 1650 Train loss 0.24 ACC 0.5 on epoch=824
03/11/2022 02:20:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.25 on epoch=829
03/11/2022 02:20:52 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=834
03/11/2022 02:20:55 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.26 on epoch=839
03/11/2022 02:20:57 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=844
03/11/2022 02:20:59 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=849
03/11/2022 02:21:00 - INFO - __main__ - Global step 1700 Train loss 0.25 ACC 0.53125 on epoch=849
03/11/2022 02:21:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.26 on epoch=854
03/11/2022 02:21:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.27 on epoch=859
03/11/2022 02:21:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=864
03/11/2022 02:21:09 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.27 on epoch=869
03/11/2022 02:21:11 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=874
03/11/2022 02:21:12 - INFO - __main__ - Global step 1750 Train loss 0.26 ACC 0.46875 on epoch=874
03/11/2022 02:21:14 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=879
03/11/2022 02:21:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.25 on epoch=884
03/11/2022 02:21:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.26 on epoch=889
03/11/2022 02:21:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.26 on epoch=894
03/11/2022 02:21:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.28 on epoch=899
03/11/2022 02:21:24 - INFO - __main__ - Global step 1800 Train loss 0.26 ACC 0.5625 on epoch=899
03/11/2022 02:21:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=904
03/11/2022 02:21:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.26 on epoch=909
03/11/2022 02:21:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=914
03/11/2022 02:21:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=919
03/11/2022 02:21:36 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=924
03/11/2022 02:21:36 - INFO - __main__ - Global step 1850 Train loss 0.25 ACC 0.5 on epoch=924
03/11/2022 02:21:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.27 on epoch=929
03/11/2022 02:21:41 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=934
03/11/2022 02:21:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.25 on epoch=939
03/11/2022 02:21:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=944
03/11/2022 02:21:48 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=949
03/11/2022 02:21:48 - INFO - __main__ - Global step 1900 Train loss 0.25 ACC 0.46875 on epoch=949
03/11/2022 02:21:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=954
03/11/2022 02:21:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=959
03/11/2022 02:21:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.24 on epoch=964
03/11/2022 02:21:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=969
03/11/2022 02:22:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=974
03/11/2022 02:22:00 - INFO - __main__ - Global step 1950 Train loss 0.25 ACC 0.53125 on epoch=974
03/11/2022 02:22:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=979
03/11/2022 02:22:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.24 on epoch=984
03/11/2022 02:22:07 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.25 on epoch=989
03/11/2022 02:22:09 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=994
03/11/2022 02:22:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.26 on epoch=999
03/11/2022 02:22:12 - INFO - __main__ - Global step 2000 Train loss 0.25 ACC 0.53125 on epoch=999
03/11/2022 02:22:12 - INFO - __main__ - save last model!
03/11/2022 02:22:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 02:22:12 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 02:22:12 - INFO - __main__ - Printing 3 examples
03/11/2022 02:22:12 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 02:22:12 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:12 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 02:22:12 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:12 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 02:22:12 - INFO - __main__ - ['sentence 1']
03/11/2022 02:22:12 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:22:12 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:22:13 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 02:22:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:22:13 - INFO - __main__ - Printing 3 examples
03/11/2022 02:22:13 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/11/2022 02:22:13 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:13 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/11/2022 02:22:13 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:13 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/11/2022 02:22:13 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 02:22:13 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:22:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:22:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:22:13 - INFO - __main__ - Printing 3 examples
03/11/2022 02:22:13 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/11/2022 02:22:13 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:13 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/11/2022 02:22:13 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:13 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/11/2022 02:22:13 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:13 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:22:13 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:22:13 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:22:16 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_21_0.4_8_predictions.txt
03/11/2022 02:22:16 - INFO - __main__ - ACC on test data: 0.4400
03/11/2022 02:22:17 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.4, bsz=8, dev_performance=0.65625, test_performance=0.44
03/11/2022 02:22:17 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.3, bsz=8 ...
03/11/2022 02:22:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:22:18 - INFO - __main__ - Printing 3 examples
03/11/2022 02:22:18 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/11/2022 02:22:18 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:18 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/11/2022 02:22:18 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:18 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/11/2022 02:22:18 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 02:22:18 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:22:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:22:18 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:22:18 - INFO - __main__ - Printing 3 examples
03/11/2022 02:22:18 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/11/2022 02:22:18 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:18 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/11/2022 02:22:18 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:18 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/11/2022 02:22:18 - INFO - __main__ - ['sentence 2']
03/11/2022 02:22:18 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:22:18 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:22:18 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:22:26 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:22:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:22:26 - INFO - __main__ - Starting training!
03/11/2022 02:22:30 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:22:31 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:22:31 - INFO - __main__ - Starting training!
03/11/2022 02:22:34 - INFO - __main__ - Step 10 Global step 10 Train loss 1.35 on epoch=4
03/11/2022 02:22:36 - INFO - __main__ - Step 20 Global step 20 Train loss 0.60 on epoch=9
03/11/2022 02:22:38 - INFO - __main__ - Step 30 Global step 30 Train loss 0.43 on epoch=14
03/11/2022 02:22:40 - INFO - __main__ - Step 40 Global step 40 Train loss 0.35 on epoch=19
03/11/2022 02:22:43 - INFO - __main__ - Step 50 Global step 50 Train loss 0.36 on epoch=24
03/11/2022 02:22:43 - INFO - __main__ - Global step 50 Train loss 0.62 ACC 0.5 on epoch=24
03/11/2022 02:22:43 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 02:22:45 - INFO - __main__ - Step 60 Global step 60 Train loss 0.32 on epoch=29
03/11/2022 02:22:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
03/11/2022 02:22:50 - INFO - __main__ - Step 80 Global step 80 Train loss 0.33 on epoch=39
03/11/2022 02:22:52 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=44
03/11/2022 02:22:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=49
03/11/2022 02:22:54 - INFO - __main__ - Global step 100 Train loss 0.31 ACC 0.5 on epoch=49
03/11/2022 02:22:57 - INFO - __main__ - Step 110 Global step 110 Train loss 0.29 on epoch=54
03/11/2022 02:22:59 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
03/11/2022 02:23:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
03/11/2022 02:23:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
03/11/2022 02:23:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
03/11/2022 02:23:06 - INFO - __main__ - Global step 150 Train loss 0.27 ACC 0.5 on epoch=74
03/11/2022 02:23:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
03/11/2022 02:23:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=84
03/11/2022 02:23:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=89
03/11/2022 02:23:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=94
03/11/2022 02:23:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/11/2022 02:23:17 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.59375 on epoch=99
03/11/2022 02:23:17 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=99, global_step=200
03/11/2022 02:23:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
03/11/2022 02:23:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
03/11/2022 02:23:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=114
03/11/2022 02:23:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
03/11/2022 02:23:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=124
03/11/2022 02:23:28 - INFO - __main__ - Global step 250 Train loss 0.26 ACC 0.5 on epoch=124
03/11/2022 02:23:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=129
03/11/2022 02:23:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
03/11/2022 02:23:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=139
03/11/2022 02:23:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.31 on epoch=144
03/11/2022 02:23:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
03/11/2022 02:23:40 - INFO - __main__ - Global step 300 Train loss 0.30 ACC 0.53125 on epoch=149
03/11/2022 02:23:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
03/11/2022 02:23:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 02:23:46 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=164
03/11/2022 02:23:48 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
03/11/2022 02:23:50 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
03/11/2022 02:23:51 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.5625 on epoch=174
03/11/2022 02:23:53 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 02:23:55 - INFO - __main__ - Step 370 Global step 370 Train loss 0.28 on epoch=184
03/11/2022 02:23:57 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
03/11/2022 02:24:00 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 02:24:02 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 02:24:02 - INFO - __main__ - Global step 400 Train loss 0.26 ACC 0.59375 on epoch=199
03/11/2022 02:24:04 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=204
03/11/2022 02:24:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
03/11/2022 02:24:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
03/11/2022 02:24:11 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
03/11/2022 02:24:13 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
03/11/2022 02:24:14 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.65625 on epoch=224
03/11/2022 02:24:14 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=224, global_step=450
03/11/2022 02:24:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
03/11/2022 02:24:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
03/11/2022 02:24:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
03/11/2022 02:24:22 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
03/11/2022 02:24:24 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
03/11/2022 02:24:25 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5 on epoch=249
03/11/2022 02:24:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 02:24:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.27 on epoch=259
03/11/2022 02:24:31 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
03/11/2022 02:24:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.27 on epoch=269
03/11/2022 02:24:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=274
03/11/2022 02:24:36 - INFO - __main__ - Global step 550 Train loss 0.26 ACC 0.5 on epoch=274
03/11/2022 02:24:39 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
03/11/2022 02:24:41 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=284
03/11/2022 02:24:43 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
03/11/2022 02:24:45 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/11/2022 02:24:47 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=299
03/11/2022 02:24:48 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.5 on epoch=299
03/11/2022 02:24:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=304
03/11/2022 02:24:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
03/11/2022 02:24:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
03/11/2022 02:24:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
03/11/2022 02:24:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=324
03/11/2022 02:24:59 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.5 on epoch=324
03/11/2022 02:25:01 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=329
03/11/2022 02:25:03 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=334
03/11/2022 02:25:05 - INFO - __main__ - Step 680 Global step 680 Train loss 0.26 on epoch=339
03/11/2022 02:25:07 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=344
03/11/2022 02:25:10 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
03/11/2022 02:25:10 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.5 on epoch=349
03/11/2022 02:25:12 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=354
03/11/2022 02:25:14 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
03/11/2022 02:25:17 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
03/11/2022 02:25:19 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
03/11/2022 02:25:21 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 02:25:21 - INFO - __main__ - Global step 750 Train loss 0.25 ACC 0.65625 on epoch=374
03/11/2022 02:25:24 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=379
03/11/2022 02:25:26 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
03/11/2022 02:25:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
03/11/2022 02:25:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=394
03/11/2022 02:25:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.26 on epoch=399
03/11/2022 02:25:33 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.5 on epoch=399
03/11/2022 02:25:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
03/11/2022 02:25:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
03/11/2022 02:25:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=414
03/11/2022 02:25:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/11/2022 02:25:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.27 on epoch=424
03/11/2022 02:25:44 - INFO - __main__ - Global step 850 Train loss 0.26 ACC 0.5 on epoch=424
03/11/2022 02:25:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=429
03/11/2022 02:25:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.27 on epoch=434
03/11/2022 02:25:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=439
03/11/2022 02:25:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=444
03/11/2022 02:25:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
03/11/2022 02:25:55 - INFO - __main__ - Global step 900 Train loss 0.25 ACC 0.5625 on epoch=449
03/11/2022 02:25:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=454
03/11/2022 02:25:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=459
03/11/2022 02:26:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.23 on epoch=464
03/11/2022 02:26:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=469
03/11/2022 02:26:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=474
03/11/2022 02:26:06 - INFO - __main__ - Global step 950 Train loss 0.24 ACC 0.5 on epoch=474
03/11/2022 02:26:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.25 on epoch=479
03/11/2022 02:26:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=484
03/11/2022 02:26:13 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=489
03/11/2022 02:26:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.25 on epoch=494
03/11/2022 02:26:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=499
03/11/2022 02:26:17 - INFO - __main__ - Global step 1000 Train loss 0.24 ACC 0.5 on epoch=499
03/11/2022 02:26:20 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=504
03/11/2022 02:26:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=509
03/11/2022 02:26:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=514
03/11/2022 02:26:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=519
03/11/2022 02:26:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=524
03/11/2022 02:26:29 - INFO - __main__ - Global step 1050 Train loss 0.24 ACC 0.53125 on epoch=524
03/11/2022 02:26:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
03/11/2022 02:26:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=534
03/11/2022 02:26:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=539
03/11/2022 02:26:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
03/11/2022 02:26:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=549
03/11/2022 02:26:40 - INFO - __main__ - Global step 1100 Train loss 0.24 ACC 0.59375 on epoch=549
03/11/2022 02:26:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=554
03/11/2022 02:26:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=559
03/11/2022 02:26:46 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=564
03/11/2022 02:26:48 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.25 on epoch=569
03/11/2022 02:26:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.25 on epoch=574
03/11/2022 02:26:51 - INFO - __main__ - Global step 1150 Train loss 0.24 ACC 0.5 on epoch=574
03/11/2022 02:26:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=579
03/11/2022 02:26:55 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=584
03/11/2022 02:26:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=589
03/11/2022 02:27:00 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/11/2022 02:27:02 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.23 on epoch=599
03/11/2022 02:27:02 - INFO - __main__ - Global step 1200 Train loss 0.24 ACC 0.5 on epoch=599
03/11/2022 02:27:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
03/11/2022 02:27:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=609
03/11/2022 02:27:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=614
03/11/2022 02:27:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=619
03/11/2022 02:27:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/11/2022 02:27:14 - INFO - __main__ - Global step 1250 Train loss 0.24 ACC 0.53125 on epoch=624
03/11/2022 02:27:16 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.23 on epoch=629
03/11/2022 02:27:18 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=634
03/11/2022 02:27:20 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=639
03/11/2022 02:27:22 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.22 on epoch=644
03/11/2022 02:27:24 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=649
03/11/2022 02:27:25 - INFO - __main__ - Global step 1300 Train loss 0.23 ACC 0.46875 on epoch=649
03/11/2022 02:27:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=654
03/11/2022 02:27:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
03/11/2022 02:27:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=664
03/11/2022 02:27:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=669
03/11/2022 02:27:36 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
03/11/2022 02:27:36 - INFO - __main__ - Global step 1350 Train loss 0.24 ACC 0.5 on epoch=674
03/11/2022 02:27:38 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=679
03/11/2022 02:27:40 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=684
03/11/2022 02:27:43 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=689
03/11/2022 02:27:45 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.24 on epoch=694
03/11/2022 02:27:47 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=699
03/11/2022 02:27:48 - INFO - __main__ - Global step 1400 Train loss 0.24 ACC 0.5625 on epoch=699
03/11/2022 02:27:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.24 on epoch=704
03/11/2022 02:27:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=709
03/11/2022 02:27:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=714
03/11/2022 02:27:56 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.23 on epoch=719
03/11/2022 02:27:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=724
03/11/2022 02:27:59 - INFO - __main__ - Global step 1450 Train loss 0.23 ACC 0.65625 on epoch=724
03/11/2022 02:28:01 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=729
03/11/2022 02:28:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=734
03/11/2022 02:28:06 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=739
03/11/2022 02:28:08 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=744
03/11/2022 02:28:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=749
03/11/2022 02:28:10 - INFO - __main__ - Global step 1500 Train loss 0.23 ACC 0.46875 on epoch=749
03/11/2022 02:28:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=754
03/11/2022 02:28:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=759
03/11/2022 02:28:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=764
03/11/2022 02:28:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=769
03/11/2022 02:28:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
03/11/2022 02:28:22 - INFO - __main__ - Global step 1550 Train loss 0.24 ACC 0.53125 on epoch=774
03/11/2022 02:28:24 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=779
03/11/2022 02:28:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=784
03/11/2022 02:28:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=789
03/11/2022 02:28:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.25 on epoch=794
03/11/2022 02:28:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=799
03/11/2022 02:28:33 - INFO - __main__ - Global step 1600 Train loss 0.24 ACC 0.5625 on epoch=799
03/11/2022 02:28:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.26 on epoch=804
03/11/2022 02:28:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=809
03/11/2022 02:28:40 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.25 on epoch=814
03/11/2022 02:28:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=819
03/11/2022 02:28:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=824
03/11/2022 02:28:45 - INFO - __main__ - Global step 1650 Train loss 0.25 ACC 0.53125 on epoch=824
03/11/2022 02:28:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
03/11/2022 02:28:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=834
03/11/2022 02:28:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=839
03/11/2022 02:28:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=844
03/11/2022 02:28:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=849
03/11/2022 02:28:56 - INFO - __main__ - Global step 1700 Train loss 0.24 ACC 0.6875 on epoch=849
03/11/2022 02:28:56 - INFO - __main__ - Saving model with best ACC: 0.65625 -> 0.6875 on epoch=849, global_step=1700
03/11/2022 02:28:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=854
03/11/2022 02:29:00 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
03/11/2022 02:29:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.22 on epoch=864
03/11/2022 02:29:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.23 on epoch=869
03/11/2022 02:29:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.23 on epoch=874
03/11/2022 02:29:07 - INFO - __main__ - Global step 1750 Train loss 0.23 ACC 0.625 on epoch=874
03/11/2022 02:29:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=879
03/11/2022 02:29:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=884
03/11/2022 02:29:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.24 on epoch=889
03/11/2022 02:29:16 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.24 on epoch=894
03/11/2022 02:29:18 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=899
03/11/2022 02:29:19 - INFO - __main__ - Global step 1800 Train loss 0.23 ACC 0.625 on epoch=899
03/11/2022 02:29:21 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=904
03/11/2022 02:29:23 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=909
03/11/2022 02:29:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.22 on epoch=914
03/11/2022 02:29:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=919
03/11/2022 02:29:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=924
03/11/2022 02:29:30 - INFO - __main__ - Global step 1850 Train loss 0.22 ACC 0.5 on epoch=924
03/11/2022 02:29:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=929
03/11/2022 02:29:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=934
03/11/2022 02:29:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.26 on epoch=939
03/11/2022 02:29:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.23 on epoch=944
03/11/2022 02:29:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.22 on epoch=949
03/11/2022 02:29:41 - INFO - __main__ - Global step 1900 Train loss 0.23 ACC 0.46875 on epoch=949
03/11/2022 02:29:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=954
03/11/2022 02:29:46 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.21 on epoch=959
03/11/2022 02:29:48 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=964
03/11/2022 02:29:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=969
03/11/2022 02:29:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=974
03/11/2022 02:29:53 - INFO - __main__ - Global step 1950 Train loss 0.19 ACC 0.5 on epoch=974
03/11/2022 02:29:55 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=979
03/11/2022 02:29:57 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=984
03/11/2022 02:29:59 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.14 on epoch=989
03/11/2022 02:30:01 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.15 on epoch=994
03/11/2022 02:30:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=999
03/11/2022 02:30:04 - INFO - __main__ - Global step 2000 Train loss 0.16 ACC 0.71875 on epoch=999
03/11/2022 02:30:04 - INFO - __main__ - Saving model with best ACC: 0.6875 -> 0.71875 on epoch=999, global_step=2000
03/11/2022 02:30:04 - INFO - __main__ - save last model!
03/11/2022 02:30:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 02:30:04 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 02:30:04 - INFO - __main__ - Printing 3 examples
03/11/2022 02:30:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 02:30:04 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 02:30:04 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:04 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 02:30:04 - INFO - __main__ - ['sentence 1']
03/11/2022 02:30:04 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:30:04 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:30:04 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 02:30:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:30:05 - INFO - __main__ - Printing 3 examples
03/11/2022 02:30:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/11/2022 02:30:05 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/11/2022 02:30:05 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/11/2022 02:30:05 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 02:30:05 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:30:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:30:05 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:30:05 - INFO - __main__ - Printing 3 examples
03/11/2022 02:30:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/11/2022 02:30:05 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/11/2022 02:30:05 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:05 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/11/2022 02:30:05 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:05 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:30:05 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:30:05 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:30:08 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_21_0.3_8_predictions.txt
03/11/2022 02:30:08 - INFO - __main__ - ACC on test data: 0.7450
03/11/2022 02:30:08 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.3, bsz=8, dev_performance=0.71875, test_performance=0.745
03/11/2022 02:30:08 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.2, bsz=8 ...
03/11/2022 02:30:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:30:09 - INFO - __main__ - Printing 3 examples
03/11/2022 02:30:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Elizabeth had fortunately ever asked Alexander's ex-girlfriend to practice. [SEP] sentence 2: Elizabeth had not ever asked Alexander's ex-girlfriend to practice.
03/11/2022 02:30:09 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every convertible might really ever steer. [SEP] sentence 2: Every convertible might not ever steer.
03/11/2022 02:30:09 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Rachelle has fortunately ever needed to donate. [SEP] sentence 2: Rachelle has not ever needed to donate.
03/11/2022 02:30:09 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 02:30:09 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:30:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:30:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:30:09 - INFO - __main__ - Printing 3 examples
03/11/2022 02:30:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Every river has probably ever vaporized. [SEP] sentence 2: Every river has not ever vaporized.
03/11/2022 02:30:09 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Carla had probably ever attacked Phillip. [SEP] sentence 2: Carla had not ever attacked Phillip.
03/11/2022 02:30:09 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:09 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: A bike would probably ever tip over. [SEP] sentence 2: A bike would not ever tip over.
03/11/2022 02:30:09 - INFO - __main__ - ['sentence 2']
03/11/2022 02:30:09 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:30:09 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:30:09 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:30:17 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:30:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:30:18 - INFO - __main__ - Starting training!
03/11/2022 02:30:21 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:30:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:30:22 - INFO - __main__ - Starting training!
03/11/2022 02:30:25 - INFO - __main__ - Step 10 Global step 10 Train loss 1.33 on epoch=4
03/11/2022 02:30:27 - INFO - __main__ - Step 20 Global step 20 Train loss 0.69 on epoch=9
03/11/2022 02:30:29 - INFO - __main__ - Step 30 Global step 30 Train loss 0.48 on epoch=14
03/11/2022 02:30:31 - INFO - __main__ - Step 40 Global step 40 Train loss 0.40 on epoch=19
03/11/2022 02:30:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.39 on epoch=24
03/11/2022 02:30:34 - INFO - __main__ - Global step 50 Train loss 0.66 ACC 0.5 on epoch=24
03/11/2022 02:30:34 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 02:30:36 - INFO - __main__ - Step 60 Global step 60 Train loss 0.36 on epoch=29
03/11/2022 02:30:39 - INFO - __main__ - Step 70 Global step 70 Train loss 0.33 on epoch=34
03/11/2022 02:30:41 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=39
03/11/2022 02:30:43 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=44
03/11/2022 02:30:45 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=49
03/11/2022 02:30:46 - INFO - __main__ - Global step 100 Train loss 0.32 ACC 0.5 on epoch=49
03/11/2022 02:30:48 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=54
03/11/2022 02:30:50 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=59
03/11/2022 02:30:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
03/11/2022 02:30:55 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
03/11/2022 02:30:57 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
03/11/2022 02:30:58 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.40625 on epoch=74
03/11/2022 02:31:00 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
03/11/2022 02:31:02 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=84
03/11/2022 02:31:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/11/2022 02:31:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.24 on epoch=94
03/11/2022 02:31:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/11/2022 02:31:10 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.5625 on epoch=99
03/11/2022 02:31:10 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=99, global_step=200
03/11/2022 02:31:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 02:31:14 - INFO - __main__ - Step 220 Global step 220 Train loss 0.33 on epoch=109
03/11/2022 02:31:16 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
03/11/2022 02:31:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=119
03/11/2022 02:31:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
03/11/2022 02:31:21 - INFO - __main__ - Global step 250 Train loss 0.29 ACC 0.53125 on epoch=124
03/11/2022 02:31:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
03/11/2022 02:31:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 02:31:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
03/11/2022 02:31:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=144
03/11/2022 02:31:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=149
03/11/2022 02:31:33 - INFO - __main__ - Global step 300 Train loss 0.27 ACC 0.5 on epoch=149
03/11/2022 02:31:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 02:31:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 02:31:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
03/11/2022 02:31:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
03/11/2022 02:31:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
03/11/2022 02:31:45 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.46875 on epoch=174
03/11/2022 02:31:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 02:31:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 02:31:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
03/11/2022 02:31:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=194
03/11/2022 02:31:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 02:31:57 - INFO - __main__ - Global step 400 Train loss 0.26 ACC 0.5625 on epoch=199
03/11/2022 02:31:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
03/11/2022 02:32:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
03/11/2022 02:32:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=214
03/11/2022 02:32:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
03/11/2022 02:32:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
03/11/2022 02:32:09 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.5 on epoch=224
03/11/2022 02:32:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
03/11/2022 02:32:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=234
03/11/2022 02:32:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 02:32:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
03/11/2022 02:32:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=249
03/11/2022 02:32:21 - INFO - __main__ - Global step 500 Train loss 0.26 ACC 0.5 on epoch=249
03/11/2022 02:32:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
03/11/2022 02:32:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
03/11/2022 02:32:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.27 on epoch=264
03/11/2022 02:32:30 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
03/11/2022 02:32:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=274
03/11/2022 02:32:32 - INFO - __main__ - Global step 550 Train loss 0.26 ACC 0.5 on epoch=274
03/11/2022 02:32:35 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
03/11/2022 02:32:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
03/11/2022 02:32:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
03/11/2022 02:32:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/11/2022 02:32:44 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=299
03/11/2022 02:32:44 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.6875 on epoch=299
03/11/2022 02:32:44 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.6875 on epoch=299, global_step=600
03/11/2022 02:32:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
03/11/2022 02:32:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.25 on epoch=309
03/11/2022 02:32:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.27 on epoch=314
03/11/2022 02:32:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=319
03/11/2022 02:32:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
03/11/2022 02:32:56 - INFO - __main__ - Global step 650 Train loss 0.26 ACC 0.46875 on epoch=324
03/11/2022 02:32:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=329
03/11/2022 02:33:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=334
03/11/2022 02:33:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
03/11/2022 02:33:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
03/11/2022 02:33:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=349
03/11/2022 02:33:08 - INFO - __main__ - Global step 700 Train loss 0.24 ACC 0.5 on epoch=349
03/11/2022 02:33:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=354
03/11/2022 02:33:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
03/11/2022 02:33:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=364
03/11/2022 02:33:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
03/11/2022 02:33:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=374
03/11/2022 02:33:19 - INFO - __main__ - Global step 750 Train loss 0.24 ACC 0.46875 on epoch=374
03/11/2022 02:33:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.27 on epoch=379
03/11/2022 02:33:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
03/11/2022 02:33:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.25 on epoch=389
03/11/2022 02:33:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=394
03/11/2022 02:33:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
03/11/2022 02:33:31 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.53125 on epoch=399
03/11/2022 02:33:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=404
03/11/2022 02:33:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=409
03/11/2022 02:33:38 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=414
03/11/2022 02:33:40 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/11/2022 02:33:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
03/11/2022 02:33:43 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.5 on epoch=424
03/11/2022 02:33:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.27 on epoch=429
03/11/2022 02:33:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=434
03/11/2022 02:33:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=439
03/11/2022 02:33:52 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=444
03/11/2022 02:33:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=449
03/11/2022 02:33:55 - INFO - __main__ - Global step 900 Train loss 0.25 ACC 0.5 on epoch=449
03/11/2022 02:33:57 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
03/11/2022 02:34:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=459
03/11/2022 02:34:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=464
03/11/2022 02:34:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=469
03/11/2022 02:34:06 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=474
03/11/2022 02:34:07 - INFO - __main__ - Global step 950 Train loss 0.25 ACC 0.53125 on epoch=474
03/11/2022 02:34:09 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=479
03/11/2022 02:34:11 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=484
03/11/2022 02:34:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=489
03/11/2022 02:34:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.21 on epoch=494
03/11/2022 02:34:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=499
03/11/2022 02:34:19 - INFO - __main__ - Global step 1000 Train loss 0.23 ACC 0.5 on epoch=499
03/11/2022 02:34:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=504
03/11/2022 02:34:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.24 on epoch=509
03/11/2022 02:34:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=514
03/11/2022 02:34:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=519
03/11/2022 02:34:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.25 on epoch=524
03/11/2022 02:34:31 - INFO - __main__ - Global step 1050 Train loss 0.25 ACC 0.53125 on epoch=524
03/11/2022 02:34:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=529
03/11/2022 02:34:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=534
03/11/2022 02:34:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=539
03/11/2022 02:34:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.25 on epoch=544
03/11/2022 02:34:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.25 on epoch=549
03/11/2022 02:34:43 - INFO - __main__ - Global step 1100 Train loss 0.24 ACC 0.5 on epoch=549
03/11/2022 02:34:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=554
03/11/2022 02:34:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=559
03/11/2022 02:34:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.25 on epoch=564
03/11/2022 02:34:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=569
03/11/2022 02:34:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=574
03/11/2022 02:34:54 - INFO - __main__ - Global step 1150 Train loss 0.24 ACC 0.5 on epoch=574
03/11/2022 02:34:57 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=579
03/11/2022 02:34:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=584
03/11/2022 02:35:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.25 on epoch=589
03/11/2022 02:35:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=594
03/11/2022 02:35:06 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.24 on epoch=599
03/11/2022 02:35:06 - INFO - __main__ - Global step 1200 Train loss 0.24 ACC 0.5 on epoch=599
03/11/2022 02:35:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
03/11/2022 02:35:11 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=609
03/11/2022 02:35:13 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=614
03/11/2022 02:35:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.24 on epoch=619
03/11/2022 02:35:18 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=624
03/11/2022 02:35:18 - INFO - __main__ - Global step 1250 Train loss 0.23 ACC 0.5 on epoch=624
03/11/2022 02:35:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=629
03/11/2022 02:35:23 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=634
03/11/2022 02:35:25 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=639
03/11/2022 02:35:27 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=644
03/11/2022 02:35:29 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=649
03/11/2022 02:35:30 - INFO - __main__ - Global step 1300 Train loss 0.24 ACC 0.59375 on epoch=649
03/11/2022 02:35:32 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=654
03/11/2022 02:35:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
03/11/2022 02:35:37 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=664
03/11/2022 02:35:39 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=669
03/11/2022 02:35:41 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
03/11/2022 02:35:42 - INFO - __main__ - Global step 1350 Train loss 0.24 ACC 0.5 on epoch=674
03/11/2022 02:35:44 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.24 on epoch=679
03/11/2022 02:35:46 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=684
03/11/2022 02:35:49 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=689
03/11/2022 02:35:51 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.26 on epoch=694
03/11/2022 02:35:53 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=699
03/11/2022 02:35:54 - INFO - __main__ - Global step 1400 Train loss 0.25 ACC 0.53125 on epoch=699
03/11/2022 02:35:56 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.23 on epoch=704
03/11/2022 02:35:58 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.22 on epoch=709
03/11/2022 02:36:01 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=714
03/11/2022 02:36:03 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=719
03/11/2022 02:36:05 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=724
03/11/2022 02:36:06 - INFO - __main__ - Global step 1450 Train loss 0.23 ACC 0.5 on epoch=724
03/11/2022 02:36:08 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=729
03/11/2022 02:36:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=734
03/11/2022 02:36:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.24 on epoch=739
03/11/2022 02:36:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=744
03/11/2022 02:36:17 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=749
03/11/2022 02:36:17 - INFO - __main__ - Global step 1500 Train loss 0.24 ACC 0.5625 on epoch=749
03/11/2022 02:36:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.26 on epoch=754
03/11/2022 02:36:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=759
03/11/2022 02:36:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=764
03/11/2022 02:36:26 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=769
03/11/2022 02:36:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.25 on epoch=774
03/11/2022 02:36:29 - INFO - __main__ - Global step 1550 Train loss 0.25 ACC 0.53125 on epoch=774
03/11/2022 02:36:31 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=779
03/11/2022 02:36:33 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.23 on epoch=784
03/11/2022 02:36:36 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=789
03/11/2022 02:36:38 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.24 on epoch=794
03/11/2022 02:36:40 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
03/11/2022 02:36:41 - INFO - __main__ - Global step 1600 Train loss 0.23 ACC 0.5625 on epoch=799
03/11/2022 02:36:43 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
03/11/2022 02:36:45 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=809
03/11/2022 02:36:47 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.23 on epoch=814
03/11/2022 02:36:50 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=819
03/11/2022 02:36:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=824
03/11/2022 02:36:52 - INFO - __main__ - Global step 1650 Train loss 0.23 ACC 0.5625 on epoch=824
03/11/2022 02:36:55 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.28 on epoch=829
03/11/2022 02:36:57 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=834
03/11/2022 02:36:59 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=839
03/11/2022 02:37:01 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=844
03/11/2022 02:37:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=849
03/11/2022 02:37:04 - INFO - __main__ - Global step 1700 Train loss 0.25 ACC 0.53125 on epoch=849
03/11/2022 02:37:06 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=854
03/11/2022 02:37:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=859
03/11/2022 02:37:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=864
03/11/2022 02:37:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=869
03/11/2022 02:37:15 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=874
03/11/2022 02:37:16 - INFO - __main__ - Global step 1750 Train loss 0.24 ACC 0.5625 on epoch=874
03/11/2022 02:37:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.25 on epoch=879
03/11/2022 02:37:20 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.26 on epoch=884
03/11/2022 02:37:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.23 on epoch=889
03/11/2022 02:37:25 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.25 on epoch=894
03/11/2022 02:37:27 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.24 on epoch=899
03/11/2022 02:37:27 - INFO - __main__ - Global step 1800 Train loss 0.25 ACC 0.5625 on epoch=899
03/11/2022 02:37:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.25 on epoch=904
03/11/2022 02:37:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.25 on epoch=909
03/11/2022 02:37:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.24 on epoch=914
03/11/2022 02:37:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.24 on epoch=919
03/11/2022 02:37:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.24 on epoch=924
03/11/2022 02:37:39 - INFO - __main__ - Global step 1850 Train loss 0.24 ACC 0.53125 on epoch=924
03/11/2022 02:37:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=929
03/11/2022 02:37:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=934
03/11/2022 02:37:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.29 on epoch=939
03/11/2022 02:37:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.24 on epoch=944
03/11/2022 02:37:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.24 on epoch=949
03/11/2022 02:37:51 - INFO - __main__ - Global step 1900 Train loss 0.25 ACC 0.5 on epoch=949
03/11/2022 02:37:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.23 on epoch=954
03/11/2022 02:37:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.25 on epoch=959
03/11/2022 02:37:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.25 on epoch=964
03/11/2022 02:38:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=969
03/11/2022 02:38:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.24 on epoch=974
03/11/2022 02:38:03 - INFO - __main__ - Global step 1950 Train loss 0.24 ACC 0.5625 on epoch=974
03/11/2022 02:38:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=979
03/11/2022 02:38:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=984
03/11/2022 02:38:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.23 on epoch=989
03/11/2022 02:38:11 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.25 on epoch=994
03/11/2022 02:38:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=999
03/11/2022 02:38:14 - INFO - __main__ - Global step 2000 Train loss 0.24 ACC 0.5625 on epoch=999
03/11/2022 02:38:14 - INFO - __main__ - save last model!
03/11/2022 02:38:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 02:38:14 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 02:38:14 - INFO - __main__ - Printing 3 examples
03/11/2022 02:38:14 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 02:38:14 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:14 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 02:38:14 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:14 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 02:38:14 - INFO - __main__ - ['sentence 1']
03/11/2022 02:38:14 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:38:14 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:38:14 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 02:38:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:38:15 - INFO - __main__ - Printing 3 examples
03/11/2022 02:38:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/11/2022 02:38:15 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/11/2022 02:38:15 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/11/2022 02:38:15 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 02:38:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:38:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:38:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:38:15 - INFO - __main__ - Printing 3 examples
03/11/2022 02:38:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/11/2022 02:38:15 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/11/2022 02:38:15 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/11/2022 02:38:15 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:15 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:38:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:38:15 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:38:18 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_21_0.2_8_predictions.txt
03/11/2022 02:38:18 - INFO - __main__ - ACC on test data: 0.4950
03/11/2022 02:38:18 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_21, lr=0.2, bsz=8, dev_performance=0.6875, test_performance=0.495
03/11/2022 02:38:18 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.5, bsz=8 ...
03/11/2022 02:38:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:38:19 - INFO - __main__ - Printing 3 examples
03/11/2022 02:38:19 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/11/2022 02:38:19 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:19 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/11/2022 02:38:19 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:19 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/11/2022 02:38:19 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 02:38:19 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:38:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:38:19 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:38:19 - INFO - __main__ - Printing 3 examples
03/11/2022 02:38:19 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/11/2022 02:38:19 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:19 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/11/2022 02:38:19 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:19 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/11/2022 02:38:19 - INFO - __main__ - ['sentence 2']
03/11/2022 02:38:19 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:38:19 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:38:19 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:38:29 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:38:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:38:30 - INFO - __main__ - Starting training!
03/11/2022 02:38:34 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:38:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:38:34 - INFO - __main__ - Starting training!
03/11/2022 02:38:37 - INFO - __main__ - Step 10 Global step 10 Train loss 1.00 on epoch=4
03/11/2022 02:38:39 - INFO - __main__ - Step 20 Global step 20 Train loss 0.42 on epoch=9
03/11/2022 02:38:41 - INFO - __main__ - Step 30 Global step 30 Train loss 0.34 on epoch=14
03/11/2022 02:38:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
03/11/2022 02:38:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=24
03/11/2022 02:38:47 - INFO - __main__ - Global step 50 Train loss 0.49 ACC 0.5 on epoch=24
03/11/2022 02:38:47 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 02:38:49 - INFO - __main__ - Step 60 Global step 60 Train loss 0.29 on epoch=29
03/11/2022 02:38:52 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
03/11/2022 02:38:54 - INFO - __main__ - Step 80 Global step 80 Train loss 0.28 on epoch=39
03/11/2022 02:38:56 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=44
03/11/2022 02:38:58 - INFO - __main__ - Step 100 Global step 100 Train loss 0.27 on epoch=49
03/11/2022 02:38:59 - INFO - __main__ - Global step 100 Train loss 0.28 ACC 0.5 on epoch=49
03/11/2022 02:39:01 - INFO - __main__ - Step 110 Global step 110 Train loss 0.26 on epoch=54
03/11/2022 02:39:03 - INFO - __main__ - Step 120 Global step 120 Train loss 0.24 on epoch=59
03/11/2022 02:39:06 - INFO - __main__ - Step 130 Global step 130 Train loss 0.23 on epoch=64
03/11/2022 02:39:08 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
03/11/2022 02:39:10 - INFO - __main__ - Step 150 Global step 150 Train loss 0.25 on epoch=74
03/11/2022 02:39:11 - INFO - __main__ - Global step 150 Train loss 0.25 ACC 0.40625 on epoch=74
03/11/2022 02:39:13 - INFO - __main__ - Step 160 Global step 160 Train loss 0.25 on epoch=79
03/11/2022 02:39:15 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
03/11/2022 02:39:17 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/11/2022 02:39:19 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
03/11/2022 02:39:22 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
03/11/2022 02:39:22 - INFO - __main__ - Global step 200 Train loss 0.24 ACC 0.5 on epoch=99
03/11/2022 02:39:24 - INFO - __main__ - Step 210 Global step 210 Train loss 0.23 on epoch=104
03/11/2022 02:39:27 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
03/11/2022 02:39:29 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
03/11/2022 02:39:31 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
03/11/2022 02:39:33 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
03/11/2022 02:39:34 - INFO - __main__ - Global step 250 Train loss 0.25 ACC 0.5 on epoch=124
03/11/2022 02:39:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
03/11/2022 02:39:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
03/11/2022 02:39:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 02:39:43 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=144
03/11/2022 02:39:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
03/11/2022 02:39:46 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.5 on epoch=149
03/11/2022 02:39:48 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 02:39:50 - INFO - __main__ - Step 320 Global step 320 Train loss 0.24 on epoch=159
03/11/2022 02:39:52 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
03/11/2022 02:39:55 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 02:39:57 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
03/11/2022 02:39:57 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.5 on epoch=174
03/11/2022 02:40:00 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 02:40:02 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 02:40:04 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
03/11/2022 02:40:06 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 02:40:08 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
03/11/2022 02:40:09 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.5 on epoch=199
03/11/2022 02:40:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
03/11/2022 02:40:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
03/11/2022 02:40:16 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
03/11/2022 02:40:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
03/11/2022 02:40:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=224
03/11/2022 02:40:21 - INFO - __main__ - Global step 450 Train loss 0.24 ACC 0.4375 on epoch=224
03/11/2022 02:40:23 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
03/11/2022 02:40:25 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 02:40:27 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
03/11/2022 02:40:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=244
03/11/2022 02:40:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
03/11/2022 02:40:32 - INFO - __main__ - Global step 500 Train loss 0.24 ACC 0.4375 on epoch=249
03/11/2022 02:40:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 02:40:37 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
03/11/2022 02:40:39 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
03/11/2022 02:40:41 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=269
03/11/2022 02:40:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=274
03/11/2022 02:40:44 - INFO - __main__ - Global step 550 Train loss 0.22 ACC 0.4375 on epoch=274
03/11/2022 02:40:46 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=279
03/11/2022 02:40:49 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=284
03/11/2022 02:40:51 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=289
03/11/2022 02:40:53 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
03/11/2022 02:40:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.21 on epoch=299
03/11/2022 02:40:56 - INFO - __main__ - Global step 600 Train loss 0.20 ACC 0.4375 on epoch=299
03/11/2022 02:40:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
03/11/2022 02:41:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.17 on epoch=309
03/11/2022 02:41:03 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=314
03/11/2022 02:41:05 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=319
03/11/2022 02:41:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.16 on epoch=324
03/11/2022 02:41:08 - INFO - __main__ - Global step 650 Train loss 0.19 ACC 0.40625 on epoch=324
03/11/2022 02:41:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=329
03/11/2022 02:41:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=334
03/11/2022 02:41:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=339
03/11/2022 02:41:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=344
03/11/2022 02:41:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
03/11/2022 02:41:20 - INFO - __main__ - Global step 700 Train loss 0.15 ACC 0.4375 on epoch=349
03/11/2022 02:41:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=354
03/11/2022 02:41:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=359
03/11/2022 02:41:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=364
03/11/2022 02:41:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.19 on epoch=369
03/11/2022 02:41:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.16 on epoch=374
03/11/2022 02:41:31 - INFO - __main__ - Global step 750 Train loss 0.16 ACC 0.5 on epoch=374
03/11/2022 02:41:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=379
03/11/2022 02:41:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.12 on epoch=384
03/11/2022 02:41:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.13 on epoch=389
03/11/2022 02:41:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=394
03/11/2022 02:41:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=399
03/11/2022 02:41:43 - INFO - __main__ - Global step 800 Train loss 0.12 ACC 0.4375 on epoch=399
03/11/2022 02:41:45 - INFO - __main__ - Step 810 Global step 810 Train loss 0.09 on epoch=404
03/11/2022 02:41:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=409
03/11/2022 02:41:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=414
03/11/2022 02:41:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=419
03/11/2022 02:41:54 - INFO - __main__ - Step 850 Global step 850 Train loss 0.12 on epoch=424
03/11/2022 02:41:55 - INFO - __main__ - Global step 850 Train loss 0.11 ACC 0.5 on epoch=424
03/11/2022 02:41:57 - INFO - __main__ - Step 860 Global step 860 Train loss 0.10 on epoch=429
03/11/2022 02:41:59 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=434
03/11/2022 02:42:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=439
03/11/2022 02:42:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=444
03/11/2022 02:42:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.12 on epoch=449
03/11/2022 02:42:07 - INFO - __main__ - Global step 900 Train loss 0.11 ACC 0.4375 on epoch=449
03/11/2022 02:42:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=454
03/11/2022 02:42:11 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=459
03/11/2022 02:42:13 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=464
03/11/2022 02:42:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.08 on epoch=469
03/11/2022 02:42:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.08 on epoch=474
03/11/2022 02:42:18 - INFO - __main__ - Global step 950 Train loss 0.09 ACC 0.46875 on epoch=474
03/11/2022 02:42:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.10 on epoch=479
03/11/2022 02:42:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.08 on epoch=484
03/11/2022 02:42:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.06 on epoch=489
03/11/2022 02:42:27 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=494
03/11/2022 02:42:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.10 on epoch=499
03/11/2022 02:42:30 - INFO - __main__ - Global step 1000 Train loss 0.09 ACC 0.4375 on epoch=499
03/11/2022 02:42:32 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.06 on epoch=504
03/11/2022 02:42:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.09 on epoch=509
03/11/2022 02:42:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=514
03/11/2022 02:42:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=519
03/11/2022 02:42:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
03/11/2022 02:42:42 - INFO - __main__ - Global step 1050 Train loss 0.07 ACC 0.4375 on epoch=524
03/11/2022 02:42:44 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
03/11/2022 02:42:46 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
03/11/2022 02:42:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
03/11/2022 02:42:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.04 on epoch=544
03/11/2022 02:42:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.06 on epoch=549
03/11/2022 02:42:54 - INFO - __main__ - Global step 1100 Train loss 0.05 ACC 0.40625 on epoch=549
03/11/2022 02:42:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
03/11/2022 02:42:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=559
03/11/2022 02:43:00 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.05 on epoch=564
03/11/2022 02:43:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=569
03/11/2022 02:43:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
03/11/2022 02:43:05 - INFO - __main__ - Global step 1150 Train loss 0.07 ACC 0.40625 on epoch=574
03/11/2022 02:43:07 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.03 on epoch=579
03/11/2022 02:43:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.05 on epoch=584
03/11/2022 02:43:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
03/11/2022 02:43:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.06 on epoch=594
03/11/2022 02:43:16 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=599
03/11/2022 02:43:17 - INFO - __main__ - Global step 1200 Train loss 0.05 ACC 0.40625 on epoch=599
03/11/2022 02:43:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
03/11/2022 02:43:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.03 on epoch=609
03/11/2022 02:43:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
03/11/2022 02:43:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
03/11/2022 02:43:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
03/11/2022 02:43:29 - INFO - __main__ - Global step 1250 Train loss 0.03 ACC 0.5625 on epoch=624
03/11/2022 02:43:29 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=624, global_step=1250
03/11/2022 02:43:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
03/11/2022 02:43:33 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/11/2022 02:43:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/11/2022 02:43:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
03/11/2022 02:43:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
03/11/2022 02:43:41 - INFO - __main__ - Global step 1300 Train loss 0.02 ACC 0.34375 on epoch=649
03/11/2022 02:43:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
03/11/2022 02:43:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.05 on epoch=659
03/11/2022 02:43:47 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
03/11/2022 02:43:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
03/11/2022 02:43:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
03/11/2022 02:43:53 - INFO - __main__ - Global step 1350 Train loss 0.03 ACC 0.375 on epoch=674
03/11/2022 02:43:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
03/11/2022 02:43:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/11/2022 02:44:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
03/11/2022 02:44:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.02 on epoch=694
03/11/2022 02:44:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/11/2022 02:44:05 - INFO - __main__ - Global step 1400 Train loss 0.03 ACC 0.40625 on epoch=699
03/11/2022 02:44:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
03/11/2022 02:44:10 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/11/2022 02:44:12 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/11/2022 02:44:14 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/11/2022 02:44:16 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
03/11/2022 02:44:17 - INFO - __main__ - Global step 1450 Train loss 0.02 ACC 0.375 on epoch=724
03/11/2022 02:44:19 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.03 on epoch=729
03/11/2022 02:44:22 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/11/2022 02:44:24 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/11/2022 02:44:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
03/11/2022 02:44:28 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
03/11/2022 02:44:29 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 0.40625 on epoch=749
03/11/2022 02:44:31 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/11/2022 02:44:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/11/2022 02:44:36 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=764
03/11/2022 02:44:38 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/11/2022 02:44:40 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/11/2022 02:44:41 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.375 on epoch=774
03/11/2022 02:44:43 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 02:44:45 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
03/11/2022 02:44:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/11/2022 02:44:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/11/2022 02:44:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 02:44:53 - INFO - __main__ - Global step 1600 Train loss 0.01 ACC 0.40625 on epoch=799
03/11/2022 02:44:55 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 02:44:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 02:44:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 02:45:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 02:45:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 02:45:05 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 0.40625 on epoch=824
03/11/2022 02:45:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
03/11/2022 02:45:09 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
03/11/2022 02:45:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/11/2022 02:45:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
03/11/2022 02:45:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 02:45:17 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.375 on epoch=849
03/11/2022 02:45:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/11/2022 02:45:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/11/2022 02:45:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 02:45:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
03/11/2022 02:45:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=874
03/11/2022 02:45:29 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.4375 on epoch=874
03/11/2022 02:45:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/11/2022 02:45:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/11/2022 02:45:36 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 02:45:38 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
03/11/2022 02:45:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 02:45:41 - INFO - __main__ - Global step 1800 Train loss 0.01 ACC 0.40625 on epoch=899
03/11/2022 02:45:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 02:45:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/11/2022 02:45:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 02:45:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 02:45:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 02:45:53 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.375 on epoch=924
03/11/2022 02:45:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/11/2022 02:45:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/11/2022 02:46:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 02:46:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 02:46:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 02:46:06 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 0.4375 on epoch=949
03/11/2022 02:46:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
03/11/2022 02:46:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 02:46:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 02:46:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 02:46:17 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 02:46:18 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.34375 on epoch=974
03/11/2022 02:46:20 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 02:46:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/11/2022 02:46:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 02:46:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 02:46:29 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 02:46:30 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 0.46875 on epoch=999
03/11/2022 02:46:30 - INFO - __main__ - save last model!
03/11/2022 02:46:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 02:46:30 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 02:46:30 - INFO - __main__ - Printing 3 examples
03/11/2022 02:46:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 02:46:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 02:46:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 02:46:30 - INFO - __main__ - ['sentence 1']
03/11/2022 02:46:30 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:46:30 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:46:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:46:30 - INFO - __main__ - Printing 3 examples
03/11/2022 02:46:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/11/2022 02:46:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/11/2022 02:46:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/11/2022 02:46:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:30 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 02:46:30 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 02:46:30 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:46:30 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:46:30 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:46:30 - INFO - __main__ - Printing 3 examples
03/11/2022 02:46:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/11/2022 02:46:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/11/2022 02:46:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/11/2022 02:46:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:30 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:46:30 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:46:30 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:46:35 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_42_0.5_8_predictions.txt
03/11/2022 02:46:35 - INFO - __main__ - ACC on test data: 0.5450
03/11/2022 02:46:35 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.5, bsz=8, dev_performance=0.5625, test_performance=0.545
03/11/2022 02:46:35 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.4, bsz=8 ...
03/11/2022 02:46:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:46:36 - INFO - __main__ - Printing 3 examples
03/11/2022 02:46:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/11/2022 02:46:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/11/2022 02:46:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/11/2022 02:46:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 02:46:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:46:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:46:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:46:36 - INFO - __main__ - Printing 3 examples
03/11/2022 02:46:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/11/2022 02:46:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/11/2022 02:46:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/11/2022 02:46:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:46:36 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:46:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:46:36 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:46:44 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:46:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:46:45 - INFO - __main__ - Starting training!
03/11/2022 02:46:48 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:46:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:46:49 - INFO - __main__ - Starting training!
03/11/2022 02:46:54 - INFO - __main__ - Step 10 Global step 10 Train loss 1.21 on epoch=4
03/11/2022 02:46:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.50 on epoch=9
03/11/2022 02:46:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.37 on epoch=14
03/11/2022 02:47:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.35 on epoch=19
03/11/2022 02:47:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.33 on epoch=24
03/11/2022 02:47:03 - INFO - __main__ - Global step 50 Train loss 0.55 ACC 0.5 on epoch=24
03/11/2022 02:47:03 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 02:47:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.30 on epoch=29
03/11/2022 02:47:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
03/11/2022 02:47:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.27 on epoch=39
03/11/2022 02:47:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.29 on epoch=44
03/11/2022 02:47:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=49
03/11/2022 02:47:14 - INFO - __main__ - Global step 100 Train loss 0.28 ACC 0.5 on epoch=49
03/11/2022 02:47:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.25 on epoch=54
03/11/2022 02:47:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.24 on epoch=59
03/11/2022 02:47:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.26 on epoch=64
03/11/2022 02:47:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=69
03/11/2022 02:47:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
03/11/2022 02:47:26 - INFO - __main__ - Global step 150 Train loss 0.26 ACC 0.5 on epoch=74
03/11/2022 02:47:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
03/11/2022 02:47:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.24 on epoch=84
03/11/2022 02:47:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
03/11/2022 02:47:34 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
03/11/2022 02:47:36 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/11/2022 02:47:37 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.5 on epoch=99
03/11/2022 02:47:39 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
03/11/2022 02:47:41 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=109
03/11/2022 02:47:43 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
03/11/2022 02:47:45 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
03/11/2022 02:47:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=124
03/11/2022 02:47:48 - INFO - __main__ - Global step 250 Train loss 0.25 ACC 0.40625 on epoch=124
03/11/2022 02:47:50 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
03/11/2022 02:47:52 - INFO - __main__ - Step 270 Global step 270 Train loss 0.24 on epoch=134
03/11/2022 02:47:55 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
03/11/2022 02:47:57 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
03/11/2022 02:47:59 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
03/11/2022 02:47:59 - INFO - __main__ - Global step 300 Train loss 0.24 ACC 0.5 on epoch=149
03/11/2022 02:48:02 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
03/11/2022 02:48:04 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
03/11/2022 02:48:06 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=164
03/11/2022 02:48:08 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 02:48:10 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=174
03/11/2022 02:48:11 - INFO - __main__ - Global step 350 Train loss 0.24 ACC 0.46875 on epoch=174
03/11/2022 02:48:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
03/11/2022 02:48:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 02:48:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
03/11/2022 02:48:19 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 02:48:21 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
03/11/2022 02:48:22 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.46875 on epoch=199
03/11/2022 02:48:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 02:48:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
03/11/2022 02:48:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
03/11/2022 02:48:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
03/11/2022 02:48:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
03/11/2022 02:48:33 - INFO - __main__ - Global step 450 Train loss 0.23 ACC 0.5 on epoch=224
03/11/2022 02:48:35 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 02:48:38 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 02:48:40 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
03/11/2022 02:48:42 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
03/11/2022 02:48:44 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
03/11/2022 02:48:45 - INFO - __main__ - Global step 500 Train loss 0.23 ACC 0.4375 on epoch=249
03/11/2022 02:48:47 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
03/11/2022 02:48:49 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
03/11/2022 02:48:51 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
03/11/2022 02:48:53 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
03/11/2022 02:48:55 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=274
03/11/2022 02:48:57 - INFO - __main__ - Global step 550 Train loss 0.21 ACC 0.4375 on epoch=274
03/11/2022 02:48:59 - INFO - __main__ - Step 560 Global step 560 Train loss 0.17 on epoch=279
03/11/2022 02:49:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=284
03/11/2022 02:49:03 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=289
03/11/2022 02:49:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
03/11/2022 02:49:07 - INFO - __main__ - Step 600 Global step 600 Train loss 0.17 on epoch=299
03/11/2022 02:49:08 - INFO - __main__ - Global step 600 Train loss 0.19 ACC 0.4375 on epoch=299
03/11/2022 02:49:10 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
03/11/2022 02:49:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=309
03/11/2022 02:49:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=314
03/11/2022 02:49:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=319
03/11/2022 02:49:19 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=324
03/11/2022 02:49:20 - INFO - __main__ - Global step 650 Train loss 0.18 ACC 0.4375 on epoch=324
03/11/2022 02:49:22 - INFO - __main__ - Step 660 Global step 660 Train loss 0.15 on epoch=329
03/11/2022 02:49:24 - INFO - __main__ - Step 670 Global step 670 Train loss 0.15 on epoch=334
03/11/2022 02:49:26 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=339
03/11/2022 02:49:29 - INFO - __main__ - Step 690 Global step 690 Train loss 0.15 on epoch=344
03/11/2022 02:49:31 - INFO - __main__ - Step 700 Global step 700 Train loss 0.14 on epoch=349
03/11/2022 02:49:32 - INFO - __main__ - Global step 700 Train loss 0.15 ACC 0.5 on epoch=349
03/11/2022 02:49:34 - INFO - __main__ - Step 710 Global step 710 Train loss 0.14 on epoch=354
03/11/2022 02:49:36 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=359
03/11/2022 02:49:38 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=364
03/11/2022 02:49:40 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=369
03/11/2022 02:49:42 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=374
03/11/2022 02:49:43 - INFO - __main__ - Global step 750 Train loss 0.16 ACC 0.4375 on epoch=374
03/11/2022 02:49:45 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=379
03/11/2022 02:49:47 - INFO - __main__ - Step 770 Global step 770 Train loss 0.16 on epoch=384
03/11/2022 02:49:49 - INFO - __main__ - Step 780 Global step 780 Train loss 0.17 on epoch=389
03/11/2022 02:49:52 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=394
03/11/2022 02:49:54 - INFO - __main__ - Step 800 Global step 800 Train loss 0.15 on epoch=399
03/11/2022 02:49:55 - INFO - __main__ - Global step 800 Train loss 0.15 ACC 0.53125 on epoch=399
03/11/2022 02:49:55 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=399, global_step=800
03/11/2022 02:49:57 - INFO - __main__ - Step 810 Global step 810 Train loss 0.14 on epoch=404
03/11/2022 02:49:59 - INFO - __main__ - Step 820 Global step 820 Train loss 0.13 on epoch=409
03/11/2022 02:50:01 - INFO - __main__ - Step 830 Global step 830 Train loss 0.11 on epoch=414
03/11/2022 02:50:03 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=419
03/11/2022 02:50:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.14 on epoch=424
03/11/2022 02:50:07 - INFO - __main__ - Global step 850 Train loss 0.12 ACC 0.40625 on epoch=424
03/11/2022 02:50:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.14 on epoch=429
03/11/2022 02:50:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=434
03/11/2022 02:50:13 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=439
03/11/2022 02:50:15 - INFO - __main__ - Step 890 Global step 890 Train loss 0.09 on epoch=444
03/11/2022 02:50:17 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=449
03/11/2022 02:50:18 - INFO - __main__ - Global step 900 Train loss 0.12 ACC 0.53125 on epoch=449
03/11/2022 02:50:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.09 on epoch=454
03/11/2022 02:50:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.10 on epoch=459
03/11/2022 02:50:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=464
03/11/2022 02:50:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.13 on epoch=469
03/11/2022 02:50:29 - INFO - __main__ - Step 950 Global step 950 Train loss 0.10 on epoch=474
03/11/2022 02:50:30 - INFO - __main__ - Global step 950 Train loss 0.10 ACC 0.46875 on epoch=474
03/11/2022 02:50:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.13 on epoch=479
03/11/2022 02:50:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=484
03/11/2022 02:50:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.10 on epoch=489
03/11/2022 02:50:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.09 on epoch=494
03/11/2022 02:50:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=499
03/11/2022 02:50:42 - INFO - __main__ - Global step 1000 Train loss 0.10 ACC 0.40625 on epoch=499
03/11/2022 02:50:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=504
03/11/2022 02:50:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=509
03/11/2022 02:50:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
03/11/2022 02:50:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.09 on epoch=519
03/11/2022 02:50:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.06 on epoch=524
03/11/2022 02:50:53 - INFO - __main__ - Global step 1050 Train loss 0.08 ACC 0.46875 on epoch=524
03/11/2022 02:50:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
03/11/2022 02:50:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.05 on epoch=534
03/11/2022 02:51:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
03/11/2022 02:51:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.07 on epoch=544
03/11/2022 02:51:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.05 on epoch=549
03/11/2022 02:51:05 - INFO - __main__ - Global step 1100 Train loss 0.06 ACC 0.46875 on epoch=549
03/11/2022 02:51:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=554
03/11/2022 02:51:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.08 on epoch=559
03/11/2022 02:51:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
03/11/2022 02:51:14 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=569
03/11/2022 02:51:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
03/11/2022 02:51:16 - INFO - __main__ - Global step 1150 Train loss 0.06 ACC 0.46875 on epoch=574
03/11/2022 02:51:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.04 on epoch=579
03/11/2022 02:51:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.04 on epoch=584
03/11/2022 02:51:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
03/11/2022 02:51:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
03/11/2022 02:51:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=599
03/11/2022 02:51:28 - INFO - __main__ - Global step 1200 Train loss 0.05 ACC 0.46875 on epoch=599
03/11/2022 02:51:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
03/11/2022 02:51:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.04 on epoch=609
03/11/2022 02:51:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=614
03/11/2022 02:51:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=619
03/11/2022 02:51:38 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
03/11/2022 02:51:39 - INFO - __main__ - Global step 1250 Train loss 0.10 ACC 0.53125 on epoch=624
03/11/2022 02:51:42 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=629
03/11/2022 02:51:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.06 on epoch=634
03/11/2022 02:51:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.07 on epoch=639
03/11/2022 02:51:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
03/11/2022 02:51:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
03/11/2022 02:51:51 - INFO - __main__ - Global step 1300 Train loss 0.06 ACC 0.46875 on epoch=649
03/11/2022 02:51:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.07 on epoch=654
03/11/2022 02:51:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
03/11/2022 02:51:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
03/11/2022 02:51:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.05 on epoch=669
03/11/2022 02:52:01 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.04 on epoch=674
03/11/2022 02:52:02 - INFO - __main__ - Global step 1350 Train loss 0.04 ACC 0.4375 on epoch=674
03/11/2022 02:52:04 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.05 on epoch=679
03/11/2022 02:52:06 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
03/11/2022 02:52:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.05 on epoch=689
03/11/2022 02:52:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
03/11/2022 02:52:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
03/11/2022 02:52:14 - INFO - __main__ - Global step 1400 Train loss 0.04 ACC 0.53125 on epoch=699
03/11/2022 02:52:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
03/11/2022 02:52:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
03/11/2022 02:52:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
03/11/2022 02:52:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
03/11/2022 02:52:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
03/11/2022 02:52:25 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 0.4375 on epoch=724
03/11/2022 02:52:27 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/11/2022 02:52:29 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
03/11/2022 02:52:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
03/11/2022 02:52:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
03/11/2022 02:52:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
03/11/2022 02:52:37 - INFO - __main__ - Global step 1500 Train loss 0.04 ACC 0.40625 on epoch=749
03/11/2022 02:52:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
03/11/2022 02:52:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/11/2022 02:52:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
03/11/2022 02:52:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
03/11/2022 02:52:47 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/11/2022 02:52:48 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 0.4375 on epoch=774
03/11/2022 02:52:50 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
03/11/2022 02:52:52 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
03/11/2022 02:52:54 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.03 on epoch=789
03/11/2022 02:52:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
03/11/2022 02:52:59 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
03/11/2022 02:52:59 - INFO - __main__ - Global step 1600 Train loss 0.03 ACC 0.5 on epoch=799
03/11/2022 02:53:01 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 02:53:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
03/11/2022 02:53:06 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=814
03/11/2022 02:53:08 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
03/11/2022 02:53:10 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
03/11/2022 02:53:11 - INFO - __main__ - Global step 1650 Train loss 0.03 ACC 0.46875 on epoch=824
03/11/2022 02:53:13 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.03 on epoch=829
03/11/2022 02:53:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
03/11/2022 02:53:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
03/11/2022 02:53:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.03 on epoch=844
03/11/2022 02:53:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 02:53:22 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 0.46875 on epoch=849
03/11/2022 02:53:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
03/11/2022 02:53:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
03/11/2022 02:53:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/11/2022 02:53:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
03/11/2022 02:53:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/11/2022 02:53:33 - INFO - __main__ - Global step 1750 Train loss 0.02 ACC 0.40625 on epoch=874
03/11/2022 02:53:35 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/11/2022 02:53:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
03/11/2022 02:53:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
03/11/2022 02:53:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
03/11/2022 02:53:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
03/11/2022 02:53:45 - INFO - __main__ - Global step 1800 Train loss 0.02 ACC 0.375 on epoch=899
03/11/2022 02:53:47 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.03 on epoch=904
03/11/2022 02:53:49 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/11/2022 02:53:51 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
03/11/2022 02:53:53 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/11/2022 02:53:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/11/2022 02:53:56 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 0.40625 on epoch=924
03/11/2022 02:53:58 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 02:54:00 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 02:54:03 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/11/2022 02:54:05 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.01 on epoch=944
03/11/2022 02:54:07 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.02 on epoch=949
03/11/2022 02:54:08 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 0.375 on epoch=949
03/11/2022 02:54:10 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/11/2022 02:54:12 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.04 on epoch=959
03/11/2022 02:54:14 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
03/11/2022 02:54:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/11/2022 02:54:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/11/2022 02:54:19 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 0.46875 on epoch=974
03/11/2022 02:54:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 02:54:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.02 on epoch=984
03/11/2022 02:54:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 02:54:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
03/11/2022 02:54:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 02:54:30 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 0.40625 on epoch=999
03/11/2022 02:54:30 - INFO - __main__ - save last model!
03/11/2022 02:54:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 02:54:30 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 02:54:30 - INFO - __main__ - Printing 3 examples
03/11/2022 02:54:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 02:54:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 02:54:30 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:30 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 02:54:30 - INFO - __main__ - ['sentence 1']
03/11/2022 02:54:30 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:54:30 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:54:31 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 02:54:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:54:31 - INFO - __main__ - Printing 3 examples
03/11/2022 02:54:31 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/11/2022 02:54:31 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:31 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/11/2022 02:54:31 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:31 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/11/2022 02:54:31 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 02:54:31 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:54:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:54:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:54:31 - INFO - __main__ - Printing 3 examples
03/11/2022 02:54:31 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/11/2022 02:54:31 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:31 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/11/2022 02:54:31 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:31 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/11/2022 02:54:31 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:31 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:54:31 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:54:31 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:54:35 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_42_0.4_8_predictions.txt
03/11/2022 02:54:35 - INFO - __main__ - ACC on test data: 0.5550
03/11/2022 02:54:35 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.555
03/11/2022 02:54:35 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.3, bsz=8 ...
03/11/2022 02:54:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:54:36 - INFO - __main__ - Printing 3 examples
03/11/2022 02:54:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/11/2022 02:54:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/11/2022 02:54:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/11/2022 02:54:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 02:54:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:54:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 02:54:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 02:54:36 - INFO - __main__ - Printing 3 examples
03/11/2022 02:54:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/11/2022 02:54:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/11/2022 02:54:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/11/2022 02:54:36 - INFO - __main__ - ['sentence 2']
03/11/2022 02:54:36 - INFO - __main__ - Tokenizing Input ...
03/11/2022 02:54:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 02:54:36 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 02:54:45 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:54:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:54:46 - INFO - __main__ - Starting training!
03/11/2022 02:54:48 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 02:54:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 02:54:49 - INFO - __main__ - Starting training!
03/11/2022 02:54:54 - INFO - __main__ - Step 10 Global step 10 Train loss 1.25 on epoch=4
03/11/2022 02:54:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.51 on epoch=9
03/11/2022 02:54:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.37 on epoch=14
03/11/2022 02:55:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.33 on epoch=19
03/11/2022 02:55:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.31 on epoch=24
03/11/2022 02:55:03 - INFO - __main__ - Global step 50 Train loss 0.55 ACC 0.5 on epoch=24
03/11/2022 02:55:03 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 02:55:05 - INFO - __main__ - Step 60 Global step 60 Train loss 0.31 on epoch=29
03/11/2022 02:55:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.31 on epoch=34
03/11/2022 02:55:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=39
03/11/2022 02:55:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.33 on epoch=44
03/11/2022 02:55:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=49
03/11/2022 02:55:15 - INFO - __main__ - Global step 100 Train loss 0.30 ACC 0.5 on epoch=49
03/11/2022 02:55:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=54
03/11/2022 02:55:19 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
03/11/2022 02:55:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
03/11/2022 02:55:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
03/11/2022 02:55:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
03/11/2022 02:55:27 - INFO - __main__ - Global step 150 Train loss 0.27 ACC 0.5 on epoch=74
03/11/2022 02:55:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
03/11/2022 02:55:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.23 on epoch=84
03/11/2022 02:55:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
03/11/2022 02:55:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
03/11/2022 02:55:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/11/2022 02:55:38 - INFO - __main__ - Global step 200 Train loss 0.25 ACC 0.5 on epoch=99
03/11/2022 02:55:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
03/11/2022 02:55:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 02:55:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
03/11/2022 02:55:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
03/11/2022 02:55:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
03/11/2022 02:55:50 - INFO - __main__ - Global step 250 Train loss 0.26 ACC 0.5 on epoch=124
03/11/2022 02:55:52 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
03/11/2022 02:55:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
03/11/2022 02:55:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
03/11/2022 02:55:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
03/11/2022 02:56:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=149
03/11/2022 02:56:02 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.5 on epoch=149
03/11/2022 02:56:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=154
03/11/2022 02:56:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 02:56:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=164
03/11/2022 02:56:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
03/11/2022 02:56:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
03/11/2022 02:56:14 - INFO - __main__ - Global step 350 Train loss 0.25 ACC 0.4375 on epoch=174
03/11/2022 02:56:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
03/11/2022 02:56:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
03/11/2022 02:56:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
03/11/2022 02:56:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 02:56:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
03/11/2022 02:56:26 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.5 on epoch=199
03/11/2022 02:56:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
03/11/2022 02:56:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
03/11/2022 02:56:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
03/11/2022 02:56:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
03/11/2022 02:56:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
03/11/2022 02:56:37 - INFO - __main__ - Global step 450 Train loss 0.24 ACC 0.5 on epoch=224
03/11/2022 02:56:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
03/11/2022 02:56:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
03/11/2022 02:56:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 02:56:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
03/11/2022 02:56:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
03/11/2022 02:56:49 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.53125 on epoch=249
03/11/2022 02:56:49 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=249, global_step=500
03/11/2022 02:56:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=254
03/11/2022 02:56:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
03/11/2022 02:56:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
03/11/2022 02:56:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
03/11/2022 02:57:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=274
03/11/2022 02:57:01 - INFO - __main__ - Global step 550 Train loss 0.24 ACC 0.5 on epoch=274
03/11/2022 02:57:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/11/2022 02:57:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.26 on epoch=284
03/11/2022 02:57:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
03/11/2022 02:57:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
03/11/2022 02:57:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
03/11/2022 02:57:13 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.53125 on epoch=299
03/11/2022 02:57:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
03/11/2022 02:57:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
03/11/2022 02:57:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=314
03/11/2022 02:57:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
03/11/2022 02:57:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=324
03/11/2022 02:57:24 - INFO - __main__ - Global step 650 Train loss 0.23 ACC 0.53125 on epoch=324
03/11/2022 02:57:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=329
03/11/2022 02:57:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
03/11/2022 02:57:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=339
03/11/2022 02:57:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=344
03/11/2022 02:57:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=349
03/11/2022 02:57:36 - INFO - __main__ - Global step 700 Train loss 0.21 ACC 0.625 on epoch=349
03/11/2022 02:57:36 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.625 on epoch=349, global_step=700
03/11/2022 02:57:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
03/11/2022 02:57:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=359
03/11/2022 02:57:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.14 on epoch=364
03/11/2022 02:57:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
03/11/2022 02:57:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=374
03/11/2022 02:57:48 - INFO - __main__ - Global step 750 Train loss 0.13 ACC 1.0 on epoch=374
03/11/2022 02:57:48 - INFO - __main__ - Saving model with best ACC: 0.625 -> 1.0 on epoch=374, global_step=750
03/11/2022 02:57:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.06 on epoch=379
03/11/2022 02:57:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.03 on epoch=384
03/11/2022 02:57:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
03/11/2022 02:57:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
03/11/2022 02:57:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.05 on epoch=399
03/11/2022 02:58:00 - INFO - __main__ - Global step 800 Train loss 0.05 ACC 1.0 on epoch=399
03/11/2022 02:58:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.03 on epoch=404
03/11/2022 02:58:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/11/2022 02:58:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
03/11/2022 02:58:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/11/2022 02:58:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/11/2022 02:58:12 - INFO - __main__ - Global step 850 Train loss 0.02 ACC 1.0 on epoch=424
03/11/2022 02:58:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.03 on epoch=429
03/11/2022 02:58:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/11/2022 02:58:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.02 on epoch=439
03/11/2022 02:58:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/11/2022 02:58:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
03/11/2022 02:58:24 - INFO - __main__ - Global step 900 Train loss 0.02 ACC 1.0 on epoch=449
03/11/2022 02:58:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/11/2022 02:58:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/11/2022 02:58:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/11/2022 02:58:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/11/2022 02:58:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/11/2022 02:58:35 - INFO - __main__ - Global step 950 Train loss 0.00 ACC 1.0 on epoch=474
03/11/2022 02:58:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/11/2022 02:58:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/11/2022 02:58:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/11/2022 02:58:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/11/2022 02:58:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/11/2022 02:58:47 - INFO - __main__ - Global step 1000 Train loss 0.00 ACC 1.0 on epoch=499
03/11/2022 02:58:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/11/2022 02:58:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/11/2022 02:58:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/11/2022 02:58:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/11/2022 02:58:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/11/2022 02:58:59 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 1.0 on epoch=524
03/11/2022 02:59:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/11/2022 02:59:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/11/2022 02:59:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/11/2022 02:59:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/11/2022 02:59:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/11/2022 02:59:11 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 1.0 on epoch=549
03/11/2022 02:59:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/11/2022 02:59:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/11/2022 02:59:17 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/11/2022 02:59:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/11/2022 02:59:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/11/2022 02:59:22 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 1.0 on epoch=574
03/11/2022 02:59:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/11/2022 02:59:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/11/2022 02:59:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/11/2022 02:59:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/11/2022 02:59:34 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/11/2022 02:59:34 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 1.0 on epoch=599
03/11/2022 02:59:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/11/2022 02:59:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/11/2022 02:59:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/11/2022 02:59:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.01 on epoch=619
03/11/2022 02:59:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/11/2022 02:59:46 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 1.0 on epoch=624
03/11/2022 02:59:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/11/2022 02:59:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/11/2022 02:59:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/11/2022 02:59:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/11/2022 02:59:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/11/2022 02:59:58 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 1.0 on epoch=649
03/11/2022 03:00:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/11/2022 03:00:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/11/2022 03:00:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/11/2022 03:00:07 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 03:00:09 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 03:00:10 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 1.0 on epoch=674
03/11/2022 03:00:12 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 03:00:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 03:00:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/11/2022 03:00:19 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 03:00:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/11/2022 03:00:21 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 1.0 on epoch=699
03/11/2022 03:00:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/11/2022 03:00:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 03:00:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 03:00:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 03:00:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
03/11/2022 03:00:32 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 1.0 on epoch=724
03/11/2022 03:00:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 03:00:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 03:00:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 03:00:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.05 on epoch=744
03/11/2022 03:00:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 03:00:44 - INFO - __main__ - Global step 1500 Train loss 0.01 ACC 1.0 on epoch=749
03/11/2022 03:00:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/11/2022 03:00:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 03:00:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 03:00:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 03:00:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
03/11/2022 03:00:55 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 1.0 on epoch=774
03/11/2022 03:00:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 03:00:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 03:01:01 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 03:01:03 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 03:01:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 03:01:06 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 1.0 on epoch=799
03/11/2022 03:01:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 03:01:11 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 03:01:13 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 03:01:15 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 03:01:17 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 03:01:17 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 1.0 on epoch=824
03/11/2022 03:01:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 03:01:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 03:01:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 03:01:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 03:01:28 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 03:01:29 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 1.0 on epoch=849
03/11/2022 03:01:31 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
03/11/2022 03:01:33 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 03:01:35 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 03:01:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 03:01:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 03:01:40 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 1.0 on epoch=874
03/11/2022 03:01:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 03:01:44 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 03:01:46 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 03:01:48 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 03:01:50 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 03:01:51 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 1.0 on epoch=899
03/11/2022 03:01:53 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 03:01:55 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 03:01:57 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 03:02:00 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 03:02:02 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 03:02:02 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 1.0 on epoch=924
03/11/2022 03:02:04 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 03:02:07 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 03:02:09 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 03:02:11 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 03:02:13 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 03:02:13 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 1.0 on epoch=949
03/11/2022 03:02:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 03:02:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 03:02:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 03:02:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 03:02:24 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 03:02:25 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 1.0 on epoch=974
03/11/2022 03:02:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 03:02:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 03:02:31 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 03:02:33 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 03:02:35 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 03:02:36 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 1.0 on epoch=999
03/11/2022 03:02:36 - INFO - __main__ - save last model!
03/11/2022 03:02:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 03:02:36 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 03:02:36 - INFO - __main__ - Printing 3 examples
03/11/2022 03:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 03:02:36 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 03:02:36 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 03:02:36 - INFO - __main__ - ['sentence 1']
03/11/2022 03:02:36 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:02:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:02:36 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 03:02:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:02:36 - INFO - __main__ - Printing 3 examples
03/11/2022 03:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/11/2022 03:02:36 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/11/2022 03:02:36 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/11/2022 03:02:36 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 03:02:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:02:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:02:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:02:36 - INFO - __main__ - Printing 3 examples
03/11/2022 03:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/11/2022 03:02:36 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/11/2022 03:02:36 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:36 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/11/2022 03:02:36 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:36 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:02:37 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:02:37 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:02:40 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_42_0.3_8_predictions.txt
03/11/2022 03:02:40 - INFO - __main__ - ACC on test data: 1.0000
03/11/2022 03:02:40 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.3, bsz=8, dev_performance=1.0, test_performance=1.0
03/11/2022 03:02:40 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.2, bsz=8 ...
03/11/2022 03:02:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:02:41 - INFO - __main__ - Printing 3 examples
03/11/2022 03:02:41 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Russell does really ever discuss April. [SEP] sentence 2: Russell does not ever discuss April.
03/11/2022 03:02:41 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:41 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These schools should really ever interact. [SEP] sentence 2: These schools should not ever interact.
03/11/2022 03:02:41 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:41 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The Lutherans will fortunately ever shout. [SEP] sentence 2: The Lutherans will not ever shout.
03/11/2022 03:02:41 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 03:02:41 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:02:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:02:41 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:02:41 - INFO - __main__ - Printing 3 examples
03/11/2022 03:02:41 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Randolf has fortunately ever discovered Candice. [SEP] sentence 2: Randolf has not ever discovered Candice.
03/11/2022 03:02:41 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:41 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Many mirrors should really ever aggravate the patients. [SEP] sentence 2: Many mirrors should not ever aggravate the patients.
03/11/2022 03:02:41 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:41 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The gates should fortunately ever open. [SEP] sentence 2: The gates should not ever open.
03/11/2022 03:02:41 - INFO - __main__ - ['sentence 2']
03/11/2022 03:02:41 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:02:41 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:02:41 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:02:51 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:02:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:02:52 - INFO - __main__ - Starting training!
03/11/2022 03:02:55 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:02:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:02:56 - INFO - __main__ - Starting training!
03/11/2022 03:02:59 - INFO - __main__ - Step 10 Global step 10 Train loss 1.41 on epoch=4
03/11/2022 03:03:01 - INFO - __main__ - Step 20 Global step 20 Train loss 0.66 on epoch=9
03/11/2022 03:03:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.51 on epoch=14
03/11/2022 03:03:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.42 on epoch=19
03/11/2022 03:03:08 - INFO - __main__ - Step 50 Global step 50 Train loss 0.34 on epoch=24
03/11/2022 03:03:08 - INFO - __main__ - Global step 50 Train loss 0.67 ACC 0.5 on epoch=24
03/11/2022 03:03:08 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 03:03:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.33 on epoch=29
03/11/2022 03:03:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.37 on epoch=34
03/11/2022 03:03:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
03/11/2022 03:03:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.29 on epoch=44
03/11/2022 03:03:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.28 on epoch=49
03/11/2022 03:03:20 - INFO - __main__ - Global step 100 Train loss 0.32 ACC 0.5 on epoch=49
03/11/2022 03:03:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
03/11/2022 03:03:25 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=59
03/11/2022 03:03:27 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=64
03/11/2022 03:03:29 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
03/11/2022 03:03:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
03/11/2022 03:03:32 - INFO - __main__ - Global step 150 Train loss 0.28 ACC 0.5 on epoch=74
03/11/2022 03:03:34 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=79
03/11/2022 03:03:37 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=84
03/11/2022 03:03:39 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
03/11/2022 03:03:41 - INFO - __main__ - Step 190 Global step 190 Train loss 0.26 on epoch=94
03/11/2022 03:03:43 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
03/11/2022 03:03:44 - INFO - __main__ - Global step 200 Train loss 0.27 ACC 0.5 on epoch=99
03/11/2022 03:03:46 - INFO - __main__ - Step 210 Global step 210 Train loss 0.24 on epoch=104
03/11/2022 03:03:49 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 03:03:51 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
03/11/2022 03:03:53 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
03/11/2022 03:03:55 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
03/11/2022 03:03:56 - INFO - __main__ - Global step 250 Train loss 0.26 ACC 0.5 on epoch=124
03/11/2022 03:03:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
03/11/2022 03:04:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
03/11/2022 03:04:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 03:04:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
03/11/2022 03:04:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=149
03/11/2022 03:04:08 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.5 on epoch=149
03/11/2022 03:04:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 03:04:12 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 03:04:14 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
03/11/2022 03:04:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
03/11/2022 03:04:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
03/11/2022 03:04:19 - INFO - __main__ - Global step 350 Train loss 0.26 ACC 0.46875 on epoch=174
03/11/2022 03:04:22 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 03:04:24 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 03:04:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
03/11/2022 03:04:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 03:04:31 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 03:04:31 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.5 on epoch=199
03/11/2022 03:04:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 03:04:36 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
03/11/2022 03:04:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
03/11/2022 03:04:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.26 on epoch=219
03/11/2022 03:04:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
03/11/2022 03:04:43 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.5 on epoch=224
03/11/2022 03:04:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 03:04:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=234
03/11/2022 03:04:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
03/11/2022 03:04:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
03/11/2022 03:04:55 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
03/11/2022 03:04:55 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5 on epoch=249
03/11/2022 03:04:57 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 03:05:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
03/11/2022 03:05:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=264
03/11/2022 03:05:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
03/11/2022 03:05:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
03/11/2022 03:05:07 - INFO - __main__ - Global step 550 Train loss 0.24 ACC 0.5 on epoch=274
03/11/2022 03:05:09 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/11/2022 03:05:11 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=284
03/11/2022 03:05:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
03/11/2022 03:05:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/11/2022 03:05:18 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
03/11/2022 03:05:19 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.5 on epoch=299
03/11/2022 03:05:21 - INFO - __main__ - Step 610 Global step 610 Train loss 0.25 on epoch=304
03/11/2022 03:05:23 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=309
03/11/2022 03:05:25 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=314
03/11/2022 03:05:28 - INFO - __main__ - Step 640 Global step 640 Train loss 0.27 on epoch=319
03/11/2022 03:05:30 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
03/11/2022 03:05:31 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.5 on epoch=324
03/11/2022 03:05:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=329
03/11/2022 03:05:35 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=334
03/11/2022 03:05:37 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
03/11/2022 03:05:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.24 on epoch=344
03/11/2022 03:05:42 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
03/11/2022 03:05:42 - INFO - __main__ - Global step 700 Train loss 0.24 ACC 0.5 on epoch=349
03/11/2022 03:05:45 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=354
03/11/2022 03:05:47 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=359
03/11/2022 03:05:49 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=364
03/11/2022 03:05:51 - INFO - __main__ - Step 740 Global step 740 Train loss 0.24 on epoch=369
03/11/2022 03:05:54 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=374
03/11/2022 03:05:54 - INFO - __main__ - Global step 750 Train loss 0.23 ACC 0.5 on epoch=374
03/11/2022 03:05:56 - INFO - __main__ - Step 760 Global step 760 Train loss 0.24 on epoch=379
03/11/2022 03:05:59 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
03/11/2022 03:06:01 - INFO - __main__ - Step 780 Global step 780 Train loss 0.24 on epoch=389
03/11/2022 03:06:03 - INFO - __main__ - Step 790 Global step 790 Train loss 0.24 on epoch=394
03/11/2022 03:06:05 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=399
03/11/2022 03:06:06 - INFO - __main__ - Global step 800 Train loss 0.24 ACC 0.5 on epoch=399
03/11/2022 03:06:08 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=404
03/11/2022 03:06:10 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
03/11/2022 03:06:13 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=414
03/11/2022 03:06:15 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/11/2022 03:06:17 - INFO - __main__ - Step 850 Global step 850 Train loss 0.25 on epoch=424
03/11/2022 03:06:18 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.5 on epoch=424
03/11/2022 03:06:20 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=429
03/11/2022 03:06:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
03/11/2022 03:06:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=439
03/11/2022 03:06:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.25 on epoch=444
03/11/2022 03:06:29 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=449
03/11/2022 03:06:29 - INFO - __main__ - Global step 900 Train loss 0.24 ACC 0.5 on epoch=449
03/11/2022 03:06:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.24 on epoch=454
03/11/2022 03:06:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
03/11/2022 03:06:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/11/2022 03:06:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=469
03/11/2022 03:06:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.23 on epoch=474
03/11/2022 03:06:41 - INFO - __main__ - Global step 950 Train loss 0.23 ACC 0.5 on epoch=474
03/11/2022 03:06:43 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=479
03/11/2022 03:06:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=484
03/11/2022 03:06:48 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=489
03/11/2022 03:06:50 - INFO - __main__ - Step 990 Global step 990 Train loss 0.23 on epoch=494
03/11/2022 03:06:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=499
03/11/2022 03:06:53 - INFO - __main__ - Global step 1000 Train loss 0.23 ACC 0.5 on epoch=499
03/11/2022 03:06:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.25 on epoch=504
03/11/2022 03:06:57 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.25 on epoch=509
03/11/2022 03:07:00 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=514
03/11/2022 03:07:02 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=519
03/11/2022 03:07:04 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
03/11/2022 03:07:05 - INFO - __main__ - Global step 1050 Train loss 0.24 ACC 0.59375 on epoch=524
03/11/2022 03:07:05 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.59375 on epoch=524, global_step=1050
03/11/2022 03:07:07 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.24 on epoch=529
03/11/2022 03:07:09 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=534
03/11/2022 03:07:11 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=539
03/11/2022 03:07:14 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
03/11/2022 03:07:16 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=549
03/11/2022 03:07:16 - INFO - __main__ - Global step 1100 Train loss 0.24 ACC 0.4375 on epoch=549
03/11/2022 03:07:19 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=554
03/11/2022 03:07:21 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=559
03/11/2022 03:07:23 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=564
03/11/2022 03:07:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=569
03/11/2022 03:07:28 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=574
03/11/2022 03:07:28 - INFO - __main__ - Global step 1150 Train loss 0.23 ACC 0.5 on epoch=574
03/11/2022 03:07:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=579
03/11/2022 03:07:33 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.24 on epoch=584
03/11/2022 03:07:35 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.23 on epoch=589
03/11/2022 03:07:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/11/2022 03:07:40 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=599
03/11/2022 03:07:40 - INFO - __main__ - Global step 1200 Train loss 0.23 ACC 0.625 on epoch=599
03/11/2022 03:07:40 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.625 on epoch=599, global_step=1200
03/11/2022 03:07:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.23 on epoch=604
03/11/2022 03:07:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=609
03/11/2022 03:07:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=614
03/11/2022 03:07:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=619
03/11/2022 03:07:51 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=624
03/11/2022 03:07:52 - INFO - __main__ - Global step 1250 Train loss 0.21 ACC 0.90625 on epoch=624
03/11/2022 03:07:52 - INFO - __main__ - Saving model with best ACC: 0.625 -> 0.90625 on epoch=624, global_step=1250
03/11/2022 03:07:54 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.19 on epoch=629
03/11/2022 03:07:57 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=634
03/11/2022 03:07:59 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=639
03/11/2022 03:08:01 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=644
03/11/2022 03:08:03 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.14 on epoch=649
03/11/2022 03:08:04 - INFO - __main__ - Global step 1300 Train loss 0.16 ACC 1.0 on epoch=649
03/11/2022 03:08:04 - INFO - __main__ - Saving model with best ACC: 0.90625 -> 1.0 on epoch=649, global_step=1300
03/11/2022 03:08:06 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=654
03/11/2022 03:08:08 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=659
03/11/2022 03:08:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.10 on epoch=664
03/11/2022 03:08:13 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
03/11/2022 03:08:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.08 on epoch=674
03/11/2022 03:08:16 - INFO - __main__ - Global step 1350 Train loss 0.10 ACC 1.0 on epoch=674
03/11/2022 03:08:18 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.07 on epoch=679
03/11/2022 03:08:20 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=684
03/11/2022 03:08:23 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.07 on epoch=689
03/11/2022 03:08:25 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
03/11/2022 03:08:27 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
03/11/2022 03:08:28 - INFO - __main__ - Global step 1400 Train loss 0.06 ACC 1.0 on epoch=699
03/11/2022 03:08:30 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
03/11/2022 03:08:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
03/11/2022 03:08:35 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/11/2022 03:08:37 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
03/11/2022 03:08:39 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
03/11/2022 03:08:40 - INFO - __main__ - Global step 1450 Train loss 0.03 ACC 1.0 on epoch=724
03/11/2022 03:08:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/11/2022 03:08:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
03/11/2022 03:08:47 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
03/11/2022 03:08:49 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/11/2022 03:08:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
03/11/2022 03:08:52 - INFO - __main__ - Global step 1500 Train loss 0.02 ACC 1.0 on epoch=749
03/11/2022 03:08:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/11/2022 03:08:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
03/11/2022 03:08:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/11/2022 03:09:01 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 03:09:03 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 03:09:04 - INFO - __main__ - Global step 1550 Train loss 0.01 ACC 1.0 on epoch=774
03/11/2022 03:09:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
03/11/2022 03:09:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
03/11/2022 03:09:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/11/2022 03:09:13 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
03/11/2022 03:09:15 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.03 on epoch=799
03/11/2022 03:09:16 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 1.0 on epoch=799
03/11/2022 03:09:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 03:09:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 03:09:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/11/2022 03:09:25 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/11/2022 03:09:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/11/2022 03:09:27 - INFO - __main__ - Global step 1650 Train loss 0.01 ACC 1.0 on epoch=824
03/11/2022 03:09:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/11/2022 03:09:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/11/2022 03:09:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/11/2022 03:09:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 03:09:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 03:09:39 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 1.0 on epoch=849
03/11/2022 03:09:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/11/2022 03:09:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.05 on epoch=859
03/11/2022 03:09:46 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 03:09:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 03:09:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 03:09:52 - INFO - __main__ - Global step 1750 Train loss 0.01 ACC 1.0 on epoch=874
03/11/2022 03:09:54 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/11/2022 03:09:56 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 03:09:58 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 03:10:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 03:10:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 03:10:03 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 1.0 on epoch=899
03/11/2022 03:10:06 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 03:10:08 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
03/11/2022 03:10:10 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 03:10:13 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 03:10:15 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 03:10:15 - INFO - __main__ - Global step 1850 Train loss 0.01 ACC 1.0 on epoch=924
03/11/2022 03:10:18 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 03:10:20 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 03:10:22 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
03/11/2022 03:10:24 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 03:10:27 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
03/11/2022 03:10:27 - INFO - __main__ - Global step 1900 Train loss 0.01 ACC 1.0 on epoch=949
03/11/2022 03:10:30 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/11/2022 03:10:32 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 03:10:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 03:10:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 03:10:39 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 03:10:39 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 1.0 on epoch=974
03/11/2022 03:10:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 03:10:44 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 03:10:46 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 03:10:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
03/11/2022 03:10:51 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 03:10:51 - INFO - __main__ - Global step 2000 Train loss 0.01 ACC 1.0 on epoch=999
03/11/2022 03:10:51 - INFO - __main__ - save last model!
03/11/2022 03:10:51 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 03:10:51 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 03:10:51 - INFO - __main__ - Printing 3 examples
03/11/2022 03:10:51 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 03:10:51 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:51 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 03:10:51 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:51 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 03:10:51 - INFO - __main__ - ['sentence 1']
03/11/2022 03:10:51 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:10:51 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:10:52 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 03:10:52 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:10:52 - INFO - __main__ - Printing 3 examples
03/11/2022 03:10:52 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/11/2022 03:10:52 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:52 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/11/2022 03:10:52 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:52 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/11/2022 03:10:52 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 03:10:52 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:10:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:10:53 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:10:53 - INFO - __main__ - Printing 3 examples
03/11/2022 03:10:53 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/11/2022 03:10:53 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:53 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/11/2022 03:10:53 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:53 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/11/2022 03:10:53 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:53 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:10:53 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:10:53 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:10:55 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_42_0.2_8_predictions.txt
03/11/2022 03:10:55 - INFO - __main__ - ACC on test data: 1.0000
03/11/2022 03:10:55 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_42, lr=0.2, bsz=8, dev_performance=1.0, test_performance=1.0
03/11/2022 03:10:55 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.5, bsz=8 ...
03/11/2022 03:10:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:10:56 - INFO - __main__ - Printing 3 examples
03/11/2022 03:10:56 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/11/2022 03:10:56 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:56 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/11/2022 03:10:56 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:56 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/11/2022 03:10:56 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 03:10:56 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:10:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:10:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:10:56 - INFO - __main__ - Printing 3 examples
03/11/2022 03:10:56 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/11/2022 03:10:56 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:56 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/11/2022 03:10:56 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:56 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/11/2022 03:10:56 - INFO - __main__ - ['sentence 2']
03/11/2022 03:10:56 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:10:56 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:10:56 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:11:07 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:11:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:11:08 - INFO - __main__ - Starting training!
03/11/2022 03:11:10 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:11:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:11:11 - INFO - __main__ - Starting training!
03/11/2022 03:11:14 - INFO - __main__ - Step 10 Global step 10 Train loss 0.96 on epoch=4
03/11/2022 03:11:16 - INFO - __main__ - Step 20 Global step 20 Train loss 0.42 on epoch=9
03/11/2022 03:11:19 - INFO - __main__ - Step 30 Global step 30 Train loss 0.33 on epoch=14
03/11/2022 03:11:21 - INFO - __main__ - Step 40 Global step 40 Train loss 0.31 on epoch=19
03/11/2022 03:11:23 - INFO - __main__ - Step 50 Global step 50 Train loss 0.30 on epoch=24
03/11/2022 03:11:24 - INFO - __main__ - Global step 50 Train loss 0.46 ACC 0.5 on epoch=24
03/11/2022 03:11:24 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 03:11:26 - INFO - __main__ - Step 60 Global step 60 Train loss 0.31 on epoch=29
03/11/2022 03:11:28 - INFO - __main__ - Step 70 Global step 70 Train loss 0.26 on epoch=34
03/11/2022 03:11:30 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=39
03/11/2022 03:11:33 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=44
03/11/2022 03:11:35 - INFO - __main__ - Step 100 Global step 100 Train loss 0.30 on epoch=49
03/11/2022 03:11:36 - INFO - __main__ - Global step 100 Train loss 0.28 ACC 0.53125 on epoch=49
03/11/2022 03:11:36 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=49, global_step=100
03/11/2022 03:11:38 - INFO - __main__ - Step 110 Global step 110 Train loss 0.30 on epoch=54
03/11/2022 03:11:40 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
03/11/2022 03:11:42 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=64
03/11/2022 03:11:45 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=69
03/11/2022 03:11:47 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
03/11/2022 03:11:47 - INFO - __main__ - Global step 150 Train loss 0.27 ACC 0.5 on epoch=74
03/11/2022 03:11:50 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
03/11/2022 03:11:52 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=84
03/11/2022 03:11:54 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/11/2022 03:11:57 - INFO - __main__ - Step 190 Global step 190 Train loss 0.25 on epoch=94
03/11/2022 03:11:59 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/11/2022 03:11:59 - INFO - __main__ - Global step 200 Train loss 0.26 ACC 0.5 on epoch=99
03/11/2022 03:12:02 - INFO - __main__ - Step 210 Global step 210 Train loss 0.29 on epoch=104
03/11/2022 03:12:04 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 03:12:06 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=114
03/11/2022 03:12:09 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=119
03/11/2022 03:12:11 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=124
03/11/2022 03:12:12 - INFO - __main__ - Global step 250 Train loss 0.26 ACC 0.5 on epoch=124
03/11/2022 03:12:14 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
03/11/2022 03:12:16 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 03:12:18 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=139
03/11/2022 03:12:21 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
03/11/2022 03:12:23 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=149
03/11/2022 03:12:23 - INFO - __main__ - Global step 300 Train loss 0.24 ACC 0.5 on epoch=149
03/11/2022 03:12:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 03:12:28 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
03/11/2022 03:12:30 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
03/11/2022 03:12:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 03:12:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
03/11/2022 03:12:35 - INFO - __main__ - Global step 350 Train loss 0.24 ACC 0.5 on epoch=174
03/11/2022 03:12:38 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 03:12:40 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=184
03/11/2022 03:12:42 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
03/11/2022 03:12:45 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
03/11/2022 03:12:47 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
03/11/2022 03:12:47 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.5 on epoch=199
03/11/2022 03:12:50 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
03/11/2022 03:12:52 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
03/11/2022 03:12:54 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
03/11/2022 03:12:56 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
03/11/2022 03:12:59 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
03/11/2022 03:12:59 - INFO - __main__ - Global step 450 Train loss 0.23 ACC 0.5 on epoch=224
03/11/2022 03:13:02 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
03/11/2022 03:13:04 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=234
03/11/2022 03:13:06 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 03:13:08 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
03/11/2022 03:13:11 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
03/11/2022 03:13:11 - INFO - __main__ - Global step 500 Train loss 0.24 ACC 0.5 on epoch=249
03/11/2022 03:13:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
03/11/2022 03:13:16 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
03/11/2022 03:13:18 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
03/11/2022 03:13:20 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=269
03/11/2022 03:13:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=274
03/11/2022 03:13:23 - INFO - __main__ - Global step 550 Train loss 0.22 ACC 0.5 on epoch=274
03/11/2022 03:13:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
03/11/2022 03:13:28 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
03/11/2022 03:13:30 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
03/11/2022 03:13:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
03/11/2022 03:13:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=299
03/11/2022 03:13:35 - INFO - __main__ - Global step 600 Train loss 0.23 ACC 0.5 on epoch=299
03/11/2022 03:13:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
03/11/2022 03:13:40 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
03/11/2022 03:13:42 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
03/11/2022 03:13:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=319
03/11/2022 03:13:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=324
03/11/2022 03:13:47 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.59375 on epoch=324
03/11/2022 03:13:47 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.59375 on epoch=324, global_step=650
03/11/2022 03:13:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=329
03/11/2022 03:13:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=334
03/11/2022 03:13:54 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=339
03/11/2022 03:13:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=344
03/11/2022 03:13:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.23 on epoch=349
03/11/2022 03:13:59 - INFO - __main__ - Global step 700 Train loss 0.22 ACC 0.53125 on epoch=349
03/11/2022 03:14:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=354
03/11/2022 03:14:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
03/11/2022 03:14:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
03/11/2022 03:14:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
03/11/2022 03:14:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=374
03/11/2022 03:14:11 - INFO - __main__ - Global step 750 Train loss 0.23 ACC 0.5 on epoch=374
03/11/2022 03:14:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
03/11/2022 03:14:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=384
03/11/2022 03:14:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=389
03/11/2022 03:14:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
03/11/2022 03:14:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=399
03/11/2022 03:14:22 - INFO - __main__ - Global step 800 Train loss 0.22 ACC 0.5 on epoch=399
03/11/2022 03:14:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
03/11/2022 03:14:27 - INFO - __main__ - Step 820 Global step 820 Train loss 0.23 on epoch=409
03/11/2022 03:14:29 - INFO - __main__ - Step 830 Global step 830 Train loss 0.22 on epoch=414
03/11/2022 03:14:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=419
03/11/2022 03:14:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=424
03/11/2022 03:14:34 - INFO - __main__ - Global step 850 Train loss 0.21 ACC 0.5 on epoch=424
03/11/2022 03:14:37 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=429
03/11/2022 03:14:39 - INFO - __main__ - Step 870 Global step 870 Train loss 0.20 on epoch=434
03/11/2022 03:14:41 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=439
03/11/2022 03:14:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=444
03/11/2022 03:14:46 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=449
03/11/2022 03:14:46 - INFO - __main__ - Global step 900 Train loss 0.18 ACC 0.53125 on epoch=449
03/11/2022 03:14:49 - INFO - __main__ - Step 910 Global step 910 Train loss 0.12 on epoch=454
03/11/2022 03:14:51 - INFO - __main__ - Step 920 Global step 920 Train loss 0.04 on epoch=459
03/11/2022 03:14:53 - INFO - __main__ - Step 930 Global step 930 Train loss 0.03 on epoch=464
03/11/2022 03:14:55 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
03/11/2022 03:14:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.01 on epoch=474
03/11/2022 03:14:58 - INFO - __main__ - Global step 950 Train loss 0.04 ACC 1.0 on epoch=474
03/11/2022 03:14:58 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 1.0 on epoch=474, global_step=950
03/11/2022 03:15:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/11/2022 03:15:03 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/11/2022 03:15:05 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/11/2022 03:15:07 - INFO - __main__ - Step 990 Global step 990 Train loss 0.02 on epoch=494
03/11/2022 03:15:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/11/2022 03:15:10 - INFO - __main__ - Global step 1000 Train loss 0.01 ACC 1.0 on epoch=499
03/11/2022 03:15:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/11/2022 03:15:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/11/2022 03:15:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/11/2022 03:15:19 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.00 on epoch=519
03/11/2022 03:15:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/11/2022 03:15:22 - INFO - __main__ - Global step 1050 Train loss 0.00 ACC 1.0 on epoch=524
03/11/2022 03:15:24 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/11/2022 03:15:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/11/2022 03:15:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/11/2022 03:15:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/11/2022 03:15:33 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/11/2022 03:15:34 - INFO - __main__ - Global step 1100 Train loss 0.00 ACC 1.0 on epoch=549
03/11/2022 03:15:36 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/11/2022 03:15:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/11/2022 03:15:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/11/2022 03:15:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/11/2022 03:15:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/11/2022 03:15:46 - INFO - __main__ - Global step 1150 Train loss 0.00 ACC 1.0 on epoch=574
03/11/2022 03:15:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/11/2022 03:15:50 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/11/2022 03:15:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/11/2022 03:15:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/11/2022 03:15:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/11/2022 03:15:58 - INFO - __main__ - Global step 1200 Train loss 0.00 ACC 1.0 on epoch=599
03/11/2022 03:16:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/11/2022 03:16:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/11/2022 03:16:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/11/2022 03:16:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/11/2022 03:16:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/11/2022 03:16:10 - INFO - __main__ - Global step 1250 Train loss 0.00 ACC 1.0 on epoch=624
03/11/2022 03:16:12 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/11/2022 03:16:14 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 03:16:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/11/2022 03:16:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/11/2022 03:16:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/11/2022 03:16:22 - INFO - __main__ - Global step 1300 Train loss 0.00 ACC 1.0 on epoch=649
03/11/2022 03:16:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/11/2022 03:16:27 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/11/2022 03:16:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.01 on epoch=664
03/11/2022 03:16:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 03:16:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 03:16:34 - INFO - __main__ - Global step 1350 Train loss 0.00 ACC 1.0 on epoch=674
03/11/2022 03:16:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 03:16:39 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 03:16:41 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/11/2022 03:16:43 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 03:16:46 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/11/2022 03:16:46 - INFO - __main__ - Global step 1400 Train loss 0.00 ACC 1.0 on epoch=699
03/11/2022 03:16:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/11/2022 03:16:51 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 03:16:53 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 03:16:55 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 03:16:58 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/11/2022 03:16:58 - INFO - __main__ - Global step 1450 Train loss 0.00 ACC 1.0 on epoch=724
03/11/2022 03:17:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 03:17:03 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 03:17:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 03:17:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 03:17:10 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 03:17:10 - INFO - __main__ - Global step 1500 Train loss 0.00 ACC 1.0 on epoch=749
03/11/2022 03:17:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/11/2022 03:17:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 03:17:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 03:17:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 03:17:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 03:17:22 - INFO - __main__ - Global step 1550 Train loss 0.00 ACC 1.0 on epoch=774
03/11/2022 03:17:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 03:17:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 03:17:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 03:17:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 03:17:34 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 03:17:34 - INFO - __main__ - Global step 1600 Train loss 0.00 ACC 1.0 on epoch=799
03/11/2022 03:17:36 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 03:17:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 03:17:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 03:17:43 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 03:17:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 03:17:46 - INFO - __main__ - Global step 1650 Train loss 0.00 ACC 1.0 on epoch=824
03/11/2022 03:17:48 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 03:17:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 03:17:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 03:17:55 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 03:17:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 03:17:58 - INFO - __main__ - Global step 1700 Train loss 0.00 ACC 1.0 on epoch=849
03/11/2022 03:18:00 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 03:18:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 03:18:05 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 03:18:07 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 03:18:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 03:18:10 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 1.0 on epoch=874
03/11/2022 03:18:12 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 03:18:15 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/11/2022 03:18:17 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 03:18:19 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 03:18:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 03:18:22 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 1.0 on epoch=899
03/11/2022 03:18:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 03:18:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 03:18:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 03:18:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 03:18:34 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 03:18:34 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 1.0 on epoch=924
03/11/2022 03:18:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 03:18:39 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 03:18:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 03:18:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 03:18:46 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 03:18:46 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 1.0 on epoch=949
03/11/2022 03:18:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 03:18:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 03:18:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 03:18:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 03:18:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 03:18:58 - INFO - __main__ - Global step 1950 Train loss 0.00 ACC 1.0 on epoch=974
03/11/2022 03:19:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 03:19:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 03:19:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 03:19:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 03:19:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 03:19:10 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 1.0 on epoch=999
03/11/2022 03:19:10 - INFO - __main__ - save last model!
03/11/2022 03:19:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 03:19:10 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 03:19:10 - INFO - __main__ - Printing 3 examples
03/11/2022 03:19:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 03:19:10 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 03:19:10 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:10 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 03:19:10 - INFO - __main__ - ['sentence 1']
03/11/2022 03:19:10 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:19:10 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:19:10 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 03:19:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:19:11 - INFO - __main__ - Printing 3 examples
03/11/2022 03:19:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/11/2022 03:19:11 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/11/2022 03:19:11 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/11/2022 03:19:11 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 03:19:11 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:19:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:19:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:19:11 - INFO - __main__ - Printing 3 examples
03/11/2022 03:19:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/11/2022 03:19:11 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/11/2022 03:19:11 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:11 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/11/2022 03:19:11 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:11 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:19:11 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:19:11 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:19:14 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_87_0.5_8_predictions.txt
03/11/2022 03:19:14 - INFO - __main__ - ACC on test data: 1.0000
03/11/2022 03:19:14 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.5, bsz=8, dev_performance=1.0, test_performance=1.0
03/11/2022 03:19:14 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.4, bsz=8 ...
03/11/2022 03:19:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:19:15 - INFO - __main__ - Printing 3 examples
03/11/2022 03:19:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/11/2022 03:19:15 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/11/2022 03:19:15 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/11/2022 03:19:15 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 03:19:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:19:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:19:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:19:15 - INFO - __main__ - Printing 3 examples
03/11/2022 03:19:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/11/2022 03:19:15 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/11/2022 03:19:15 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:15 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/11/2022 03:19:15 - INFO - __main__ - ['sentence 2']
03/11/2022 03:19:15 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:19:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:19:15 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:19:25 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:19:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:19:26 - INFO - __main__ - Starting training!
03/11/2022 03:19:28 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:19:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:19:28 - INFO - __main__ - Starting training!
03/11/2022 03:19:31 - INFO - __main__ - Step 10 Global step 10 Train loss 1.01 on epoch=4
03/11/2022 03:19:33 - INFO - __main__ - Step 20 Global step 20 Train loss 0.54 on epoch=9
03/11/2022 03:19:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.40 on epoch=14
03/11/2022 03:19:38 - INFO - __main__ - Step 40 Global step 40 Train loss 0.35 on epoch=19
03/11/2022 03:19:40 - INFO - __main__ - Step 50 Global step 50 Train loss 0.28 on epoch=24
03/11/2022 03:19:41 - INFO - __main__ - Global step 50 Train loss 0.52 ACC 0.5 on epoch=24
03/11/2022 03:19:41 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 03:19:43 - INFO - __main__ - Step 60 Global step 60 Train loss 0.31 on epoch=29
03/11/2022 03:19:45 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
03/11/2022 03:19:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.25 on epoch=39
03/11/2022 03:19:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.27 on epoch=44
03/11/2022 03:19:52 - INFO - __main__ - Step 100 Global step 100 Train loss 0.25 on epoch=49
03/11/2022 03:19:53 - INFO - __main__ - Global step 100 Train loss 0.28 ACC 0.5 on epoch=49
03/11/2022 03:19:55 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=54
03/11/2022 03:19:57 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=59
03/11/2022 03:19:59 - INFO - __main__ - Step 130 Global step 130 Train loss 0.25 on epoch=64
03/11/2022 03:20:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.39 on epoch=69
03/11/2022 03:20:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.50 on epoch=74
03/11/2022 03:20:05 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.5 on epoch=74
03/11/2022 03:20:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
03/11/2022 03:20:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
03/11/2022 03:20:11 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
03/11/2022 03:20:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
03/11/2022 03:20:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=99
03/11/2022 03:20:17 - INFO - __main__ - Global step 200 Train loss 0.29 ACC 0.5 on epoch=99
03/11/2022 03:20:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
03/11/2022 03:20:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 03:20:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
03/11/2022 03:20:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=119
03/11/2022 03:20:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
03/11/2022 03:20:28 - INFO - __main__ - Global step 250 Train loss 0.27 ACC 0.5 on epoch=124
03/11/2022 03:20:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
03/11/2022 03:20:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 03:20:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
03/11/2022 03:20:38 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
03/11/2022 03:20:40 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=149
03/11/2022 03:20:40 - INFO - __main__ - Global step 300 Train loss 0.26 ACC 0.5 on epoch=149
03/11/2022 03:20:43 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
03/11/2022 03:20:45 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=159
03/11/2022 03:20:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=164
03/11/2022 03:20:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
03/11/2022 03:20:52 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
03/11/2022 03:20:52 - INFO - __main__ - Global step 350 Train loss 0.27 ACC 0.5 on epoch=174
03/11/2022 03:20:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
03/11/2022 03:20:57 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
03/11/2022 03:20:59 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
03/11/2022 03:21:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
03/11/2022 03:21:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
03/11/2022 03:21:04 - INFO - __main__ - Global step 400 Train loss 0.26 ACC 0.5 on epoch=199
03/11/2022 03:21:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 03:21:09 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
03/11/2022 03:21:11 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
03/11/2022 03:21:13 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
03/11/2022 03:21:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
03/11/2022 03:21:16 - INFO - __main__ - Global step 450 Train loss 0.25 ACC 0.5625 on epoch=224
03/11/2022 03:21:16 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=224, global_step=450
03/11/2022 03:21:18 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
03/11/2022 03:21:21 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
03/11/2022 03:21:23 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 03:21:25 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
03/11/2022 03:21:27 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
03/11/2022 03:21:28 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5 on epoch=249
03/11/2022 03:21:30 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 03:21:32 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
03/11/2022 03:21:35 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=264
03/11/2022 03:21:37 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
03/11/2022 03:21:39 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
03/11/2022 03:21:40 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.5 on epoch=274
03/11/2022 03:21:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
03/11/2022 03:21:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=284
03/11/2022 03:21:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
03/11/2022 03:21:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/11/2022 03:21:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=299
03/11/2022 03:21:52 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.59375 on epoch=299
03/11/2022 03:21:52 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=299, global_step=600
03/11/2022 03:21:54 - INFO - __main__ - Step 610 Global step 610 Train loss 0.26 on epoch=304
03/11/2022 03:21:56 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
03/11/2022 03:21:58 - INFO - __main__ - Step 630 Global step 630 Train loss 0.26 on epoch=314
03/11/2022 03:22:01 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=319
03/11/2022 03:22:03 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
03/11/2022 03:22:03 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.5 on epoch=324
03/11/2022 03:22:06 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=329
03/11/2022 03:22:08 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
03/11/2022 03:22:10 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
03/11/2022 03:22:12 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=344
03/11/2022 03:22:15 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=349
03/11/2022 03:22:15 - INFO - __main__ - Global step 700 Train loss 0.26 ACC 0.5 on epoch=349
03/11/2022 03:22:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.30 on epoch=354
03/11/2022 03:22:20 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=359
03/11/2022 03:22:22 - INFO - __main__ - Step 730 Global step 730 Train loss 0.27 on epoch=364
03/11/2022 03:22:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=369
03/11/2022 03:22:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=374
03/11/2022 03:22:27 - INFO - __main__ - Global step 750 Train loss 0.27 ACC 0.5625 on epoch=374
03/11/2022 03:22:29 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
03/11/2022 03:22:31 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
03/11/2022 03:22:34 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=389
03/11/2022 03:22:36 - INFO - __main__ - Step 790 Global step 790 Train loss 0.26 on epoch=394
03/11/2022 03:22:38 - INFO - __main__ - Step 800 Global step 800 Train loss 0.27 on epoch=399
03/11/2022 03:22:39 - INFO - __main__ - Global step 800 Train loss 0.26 ACC 0.5 on epoch=399
03/11/2022 03:22:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
03/11/2022 03:22:43 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=409
03/11/2022 03:22:45 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=414
03/11/2022 03:22:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/11/2022 03:22:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=424
03/11/2022 03:22:51 - INFO - __main__ - Global step 850 Train loss 0.25 ACC 0.5 on epoch=424
03/11/2022 03:22:53 - INFO - __main__ - Step 860 Global step 860 Train loss 0.25 on epoch=429
03/11/2022 03:22:55 - INFO - __main__ - Step 870 Global step 870 Train loss 0.25 on epoch=434
03/11/2022 03:22:57 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=439
03/11/2022 03:22:59 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=444
03/11/2022 03:23:02 - INFO - __main__ - Step 900 Global step 900 Train loss 0.25 on epoch=449
03/11/2022 03:23:02 - INFO - __main__ - Global step 900 Train loss 0.25 ACC 0.59375 on epoch=449
03/11/2022 03:23:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=454
03/11/2022 03:23:07 - INFO - __main__ - Step 920 Global step 920 Train loss 0.25 on epoch=459
03/11/2022 03:23:09 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/11/2022 03:23:11 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=469
03/11/2022 03:23:13 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=474
03/11/2022 03:23:14 - INFO - __main__ - Global step 950 Train loss 0.25 ACC 0.5 on epoch=474
03/11/2022 03:23:16 - INFO - __main__ - Step 960 Global step 960 Train loss 0.24 on epoch=479
03/11/2022 03:23:18 - INFO - __main__ - Step 970 Global step 970 Train loss 0.26 on epoch=484
03/11/2022 03:23:21 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=489
03/11/2022 03:23:23 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=494
03/11/2022 03:23:25 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.23 on epoch=499
03/11/2022 03:23:26 - INFO - __main__ - Global step 1000 Train loss 0.24 ACC 0.5 on epoch=499
03/11/2022 03:23:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=504
03/11/2022 03:23:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
03/11/2022 03:23:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.24 on epoch=514
03/11/2022 03:23:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.24 on epoch=519
03/11/2022 03:23:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=524
03/11/2022 03:23:38 - INFO - __main__ - Global step 1050 Train loss 0.24 ACC 0.53125 on epoch=524
03/11/2022 03:23:40 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.27 on epoch=529
03/11/2022 03:23:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=534
03/11/2022 03:23:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=539
03/11/2022 03:23:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/11/2022 03:23:49 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.26 on epoch=549
03/11/2022 03:23:49 - INFO - __main__ - Global step 1100 Train loss 0.25 ACC 0.5 on epoch=549
03/11/2022 03:23:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.25 on epoch=554
03/11/2022 03:23:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=559
03/11/2022 03:23:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.24 on epoch=564
03/11/2022 03:23:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=569
03/11/2022 03:24:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=574
03/11/2022 03:24:01 - INFO - __main__ - Global step 1150 Train loss 0.25 ACC 0.5 on epoch=574
03/11/2022 03:24:04 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
03/11/2022 03:24:06 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=584
03/11/2022 03:24:08 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=589
03/11/2022 03:24:10 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=594
03/11/2022 03:24:13 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=599
03/11/2022 03:24:13 - INFO - __main__ - Global step 1200 Train loss 0.26 ACC 0.65625 on epoch=599
03/11/2022 03:24:13 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 0.65625 on epoch=599, global_step=1200
03/11/2022 03:24:15 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.35 on epoch=604
03/11/2022 03:24:18 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=609
03/11/2022 03:24:20 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.25 on epoch=614
03/11/2022 03:24:22 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=619
03/11/2022 03:24:25 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.25 on epoch=624
03/11/2022 03:24:25 - INFO - __main__ - Global step 1250 Train loss 0.27 ACC 0.5 on epoch=624
03/11/2022 03:24:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.26 on epoch=629
03/11/2022 03:24:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.25 on epoch=634
03/11/2022 03:24:33 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.24 on epoch=639
03/11/2022 03:24:35 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=644
03/11/2022 03:24:37 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/11/2022 03:24:38 - INFO - __main__ - Global step 1300 Train loss 0.25 ACC 0.53125 on epoch=649
03/11/2022 03:24:40 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=654
03/11/2022 03:24:42 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.26 on epoch=659
03/11/2022 03:24:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=664
03/11/2022 03:24:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
03/11/2022 03:24:49 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.25 on epoch=674
03/11/2022 03:24:50 - INFO - __main__ - Global step 1350 Train loss 0.25 ACC 0.5 on epoch=674
03/11/2022 03:24:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=679
03/11/2022 03:24:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=684
03/11/2022 03:24:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=689
03/11/2022 03:24:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=694
03/11/2022 03:25:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.24 on epoch=699
03/11/2022 03:25:02 - INFO - __main__ - Global step 1400 Train loss 0.24 ACC 0.53125 on epoch=699
03/11/2022 03:25:04 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.26 on epoch=704
03/11/2022 03:25:06 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=709
03/11/2022 03:25:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=714
03/11/2022 03:25:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=719
03/11/2022 03:25:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=724
03/11/2022 03:25:14 - INFO - __main__ - Global step 1450 Train loss 0.25 ACC 0.5 on epoch=724
03/11/2022 03:25:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.24 on epoch=729
03/11/2022 03:25:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.23 on epoch=734
03/11/2022 03:25:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=739
03/11/2022 03:25:23 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=744
03/11/2022 03:25:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.23 on epoch=749
03/11/2022 03:25:26 - INFO - __main__ - Global step 1500 Train loss 0.24 ACC 0.5 on epoch=749
03/11/2022 03:25:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.24 on epoch=754
03/11/2022 03:25:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.26 on epoch=759
03/11/2022 03:25:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.26 on epoch=764
03/11/2022 03:25:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=769
03/11/2022 03:25:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.24 on epoch=774
03/11/2022 03:25:37 - INFO - __main__ - Global step 1550 Train loss 0.25 ACC 0.5 on epoch=774
03/11/2022 03:25:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.26 on epoch=779
03/11/2022 03:25:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=784
03/11/2022 03:25:44 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.25 on epoch=789
03/11/2022 03:25:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.26 on epoch=794
03/11/2022 03:25:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
03/11/2022 03:25:49 - INFO - __main__ - Global step 1600 Train loss 0.25 ACC 0.5 on epoch=799
03/11/2022 03:25:52 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.24 on epoch=804
03/11/2022 03:25:54 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=809
03/11/2022 03:25:56 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.26 on epoch=814
03/11/2022 03:25:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.24 on epoch=819
03/11/2022 03:26:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=824
03/11/2022 03:26:01 - INFO - __main__ - Global step 1650 Train loss 0.25 ACC 0.5 on epoch=824
03/11/2022 03:26:03 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
03/11/2022 03:26:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.26 on epoch=834
03/11/2022 03:26:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=839
03/11/2022 03:26:09 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.23 on epoch=844
03/11/2022 03:26:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=849
03/11/2022 03:26:12 - INFO - __main__ - Global step 1700 Train loss 0.24 ACC 0.53125 on epoch=849
03/11/2022 03:26:14 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.23 on epoch=854
03/11/2022 03:26:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.25 on epoch=859
03/11/2022 03:26:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.25 on epoch=864
03/11/2022 03:26:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
03/11/2022 03:26:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.26 on epoch=874
03/11/2022 03:26:24 - INFO - __main__ - Global step 1750 Train loss 0.25 ACC 0.5 on epoch=874
03/11/2022 03:26:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=879
03/11/2022 03:26:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=884
03/11/2022 03:26:30 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.25 on epoch=889
03/11/2022 03:26:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=894
03/11/2022 03:26:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.26 on epoch=899
03/11/2022 03:26:35 - INFO - __main__ - Global step 1800 Train loss 0.24 ACC 0.59375 on epoch=899
03/11/2022 03:26:37 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.24 on epoch=904
03/11/2022 03:26:40 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.24 on epoch=909
03/11/2022 03:26:42 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.24 on epoch=914
03/11/2022 03:26:44 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.25 on epoch=919
03/11/2022 03:26:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.25 on epoch=924
03/11/2022 03:26:47 - INFO - __main__ - Global step 1850 Train loss 0.24 ACC 0.53125 on epoch=924
03/11/2022 03:26:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.25 on epoch=929
03/11/2022 03:26:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.26 on epoch=934
03/11/2022 03:26:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=939
03/11/2022 03:26:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.25 on epoch=944
03/11/2022 03:26:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=949
03/11/2022 03:26:58 - INFO - __main__ - Global step 1900 Train loss 0.25 ACC 0.53125 on epoch=949
03/11/2022 03:27:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.25 on epoch=954
03/11/2022 03:27:03 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=959
03/11/2022 03:27:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.26 on epoch=964
03/11/2022 03:27:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.25 on epoch=969
03/11/2022 03:27:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.25 on epoch=974
03/11/2022 03:27:10 - INFO - __main__ - Global step 1950 Train loss 0.25 ACC 0.5 on epoch=974
03/11/2022 03:27:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.24 on epoch=979
03/11/2022 03:27:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.25 on epoch=984
03/11/2022 03:27:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.24 on epoch=989
03/11/2022 03:27:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.24 on epoch=994
03/11/2022 03:27:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.23 on epoch=999
03/11/2022 03:27:21 - INFO - __main__ - Global step 2000 Train loss 0.24 ACC 0.5 on epoch=999
03/11/2022 03:27:21 - INFO - __main__ - save last model!
03/11/2022 03:27:21 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 03:27:21 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 03:27:21 - INFO - __main__ - Printing 3 examples
03/11/2022 03:27:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 03:27:21 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 03:27:21 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:21 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 03:27:21 - INFO - __main__ - ['sentence 1']
03/11/2022 03:27:21 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:27:21 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:27:22 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 03:27:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:27:23 - INFO - __main__ - Printing 3 examples
03/11/2022 03:27:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/11/2022 03:27:23 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/11/2022 03:27:23 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/11/2022 03:27:23 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 03:27:23 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:27:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:27:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:27:23 - INFO - __main__ - Printing 3 examples
03/11/2022 03:27:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/11/2022 03:27:23 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/11/2022 03:27:23 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:23 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/11/2022 03:27:23 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:23 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:27:23 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:27:23 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:27:25 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_87_0.4_8_predictions.txt
03/11/2022 03:27:25 - INFO - __main__ - ACC on test data: 0.4500
03/11/2022 03:27:25 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.4, bsz=8, dev_performance=0.65625, test_performance=0.45
03/11/2022 03:27:25 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.3, bsz=8 ...
03/11/2022 03:27:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:27:26 - INFO - __main__ - Printing 3 examples
03/11/2022 03:27:26 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/11/2022 03:27:26 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:26 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/11/2022 03:27:26 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:26 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/11/2022 03:27:26 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 03:27:26 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:27:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:27:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:27:26 - INFO - __main__ - Printing 3 examples
03/11/2022 03:27:26 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/11/2022 03:27:26 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:26 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/11/2022 03:27:26 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:26 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/11/2022 03:27:26 - INFO - __main__ - ['sentence 2']
03/11/2022 03:27:26 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:27:26 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:27:26 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:27:36 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:27:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:27:36 - INFO - __main__ - Starting training!
03/11/2022 03:27:39 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:27:39 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:27:39 - INFO - __main__ - Starting training!
03/11/2022 03:27:42 - INFO - __main__ - Step 10 Global step 10 Train loss 1.26 on epoch=4
03/11/2022 03:27:44 - INFO - __main__ - Step 20 Global step 20 Train loss 0.53 on epoch=9
03/11/2022 03:27:47 - INFO - __main__ - Step 30 Global step 30 Train loss 0.36 on epoch=14
03/11/2022 03:27:49 - INFO - __main__ - Step 40 Global step 40 Train loss 0.34 on epoch=19
03/11/2022 03:27:51 - INFO - __main__ - Step 50 Global step 50 Train loss 0.29 on epoch=24
03/11/2022 03:27:52 - INFO - __main__ - Global step 50 Train loss 0.56 ACC 0.5 on epoch=24
03/11/2022 03:27:52 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 03:27:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.33 on epoch=29
03/11/2022 03:27:57 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
03/11/2022 03:27:59 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
03/11/2022 03:28:01 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=44
03/11/2022 03:28:03 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=49
03/11/2022 03:28:04 - INFO - __main__ - Global step 100 Train loss 0.32 ACC 0.5 on epoch=49
03/11/2022 03:28:06 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
03/11/2022 03:28:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=59
03/11/2022 03:28:11 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=64
03/11/2022 03:28:13 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
03/11/2022 03:28:15 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
03/11/2022 03:28:16 - INFO - __main__ - Global step 150 Train loss 0.34 ACC 0.5 on epoch=74
03/11/2022 03:28:18 - INFO - __main__ - Step 160 Global step 160 Train loss 0.31 on epoch=79
03/11/2022 03:28:20 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
03/11/2022 03:28:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.32 on epoch=89
03/11/2022 03:28:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.30 on epoch=94
03/11/2022 03:28:27 - INFO - __main__ - Step 200 Global step 200 Train loss 0.31 on epoch=99
03/11/2022 03:28:27 - INFO - __main__ - Global step 200 Train loss 0.31 ACC 0.5 on epoch=99
03/11/2022 03:28:30 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
03/11/2022 03:28:32 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
03/11/2022 03:28:34 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=114
03/11/2022 03:28:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.31 on epoch=119
03/11/2022 03:28:39 - INFO - __main__ - Step 250 Global step 250 Train loss 0.32 on epoch=124
03/11/2022 03:28:39 - INFO - __main__ - Global step 250 Train loss 0.30 ACC 0.5 on epoch=124
03/11/2022 03:28:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.31 on epoch=129
03/11/2022 03:28:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.30 on epoch=134
03/11/2022 03:28:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
03/11/2022 03:28:48 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=144
03/11/2022 03:28:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=149
03/11/2022 03:28:51 - INFO - __main__ - Global step 300 Train loss 0.29 ACC 0.5 on epoch=149
03/11/2022 03:28:53 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
03/11/2022 03:28:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=159
03/11/2022 03:28:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
03/11/2022 03:29:00 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
03/11/2022 03:29:02 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
03/11/2022 03:29:02 - INFO - __main__ - Global step 350 Train loss 0.27 ACC 0.5 on epoch=174
03/11/2022 03:29:05 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 03:29:07 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
03/11/2022 03:29:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 03:29:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 03:29:14 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
03/11/2022 03:29:14 - INFO - __main__ - Global step 400 Train loss 0.25 ACC 0.5 on epoch=199
03/11/2022 03:29:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.27 on epoch=204
03/11/2022 03:29:19 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=209
03/11/2022 03:29:21 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
03/11/2022 03:29:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
03/11/2022 03:29:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
03/11/2022 03:29:26 - INFO - __main__ - Global step 450 Train loss 0.26 ACC 0.46875 on epoch=224
03/11/2022 03:29:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
03/11/2022 03:29:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.27 on epoch=234
03/11/2022 03:29:33 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
03/11/2022 03:29:35 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=244
03/11/2022 03:29:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
03/11/2022 03:29:38 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.46875 on epoch=249
03/11/2022 03:29:40 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
03/11/2022 03:29:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
03/11/2022 03:29:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.24 on epoch=264
03/11/2022 03:29:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
03/11/2022 03:29:49 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
03/11/2022 03:29:49 - INFO - __main__ - Global step 550 Train loss 0.25 ACC 0.5 on epoch=274
03/11/2022 03:29:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/11/2022 03:29:54 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
03/11/2022 03:29:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/11/2022 03:29:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=294
03/11/2022 03:30:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
03/11/2022 03:30:01 - INFO - __main__ - Global step 600 Train loss 0.25 ACC 0.5 on epoch=299
03/11/2022 03:30:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=304
03/11/2022 03:30:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=309
03/11/2022 03:30:08 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
03/11/2022 03:30:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=319
03/11/2022 03:30:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.25 on epoch=324
03/11/2022 03:30:12 - INFO - __main__ - Global step 650 Train loss 0.25 ACC 0.5625 on epoch=324
03/11/2022 03:30:13 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=324, global_step=650
03/11/2022 03:30:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.23 on epoch=329
03/11/2022 03:30:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=334
03/11/2022 03:30:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=339
03/11/2022 03:30:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=344
03/11/2022 03:30:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
03/11/2022 03:30:24 - INFO - __main__ - Global step 700 Train loss 0.24 ACC 0.46875 on epoch=349
03/11/2022 03:30:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=354
03/11/2022 03:30:29 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
03/11/2022 03:30:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=364
03/11/2022 03:30:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.27 on epoch=369
03/11/2022 03:30:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.22 on epoch=374
03/11/2022 03:30:36 - INFO - __main__ - Global step 750 Train loss 0.24 ACC 0.59375 on epoch=374
03/11/2022 03:30:36 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=374, global_step=750
03/11/2022 03:30:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.26 on epoch=379
03/11/2022 03:30:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
03/11/2022 03:30:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=389
03/11/2022 03:30:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=394
03/11/2022 03:30:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=399
03/11/2022 03:30:47 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.5625 on epoch=399
03/11/2022 03:30:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
03/11/2022 03:30:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=409
03/11/2022 03:30:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.26 on epoch=414
03/11/2022 03:30:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.24 on epoch=419
03/11/2022 03:30:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
03/11/2022 03:30:59 - INFO - __main__ - Global step 850 Train loss 0.24 ACC 0.53125 on epoch=424
03/11/2022 03:31:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=429
03/11/2022 03:31:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=434
03/11/2022 03:31:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
03/11/2022 03:31:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.27 on epoch=444
03/11/2022 03:31:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=449
03/11/2022 03:31:11 - INFO - __main__ - Global step 900 Train loss 0.23 ACC 0.5 on epoch=449
03/11/2022 03:31:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=454
03/11/2022 03:31:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=459
03/11/2022 03:31:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/11/2022 03:31:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=469
03/11/2022 03:31:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=474
03/11/2022 03:31:22 - INFO - __main__ - Global step 950 Train loss 0.24 ACC 0.59375 on epoch=474
03/11/2022 03:31:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=479
03/11/2022 03:31:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=484
03/11/2022 03:31:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=489
03/11/2022 03:31:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=494
03/11/2022 03:31:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=499
03/11/2022 03:31:34 - INFO - __main__ - Global step 1000 Train loss 0.24 ACC 0.5625 on epoch=499
03/11/2022 03:31:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.24 on epoch=504
03/11/2022 03:31:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
03/11/2022 03:31:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=514
03/11/2022 03:31:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=519
03/11/2022 03:31:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.24 on epoch=524
03/11/2022 03:31:45 - INFO - __main__ - Global step 1050 Train loss 0.23 ACC 0.5 on epoch=524
03/11/2022 03:31:48 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
03/11/2022 03:31:50 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=534
03/11/2022 03:31:52 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=539
03/11/2022 03:31:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/11/2022 03:31:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=549
03/11/2022 03:31:57 - INFO - __main__ - Global step 1100 Train loss 0.24 ACC 0.5 on epoch=549
03/11/2022 03:31:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=554
03/11/2022 03:32:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=559
03/11/2022 03:32:04 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.25 on epoch=564
03/11/2022 03:32:06 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=569
03/11/2022 03:32:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.24 on epoch=574
03/11/2022 03:32:09 - INFO - __main__ - Global step 1150 Train loss 0.24 ACC 0.5625 on epoch=574
03/11/2022 03:32:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
03/11/2022 03:32:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=584
03/11/2022 03:32:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=589
03/11/2022 03:32:18 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=594
03/11/2022 03:32:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=599
03/11/2022 03:32:20 - INFO - __main__ - Global step 1200 Train loss 0.23 ACC 0.5 on epoch=599
03/11/2022 03:32:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.23 on epoch=604
03/11/2022 03:32:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=609
03/11/2022 03:32:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.21 on epoch=614
03/11/2022 03:32:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=619
03/11/2022 03:32:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=624
03/11/2022 03:32:32 - INFO - __main__ - Global step 1250 Train loss 0.22 ACC 0.5625 on epoch=624
03/11/2022 03:32:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=629
03/11/2022 03:32:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=634
03/11/2022 03:32:39 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=639
03/11/2022 03:32:41 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=644
03/11/2022 03:32:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.23 on epoch=649
03/11/2022 03:32:44 - INFO - __main__ - Global step 1300 Train loss 0.22 ACC 0.5 on epoch=649
03/11/2022 03:32:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=654
03/11/2022 03:32:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.22 on epoch=659
03/11/2022 03:32:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=664
03/11/2022 03:32:53 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=669
03/11/2022 03:32:55 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=674
03/11/2022 03:32:55 - INFO - __main__ - Global step 1350 Train loss 0.22 ACC 0.53125 on epoch=674
03/11/2022 03:32:58 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.21 on epoch=679
03/11/2022 03:33:00 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.20 on epoch=684
03/11/2022 03:33:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=689
03/11/2022 03:33:05 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.18 on epoch=694
03/11/2022 03:33:07 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=699
03/11/2022 03:33:07 - INFO - __main__ - Global step 1400 Train loss 0.19 ACC 0.53125 on epoch=699
03/11/2022 03:33:10 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=704
03/11/2022 03:33:12 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=709
03/11/2022 03:33:14 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=714
03/11/2022 03:33:16 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=719
03/11/2022 03:33:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=724
03/11/2022 03:33:19 - INFO - __main__ - Global step 1450 Train loss 0.10 ACC 1.0 on epoch=724
03/11/2022 03:33:19 - INFO - __main__ - Saving model with best ACC: 0.59375 -> 1.0 on epoch=724, global_step=1450
03/11/2022 03:33:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
03/11/2022 03:33:24 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.04 on epoch=734
03/11/2022 03:33:26 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
03/11/2022 03:33:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
03/11/2022 03:33:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
03/11/2022 03:33:31 - INFO - __main__ - Global step 1500 Train loss 0.03 ACC 1.0 on epoch=749
03/11/2022 03:33:33 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
03/11/2022 03:33:36 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/11/2022 03:33:38 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/11/2022 03:33:40 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/11/2022 03:33:42 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
03/11/2022 03:33:43 - INFO - __main__ - Global step 1550 Train loss 0.02 ACC 1.0 on epoch=774
03/11/2022 03:33:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
03/11/2022 03:33:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.03 on epoch=784
03/11/2022 03:33:50 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/11/2022 03:33:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
03/11/2022 03:33:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/11/2022 03:33:55 - INFO - __main__ - Global step 1600 Train loss 0.02 ACC 1.0 on epoch=799
03/11/2022 03:33:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 03:33:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
03/11/2022 03:34:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 03:34:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=819
03/11/2022 03:34:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
03/11/2022 03:34:06 - INFO - __main__ - Global step 1650 Train loss 0.02 ACC 1.0 on epoch=824
03/11/2022 03:34:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 03:34:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 03:34:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=839
03/11/2022 03:34:15 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/11/2022 03:34:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 03:34:18 - INFO - __main__ - Global step 1700 Train loss 0.02 ACC 1.0 on epoch=849
03/11/2022 03:34:20 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 03:34:23 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 03:34:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 03:34:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 03:34:29 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 03:34:30 - INFO - __main__ - Global step 1750 Train loss 0.00 ACC 1.0 on epoch=874
03/11/2022 03:34:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 03:34:34 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/11/2022 03:34:37 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
03/11/2022 03:34:39 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 03:34:41 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 03:34:42 - INFO - __main__ - Global step 1800 Train loss 0.00 ACC 1.0 on epoch=899
03/11/2022 03:34:44 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 03:34:46 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 03:34:48 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 03:34:51 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 03:34:53 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 03:34:53 - INFO - __main__ - Global step 1850 Train loss 0.00 ACC 1.0 on epoch=924
03/11/2022 03:34:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 03:34:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 03:35:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 03:35:02 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 03:35:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/11/2022 03:35:05 - INFO - __main__ - Global step 1900 Train loss 0.00 ACC 1.0 on epoch=949
03/11/2022 03:35:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 03:35:10 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 03:35:12 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
03/11/2022 03:35:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 03:35:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 03:35:17 - INFO - __main__ - Global step 1950 Train loss 0.01 ACC 1.0 on epoch=974
03/11/2022 03:35:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 03:35:21 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 03:35:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 03:35:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 03:35:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 03:35:29 - INFO - __main__ - Global step 2000 Train loss 0.00 ACC 1.0 on epoch=999
03/11/2022 03:35:29 - INFO - __main__ - save last model!
03/11/2022 03:35:29 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 03:35:29 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 03:35:29 - INFO - __main__ - Printing 3 examples
03/11/2022 03:35:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 03:35:29 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 03:35:29 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 03:35:29 - INFO - __main__ - ['sentence 1']
03/11/2022 03:35:29 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:35:29 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:35:29 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 03:35:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:35:29 - INFO - __main__ - Printing 3 examples
03/11/2022 03:35:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/11/2022 03:35:29 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/11/2022 03:35:29 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/11/2022 03:35:29 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 03:35:29 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:35:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:35:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:35:29 - INFO - __main__ - Printing 3 examples
03/11/2022 03:35:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/11/2022 03:35:29 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/11/2022 03:35:29 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:29 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/11/2022 03:35:29 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:29 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:35:29 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:35:29 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:35:32 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_87_0.3_8_predictions.txt
03/11/2022 03:35:32 - INFO - __main__ - ACC on test data: 1.0000
03/11/2022 03:35:33 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.3, bsz=8, dev_performance=1.0, test_performance=1.0
03/11/2022 03:35:33 - INFO - __main__ - Running ... prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.2, bsz=8 ...
03/11/2022 03:35:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:35:34 - INFO - __main__ - Printing 3 examples
03/11/2022 03:35:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Lucille does fortunately ever alarm Tanya. [SEP] sentence 2: Lucille does not ever alarm Tanya.
03/11/2022 03:35:34 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Sharon has probably ever compromised. [SEP] sentence 2: Sharon has not ever compromised.
03/11/2022 03:35:34 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Steven had probably ever visited Edward. [SEP] sentence 2: Steven had not ever visited Edward.
03/11/2022 03:35:34 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 03:35:34 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:35:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:35:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:35:34 - INFO - __main__ - Printing 3 examples
03/11/2022 03:35:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: April had fortunately ever protested. [SEP] sentence 2: April had not ever protested.
03/11/2022 03:35:34 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: That guest will fortunately ever climb down most stairs. [SEP] sentence 2: That guest will not ever climb down most stairs.
03/11/2022 03:35:34 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:34 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: These mouths had fortunately ever wasted away. [SEP] sentence 2: These mouths had not ever wasted away.
03/11/2022 03:35:34 - INFO - __main__ - ['sentence 2']
03/11/2022 03:35:34 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:35:34 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:35:34 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:35:42 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:35:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:35:42 - INFO - __main__ - Starting training!
03/11/2022 03:35:46 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:35:47 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:35:47 - INFO - __main__ - Starting training!
03/11/2022 03:35:53 - INFO - __main__ - Step 10 Global step 10 Train loss 1.53 on epoch=4
03/11/2022 03:35:55 - INFO - __main__ - Step 20 Global step 20 Train loss 0.75 on epoch=9
03/11/2022 03:35:57 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=14
03/11/2022 03:35:59 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=19
03/11/2022 03:36:02 - INFO - __main__ - Step 50 Global step 50 Train loss 0.37 on epoch=24
03/11/2022 03:36:02 - INFO - __main__ - Global step 50 Train loss 0.71 ACC 0.5 on epoch=24
03/11/2022 03:36:02 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.5 on epoch=24, global_step=50
03/11/2022 03:36:04 - INFO - __main__ - Step 60 Global step 60 Train loss 0.32 on epoch=29
03/11/2022 03:36:07 - INFO - __main__ - Step 70 Global step 70 Train loss 0.32 on epoch=34
03/11/2022 03:36:09 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
03/11/2022 03:36:11 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=44
03/11/2022 03:36:13 - INFO - __main__ - Step 100 Global step 100 Train loss 0.28 on epoch=49
03/11/2022 03:36:14 - INFO - __main__ - Global step 100 Train loss 0.31 ACC 0.5 on epoch=49
03/11/2022 03:36:16 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
03/11/2022 03:36:18 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
03/11/2022 03:36:21 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
03/11/2022 03:36:23 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=69
03/11/2022 03:36:25 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
03/11/2022 03:36:26 - INFO - __main__ - Global step 150 Train loss 0.29 ACC 0.5 on epoch=74
03/11/2022 03:36:28 - INFO - __main__ - Step 160 Global step 160 Train loss 0.26 on epoch=79
03/11/2022 03:36:30 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
03/11/2022 03:36:32 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/11/2022 03:36:35 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=94
03/11/2022 03:36:37 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/11/2022 03:36:37 - INFO - __main__ - Global step 200 Train loss 0.25 ACC 0.5 on epoch=99
03/11/2022 03:36:40 - INFO - __main__ - Step 210 Global step 210 Train loss 0.24 on epoch=104
03/11/2022 03:36:42 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
03/11/2022 03:36:44 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
03/11/2022 03:36:46 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
03/11/2022 03:36:48 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
03/11/2022 03:36:49 - INFO - __main__ - Global step 250 Train loss 0.25 ACC 0.46875 on epoch=124
03/11/2022 03:36:51 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
03/11/2022 03:36:53 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 03:36:56 - INFO - __main__ - Step 280 Global step 280 Train loss 0.27 on epoch=139
03/11/2022 03:36:58 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
03/11/2022 03:37:00 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=149
03/11/2022 03:37:00 - INFO - __main__ - Global step 300 Train loss 0.25 ACC 0.5 on epoch=149
03/11/2022 03:37:03 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=154
03/11/2022 03:37:05 - INFO - __main__ - Step 320 Global step 320 Train loss 0.22 on epoch=159
03/11/2022 03:37:07 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=164
03/11/2022 03:37:09 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 03:37:11 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
03/11/2022 03:37:12 - INFO - __main__ - Global step 350 Train loss 0.23 ACC 0.5 on epoch=174
03/11/2022 03:37:14 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=179
03/11/2022 03:37:16 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
03/11/2022 03:37:19 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
03/11/2022 03:37:21 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 03:37:23 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 03:37:24 - INFO - __main__ - Global step 400 Train loss 0.24 ACC 0.5 on epoch=199
03/11/2022 03:37:26 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 03:37:28 - INFO - __main__ - Step 420 Global step 420 Train loss 0.20 on epoch=209
03/11/2022 03:37:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=214
03/11/2022 03:37:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
03/11/2022 03:37:35 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
03/11/2022 03:37:35 - INFO - __main__ - Global step 450 Train loss 0.23 ACC 0.5 on epoch=224
03/11/2022 03:37:37 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 03:37:39 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
03/11/2022 03:37:42 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
03/11/2022 03:37:44 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=244
03/11/2022 03:37:46 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=249
03/11/2022 03:37:47 - INFO - __main__ - Global step 500 Train loss 0.25 ACC 0.5 on epoch=249
03/11/2022 03:37:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=254
03/11/2022 03:37:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.32 on epoch=259
03/11/2022 03:37:53 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
03/11/2022 03:37:55 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
03/11/2022 03:37:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
03/11/2022 03:37:58 - INFO - __main__ - Global step 550 Train loss 0.26 ACC 0.5 on epoch=274
03/11/2022 03:38:00 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
03/11/2022 03:38:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
03/11/2022 03:38:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
03/11/2022 03:38:07 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=294
03/11/2022 03:38:09 - INFO - __main__ - Step 600 Global step 600 Train loss 0.22 on epoch=299
03/11/2022 03:38:10 - INFO - __main__ - Global step 600 Train loss 0.23 ACC 0.5 on epoch=299
03/11/2022 03:38:12 - INFO - __main__ - Step 610 Global step 610 Train loss 0.24 on epoch=304
03/11/2022 03:38:14 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
03/11/2022 03:38:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
03/11/2022 03:38:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=319
03/11/2022 03:38:21 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
03/11/2022 03:38:21 - INFO - __main__ - Global step 650 Train loss 0.24 ACC 0.5 on epoch=324
03/11/2022 03:38:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=329
03/11/2022 03:38:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=334
03/11/2022 03:38:28 - INFO - __main__ - Step 680 Global step 680 Train loss 0.24 on epoch=339
03/11/2022 03:38:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=344
03/11/2022 03:38:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=349
03/11/2022 03:38:33 - INFO - __main__ - Global step 700 Train loss 0.25 ACC 0.5 on epoch=349
03/11/2022 03:38:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.24 on epoch=354
03/11/2022 03:38:37 - INFO - __main__ - Step 720 Global step 720 Train loss 0.27 on epoch=359
03/11/2022 03:38:39 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
03/11/2022 03:38:41 - INFO - __main__ - Step 740 Global step 740 Train loss 0.26 on epoch=369
03/11/2022 03:38:43 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 03:38:44 - INFO - __main__ - Global step 750 Train loss 0.25 ACC 0.5 on epoch=374
03/11/2022 03:38:46 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
03/11/2022 03:38:48 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
03/11/2022 03:38:51 - INFO - __main__ - Step 780 Global step 780 Train loss 0.26 on epoch=389
03/11/2022 03:38:53 - INFO - __main__ - Step 790 Global step 790 Train loss 0.25 on epoch=394
03/11/2022 03:38:55 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=399
03/11/2022 03:38:55 - INFO - __main__ - Global step 800 Train loss 0.25 ACC 0.5 on epoch=399
03/11/2022 03:38:58 - INFO - __main__ - Step 810 Global step 810 Train loss 0.25 on epoch=404
03/11/2022 03:39:00 - INFO - __main__ - Step 820 Global step 820 Train loss 0.26 on epoch=409
03/11/2022 03:39:02 - INFO - __main__ - Step 830 Global step 830 Train loss 0.24 on epoch=414
03/11/2022 03:39:04 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/11/2022 03:39:06 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=424
03/11/2022 03:39:07 - INFO - __main__ - Global step 850 Train loss 0.24 ACC 0.5 on epoch=424
03/11/2022 03:39:09 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
03/11/2022 03:39:11 - INFO - __main__ - Step 870 Global step 870 Train loss 0.24 on epoch=434
03/11/2022 03:39:14 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
03/11/2022 03:39:16 - INFO - __main__ - Step 890 Global step 890 Train loss 0.24 on epoch=444
03/11/2022 03:39:18 - INFO - __main__ - Step 900 Global step 900 Train loss 0.22 on epoch=449
03/11/2022 03:39:19 - INFO - __main__ - Global step 900 Train loss 0.23 ACC 0.5 on epoch=449
03/11/2022 03:39:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=454
03/11/2022 03:39:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.24 on epoch=459
03/11/2022 03:39:25 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
03/11/2022 03:39:27 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=469
03/11/2022 03:39:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.25 on epoch=474
03/11/2022 03:39:30 - INFO - __main__ - Global step 950 Train loss 0.23 ACC 0.5 on epoch=474
03/11/2022 03:39:32 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=479
03/11/2022 03:39:34 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=484
03/11/2022 03:39:37 - INFO - __main__ - Step 980 Global step 980 Train loss 0.24 on epoch=489
03/11/2022 03:39:39 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=494
03/11/2022 03:39:41 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.22 on epoch=499
03/11/2022 03:39:42 - INFO - __main__ - Global step 1000 Train loss 0.23 ACC 0.5 on epoch=499
03/11/2022 03:39:44 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=504
03/11/2022 03:39:46 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
03/11/2022 03:39:48 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=514
03/11/2022 03:39:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=519
03/11/2022 03:39:53 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
03/11/2022 03:39:53 - INFO - __main__ - Global step 1050 Train loss 0.23 ACC 0.5625 on epoch=524
03/11/2022 03:39:53 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=524, global_step=1050
03/11/2022 03:39:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.23 on epoch=529
03/11/2022 03:39:58 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=534
03/11/2022 03:40:00 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.23 on epoch=539
03/11/2022 03:40:02 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/11/2022 03:40:04 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.23 on epoch=549
03/11/2022 03:40:05 - INFO - __main__ - Global step 1100 Train loss 0.23 ACC 0.5625 on epoch=549
03/11/2022 03:40:07 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=554
03/11/2022 03:40:09 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=559
03/11/2022 03:40:11 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=564
03/11/2022 03:40:13 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=569
03/11/2022 03:40:16 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=574
03/11/2022 03:40:16 - INFO - __main__ - Global step 1150 Train loss 0.22 ACC 0.5 on epoch=574
03/11/2022 03:40:18 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.24 on epoch=579
03/11/2022 03:40:21 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.25 on epoch=584
03/11/2022 03:40:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.24 on epoch=589
03/11/2022 03:40:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=594
03/11/2022 03:40:27 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=599
03/11/2022 03:40:28 - INFO - __main__ - Global step 1200 Train loss 0.24 ACC 0.5 on epoch=599
03/11/2022 03:40:30 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
03/11/2022 03:40:32 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.23 on epoch=609
03/11/2022 03:40:34 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=614
03/11/2022 03:40:36 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=619
03/11/2022 03:40:39 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=624
03/11/2022 03:40:39 - INFO - __main__ - Global step 1250 Train loss 0.23 ACC 0.5 on epoch=624
03/11/2022 03:40:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=629
03/11/2022 03:40:44 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=634
03/11/2022 03:40:46 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.22 on epoch=639
03/11/2022 03:40:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=644
03/11/2022 03:40:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=649
03/11/2022 03:40:51 - INFO - __main__ - Global step 1300 Train loss 0.23 ACC 0.5 on epoch=649
03/11/2022 03:40:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.23 on epoch=654
03/11/2022 03:40:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=659
03/11/2022 03:40:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=664
03/11/2022 03:41:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.23 on epoch=669
03/11/2022 03:41:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.21 on epoch=674
03/11/2022 03:41:02 - INFO - __main__ - Global step 1350 Train loss 0.23 ACC 0.5 on epoch=674
03/11/2022 03:41:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=679
03/11/2022 03:41:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.22 on epoch=684
03/11/2022 03:41:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.24 on epoch=689
03/11/2022 03:41:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.22 on epoch=694
03/11/2022 03:41:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.23 on epoch=699
03/11/2022 03:41:14 - INFO - __main__ - Global step 1400 Train loss 0.23 ACC 0.5 on epoch=699
03/11/2022 03:41:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=704
03/11/2022 03:41:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=709
03/11/2022 03:41:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=714
03/11/2022 03:41:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=719
03/11/2022 03:41:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.23 on epoch=724
03/11/2022 03:41:25 - INFO - __main__ - Global step 1450 Train loss 0.22 ACC 0.5 on epoch=724
03/11/2022 03:41:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=729
03/11/2022 03:41:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=734
03/11/2022 03:41:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=739
03/11/2022 03:41:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=744
03/11/2022 03:41:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=749
03/11/2022 03:41:37 - INFO - __main__ - Global step 1500 Train loss 0.22 ACC 0.5 on epoch=749
03/11/2022 03:41:39 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=754
03/11/2022 03:41:41 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=759
03/11/2022 03:41:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=764
03/11/2022 03:41:46 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=769
03/11/2022 03:41:48 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.21 on epoch=774
03/11/2022 03:41:49 - INFO - __main__ - Global step 1550 Train loss 0.22 ACC 0.5 on epoch=774
03/11/2022 03:41:51 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.23 on epoch=779
03/11/2022 03:41:53 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=784
03/11/2022 03:41:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.23 on epoch=789
03/11/2022 03:41:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=794
03/11/2022 03:42:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.23 on epoch=799
03/11/2022 03:42:00 - INFO - __main__ - Global step 1600 Train loss 0.22 ACC 0.5 on epoch=799
03/11/2022 03:42:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
03/11/2022 03:42:04 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=809
03/11/2022 03:42:07 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=814
03/11/2022 03:42:09 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=819
03/11/2022 03:42:11 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.25 on epoch=824
03/11/2022 03:42:12 - INFO - __main__ - Global step 1650 Train loss 0.22 ACC 0.5 on epoch=824
03/11/2022 03:42:14 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
03/11/2022 03:42:16 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.23 on epoch=834
03/11/2022 03:42:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.24 on epoch=839
03/11/2022 03:42:20 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.22 on epoch=844
03/11/2022 03:42:23 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.21 on epoch=849
03/11/2022 03:42:23 - INFO - __main__ - Global step 1700 Train loss 0.22 ACC 0.5 on epoch=849
03/11/2022 03:42:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=854
03/11/2022 03:42:28 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
03/11/2022 03:42:30 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=864
03/11/2022 03:42:32 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.21 on epoch=869
03/11/2022 03:42:34 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.22 on epoch=874
03/11/2022 03:42:35 - INFO - __main__ - Global step 1750 Train loss 0.22 ACC 0.5 on epoch=874
03/11/2022 03:42:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.22 on epoch=879
03/11/2022 03:42:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=884
03/11/2022 03:42:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=889
03/11/2022 03:42:44 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=894
03/11/2022 03:42:46 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=899
03/11/2022 03:42:46 - INFO - __main__ - Global step 1800 Train loss 0.22 ACC 0.5 on epoch=899
03/11/2022 03:42:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=904
03/11/2022 03:42:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=909
03/11/2022 03:42:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.20 on epoch=914
03/11/2022 03:42:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=919
03/11/2022 03:42:57 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=924
03/11/2022 03:42:58 - INFO - __main__ - Global step 1850 Train loss 0.21 ACC 0.5 on epoch=924
03/11/2022 03:43:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=929
03/11/2022 03:43:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.22 on epoch=934
03/11/2022 03:43:05 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=939
03/11/2022 03:43:07 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=944
03/11/2022 03:43:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=949
03/11/2022 03:43:10 - INFO - __main__ - Global step 1900 Train loss 0.20 ACC 0.5 on epoch=949
03/11/2022 03:43:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/11/2022 03:43:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.22 on epoch=959
03/11/2022 03:43:16 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=964
03/11/2022 03:43:18 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.21 on epoch=969
03/11/2022 03:43:20 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.21 on epoch=974
03/11/2022 03:43:21 - INFO - __main__ - Global step 1950 Train loss 0.21 ACC 0.5 on epoch=974
03/11/2022 03:43:23 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.19 on epoch=979
03/11/2022 03:43:25 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=984
03/11/2022 03:43:28 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=989
03/11/2022 03:43:30 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.20 on epoch=994
03/11/2022 03:43:32 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.21 on epoch=999
03/11/2022 03:43:33 - INFO - __main__ - Global step 2000 Train loss 0.20 ACC 0.71875 on epoch=999
03/11/2022 03:43:33 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.71875 on epoch=999, global_step=2000
03/11/2022 03:43:33 - INFO - __main__ - save last model!
03/11/2022 03:43:33 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 03:43:33 - INFO - __main__ - Start tokenizing ... 200 instances
03/11/2022 03:43:33 - INFO - __main__ - Printing 3 examples
03/11/2022 03:43:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: The couches did probably ever slip. [SEP] sentence 2: The couches did not ever slip.
03/11/2022 03:43:33 - INFO - __main__ - ['sentence 2']
03/11/2022 03:43:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Deborah can fortunately ever scan some play. [SEP] sentence 2: Deborah can not ever scan some play.
03/11/2022 03:43:33 - INFO - __main__ - ['sentence 2']
03/11/2022 03:43:33 - INFO - __main__ -  [blimp-sentential_negation_npi_licensor_present] sentence 1: Andrew would not ever salute. [SEP] sentence 2: Andrew would really ever salute.
03/11/2022 03:43:33 - INFO - __main__ - ['sentence 1']
03/11/2022 03:43:33 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:43:33 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:43:33 - INFO - __main__ - Loaded 200 examples from test data
03/11/2022 03:43:37 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-blimp-sentential_negation_npi_licensor_present/blimp-sentential_negation_npi_licensor_present_16_87_0.2_8_predictions.txt
03/11/2022 03:43:37 - INFO - __main__ - ACC on test data: 0.7550
03/11/2022 03:43:37 - INFO - __main__ - prefix=blimp-sentential_negation_npi_licensor_present_16_87, lr=0.2, bsz=8, dev_performance=0.71875, test_performance=0.755
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00032448768615722656 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15709", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 9780, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15710", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 9780, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 9780, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15766): No such process
Task: tweet_eval-irony, Checkpoint: models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-fomaml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_fomaml_random.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:24568
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_csn_ca62/none_ua3oyqnw
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=24568
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_csn_ca62/none_ua3oyqnw/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_csn_ca62/none_ua3oyqnw/attempt_0/1/error.json
03/11/2022 03:43:46 - INFO - __main__ - Namespace(task_dir='data/tweet_eval-irony/', task_name='tweet_eval-irony', identifier='T5-large-fomaml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/11/2022 03:43:46 - INFO - __main__ - models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony
Output directory () already exists and is not empty.
03/11/2022 03:43:46 - INFO - __main__ - Namespace(task_dir='data/tweet_eval-irony/', task_name='tweet_eval-irony', identifier='T5-large-fomaml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-fomaml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='4,5')
03/11/2022 03:43:46 - INFO - __main__ - models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony
03/11/2022 03:43:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/11/2022 03:43:46 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/11/2022 03:43:46 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/11/2022 03:43:46 - INFO - __main__ - args.device: cuda:0
03/11/2022 03:43:46 - INFO - __main__ - Using 2 gpus
03/11/2022 03:43:46 - INFO - __main__ - Fine-tuning the following samples: ['tweet_eval-irony_16_100', 'tweet_eval-irony_16_13', 'tweet_eval-irony_16_21', 'tweet_eval-irony_16_42', 'tweet_eval-irony_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/11/2022 03:43:46 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/11/2022 03:43:46 - INFO - __main__ - args.device: cuda:1
03/11/2022 03:43:46 - INFO - __main__ - Using 2 gpus
03/11/2022 03:43:46 - INFO - __main__ - Fine-tuning the following samples: ['tweet_eval-irony_16_100', 'tweet_eval-irony_16_13', 'tweet_eval-irony_16_21', 'tweet_eval-irony_16_42', 'tweet_eval-irony_16_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/11/2022 03:43:55 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_100, lr=0.5, bsz=8 ...
03/11/2022 03:43:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:43:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:43:55 - INFO - __main__ - Printing 3 examples
03/11/2022 03:43:55 - INFO - __main__ - Printing 3 examples
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:43:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 03:43:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:43:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:43:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:43:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:43:55 - INFO - __main__ - Printing 3 examples
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:43:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:43:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:43:55 - INFO - __main__ - Printing 3 examples
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/11/2022 03:43:55 - INFO - __main__ - ['hate']
03/11/2022 03:43:55 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:43:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:43:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:43:56 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:43:56 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:44:11 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:44:11 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:44:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:44:11 - INFO - __main__ - Starting training!
03/11/2022 03:44:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:44:18 - INFO - __main__ - Starting training!
03/11/2022 03:44:23 - INFO - __main__ - Step 10 Global step 10 Train loss 2.29 on epoch=4
03/11/2022 03:44:25 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=9
03/11/2022 03:44:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.52 on epoch=14
03/11/2022 03:44:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=19
03/11/2022 03:44:32 - INFO - __main__ - Step 50 Global step 50 Train loss 0.36 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/11/2022 03:44:33 - INFO - __main__ - Global step 50 Train loss 0.86 Classification-F1 0.2822966507177033 on epoch=24
03/11/2022 03:44:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.2822966507177033 on epoch=24, global_step=50
03/11/2022 03:44:35 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=29
03/11/2022 03:44:37 - INFO - __main__ - Step 70 Global step 70 Train loss 0.34 on epoch=34
03/11/2022 03:44:39 - INFO - __main__ - Step 80 Global step 80 Train loss 0.35 on epoch=39
03/11/2022 03:44:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=44
03/11/2022 03:44:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.32 on epoch=49
03/11/2022 03:44:45 - INFO - __main__ - Global step 100 Train loss 0.33 Classification-F1 0.24444444444444446 on epoch=49
03/11/2022 03:44:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=54
03/11/2022 03:44:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.28 on epoch=59
03/11/2022 03:44:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
03/11/2022 03:44:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.26 on epoch=69
03/11/2022 03:44:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
03/11/2022 03:44:57 - INFO - __main__ - Global step 150 Train loss 0.27 Classification-F1 0.24444444444444446 on epoch=74
03/11/2022 03:44:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
03/11/2022 03:45:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
03/11/2022 03:45:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.27 on epoch=89
03/11/2022 03:45:06 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
03/11/2022 03:45:08 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
03/11/2022 03:45:09 - INFO - __main__ - Global step 200 Train loss 0.28 Classification-F1 0.3571428571428572 on epoch=99
03/11/2022 03:45:09 - INFO - __main__ - Saving model with best Classification-F1: 0.2822966507177033 -> 0.3571428571428572 on epoch=99, global_step=200
03/11/2022 03:45:11 - INFO - __main__ - Step 210 Global step 210 Train loss 0.28 on epoch=104
03/11/2022 03:45:13 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
03/11/2022 03:45:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
03/11/2022 03:45:18 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
03/11/2022 03:45:20 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
03/11/2022 03:45:21 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.2222222222222222 on epoch=124
03/11/2022 03:45:23 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
03/11/2022 03:45:25 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=134
03/11/2022 03:45:27 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
03/11/2022 03:45:30 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
03/11/2022 03:45:32 - INFO - __main__ - Step 300 Global step 300 Train loss 0.23 on epoch=149
03/11/2022 03:45:33 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.3387096774193548 on epoch=149
03/11/2022 03:45:35 - INFO - __main__ - Step 310 Global step 310 Train loss 0.22 on epoch=154
03/11/2022 03:45:37 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
03/11/2022 03:45:39 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=164
03/11/2022 03:45:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
03/11/2022 03:45:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
03/11/2022 03:45:45 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.2222222222222222 on epoch=174
03/11/2022 03:45:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=179
03/11/2022 03:45:49 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=184
03/11/2022 03:45:51 - INFO - __main__ - Step 380 Global step 380 Train loss 0.22 on epoch=189
03/11/2022 03:45:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
03/11/2022 03:45:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=199
03/11/2022 03:45:57 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.2222222222222222 on epoch=199
03/11/2022 03:45:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.21 on epoch=204
03/11/2022 03:46:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
03/11/2022 03:46:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=214
03/11/2022 03:46:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=219
03/11/2022 03:46:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
03/11/2022 03:46:08 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.2566069906223359 on epoch=224
03/11/2022 03:46:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=229
03/11/2022 03:46:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.21 on epoch=234
03/11/2022 03:46:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=239
03/11/2022 03:46:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.20 on epoch=244
03/11/2022 03:46:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
03/11/2022 03:46:20 - INFO - __main__ - Global step 500 Train loss 0.21 Classification-F1 0.2222222222222222 on epoch=249
03/11/2022 03:46:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=254
03/11/2022 03:46:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
03/11/2022 03:46:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.18 on epoch=264
03/11/2022 03:46:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.17 on epoch=269
03/11/2022 03:46:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=274
03/11/2022 03:46:32 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.3373901284651792 on epoch=274
03/11/2022 03:46:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=279
03/11/2022 03:46:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=284
03/11/2022 03:46:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.31 on epoch=289
03/11/2022 03:46:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=294
03/11/2022 03:46:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=299
03/11/2022 03:46:44 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.37006237006237 on epoch=299
03/11/2022 03:46:44 - INFO - __main__ - Saving model with best Classification-F1: 0.3571428571428572 -> 0.37006237006237 on epoch=299, global_step=600
03/11/2022 03:46:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=304
03/11/2022 03:46:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=309
03/11/2022 03:46:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=314
03/11/2022 03:46:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=319
03/11/2022 03:46:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.14 on epoch=324
03/11/2022 03:46:56 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.3810483870967742 on epoch=324
03/11/2022 03:46:56 - INFO - __main__ - Saving model with best Classification-F1: 0.37006237006237 -> 0.3810483870967742 on epoch=324, global_step=650
03/11/2022 03:46:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
03/11/2022 03:47:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
03/11/2022 03:47:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.08 on epoch=339
03/11/2022 03:47:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
03/11/2022 03:47:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.04 on epoch=349
03/11/2022 03:47:08 - INFO - __main__ - Global step 700 Train loss 0.09 Classification-F1 0.3957033957033957 on epoch=349
03/11/2022 03:47:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3810483870967742 -> 0.3957033957033957 on epoch=349, global_step=700
03/11/2022 03:47:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
03/11/2022 03:47:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.07 on epoch=359
03/11/2022 03:47:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.06 on epoch=364
03/11/2022 03:47:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.03 on epoch=369
03/11/2022 03:47:19 - INFO - __main__ - Step 750 Global step 750 Train loss 0.08 on epoch=374
03/11/2022 03:47:20 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.2720588235294118 on epoch=374
03/11/2022 03:47:22 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
03/11/2022 03:47:24 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/11/2022 03:47:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.03 on epoch=389
03/11/2022 03:47:29 - INFO - __main__ - Step 790 Global step 790 Train loss 0.11 on epoch=394
03/11/2022 03:47:31 - INFO - __main__ - Step 800 Global step 800 Train loss 0.04 on epoch=399
03/11/2022 03:47:32 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.3170542635658915 on epoch=399
03/11/2022 03:47:34 - INFO - __main__ - Step 810 Global step 810 Train loss 0.05 on epoch=404
03/11/2022 03:47:36 - INFO - __main__ - Step 820 Global step 820 Train loss 0.03 on epoch=409
03/11/2022 03:47:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.03 on epoch=414
03/11/2022 03:47:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/11/2022 03:47:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.00 on epoch=424
03/11/2022 03:47:44 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.3888888888888889 on epoch=424
03/11/2022 03:47:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.01 on epoch=429
03/11/2022 03:47:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.05 on epoch=434
03/11/2022 03:47:50 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/11/2022 03:47:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/11/2022 03:47:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/11/2022 03:47:56 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.3888888888888889 on epoch=449
03/11/2022 03:47:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/11/2022 03:48:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/11/2022 03:48:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.00 on epoch=464
03/11/2022 03:48:05 - INFO - __main__ - Step 940 Global step 940 Train loss 0.00 on epoch=469
03/11/2022 03:48:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
03/11/2022 03:48:08 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.37915742793791574 on epoch=474
03/11/2022 03:48:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/11/2022 03:48:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
03/11/2022 03:48:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
03/11/2022 03:48:17 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/11/2022 03:48:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/11/2022 03:48:20 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.4147368421052631 on epoch=499
03/11/2022 03:48:20 - INFO - __main__ - Saving model with best Classification-F1: 0.3957033957033957 -> 0.4147368421052631 on epoch=499, global_step=1000
03/11/2022 03:48:22 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/11/2022 03:48:24 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.01 on epoch=509
03/11/2022 03:48:27 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
03/11/2022 03:48:29 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/11/2022 03:48:31 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.00 on epoch=524
03/11/2022 03:48:32 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.37915742793791574 on epoch=524
03/11/2022 03:48:34 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/11/2022 03:48:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/11/2022 03:48:39 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/11/2022 03:48:41 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/11/2022 03:48:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/11/2022 03:48:44 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.2724867724867725 on epoch=549
03/11/2022 03:48:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/11/2022 03:48:49 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/11/2022 03:48:51 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
03/11/2022 03:48:53 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/11/2022 03:48:55 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/11/2022 03:48:57 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.4444444444444444 on epoch=574
03/11/2022 03:48:57 - INFO - __main__ - Saving model with best Classification-F1: 0.4147368421052631 -> 0.4444444444444444 on epoch=574, global_step=1150
03/11/2022 03:48:59 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
03/11/2022 03:49:01 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/11/2022 03:49:03 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/11/2022 03:49:06 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/11/2022 03:49:08 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/11/2022 03:49:09 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.290625 on epoch=599
03/11/2022 03:49:11 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/11/2022 03:49:14 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/11/2022 03:49:16 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.02 on epoch=614
03/11/2022 03:49:18 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/11/2022 03:49:20 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/11/2022 03:49:21 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.37052631578947376 on epoch=624
03/11/2022 03:49:23 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/11/2022 03:49:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 03:49:28 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/11/2022 03:49:30 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/11/2022 03:49:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/11/2022 03:49:34 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.37052631578947376 on epoch=649
03/11/2022 03:49:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/11/2022 03:49:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/11/2022 03:49:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/11/2022 03:49:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 03:49:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 03:49:46 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.40724637681159415 on epoch=674
03/11/2022 03:49:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 03:49:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 03:49:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/11/2022 03:49:55 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/11/2022 03:49:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/11/2022 03:49:58 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.37915742793791574 on epoch=699
03/11/2022 03:50:00 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
03/11/2022 03:50:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 03:50:05 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 03:50:07 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 03:50:09 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/11/2022 03:50:10 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.40724637681159415 on epoch=724
03/11/2022 03:50:12 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 03:50:14 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 03:50:17 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 03:50:19 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 03:50:21 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 03:50:22 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.37052631578947376 on epoch=749
03/11/2022 03:50:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/11/2022 03:50:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 03:50:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 03:50:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 03:50:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 03:50:34 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.30158730158730157 on epoch=774
03/11/2022 03:50:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 03:50:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 03:50:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 03:50:43 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 03:50:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 03:50:46 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.30158730158730157 on epoch=799
03/11/2022 03:50:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 03:50:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 03:50:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 03:50:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 03:50:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 03:50:58 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.3325942350332594 on epoch=824
03/11/2022 03:51:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 03:51:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 03:51:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 03:51:07 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 03:51:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 03:51:10 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.36159420289855077 on epoch=849
03/11/2022 03:51:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 03:51:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 03:51:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 03:51:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.04 on epoch=869
03/11/2022 03:51:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 03:51:22 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.2822966507177033 on epoch=874
03/11/2022 03:51:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 03:51:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 03:51:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 03:51:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 03:51:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 03:51:34 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.3799864773495605 on epoch=899
03/11/2022 03:51:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 03:51:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 03:51:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 03:51:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 03:51:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 03:51:46 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4018817204301075 on epoch=924
03/11/2022 03:51:48 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/11/2022 03:51:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 03:51:53 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 03:51:55 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 03:51:57 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 03:51:58 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4000000000000001 on epoch=949
03/11/2022 03:52:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 03:52:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 03:52:05 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 03:52:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 03:52:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 03:52:10 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.3799864773495605 on epoch=974
03/11/2022 03:52:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 03:52:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 03:52:17 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 03:52:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 03:52:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 03:52:22 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4233870967741935 on epoch=999
03/11/2022 03:52:22 - INFO - __main__ - save last model!
03/11/2022 03:52:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 03:52:22 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 03:52:22 - INFO - __main__ - Printing 3 examples
03/11/2022 03:52:22 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 03:52:22 - INFO - __main__ - ['hate']
03/11/2022 03:52:22 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 03:52:22 - INFO - __main__ - ['non-irony']
03/11/2022 03:52:22 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 03:52:22 - INFO - __main__ - ['hate']
03/11/2022 03:52:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 03:52:22 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:52:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:52:23 - INFO - __main__ - Printing 3 examples
03/11/2022 03:52:23 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/11/2022 03:52:23 - INFO - __main__ - ['hate']
03/11/2022 03:52:23 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/11/2022 03:52:23 - INFO - __main__ - ['hate']
03/11/2022 03:52:23 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/11/2022 03:52:23 - INFO - __main__ - ['hate']
03/11/2022 03:52:23 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 03:52:23 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:52:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:52:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:52:23 - INFO - __main__ - Printing 3 examples
03/11/2022 03:52:23 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/11/2022 03:52:23 - INFO - __main__ - ['hate']
03/11/2022 03:52:23 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/11/2022 03:52:23 - INFO - __main__ - ['hate']
03/11/2022 03:52:23 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/11/2022 03:52:23 - INFO - __main__ - ['hate']
03/11/2022 03:52:23 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:52:23 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:52:23 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:52:23 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 03:52:37 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:52:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:52:38 - INFO - __main__ - Starting training!
03/11/2022 03:52:49 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_100_0.5_8_predictions.txt
03/11/2022 03:52:49 - INFO - __main__ - Classification-F1 on test data: 0.5239
03/11/2022 03:52:50 - INFO - __main__ - prefix=tweet_eval-irony_16_100, lr=0.5, bsz=8, dev_performance=0.4444444444444444, test_performance=0.5238926663855508
03/11/2022 03:52:50 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_100, lr=0.4, bsz=8 ...
03/11/2022 03:52:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:52:51 - INFO - __main__ - Printing 3 examples
03/11/2022 03:52:51 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/11/2022 03:52:51 - INFO - __main__ - ['hate']
03/11/2022 03:52:51 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/11/2022 03:52:51 - INFO - __main__ - ['hate']
03/11/2022 03:52:51 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/11/2022 03:52:51 - INFO - __main__ - ['hate']
03/11/2022 03:52:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 03:52:51 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:52:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 03:52:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 03:52:51 - INFO - __main__ - Printing 3 examples
03/11/2022 03:52:51 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/11/2022 03:52:51 - INFO - __main__ - ['hate']
03/11/2022 03:52:51 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/11/2022 03:52:51 - INFO - __main__ - ['hate']
03/11/2022 03:52:51 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/11/2022 03:52:51 - INFO - __main__ - ['hate']
03/11/2022 03:52:51 - INFO - __main__ - Tokenizing Input ...
03/11/2022 03:52:51 - INFO - __main__ - Tokenizing Output ...
03/11/2022 03:52:51 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 03:53:05 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 03:53:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 03:53:06 - INFO - __main__ - Starting training!
03/11/2022 03:53:09 - INFO - __main__ - Step 10 Global step 10 Train loss 2.72 on epoch=4
03/11/2022 03:53:11 - INFO - __main__ - Step 20 Global step 20 Train loss 0.89 on epoch=9
03/11/2022 03:53:13 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=14
03/11/2022 03:53:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.41 on epoch=19
03/11/2022 03:53:18 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=24
03/11/2022 03:53:20 - INFO - __main__ - Global step 50 Train loss 1.00 Classification-F1 0.24444444444444446 on epoch=24
03/11/2022 03:53:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.24444444444444446 on epoch=24, global_step=50
03/11/2022 03:53:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.45 on epoch=29
03/11/2022 03:53:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.39 on epoch=34
03/11/2022 03:53:27 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=39
03/11/2022 03:53:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.36 on epoch=44
03/11/2022 03:53:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=49
03/11/2022 03:53:32 - INFO - __main__ - Global step 100 Train loss 0.38 Classification-F1 0.29629629629629634 on epoch=49
03/11/2022 03:53:32 - INFO - __main__ - Saving model with best Classification-F1: 0.24444444444444446 -> 0.29629629629629634 on epoch=49, global_step=100
03/11/2022 03:53:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.30 on epoch=54
03/11/2022 03:53:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
03/11/2022 03:53:39 - INFO - __main__ - Step 130 Global step 130 Train loss 0.31 on epoch=64
03/11/2022 03:53:41 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
03/11/2022 03:53:43 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
03/11/2022 03:53:44 - INFO - __main__ - Global step 150 Train loss 0.29 Classification-F1 0.21276595744680848 on epoch=74
03/11/2022 03:53:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
03/11/2022 03:53:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
03/11/2022 03:53:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.31 on epoch=89
03/11/2022 03:53:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=94
03/11/2022 03:53:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.22 on epoch=99
03/11/2022 03:53:56 - INFO - __main__ - Global step 200 Train loss 0.29 Classification-F1 0.2222222222222222 on epoch=99
03/11/2022 03:53:58 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 03:54:00 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=109
03/11/2022 03:54:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
03/11/2022 03:54:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
03/11/2022 03:54:07 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=124
03/11/2022 03:54:08 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.34401709401709396 on epoch=124
03/11/2022 03:54:08 - INFO - __main__ - Saving model with best Classification-F1: 0.29629629629629634 -> 0.34401709401709396 on epoch=124, global_step=250
03/11/2022 03:54:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=129
03/11/2022 03:54:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
03/11/2022 03:54:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.22 on epoch=139
03/11/2022 03:54:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.19 on epoch=144
03/11/2022 03:54:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
03/11/2022 03:54:20 - INFO - __main__ - Global step 300 Train loss 0.23 Classification-F1 0.35959595959595964 on epoch=149
03/11/2022 03:54:20 - INFO - __main__ - Saving model with best Classification-F1: 0.34401709401709396 -> 0.35959595959595964 on epoch=149, global_step=300
03/11/2022 03:54:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 03:54:24 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 03:54:26 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
03/11/2022 03:54:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
03/11/2022 03:54:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
03/11/2022 03:54:32 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.2222222222222222 on epoch=174
03/11/2022 03:54:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=179
03/11/2022 03:54:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 03:54:38 - INFO - __main__ - Step 380 Global step 380 Train loss 0.28 on epoch=189
03/11/2022 03:54:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=194
03/11/2022 03:54:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
03/11/2022 03:54:43 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.2222222222222222 on epoch=199
03/11/2022 03:54:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
03/11/2022 03:54:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
03/11/2022 03:54:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
03/11/2022 03:54:52 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
03/11/2022 03:54:54 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
03/11/2022 03:54:55 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.2222222222222222 on epoch=224
03/11/2022 03:54:57 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 03:55:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 03:55:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=239
03/11/2022 03:55:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=244
03/11/2022 03:55:06 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
03/11/2022 03:55:07 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3159420289855072 on epoch=249
03/11/2022 03:55:09 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 03:55:11 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
03/11/2022 03:55:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
03/11/2022 03:55:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=269
03/11/2022 03:55:18 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
03/11/2022 03:55:19 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.24444444444444446 on epoch=274
03/11/2022 03:55:21 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
03/11/2022 03:55:23 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
03/11/2022 03:55:25 - INFO - __main__ - Step 580 Global step 580 Train loss 0.20 on epoch=289
03/11/2022 03:55:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
03/11/2022 03:55:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
03/11/2022 03:55:31 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.37915742793791574 on epoch=299
03/11/2022 03:55:31 - INFO - __main__ - Saving model with best Classification-F1: 0.35959595959595964 -> 0.37915742793791574 on epoch=299, global_step=600
03/11/2022 03:55:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
03/11/2022 03:55:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=309
03/11/2022 03:55:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=314
03/11/2022 03:55:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=319
03/11/2022 03:55:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=324
03/11/2022 03:55:42 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.34401709401709407 on epoch=324
03/11/2022 03:55:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=329
03/11/2022 03:55:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=334
03/11/2022 03:55:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=339
03/11/2022 03:55:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=344
03/11/2022 03:55:53 - INFO - __main__ - Step 700 Global step 700 Train loss 0.19 on epoch=349
03/11/2022 03:55:54 - INFO - __main__ - Global step 700 Train loss 0.19 Classification-F1 0.3159420289855072 on epoch=349
03/11/2022 03:55:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.23 on epoch=354
03/11/2022 03:55:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=359
03/11/2022 03:56:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=364
03/11/2022 03:56:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=369
03/11/2022 03:56:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.23 on epoch=374
03/11/2022 03:56:06 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.24444444444444446 on epoch=374
03/11/2022 03:56:08 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=379
03/11/2022 03:56:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
03/11/2022 03:56:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=389
03/11/2022 03:56:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=394
03/11/2022 03:56:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=399
03/11/2022 03:56:18 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.30158730158730157 on epoch=399
03/11/2022 03:56:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=404
03/11/2022 03:56:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=409
03/11/2022 03:56:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=414
03/11/2022 03:56:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=419
03/11/2022 03:56:28 - INFO - __main__ - Step 850 Global step 850 Train loss 0.18 on epoch=424
03/11/2022 03:56:29 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.21276595744680848 on epoch=424
03/11/2022 03:56:31 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
03/11/2022 03:56:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=434
03/11/2022 03:56:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=439
03/11/2022 03:56:38 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=444
03/11/2022 03:56:40 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=449
03/11/2022 03:56:41 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.3571428571428572 on epoch=449
03/11/2022 03:56:43 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=454
03/11/2022 03:56:45 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=459
03/11/2022 03:56:47 - INFO - __main__ - Step 930 Global step 930 Train loss 0.21 on epoch=464
03/11/2022 03:56:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=469
03/11/2022 03:56:52 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
03/11/2022 03:56:53 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.29047619047619044 on epoch=474
03/11/2022 03:56:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=479
03/11/2022 03:56:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=484
03/11/2022 03:56:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=489
03/11/2022 03:57:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=494
03/11/2022 03:57:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=499
03/11/2022 03:57:04 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.4225828262339418 on epoch=499
03/11/2022 03:57:04 - INFO - __main__ - Saving model with best Classification-F1: 0.37915742793791574 -> 0.4225828262339418 on epoch=499, global_step=1000
03/11/2022 03:57:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=504
03/11/2022 03:57:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=509
03/11/2022 03:57:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.18 on epoch=514
03/11/2022 03:57:13 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.14 on epoch=519
03/11/2022 03:57:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=524
03/11/2022 03:57:16 - INFO - __main__ - Global step 1050 Train loss 0.17 Classification-F1 0.2222222222222222 on epoch=524
03/11/2022 03:57:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.15 on epoch=529
03/11/2022 03:57:20 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=534
03/11/2022 03:57:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=539
03/11/2022 03:57:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=544
03/11/2022 03:57:27 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=549
03/11/2022 03:57:28 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.29012345679012347 on epoch=549
03/11/2022 03:57:30 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=554
03/11/2022 03:57:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=559
03/11/2022 03:57:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=564
03/11/2022 03:57:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.09 on epoch=569
03/11/2022 03:57:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.10 on epoch=574
03/11/2022 03:57:40 - INFO - __main__ - Global step 1150 Train loss 0.11 Classification-F1 0.3142857142857143 on epoch=574
03/11/2022 03:57:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=579
03/11/2022 03:57:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.11 on epoch=584
03/11/2022 03:57:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.05 on epoch=589
03/11/2022 03:57:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.04 on epoch=594
03/11/2022 03:57:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
03/11/2022 03:57:53 - INFO - __main__ - Global step 1200 Train loss 0.06 Classification-F1 0.29914529914529914 on epoch=599
03/11/2022 03:57:55 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
03/11/2022 03:57:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=609
03/11/2022 03:58:00 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
03/11/2022 03:58:02 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
03/11/2022 03:58:04 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.04 on epoch=624
03/11/2022 03:58:05 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.35204435204435197 on epoch=624
03/11/2022 03:58:07 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
03/11/2022 03:58:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.02 on epoch=634
03/11/2022 03:58:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/11/2022 03:58:14 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.04 on epoch=644
03/11/2022 03:58:17 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
03/11/2022 03:58:17 - INFO - __main__ - Global step 1300 Train loss 0.03 Classification-F1 0.35185185185185186 on epoch=649
03/11/2022 03:58:20 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/11/2022 03:58:22 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/11/2022 03:58:24 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
03/11/2022 03:58:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/11/2022 03:58:29 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/11/2022 03:58:30 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.3799864773495605 on epoch=674
03/11/2022 03:58:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/11/2022 03:58:34 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/11/2022 03:58:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/11/2022 03:58:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/11/2022 03:58:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/11/2022 03:58:42 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.3571428571428572 on epoch=699
03/11/2022 03:58:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/11/2022 03:58:47 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 03:58:49 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/11/2022 03:58:51 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/11/2022 03:58:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/11/2022 03:58:54 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3144016227180528 on epoch=724
03/11/2022 03:58:57 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/11/2022 03:58:59 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 03:59:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.01 on epoch=739
03/11/2022 03:59:04 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/11/2022 03:59:06 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
03/11/2022 03:59:07 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.30838530838530837 on epoch=749
03/11/2022 03:59:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/11/2022 03:59:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 03:59:14 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 03:59:16 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/11/2022 03:59:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
03/11/2022 03:59:19 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.3810483870967742 on epoch=774
03/11/2022 03:59:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 03:59:24 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
03/11/2022 03:59:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/11/2022 03:59:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/11/2022 03:59:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 03:59:31 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.30838530838530837 on epoch=799
03/11/2022 03:59:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 03:59:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 03:59:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 03:59:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 03:59:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 03:59:44 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.3571428571428572 on epoch=824
03/11/2022 03:59:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
03/11/2022 03:59:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 03:59:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
03/11/2022 03:59:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 03:59:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/11/2022 03:59:56 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.2714285714285714 on epoch=849
03/11/2022 03:59:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/11/2022 04:00:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/11/2022 04:00:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/11/2022 04:00:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 04:00:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 04:00:09 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.29479377958079783 on epoch=874
03/11/2022 04:00:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 04:00:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 04:00:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 04:00:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/11/2022 04:00:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 04:00:21 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.3373737373737374 on epoch=899
03/11/2022 04:00:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
03/11/2022 04:00:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 04:00:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 04:00:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 04:00:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 04:00:34 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.3373901284651792 on epoch=924
03/11/2022 04:00:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 04:00:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 04:00:41 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 04:00:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 04:00:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 04:00:46 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.3142857142857143 on epoch=949
03/11/2022 04:00:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 04:00:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 04:00:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 04:00:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 04:00:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 04:00:59 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.26472626472626476 on epoch=974
03/11/2022 04:01:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 04:01:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 04:01:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 04:01:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 04:01:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 04:01:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:01:11 - INFO - __main__ - Printing 3 examples
03/11/2022 04:01:11 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/11/2022 04:01:11 - INFO - __main__ - ['hate']
03/11/2022 04:01:11 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/11/2022 04:01:11 - INFO - __main__ - ['hate']
03/11/2022 04:01:11 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/11/2022 04:01:11 - INFO - __main__ - ['hate']
03/11/2022 04:01:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 04:01:11 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:01:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:01:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:01:11 - INFO - __main__ - Printing 3 examples
03/11/2022 04:01:11 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/11/2022 04:01:11 - INFO - __main__ - ['hate']
03/11/2022 04:01:11 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/11/2022 04:01:11 - INFO - __main__ - ['hate']
03/11/2022 04:01:11 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/11/2022 04:01:11 - INFO - __main__ - ['hate']
03/11/2022 04:01:11 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:01:11 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:01:11 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:01:12 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.2821052631578947 on epoch=999
03/11/2022 04:01:12 - INFO - __main__ - save last model!
03/11/2022 04:01:12 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 04:01:12 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 04:01:12 - INFO - __main__ - Printing 3 examples
03/11/2022 04:01:12 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 04:01:12 - INFO - __main__ - ['hate']
03/11/2022 04:01:12 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 04:01:12 - INFO - __main__ - ['non-irony']
03/11/2022 04:01:12 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 04:01:12 - INFO - __main__ - ['hate']
03/11/2022 04:01:12 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:01:12 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:01:13 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 04:01:24 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:01:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:01:25 - INFO - __main__ - Starting training!
03/11/2022 04:01:53 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_100_0.4_8_predictions.txt
03/11/2022 04:01:53 - INFO - __main__ - Classification-F1 on test data: 0.5391
03/11/2022 04:01:53 - INFO - __main__ - prefix=tweet_eval-irony_16_100, lr=0.4, bsz=8, dev_performance=0.4225828262339418, test_performance=0.5390746827979103
03/11/2022 04:01:53 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_100, lr=0.3, bsz=8 ...
03/11/2022 04:01:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:01:54 - INFO - __main__ - Printing 3 examples
03/11/2022 04:01:54 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/11/2022 04:01:54 - INFO - __main__ - ['hate']
03/11/2022 04:01:54 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/11/2022 04:01:54 - INFO - __main__ - ['hate']
03/11/2022 04:01:54 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/11/2022 04:01:54 - INFO - __main__ - ['hate']
03/11/2022 04:01:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:01:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:01:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:01:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:01:54 - INFO - __main__ - Printing 3 examples
03/11/2022 04:01:54 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/11/2022 04:01:54 - INFO - __main__ - ['hate']
03/11/2022 04:01:54 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/11/2022 04:01:54 - INFO - __main__ - ['hate']
03/11/2022 04:01:54 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/11/2022 04:01:54 - INFO - __main__ - ['hate']
03/11/2022 04:01:54 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:01:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:01:54 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:02:07 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:02:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:02:08 - INFO - __main__ - Starting training!
03/11/2022 04:02:10 - INFO - __main__ - Step 10 Global step 10 Train loss 2.72 on epoch=4
03/11/2022 04:02:13 - INFO - __main__ - Step 20 Global step 20 Train loss 1.06 on epoch=9
03/11/2022 04:02:15 - INFO - __main__ - Step 30 Global step 30 Train loss 0.66 on epoch=14
03/11/2022 04:02:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.53 on epoch=19
03/11/2022 04:02:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.43 on epoch=24
03/11/2022 04:02:22 - INFO - __main__ - Global step 50 Train loss 1.08 Classification-F1 0.3263157894736842 on epoch=24
03/11/2022 04:02:22 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3263157894736842 on epoch=24, global_step=50
03/11/2022 04:02:24 - INFO - __main__ - Step 60 Global step 60 Train loss 0.43 on epoch=29
03/11/2022 04:02:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.41 on epoch=34
03/11/2022 04:02:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
03/11/2022 04:02:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.36 on epoch=44
03/11/2022 04:02:33 - INFO - __main__ - Step 100 Global step 100 Train loss 0.29 on epoch=49
03/11/2022 04:02:34 - INFO - __main__ - Global step 100 Train loss 0.36 Classification-F1 0.26861702127659576 on epoch=49
03/11/2022 04:02:36 - INFO - __main__ - Step 110 Global step 110 Train loss 0.34 on epoch=54
03/11/2022 04:02:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
03/11/2022 04:02:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
03/11/2022 04:02:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
03/11/2022 04:02:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=74
03/11/2022 04:02:46 - INFO - __main__ - Global step 150 Train loss 0.33 Classification-F1 0.24444444444444446 on epoch=74
03/11/2022 04:02:49 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
03/11/2022 04:02:51 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
03/11/2022 04:02:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
03/11/2022 04:02:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
03/11/2022 04:02:58 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
03/11/2022 04:02:58 - INFO - __main__ - Global step 200 Train loss 0.28 Classification-F1 0.2222222222222222 on epoch=99
03/11/2022 04:03:01 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
03/11/2022 04:03:03 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 04:03:05 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=114
03/11/2022 04:03:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.28 on epoch=119
03/11/2022 04:03:10 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=124
03/11/2022 04:03:10 - INFO - __main__ - Global step 250 Train loss 0.27 Classification-F1 0.24444444444444446 on epoch=124
03/11/2022 04:03:13 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=129
03/11/2022 04:03:15 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
03/11/2022 04:03:17 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
03/11/2022 04:03:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
03/11/2022 04:03:22 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
03/11/2022 04:03:22 - INFO - __main__ - Global step 300 Train loss 0.25 Classification-F1 0.3325942350332594 on epoch=149
03/11/2022 04:03:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3263157894736842 -> 0.3325942350332594 on epoch=149, global_step=300
03/11/2022 04:03:25 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=154
03/11/2022 04:03:27 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=159
03/11/2022 04:03:29 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
03/11/2022 04:03:31 - INFO - __main__ - Step 340 Global step 340 Train loss 0.21 on epoch=169
03/11/2022 04:03:34 - INFO - __main__ - Step 350 Global step 350 Train loss 0.21 on epoch=174
03/11/2022 04:03:34 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.29479377958079783 on epoch=174
03/11/2022 04:03:37 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=179
03/11/2022 04:03:39 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=184
03/11/2022 04:03:41 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
03/11/2022 04:03:43 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
03/11/2022 04:03:46 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
03/11/2022 04:03:46 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.24444444444444446 on epoch=199
03/11/2022 04:03:49 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=204
03/11/2022 04:03:51 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
03/11/2022 04:03:53 - INFO - __main__ - Step 430 Global step 430 Train loss 0.20 on epoch=214
03/11/2022 04:03:55 - INFO - __main__ - Step 440 Global step 440 Train loss 0.20 on epoch=219
03/11/2022 04:03:58 - INFO - __main__ - Step 450 Global step 450 Train loss 0.23 on epoch=224
03/11/2022 04:03:58 - INFO - __main__ - Global step 450 Train loss 0.21 Classification-F1 0.22106722106722107 on epoch=224
03/11/2022 04:04:01 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
03/11/2022 04:04:03 - INFO - __main__ - Step 470 Global step 470 Train loss 0.18 on epoch=234
03/11/2022 04:04:05 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=239
03/11/2022 04:04:07 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
03/11/2022 04:04:10 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
03/11/2022 04:04:10 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.3571428571428572 on epoch=249
03/11/2022 04:04:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3325942350332594 -> 0.3571428571428572 on epoch=249, global_step=500
03/11/2022 04:04:13 - INFO - __main__ - Step 510 Global step 510 Train loss 0.20 on epoch=254
03/11/2022 04:04:15 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=259
03/11/2022 04:04:17 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=264
03/11/2022 04:04:19 - INFO - __main__ - Step 540 Global step 540 Train loss 0.26 on epoch=269
03/11/2022 04:04:22 - INFO - __main__ - Step 550 Global step 550 Train loss 0.17 on epoch=274
03/11/2022 04:04:22 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.24444444444444446 on epoch=274
03/11/2022 04:04:25 - INFO - __main__ - Step 560 Global step 560 Train loss 0.25 on epoch=279
03/11/2022 04:04:27 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=284
03/11/2022 04:04:29 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=289
03/11/2022 04:04:32 - INFO - __main__ - Step 590 Global step 590 Train loss 0.17 on epoch=294
03/11/2022 04:04:34 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=299
03/11/2022 04:04:35 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.3373901284651792 on epoch=299
03/11/2022 04:04:37 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
03/11/2022 04:04:39 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=309
03/11/2022 04:04:41 - INFO - __main__ - Step 630 Global step 630 Train loss 0.19 on epoch=314
03/11/2022 04:04:44 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=319
03/11/2022 04:04:46 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=324
03/11/2022 04:04:47 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.3333333333333333 on epoch=324
03/11/2022 04:04:49 - INFO - __main__ - Step 660 Global step 660 Train loss 0.14 on epoch=329
03/11/2022 04:04:51 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
03/11/2022 04:04:53 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=339
03/11/2022 04:04:56 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=344
03/11/2022 04:04:58 - INFO - __main__ - Step 700 Global step 700 Train loss 0.15 on epoch=349
03/11/2022 04:04:59 - INFO - __main__ - Global step 700 Train loss 0.16 Classification-F1 0.3761904761904762 on epoch=349
03/11/2022 04:04:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3571428571428572 -> 0.3761904761904762 on epoch=349, global_step=700
03/11/2022 04:05:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.13 on epoch=354
03/11/2022 04:05:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.17 on epoch=359
03/11/2022 04:05:06 - INFO - __main__ - Step 730 Global step 730 Train loss 0.17 on epoch=364
03/11/2022 04:05:08 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
03/11/2022 04:05:10 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=374
03/11/2022 04:05:11 - INFO - __main__ - Global step 750 Train loss 0.14 Classification-F1 0.29964912280701755 on epoch=374
03/11/2022 04:05:13 - INFO - __main__ - Step 760 Global step 760 Train loss 0.15 on epoch=379
03/11/2022 04:05:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.14 on epoch=384
03/11/2022 04:05:18 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=389
03/11/2022 04:05:20 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=394
03/11/2022 04:05:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
03/11/2022 04:05:23 - INFO - __main__ - Global step 800 Train loss 0.14 Classification-F1 0.4865591397849462 on epoch=399
03/11/2022 04:05:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3761904761904762 -> 0.4865591397849462 on epoch=399, global_step=800
03/11/2022 04:05:25 - INFO - __main__ - Step 810 Global step 810 Train loss 0.12 on epoch=404
03/11/2022 04:05:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=409
03/11/2022 04:05:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.06 on epoch=414
03/11/2022 04:05:32 - INFO - __main__ - Step 840 Global step 840 Train loss 0.04 on epoch=419
03/11/2022 04:05:34 - INFO - __main__ - Step 850 Global step 850 Train loss 0.08 on epoch=424
03/11/2022 04:05:35 - INFO - __main__ - Global step 850 Train loss 0.08 Classification-F1 0.30158730158730157 on epoch=424
03/11/2022 04:05:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.13 on epoch=429
03/11/2022 04:05:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
03/11/2022 04:05:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.04 on epoch=439
03/11/2022 04:05:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.05 on epoch=444
03/11/2022 04:05:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.03 on epoch=449
03/11/2022 04:05:47 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.419753086419753 on epoch=449
03/11/2022 04:05:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.04 on epoch=454
03/11/2022 04:05:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.03 on epoch=459
03/11/2022 04:05:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.06 on epoch=464
03/11/2022 04:05:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.02 on epoch=469
03/11/2022 04:05:59 - INFO - __main__ - Step 950 Global step 950 Train loss 0.03 on epoch=474
03/11/2022 04:06:00 - INFO - __main__ - Global step 950 Train loss 0.03 Classification-F1 0.3810483870967742 on epoch=474
03/11/2022 04:06:02 - INFO - __main__ - Step 960 Global step 960 Train loss 0.07 on epoch=479
03/11/2022 04:06:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.03 on epoch=484
03/11/2022 04:06:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.04 on epoch=489
03/11/2022 04:06:09 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=494
03/11/2022 04:06:11 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
03/11/2022 04:06:12 - INFO - __main__ - Global step 1000 Train loss 0.05 Classification-F1 0.3325942350332594 on epoch=499
03/11/2022 04:06:14 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
03/11/2022 04:06:16 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.03 on epoch=509
03/11/2022 04:06:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.01 on epoch=514
03/11/2022 04:06:21 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.04 on epoch=519
03/11/2022 04:06:23 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.03 on epoch=524
03/11/2022 04:06:24 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.419753086419753 on epoch=524
03/11/2022 04:06:26 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
03/11/2022 04:06:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/11/2022 04:06:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.02 on epoch=539
03/11/2022 04:06:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.01 on epoch=544
03/11/2022 04:06:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
03/11/2022 04:06:36 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.402020202020202 on epoch=549
03/11/2022 04:06:39 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/11/2022 04:06:41 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/11/2022 04:06:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/11/2022 04:06:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/11/2022 04:06:48 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/11/2022 04:06:49 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.3595430107526882 on epoch=574
03/11/2022 04:06:51 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/11/2022 04:06:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/11/2022 04:06:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/11/2022 04:06:58 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/11/2022 04:07:00 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
03/11/2022 04:07:02 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.4428571428571429 on epoch=599
03/11/2022 04:07:04 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/11/2022 04:07:06 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/11/2022 04:07:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/11/2022 04:07:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/11/2022 04:07:13 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
03/11/2022 04:07:15 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.37052631578947376 on epoch=624
03/11/2022 04:07:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
03/11/2022 04:07:19 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.01 on epoch=634
03/11/2022 04:07:21 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/11/2022 04:07:23 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/11/2022 04:07:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
03/11/2022 04:07:27 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.402020202020202 on epoch=649
03/11/2022 04:07:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/11/2022 04:07:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/11/2022 04:07:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
03/11/2022 04:07:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 04:07:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 04:07:39 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.402020202020202 on epoch=674
03/11/2022 04:07:42 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 04:07:44 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 04:07:46 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/11/2022 04:07:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/11/2022 04:07:51 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/11/2022 04:07:52 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.4000000000000001 on epoch=699
03/11/2022 04:07:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/11/2022 04:07:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 04:07:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/11/2022 04:08:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 04:08:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/11/2022 04:08:05 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.37654320987654316 on epoch=724
03/11/2022 04:08:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 04:08:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 04:08:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 04:08:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 04:08:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 04:08:18 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.3810483870967742 on epoch=749
03/11/2022 04:08:20 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/11/2022 04:08:22 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 04:08:24 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 04:08:27 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/11/2022 04:08:29 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 04:08:30 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.34920634920634924 on epoch=774
03/11/2022 04:08:32 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
03/11/2022 04:08:34 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 04:08:37 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 04:08:39 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 04:08:41 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 04:08:43 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.35204435204435197 on epoch=799
03/11/2022 04:08:45 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 04:08:47 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=809
03/11/2022 04:08:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 04:08:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 04:08:54 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 04:08:55 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.2724867724867725 on epoch=824
03/11/2022 04:08:57 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 04:09:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/11/2022 04:09:02 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 04:09:04 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/11/2022 04:09:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 04:09:08 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.4000000000000001 on epoch=849
03/11/2022 04:09:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 04:09:12 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/11/2022 04:09:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 04:09:17 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 04:09:19 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 04:09:20 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.419753086419753 on epoch=874
03/11/2022 04:09:23 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 04:09:25 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 04:09:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 04:09:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 04:09:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.03 on epoch=899
03/11/2022 04:09:34 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.28991596638655465 on epoch=899
03/11/2022 04:09:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 04:09:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.02 on epoch=909
03/11/2022 04:09:41 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 04:09:43 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 04:09:45 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 04:09:47 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.3810483870967742 on epoch=924
03/11/2022 04:09:49 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 04:09:51 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 04:09:54 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/11/2022 04:09:56 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 04:09:58 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 04:09:59 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.36159420289855077 on epoch=949
03/11/2022 04:10:01 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/11/2022 04:10:04 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 04:10:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
03/11/2022 04:10:08 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 04:10:11 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 04:10:12 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.37052631578947376 on epoch=974
03/11/2022 04:10:14 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 04:10:16 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 04:10:18 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 04:10:21 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
03/11/2022 04:10:23 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 04:10:24 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.34401709401709407 on epoch=999
03/11/2022 04:10:24 - INFO - __main__ - save last model!
03/11/2022 04:10:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:10:24 - INFO - __main__ - Printing 3 examples
03/11/2022 04:10:24 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/11/2022 04:10:24 - INFO - __main__ - ['hate']
03/11/2022 04:10:24 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/11/2022 04:10:24 - INFO - __main__ - ['hate']
03/11/2022 04:10:24 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/11/2022 04:10:24 - INFO - __main__ - ['hate']
03/11/2022 04:10:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 04:10:24 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:10:24 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 04:10:24 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 04:10:24 - INFO - __main__ - Printing 3 examples
03/11/2022 04:10:24 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 04:10:24 - INFO - __main__ - ['hate']
03/11/2022 04:10:24 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 04:10:24 - INFO - __main__ - ['non-irony']
03/11/2022 04:10:24 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 04:10:24 - INFO - __main__ - ['hate']
03/11/2022 04:10:24 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:10:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:10:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:10:24 - INFO - __main__ - Printing 3 examples
03/11/2022 04:10:24 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/11/2022 04:10:24 - INFO - __main__ - ['hate']
03/11/2022 04:10:24 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/11/2022 04:10:24 - INFO - __main__ - ['hate']
03/11/2022 04:10:24 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/11/2022 04:10:24 - INFO - __main__ - ['hate']
03/11/2022 04:10:24 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:10:24 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:10:24 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:10:25 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:10:26 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 04:10:37 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:10:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:10:38 - INFO - __main__ - Starting training!
03/11/2022 04:11:15 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_100_0.3_8_predictions.txt
03/11/2022 04:11:15 - INFO - __main__ - Classification-F1 on test data: 0.2029
03/11/2022 04:11:15 - INFO - __main__ - prefix=tweet_eval-irony_16_100, lr=0.3, bsz=8, dev_performance=0.4865591397849462, test_performance=0.2028504332334602
03/11/2022 04:11:15 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_100, lr=0.2, bsz=8 ...
03/11/2022 04:11:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:11:16 - INFO - __main__ - Printing 3 examples
03/11/2022 04:11:16 - INFO - __main__ -  [tweet_eval-irony] .@NSRoadsPolicing @user oh look an Audi driver breaking the law...how strange
03/11/2022 04:11:16 - INFO - __main__ - ['hate']
03/11/2022 04:11:16 - INFO - __main__ -  [tweet_eval-irony] I have such a loving family
03/11/2022 04:11:16 - INFO - __main__ - ['hate']
03/11/2022 04:11:16 - INFO - __main__ -  [tweet_eval-irony] @user funny joke there bae...
03/11/2022 04:11:16 - INFO - __main__ - ['hate']
03/11/2022 04:11:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:11:16 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:11:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:11:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:11:16 - INFO - __main__ - Printing 3 examples
03/11/2022 04:11:16 - INFO - __main__ -  [tweet_eval-irony] The worst is when they consider things like "conservative tribune" as a credible source of information. Totally not bias.
03/11/2022 04:11:16 - INFO - __main__ - ['hate']
03/11/2022 04:11:16 - INFO - __main__ -  [tweet_eval-irony] Come to jhb to play volleyball on a fake beach  #work
03/11/2022 04:11:16 - INFO - __main__ - ['hate']
03/11/2022 04:11:16 - INFO - __main__ -  [tweet_eval-irony] Damit, this fatima bhutto has an instagram account but not pics of her. Some random shit...and then ppl i follow keep posting pics.
03/11/2022 04:11:16 - INFO - __main__ - ['hate']
03/11/2022 04:11:16 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:11:16 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:11:16 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:11:28 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:11:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:11:29 - INFO - __main__ - Starting training!
03/11/2022 04:11:32 - INFO - __main__ - Step 10 Global step 10 Train loss 3.27 on epoch=4
03/11/2022 04:11:34 - INFO - __main__ - Step 20 Global step 20 Train loss 1.91 on epoch=9
03/11/2022 04:11:36 - INFO - __main__ - Step 30 Global step 30 Train loss 1.09 on epoch=14
03/11/2022 04:11:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.67 on epoch=19
03/11/2022 04:11:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.65 on epoch=24
03/11/2022 04:11:42 - INFO - __main__ - Global step 50 Train loss 1.52 Classification-F1 0.33260869565217394 on epoch=24
03/11/2022 04:11:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.33260869565217394 on epoch=24, global_step=50
03/11/2022 04:11:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
03/11/2022 04:11:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=34
03/11/2022 04:11:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
03/11/2022 04:11:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.42 on epoch=44
03/11/2022 04:11:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.41 on epoch=49
03/11/2022 04:11:54 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.26861702127659576 on epoch=49
03/11/2022 04:11:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
03/11/2022 04:11:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=59
03/11/2022 04:12:01 - INFO - __main__ - Step 130 Global step 130 Train loss 0.35 on epoch=64
03/11/2022 04:12:03 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=69
03/11/2022 04:12:05 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=74
03/11/2022 04:12:06 - INFO - __main__ - Global step 150 Train loss 0.36 Classification-F1 0.2222222222222222 on epoch=74
03/11/2022 04:12:08 - INFO - __main__ - Step 160 Global step 160 Train loss 0.31 on epoch=79
03/11/2022 04:12:10 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
03/11/2022 04:12:13 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
03/11/2022 04:12:15 - INFO - __main__ - Step 190 Global step 190 Train loss 0.36 on epoch=94
03/11/2022 04:12:17 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=99
03/11/2022 04:12:18 - INFO - __main__ - Global step 200 Train loss 0.32 Classification-F1 0.2222222222222222 on epoch=99
03/11/2022 04:12:20 - INFO - __main__ - Step 210 Global step 210 Train loss 0.28 on epoch=104
03/11/2022 04:12:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
03/11/2022 04:12:25 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=114
03/11/2022 04:12:27 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=119
03/11/2022 04:12:29 - INFO - __main__ - Step 250 Global step 250 Train loss 0.25 on epoch=124
03/11/2022 04:12:30 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.2222222222222222 on epoch=124
03/11/2022 04:12:32 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=129
03/11/2022 04:12:35 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
03/11/2022 04:12:37 - INFO - __main__ - Step 280 Global step 280 Train loss 0.31 on epoch=139
03/11/2022 04:12:39 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=144
03/11/2022 04:12:41 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
03/11/2022 04:12:42 - INFO - __main__ - Global step 300 Train loss 0.29 Classification-F1 0.29479377958079783 on epoch=149
03/11/2022 04:12:45 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=154
03/11/2022 04:12:47 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 04:12:49 - INFO - __main__ - Step 330 Global step 330 Train loss 0.23 on epoch=164
03/11/2022 04:12:51 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
03/11/2022 04:12:54 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
03/11/2022 04:12:54 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.3325358851674641 on epoch=174
03/11/2022 04:12:57 - INFO - __main__ - Step 360 Global step 360 Train loss 0.22 on epoch=179
03/11/2022 04:12:59 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
03/11/2022 04:13:01 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=189
03/11/2022 04:13:04 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=194
03/11/2022 04:13:06 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
03/11/2022 04:13:07 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.2222222222222222 on epoch=199
03/11/2022 04:13:09 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=204
03/11/2022 04:13:11 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
03/11/2022 04:13:13 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=214
03/11/2022 04:13:16 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
03/11/2022 04:13:18 - INFO - __main__ - Step 450 Global step 450 Train loss 0.24 on epoch=224
03/11/2022 04:13:19 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.2822966507177033 on epoch=224
03/11/2022 04:13:21 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=229
03/11/2022 04:13:23 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=234
03/11/2022 04:13:26 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 04:13:28 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=244
03/11/2022 04:13:30 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
03/11/2022 04:13:31 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.2222222222222222 on epoch=249
03/11/2022 04:13:33 - INFO - __main__ - Step 510 Global step 510 Train loss 0.27 on epoch=254
03/11/2022 04:13:36 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
03/11/2022 04:13:38 - INFO - __main__ - Step 530 Global step 530 Train loss 0.25 on epoch=264
03/11/2022 04:13:40 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=269
03/11/2022 04:13:42 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=274
03/11/2022 04:13:43 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.24444444444444446 on epoch=274
03/11/2022 04:13:45 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
03/11/2022 04:13:48 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=284
03/11/2022 04:13:50 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
03/11/2022 04:13:52 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=294
03/11/2022 04:13:55 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
03/11/2022 04:13:55 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.29914529914529914 on epoch=299
03/11/2022 04:13:58 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=304
03/11/2022 04:14:00 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
03/11/2022 04:14:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
03/11/2022 04:14:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=319
03/11/2022 04:14:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
03/11/2022 04:14:08 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.24444444444444446 on epoch=324
03/11/2022 04:14:10 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=329
03/11/2022 04:14:12 - INFO - __main__ - Step 670 Global step 670 Train loss 0.20 on epoch=334
03/11/2022 04:14:14 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=339
03/11/2022 04:14:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.27 on epoch=344
03/11/2022 04:14:19 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
03/11/2022 04:14:20 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.2822966507177033 on epoch=349
03/11/2022 04:14:22 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=354
03/11/2022 04:14:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=359
03/11/2022 04:14:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
03/11/2022 04:14:29 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=369
03/11/2022 04:14:31 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=374
03/11/2022 04:14:32 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.24444444444444446 on epoch=374
03/11/2022 04:14:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.23 on epoch=379
03/11/2022 04:14:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
03/11/2022 04:14:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=389
03/11/2022 04:14:41 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=394
03/11/2022 04:14:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.25 on epoch=399
03/11/2022 04:14:44 - INFO - __main__ - Global step 800 Train loss 0.23 Classification-F1 0.25252525252525254 on epoch=399
03/11/2022 04:14:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
03/11/2022 04:14:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=409
03/11/2022 04:14:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.25 on epoch=414
03/11/2022 04:14:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.25 on epoch=419
03/11/2022 04:14:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=424
03/11/2022 04:14:56 - INFO - __main__ - Global step 850 Train loss 0.23 Classification-F1 0.26821705426356585 on epoch=424
03/11/2022 04:14:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
03/11/2022 04:15:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=434
03/11/2022 04:15:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.25 on epoch=439
03/11/2022 04:15:04 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=444
03/11/2022 04:15:06 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=449
03/11/2022 04:15:07 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.26821705426356585 on epoch=449
03/11/2022 04:15:09 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=454
03/11/2022 04:15:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.23 on epoch=459
03/11/2022 04:15:14 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
03/11/2022 04:15:16 - INFO - __main__ - Step 940 Global step 940 Train loss 0.24 on epoch=469
03/11/2022 04:15:18 - INFO - __main__ - Step 950 Global step 950 Train loss 0.24 on epoch=474
03/11/2022 04:15:19 - INFO - __main__ - Global step 950 Train loss 0.23 Classification-F1 0.24444444444444446 on epoch=474
03/11/2022 04:15:21 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=479
03/11/2022 04:15:23 - INFO - __main__ - Step 970 Global step 970 Train loss 0.22 on epoch=484
03/11/2022 04:15:25 - INFO - __main__ - Step 980 Global step 980 Train loss 0.23 on epoch=489
03/11/2022 04:15:28 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=494
03/11/2022 04:15:30 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.20 on epoch=499
03/11/2022 04:15:31 - INFO - __main__ - Global step 1000 Train loss 0.21 Classification-F1 0.24444444444444446 on epoch=499
03/11/2022 04:15:33 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.21 on epoch=504
03/11/2022 04:15:35 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=509
03/11/2022 04:15:37 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.19 on epoch=514
03/11/2022 04:15:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.20 on epoch=519
03/11/2022 04:15:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=524
03/11/2022 04:15:42 - INFO - __main__ - Global step 1050 Train loss 0.20 Classification-F1 0.25396825396825395 on epoch=524
03/11/2022 04:15:45 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=529
03/11/2022 04:15:47 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=534
03/11/2022 04:15:49 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.17 on epoch=539
03/11/2022 04:15:51 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.22 on epoch=544
03/11/2022 04:15:53 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.22 on epoch=549
03/11/2022 04:15:54 - INFO - __main__ - Global step 1100 Train loss 0.21 Classification-F1 0.25396825396825395 on epoch=549
03/11/2022 04:15:56 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.23 on epoch=554
03/11/2022 04:15:58 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.15 on epoch=559
03/11/2022 04:16:01 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=564
03/11/2022 04:16:03 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.21 on epoch=569
03/11/2022 04:16:05 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.20 on epoch=574
03/11/2022 04:16:06 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.29914529914529914 on epoch=574
03/11/2022 04:16:08 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=579
03/11/2022 04:16:10 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=584
03/11/2022 04:16:12 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=589
03/11/2022 04:16:14 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.16 on epoch=594
03/11/2022 04:16:17 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=599
03/11/2022 04:16:17 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.30838530838530837 on epoch=599
03/11/2022 04:16:20 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
03/11/2022 04:16:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=609
03/11/2022 04:16:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.18 on epoch=614
03/11/2022 04:16:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=619
03/11/2022 04:16:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.22 on epoch=624
03/11/2022 04:16:29 - INFO - __main__ - Global step 1250 Train loss 0.19 Classification-F1 0.25396825396825395 on epoch=624
03/11/2022 04:16:31 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=629
03/11/2022 04:16:34 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.21 on epoch=634
03/11/2022 04:16:36 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=639
03/11/2022 04:16:38 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.17 on epoch=644
03/11/2022 04:16:40 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.16 on epoch=649
03/11/2022 04:16:41 - INFO - __main__ - Global step 1300 Train loss 0.18 Classification-F1 0.24691358024691357 on epoch=649
03/11/2022 04:16:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=654
03/11/2022 04:16:45 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=659
03/11/2022 04:16:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=664
03/11/2022 04:16:50 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.18 on epoch=669
03/11/2022 04:16:52 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.16 on epoch=674
03/11/2022 04:16:53 - INFO - __main__ - Global step 1350 Train loss 0.19 Classification-F1 0.30838530838530837 on epoch=674
03/11/2022 04:16:55 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=679
03/11/2022 04:16:57 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.15 on epoch=684
03/11/2022 04:16:59 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=689
03/11/2022 04:17:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
03/11/2022 04:17:04 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=699
03/11/2022 04:17:05 - INFO - __main__ - Global step 1400 Train loss 0.16 Classification-F1 0.30838530838530837 on epoch=699
03/11/2022 04:17:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=704
03/11/2022 04:17:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.13 on epoch=709
03/11/2022 04:17:11 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.15 on epoch=714
03/11/2022 04:17:13 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=719
03/11/2022 04:17:15 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.14 on epoch=724
03/11/2022 04:17:16 - INFO - __main__ - Global step 1450 Train loss 0.15 Classification-F1 0.29914529914529914 on epoch=724
03/11/2022 04:17:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=729
03/11/2022 04:17:21 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=734
03/11/2022 04:17:23 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.14 on epoch=739
03/11/2022 04:17:25 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=744
03/11/2022 04:17:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=749
03/11/2022 04:17:28 - INFO - __main__ - Global step 1500 Train loss 0.14 Classification-F1 0.3142857142857143 on epoch=749
03/11/2022 04:17:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=754
03/11/2022 04:17:32 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=759
03/11/2022 04:17:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=764
03/11/2022 04:17:37 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=769
03/11/2022 04:17:39 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.13 on epoch=774
03/11/2022 04:17:40 - INFO - __main__ - Global step 1550 Train loss 0.15 Classification-F1 0.3264033264033264 on epoch=774
03/11/2022 04:17:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=779
03/11/2022 04:17:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.14 on epoch=784
03/11/2022 04:17:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=789
03/11/2022 04:17:49 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.13 on epoch=794
03/11/2022 04:17:51 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=799
03/11/2022 04:17:52 - INFO - __main__ - Global step 1600 Train loss 0.13 Classification-F1 0.308641975308642 on epoch=799
03/11/2022 04:17:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=804
03/11/2022 04:17:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=809
03/11/2022 04:17:58 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=814
03/11/2022 04:18:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=819
03/11/2022 04:18:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=824
03/11/2022 04:18:03 - INFO - __main__ - Global step 1650 Train loss 0.13 Classification-F1 0.35714285714285715 on epoch=824
03/11/2022 04:18:03 - INFO - __main__ - Saving model with best Classification-F1: 0.33260869565217394 -> 0.35714285714285715 on epoch=824, global_step=1650
03/11/2022 04:18:06 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=829
03/11/2022 04:18:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.08 on epoch=834
03/11/2022 04:18:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=839
03/11/2022 04:18:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=844
03/11/2022 04:18:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=849
03/11/2022 04:18:15 - INFO - __main__ - Global step 1700 Train loss 0.11 Classification-F1 0.28695652173913044 on epoch=849
03/11/2022 04:18:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.10 on epoch=854
03/11/2022 04:18:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=859
03/11/2022 04:18:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.13 on epoch=864
03/11/2022 04:18:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=869
03/11/2022 04:18:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=874
03/11/2022 04:18:27 - INFO - __main__ - Global step 1750 Train loss 0.10 Classification-F1 0.32631578947368417 on epoch=874
03/11/2022 04:18:29 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=879
03/11/2022 04:18:31 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=884
03/11/2022 04:18:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.06 on epoch=889
03/11/2022 04:18:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.13 on epoch=894
03/11/2022 04:18:38 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=899
03/11/2022 04:18:39 - INFO - __main__ - Global step 1800 Train loss 0.10 Classification-F1 0.39959432048681537 on epoch=899
03/11/2022 04:18:39 - INFO - __main__ - Saving model with best Classification-F1: 0.35714285714285715 -> 0.39959432048681537 on epoch=899, global_step=1800
03/11/2022 04:18:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
03/11/2022 04:18:43 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=909
03/11/2022 04:18:45 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=914
03/11/2022 04:18:47 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=919
03/11/2022 04:18:50 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.05 on epoch=924
03/11/2022 04:18:50 - INFO - __main__ - Global step 1850 Train loss 0.06 Classification-F1 0.3761904761904762 on epoch=924
03/11/2022 04:18:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=929
03/11/2022 04:18:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.06 on epoch=934
03/11/2022 04:18:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
03/11/2022 04:18:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=944
03/11/2022 04:19:01 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=949
03/11/2022 04:19:02 - INFO - __main__ - Global step 1900 Train loss 0.06 Classification-F1 0.4421906693711968 on epoch=949
03/11/2022 04:19:02 - INFO - __main__ - Saving model with best Classification-F1: 0.39959432048681537 -> 0.4421906693711968 on epoch=949, global_step=1900
03/11/2022 04:19:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=954
03/11/2022 04:19:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
03/11/2022 04:19:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
03/11/2022 04:19:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.03 on epoch=969
03/11/2022 04:19:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
03/11/2022 04:19:14 - INFO - __main__ - Global step 1950 Train loss 0.04 Classification-F1 0.3799864773495605 on epoch=974
03/11/2022 04:19:16 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.05 on epoch=979
03/11/2022 04:19:18 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=984
03/11/2022 04:19:21 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=989
03/11/2022 04:19:23 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=994
03/11/2022 04:19:25 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.06 on epoch=999
03/11/2022 04:19:26 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.3799864773495605 on epoch=999
03/11/2022 04:19:26 - INFO - __main__ - save last model!
03/11/2022 04:19:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 04:19:26 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 04:19:26 - INFO - __main__ - Printing 3 examples
03/11/2022 04:19:26 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 04:19:26 - INFO - __main__ - ['hate']
03/11/2022 04:19:26 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 04:19:26 - INFO - __main__ - ['non-irony']
03/11/2022 04:19:26 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 04:19:26 - INFO - __main__ - ['hate']
03/11/2022 04:19:26 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:19:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:19:26 - INFO - __main__ - Printing 3 examples
03/11/2022 04:19:26 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/11/2022 04:19:26 - INFO - __main__ - ['hate']
03/11/2022 04:19:26 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/11/2022 04:19:26 - INFO - __main__ - ['hate']
03/11/2022 04:19:26 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/11/2022 04:19:26 - INFO - __main__ - ['hate']
03/11/2022 04:19:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 04:19:26 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:19:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:19:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:19:26 - INFO - __main__ - Printing 3 examples
03/11/2022 04:19:26 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/11/2022 04:19:26 - INFO - __main__ - ['hate']
03/11/2022 04:19:26 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/11/2022 04:19:26 - INFO - __main__ - ['hate']
03/11/2022 04:19:26 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/11/2022 04:19:26 - INFO - __main__ - ['hate']
03/11/2022 04:19:26 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:19:26 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:19:26 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:19:26 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:19:27 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 04:19:41 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:19:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:19:42 - INFO - __main__ - Starting training!
03/11/2022 04:19:52 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_100_0.2_8_predictions.txt
03/11/2022 04:19:52 - INFO - __main__ - Classification-F1 on test data: 0.1764
03/11/2022 04:19:53 - INFO - __main__ - prefix=tweet_eval-irony_16_100, lr=0.2, bsz=8, dev_performance=0.4421906693711968, test_performance=0.17640369345025944
03/11/2022 04:19:53 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_13, lr=0.5, bsz=8 ...
03/11/2022 04:19:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:19:54 - INFO - __main__ - Printing 3 examples
03/11/2022 04:19:54 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/11/2022 04:19:54 - INFO - __main__ - ['hate']
03/11/2022 04:19:54 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/11/2022 04:19:54 - INFO - __main__ - ['hate']
03/11/2022 04:19:54 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/11/2022 04:19:54 - INFO - __main__ - ['hate']
03/11/2022 04:19:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:19:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:19:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:19:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:19:54 - INFO - __main__ - Printing 3 examples
03/11/2022 04:19:54 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/11/2022 04:19:54 - INFO - __main__ - ['hate']
03/11/2022 04:19:54 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/11/2022 04:19:54 - INFO - __main__ - ['hate']
03/11/2022 04:19:54 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/11/2022 04:19:54 - INFO - __main__ - ['hate']
03/11/2022 04:19:54 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:19:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:19:54 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:20:08 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:20:08 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:20:08 - INFO - __main__ - Starting training!
03/11/2022 04:20:11 - INFO - __main__ - Step 10 Global step 10 Train loss 2.39 on epoch=4
03/11/2022 04:20:13 - INFO - __main__ - Step 20 Global step 20 Train loss 0.67 on epoch=9
03/11/2022 04:20:16 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=14
03/11/2022 04:20:18 - INFO - __main__ - Step 40 Global step 40 Train loss 0.45 on epoch=19
03/11/2022 04:20:20 - INFO - __main__ - Step 50 Global step 50 Train loss 0.36 on epoch=24
03/11/2022 04:20:21 - INFO - __main__ - Global step 50 Train loss 0.87 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 04:20:21 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 04:20:23 - INFO - __main__ - Step 60 Global step 60 Train loss 0.32 on epoch=29
03/11/2022 04:20:26 - INFO - __main__ - Step 70 Global step 70 Train loss 0.38 on epoch=34
03/11/2022 04:20:28 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=39
03/11/2022 04:20:30 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=44
03/11/2022 04:20:32 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=49
03/11/2022 04:20:33 - INFO - __main__ - Global step 100 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 04:20:35 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=54
03/11/2022 04:20:38 - INFO - __main__ - Step 120 Global step 120 Train loss 0.31 on epoch=59
03/11/2022 04:20:40 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
03/11/2022 04:20:42 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
03/11/2022 04:20:44 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
03/11/2022 04:20:45 - INFO - __main__ - Global step 150 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 04:20:47 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
03/11/2022 04:20:49 - INFO - __main__ - Step 170 Global step 170 Train loss 0.25 on epoch=84
03/11/2022 04:20:52 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=89
03/11/2022 04:20:54 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
03/11/2022 04:20:56 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=99
03/11/2022 04:20:57 - INFO - __main__ - Global step 200 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 04:20:59 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
03/11/2022 04:21:01 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=109
03/11/2022 04:21:03 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
03/11/2022 04:21:05 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
03/11/2022 04:21:08 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
03/11/2022 04:21:08 - INFO - __main__ - Global step 250 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 04:21:11 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=129
03/11/2022 04:21:13 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 04:21:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 04:21:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=144
03/11/2022 04:21:20 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
03/11/2022 04:21:20 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 04:21:22 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=154
03/11/2022 04:21:25 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
03/11/2022 04:21:27 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
03/11/2022 04:21:29 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 04:21:31 - INFO - __main__ - Step 350 Global step 350 Train loss 0.24 on epoch=174
03/11/2022 04:21:32 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 04:21:34 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
03/11/2022 04:21:36 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
03/11/2022 04:21:39 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 04:21:41 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=194
03/11/2022 04:21:43 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
03/11/2022 04:21:44 - INFO - __main__ - Global step 400 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=199
03/11/2022 04:21:46 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=204
03/11/2022 04:21:48 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
03/11/2022 04:21:50 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=214
03/11/2022 04:21:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
03/11/2022 04:21:55 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=224
03/11/2022 04:21:56 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3816425120772947 on epoch=224
03/11/2022 04:21:56 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3816425120772947 on epoch=224, global_step=450
03/11/2022 04:21:58 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=229
03/11/2022 04:22:00 - INFO - __main__ - Step 470 Global step 470 Train loss 0.20 on epoch=234
03/11/2022 04:22:02 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
03/11/2022 04:22:04 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=244
03/11/2022 04:22:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
03/11/2022 04:22:07 - INFO - __main__ - Global step 500 Train loss 0.22 Classification-F1 0.4554554554554554 on epoch=249
03/11/2022 04:22:08 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.4554554554554554 on epoch=249, global_step=500
03/11/2022 04:22:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.16 on epoch=254
03/11/2022 04:22:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.22 on epoch=259
03/11/2022 04:22:14 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
03/11/2022 04:22:16 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=269
03/11/2022 04:22:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=274
03/11/2022 04:22:19 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3266888150609081 on epoch=274
03/11/2022 04:22:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.21 on epoch=279
03/11/2022 04:22:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=284
03/11/2022 04:22:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.18 on epoch=289
03/11/2022 04:22:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
03/11/2022 04:22:30 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
03/11/2022 04:22:31 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.3454545454545454 on epoch=299
03/11/2022 04:22:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=304
03/11/2022 04:22:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.16 on epoch=309
03/11/2022 04:22:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.09 on epoch=314
03/11/2022 04:22:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.19 on epoch=319
03/11/2022 04:22:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.13 on epoch=324
03/11/2022 04:22:43 - INFO - __main__ - Global step 650 Train loss 0.15 Classification-F1 0.4458874458874459 on epoch=324
03/11/2022 04:22:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.11 on epoch=329
03/11/2022 04:22:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.09 on epoch=334
03/11/2022 04:22:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.09 on epoch=339
03/11/2022 04:22:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
03/11/2022 04:22:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.09 on epoch=349
03/11/2022 04:22:54 - INFO - __main__ - Global step 700 Train loss 0.10 Classification-F1 0.5901477832512315 on epoch=349
03/11/2022 04:22:54 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.5901477832512315 on epoch=349, global_step=700
03/11/2022 04:22:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.05 on epoch=354
03/11/2022 04:22:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
03/11/2022 04:23:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
03/11/2022 04:23:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.01 on epoch=369
03/11/2022 04:23:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.06 on epoch=374
03/11/2022 04:23:07 - INFO - __main__ - Global step 750 Train loss 0.04 Classification-F1 0.5555555555555556 on epoch=374
03/11/2022 04:23:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.04 on epoch=379
03/11/2022 04:23:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.01 on epoch=384
03/11/2022 04:23:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.06 on epoch=389
03/11/2022 04:23:16 - INFO - __main__ - Step 790 Global step 790 Train loss 0.06 on epoch=394
03/11/2022 04:23:18 - INFO - __main__ - Step 800 Global step 800 Train loss 0.00 on epoch=399
03/11/2022 04:23:19 - INFO - __main__ - Global step 800 Train loss 0.03 Classification-F1 0.4682306940371457 on epoch=399
03/11/2022 04:23:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.01 on epoch=404
03/11/2022 04:23:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.02 on epoch=409
03/11/2022 04:23:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
03/11/2022 04:23:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.00 on epoch=419
03/11/2022 04:23:30 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/11/2022 04:23:31 - INFO - __main__ - Global step 850 Train loss 0.01 Classification-F1 0.4666666666666667 on epoch=424
03/11/2022 04:23:33 - INFO - __main__ - Step 860 Global step 860 Train loss 0.04 on epoch=429
03/11/2022 04:23:36 - INFO - __main__ - Step 870 Global step 870 Train loss 0.01 on epoch=434
03/11/2022 04:23:38 - INFO - __main__ - Step 880 Global step 880 Train loss 0.00 on epoch=439
03/11/2022 04:23:40 - INFO - __main__ - Step 890 Global step 890 Train loss 0.00 on epoch=444
03/11/2022 04:23:42 - INFO - __main__ - Step 900 Global step 900 Train loss 0.00 on epoch=449
03/11/2022 04:23:44 - INFO - __main__ - Global step 900 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=449
03/11/2022 04:23:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/11/2022 04:23:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.00 on epoch=459
03/11/2022 04:23:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/11/2022 04:23:52 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/11/2022 04:23:54 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/11/2022 04:23:56 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.4980392156862745 on epoch=474
03/11/2022 04:23:58 - INFO - __main__ - Step 960 Global step 960 Train loss 0.00 on epoch=479
03/11/2022 04:24:00 - INFO - __main__ - Step 970 Global step 970 Train loss 0.00 on epoch=484
03/11/2022 04:24:02 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/11/2022 04:24:04 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/11/2022 04:24:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.00 on epoch=499
03/11/2022 04:24:08 - INFO - __main__ - Global step 1000 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=499
03/11/2022 04:24:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/11/2022 04:24:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/11/2022 04:24:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/11/2022 04:24:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
03/11/2022 04:24:19 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.01 on epoch=524
03/11/2022 04:24:20 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=524
03/11/2022 04:24:23 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/11/2022 04:24:25 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/11/2022 04:24:27 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/11/2022 04:24:29 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/11/2022 04:24:31 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/11/2022 04:24:32 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=549
03/11/2022 04:24:35 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/11/2022 04:24:37 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/11/2022 04:24:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/11/2022 04:24:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/11/2022 04:24:43 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/11/2022 04:24:45 - INFO - __main__ - Global step 1150 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=574
03/11/2022 04:24:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/11/2022 04:24:49 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/11/2022 04:24:51 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/11/2022 04:24:54 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/11/2022 04:24:56 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/11/2022 04:24:57 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=599
03/11/2022 04:24:59 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/11/2022 04:25:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/11/2022 04:25:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/11/2022 04:25:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/11/2022 04:25:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/11/2022 04:25:09 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.5625 on epoch=624
03/11/2022 04:25:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/11/2022 04:25:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 04:25:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/11/2022 04:25:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/11/2022 04:25:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/11/2022 04:25:21 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=649
03/11/2022 04:25:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/11/2022 04:25:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/11/2022 04:25:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/11/2022 04:25:29 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 04:25:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/11/2022 04:25:33 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=674
03/11/2022 04:25:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 04:25:37 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 04:25:39 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/11/2022 04:25:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 04:25:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/11/2022 04:25:45 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=699
03/11/2022 04:25:47 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/11/2022 04:25:49 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 04:25:51 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 04:25:53 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 04:25:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/11/2022 04:25:57 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=724
03/11/2022 04:25:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 04:26:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 04:26:03 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 04:26:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 04:26:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 04:26:09 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4285714285714286 on epoch=749
03/11/2022 04:26:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/11/2022 04:26:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 04:26:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 04:26:18 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 04:26:20 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/11/2022 04:26:21 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=774
03/11/2022 04:26:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 04:26:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 04:26:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 04:26:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 04:26:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 04:26:33 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.40566959921798634 on epoch=799
03/11/2022 04:26:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 04:26:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 04:26:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 04:26:42 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 04:26:44 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 04:26:45 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.4009852216748768 on epoch=824
03/11/2022 04:26:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/11/2022 04:26:49 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 04:26:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 04:26:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 04:26:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 04:26:57 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.40566959921798634 on epoch=849
03/11/2022 04:26:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 04:27:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 04:27:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 04:27:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 04:27:08 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 04:27:09 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=874
03/11/2022 04:27:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 04:27:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 04:27:16 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 04:27:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 04:27:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 04:27:21 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5333333333333333 on epoch=899
03/11/2022 04:27:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 04:27:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 04:27:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 04:27:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 04:27:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 04:27:34 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4980392156862745 on epoch=924
03/11/2022 04:27:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 04:27:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 04:27:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
03/11/2022 04:27:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 04:27:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 04:27:45 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=949
03/11/2022 04:27:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 04:27:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 04:27:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 04:27:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 04:27:56 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 04:27:57 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=974
03/11/2022 04:27:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 04:28:02 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 04:28:04 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 04:28:06 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 04:28:08 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 04:28:09 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=999
03/11/2022 04:28:09 - INFO - __main__ - save last model!
03/11/2022 04:28:09 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 04:28:09 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 04:28:09 - INFO - __main__ - Printing 3 examples
03/11/2022 04:28:09 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 04:28:09 - INFO - __main__ - ['hate']
03/11/2022 04:28:09 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 04:28:09 - INFO - __main__ - ['non-irony']
03/11/2022 04:28:09 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 04:28:09 - INFO - __main__ - ['hate']
03/11/2022 04:28:09 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:28:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:28:09 - INFO - __main__ - Printing 3 examples
03/11/2022 04:28:09 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/11/2022 04:28:09 - INFO - __main__ - ['hate']
03/11/2022 04:28:09 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/11/2022 04:28:09 - INFO - __main__ - ['hate']
03/11/2022 04:28:09 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/11/2022 04:28:09 - INFO - __main__ - ['hate']
03/11/2022 04:28:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 04:28:09 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:28:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:28:09 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:28:09 - INFO - __main__ - Printing 3 examples
03/11/2022 04:28:09 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/11/2022 04:28:09 - INFO - __main__ - ['hate']
03/11/2022 04:28:09 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/11/2022 04:28:09 - INFO - __main__ - ['hate']
03/11/2022 04:28:09 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/11/2022 04:28:09 - INFO - __main__ - ['hate']
03/11/2022 04:28:09 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:28:09 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:28:10 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:28:10 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:28:11 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 04:28:24 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:28:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:28:25 - INFO - __main__ - Starting training!
03/11/2022 04:28:34 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_13_0.5_8_predictions.txt
03/11/2022 04:28:34 - INFO - __main__ - Classification-F1 on test data: 0.5653
03/11/2022 04:28:35 - INFO - __main__ - prefix=tweet_eval-irony_16_13, lr=0.5, bsz=8, dev_performance=0.5901477832512315, test_performance=0.5652905941750639
03/11/2022 04:28:35 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_13, lr=0.4, bsz=8 ...
03/11/2022 04:28:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:28:36 - INFO - __main__ - Printing 3 examples
03/11/2022 04:28:36 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/11/2022 04:28:36 - INFO - __main__ - ['hate']
03/11/2022 04:28:36 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/11/2022 04:28:36 - INFO - __main__ - ['hate']
03/11/2022 04:28:36 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/11/2022 04:28:36 - INFO - __main__ - ['hate']
03/11/2022 04:28:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:28:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:28:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:28:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:28:36 - INFO - __main__ - Printing 3 examples
03/11/2022 04:28:36 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/11/2022 04:28:36 - INFO - __main__ - ['hate']
03/11/2022 04:28:36 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/11/2022 04:28:36 - INFO - __main__ - ['hate']
03/11/2022 04:28:36 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/11/2022 04:28:36 - INFO - __main__ - ['hate']
03/11/2022 04:28:36 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:28:36 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:28:36 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:28:50 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:28:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:28:51 - INFO - __main__ - Starting training!
03/11/2022 04:28:54 - INFO - __main__ - Step 10 Global step 10 Train loss 2.70 on epoch=4
03/11/2022 04:28:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.97 on epoch=9
03/11/2022 04:28:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.57 on epoch=14
03/11/2022 04:29:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=19
03/11/2022 04:29:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=24
03/11/2022 04:29:04 - INFO - __main__ - Global step 50 Train loss 1.03 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 04:29:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 04:29:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.35 on epoch=29
03/11/2022 04:29:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.34 on epoch=34
03/11/2022 04:29:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=39
03/11/2022 04:29:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.34 on epoch=44
03/11/2022 04:29:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.35 on epoch=49
03/11/2022 04:29:16 - INFO - __main__ - Global step 100 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 04:29:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.34 on epoch=54
03/11/2022 04:29:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.43 on epoch=59
03/11/2022 04:29:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
03/11/2022 04:29:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
03/11/2022 04:29:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
03/11/2022 04:29:27 - INFO - __main__ - Global step 150 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 04:29:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.34 on epoch=79
03/11/2022 04:29:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.27 on epoch=84
03/11/2022 04:29:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.31 on epoch=89
03/11/2022 04:29:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=94
03/11/2022 04:29:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=99
03/11/2022 04:29:39 - INFO - __main__ - Global step 200 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 04:29:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
03/11/2022 04:29:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.26 on epoch=109
03/11/2022 04:29:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.30 on epoch=114
03/11/2022 04:29:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.32 on epoch=119
03/11/2022 04:29:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
03/11/2022 04:29:51 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.46843853820598 on epoch=124
03/11/2022 04:29:51 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.46843853820598 on epoch=124, global_step=250
03/11/2022 04:29:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
03/11/2022 04:29:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
03/11/2022 04:29:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
03/11/2022 04:29:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
03/11/2022 04:30:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=149
03/11/2022 04:30:02 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.5307917888563051 on epoch=149
03/11/2022 04:30:02 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5307917888563051 on epoch=149, global_step=300
03/11/2022 04:30:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=154
03/11/2022 04:30:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
03/11/2022 04:30:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
03/11/2022 04:30:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
03/11/2022 04:30:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=174
03/11/2022 04:30:14 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 04:30:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
03/11/2022 04:30:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
03/11/2022 04:30:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=189
03/11/2022 04:30:23 - INFO - __main__ - Step 390 Global step 390 Train loss 0.29 on epoch=194
03/11/2022 04:30:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
03/11/2022 04:30:26 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=199
03/11/2022 04:30:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
03/11/2022 04:30:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
03/11/2022 04:30:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
03/11/2022 04:30:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
03/11/2022 04:30:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
03/11/2022 04:30:38 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=224
03/11/2022 04:30:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
03/11/2022 04:30:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
03/11/2022 04:30:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 04:30:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
03/11/2022 04:30:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.20 on epoch=249
03/11/2022 04:30:49 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=249
03/11/2022 04:30:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.23 on epoch=254
03/11/2022 04:30:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
03/11/2022 04:30:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=264
03/11/2022 04:30:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=269
03/11/2022 04:31:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=274
03/11/2022 04:31:01 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=274
03/11/2022 04:31:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=279
03/11/2022 04:31:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=284
03/11/2022 04:31:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=289
03/11/2022 04:31:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=294
03/11/2022 04:31:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=299
03/11/2022 04:31:13 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3191489361702127 on epoch=299
03/11/2022 04:31:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
03/11/2022 04:31:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.19 on epoch=309
03/11/2022 04:31:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=314
03/11/2022 04:31:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=319
03/11/2022 04:31:23 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=324
03/11/2022 04:31:24 - INFO - __main__ - Global step 650 Train loss 0.19 Classification-F1 0.3764102564102564 on epoch=324
03/11/2022 04:31:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=329
03/11/2022 04:31:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=334
03/11/2022 04:31:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.12 on epoch=339
03/11/2022 04:31:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.13 on epoch=344
03/11/2022 04:31:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
03/11/2022 04:31:36 - INFO - __main__ - Global step 700 Train loss 0.14 Classification-F1 0.4458874458874459 on epoch=349
03/11/2022 04:31:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
03/11/2022 04:31:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.08 on epoch=359
03/11/2022 04:31:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.08 on epoch=364
03/11/2022 04:31:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.07 on epoch=369
03/11/2022 04:31:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.07 on epoch=374
03/11/2022 04:31:48 - INFO - __main__ - Global step 750 Train loss 0.08 Classification-F1 0.3552492046659597 on epoch=374
03/11/2022 04:31:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.09 on epoch=379
03/11/2022 04:31:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
03/11/2022 04:31:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.05 on epoch=389
03/11/2022 04:31:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.05 on epoch=394
03/11/2022 04:31:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.06 on epoch=399
03/11/2022 04:31:59 - INFO - __main__ - Global step 800 Train loss 0.06 Classification-F1 0.3333333333333333 on epoch=399
03/11/2022 04:32:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
03/11/2022 04:32:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.05 on epoch=409
03/11/2022 04:32:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
03/11/2022 04:32:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.03 on epoch=419
03/11/2022 04:32:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.01 on epoch=424
03/11/2022 04:32:11 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.3650793650793651 on epoch=424
03/11/2022 04:32:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.05 on epoch=429
03/11/2022 04:32:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.02 on epoch=434
03/11/2022 04:32:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.05 on epoch=439
03/11/2022 04:32:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.02 on epoch=444
03/11/2022 04:32:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/11/2022 04:32:24 - INFO - __main__ - Global step 900 Train loss 0.03 Classification-F1 0.3650793650793651 on epoch=449
03/11/2022 04:32:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.00 on epoch=454
03/11/2022 04:32:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/11/2022 04:32:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/11/2022 04:32:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.01 on epoch=469
03/11/2022 04:32:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.00 on epoch=474
03/11/2022 04:32:36 - INFO - __main__ - Global step 950 Train loss 0.01 Classification-F1 0.39139139139139134 on epoch=474
03/11/2022 04:32:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/11/2022 04:32:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
03/11/2022 04:32:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/11/2022 04:32:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.01 on epoch=494
03/11/2022 04:32:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/11/2022 04:32:49 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.4009852216748768 on epoch=499
03/11/2022 04:32:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.00 on epoch=504
03/11/2022 04:32:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.00 on epoch=509
03/11/2022 04:32:56 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.00 on epoch=514
03/11/2022 04:32:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/11/2022 04:33:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/11/2022 04:33:02 - INFO - __main__ - Global step 1050 Train loss 0.01 Classification-F1 0.3273273273273273 on epoch=524
03/11/2022 04:33:04 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.01 on epoch=529
03/11/2022 04:33:06 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/11/2022 04:33:09 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.01 on epoch=539
03/11/2022 04:33:11 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/11/2022 04:33:13 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/11/2022 04:33:15 - INFO - __main__ - Global step 1100 Train loss 0.01 Classification-F1 0.3650793650793651 on epoch=549
03/11/2022 04:33:17 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/11/2022 04:33:20 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/11/2022 04:33:22 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/11/2022 04:33:24 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
03/11/2022 04:33:26 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/11/2022 04:33:28 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.3273273273273273 on epoch=574
03/11/2022 04:33:30 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/11/2022 04:33:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/11/2022 04:33:34 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/11/2022 04:33:37 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/11/2022 04:33:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/11/2022 04:33:41 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.3273273273273273 on epoch=599
03/11/2022 04:33:43 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/11/2022 04:33:45 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/11/2022 04:33:47 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/11/2022 04:33:49 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/11/2022 04:33:52 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.03 on epoch=624
03/11/2022 04:33:54 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.3273273273273273 on epoch=624
03/11/2022 04:33:56 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.00 on epoch=629
03/11/2022 04:33:58 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 04:34:00 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/11/2022 04:34:03 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/11/2022 04:34:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/11/2022 04:34:07 - INFO - __main__ - Global step 1300 Train loss 0.00 Classification-F1 0.3650793650793651 on epoch=649
03/11/2022 04:34:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/11/2022 04:34:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/11/2022 04:34:14 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/11/2022 04:34:16 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 04:34:18 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 04:34:20 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.3650793650793651 on epoch=674
03/11/2022 04:34:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 04:34:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 04:34:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
03/11/2022 04:34:30 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 04:34:32 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/11/2022 04:34:34 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.3650793650793651 on epoch=699
03/11/2022 04:34:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/11/2022 04:34:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 04:34:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 04:34:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 04:34:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/11/2022 04:34:47 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.3650793650793651 on epoch=724
03/11/2022 04:34:50 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 04:34:52 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 04:34:54 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 04:34:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 04:34:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 04:35:01 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.33793103448275863 on epoch=749
03/11/2022 04:35:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/11/2022 04:35:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/11/2022 04:35:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/11/2022 04:35:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 04:35:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 04:35:14 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.3650793650793651 on epoch=774
03/11/2022 04:35:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.02 on epoch=779
03/11/2022 04:35:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 04:35:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
03/11/2022 04:35:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 04:35:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 04:35:28 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.3650793650793651 on epoch=799
03/11/2022 04:35:30 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 04:35:32 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 04:35:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 04:35:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 04:35:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 04:35:41 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.3273273273273273 on epoch=824
03/11/2022 04:35:44 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 04:35:46 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 04:35:48 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
03/11/2022 04:35:51 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 04:35:53 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 04:35:55 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.3273273273273273 on epoch=849
03/11/2022 04:35:57 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 04:35:59 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
03/11/2022 04:36:02 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 04:36:04 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
03/11/2022 04:36:06 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 04:36:08 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.3273273273273273 on epoch=874
03/11/2022 04:36:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 04:36:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 04:36:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 04:36:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 04:36:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 04:36:22 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.3273273273273273 on epoch=899
03/11/2022 04:36:24 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 04:36:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 04:36:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 04:36:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 04:36:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
03/11/2022 04:36:35 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.3273273273273273 on epoch=924
03/11/2022 04:36:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 04:36:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=934
03/11/2022 04:36:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 04:36:44 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 04:36:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 04:36:49 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.3273273273273273 on epoch=949
03/11/2022 04:36:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 04:36:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/11/2022 04:36:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 04:36:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/11/2022 04:37:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 04:37:03 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.3273273273273273 on epoch=974
03/11/2022 04:37:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 04:37:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/11/2022 04:37:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 04:37:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 04:37:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 04:37:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:37:15 - INFO - __main__ - Printing 3 examples
03/11/2022 04:37:15 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/11/2022 04:37:15 - INFO - __main__ - ['hate']
03/11/2022 04:37:15 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/11/2022 04:37:15 - INFO - __main__ - ['hate']
03/11/2022 04:37:15 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/11/2022 04:37:15 - INFO - __main__ - ['hate']
03/11/2022 04:37:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 04:37:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:37:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:37:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:37:15 - INFO - __main__ - Printing 3 examples
03/11/2022 04:37:15 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/11/2022 04:37:15 - INFO - __main__ - ['hate']
03/11/2022 04:37:15 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/11/2022 04:37:15 - INFO - __main__ - ['hate']
03/11/2022 04:37:15 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/11/2022 04:37:15 - INFO - __main__ - ['hate']
03/11/2022 04:37:15 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:37:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:37:15 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:37:16 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.3650793650793651 on epoch=999
03/11/2022 04:37:16 - INFO - __main__ - save last model!
03/11/2022 04:37:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 04:37:16 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 04:37:16 - INFO - __main__ - Printing 3 examples
03/11/2022 04:37:16 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 04:37:16 - INFO - __main__ - ['hate']
03/11/2022 04:37:16 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 04:37:16 - INFO - __main__ - ['non-irony']
03/11/2022 04:37:16 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 04:37:16 - INFO - __main__ - ['hate']
03/11/2022 04:37:16 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:37:17 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:37:18 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 04:37:28 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:37:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:37:29 - INFO - __main__ - Starting training!
03/11/2022 04:38:09 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_13_0.4_8_predictions.txt
03/11/2022 04:38:09 - INFO - __main__ - Classification-F1 on test data: 0.5842
03/11/2022 04:38:09 - INFO - __main__ - prefix=tweet_eval-irony_16_13, lr=0.4, bsz=8, dev_performance=0.5307917888563051, test_performance=0.5841532313628222
03/11/2022 04:38:09 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_13, lr=0.3, bsz=8 ...
03/11/2022 04:38:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:38:10 - INFO - __main__ - Printing 3 examples
03/11/2022 04:38:10 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/11/2022 04:38:10 - INFO - __main__ - ['hate']
03/11/2022 04:38:10 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/11/2022 04:38:10 - INFO - __main__ - ['hate']
03/11/2022 04:38:10 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/11/2022 04:38:10 - INFO - __main__ - ['hate']
03/11/2022 04:38:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:38:10 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:38:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:38:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:38:10 - INFO - __main__ - Printing 3 examples
03/11/2022 04:38:10 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/11/2022 04:38:10 - INFO - __main__ - ['hate']
03/11/2022 04:38:10 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/11/2022 04:38:10 - INFO - __main__ - ['hate']
03/11/2022 04:38:10 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/11/2022 04:38:10 - INFO - __main__ - ['hate']
03/11/2022 04:38:10 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:38:10 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:38:10 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:38:22 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:38:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:38:23 - INFO - __main__ - Starting training!
03/11/2022 04:38:26 - INFO - __main__ - Step 10 Global step 10 Train loss 2.64 on epoch=4
03/11/2022 04:38:28 - INFO - __main__ - Step 20 Global step 20 Train loss 1.01 on epoch=9
03/11/2022 04:38:30 - INFO - __main__ - Step 30 Global step 30 Train loss 0.55 on epoch=14
03/11/2022 04:38:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.48 on epoch=19
03/11/2022 04:38:35 - INFO - __main__ - Step 50 Global step 50 Train loss 0.41 on epoch=24
03/11/2022 04:38:35 - INFO - __main__ - Global step 50 Train loss 1.02 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 04:38:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 04:38:38 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=29
03/11/2022 04:38:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=34
03/11/2022 04:38:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=39
03/11/2022 04:38:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.35 on epoch=44
03/11/2022 04:38:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.36 on epoch=49
03/11/2022 04:38:47 - INFO - __main__ - Global step 100 Train loss 0.37 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 04:38:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.32 on epoch=54
03/11/2022 04:38:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.34 on epoch=59
03/11/2022 04:38:54 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
03/11/2022 04:38:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=69
03/11/2022 04:38:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
03/11/2022 04:38:59 - INFO - __main__ - Global step 150 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 04:39:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
03/11/2022 04:39:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=84
03/11/2022 04:39:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.28 on epoch=89
03/11/2022 04:39:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.31 on epoch=94
03/11/2022 04:39:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.23 on epoch=99
03/11/2022 04:39:10 - INFO - __main__ - Global step 200 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 04:39:13 - INFO - __main__ - Step 210 Global step 210 Train loss 0.29 on epoch=104
03/11/2022 04:39:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.35 on epoch=109
03/11/2022 04:39:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.25 on epoch=114
03/11/2022 04:39:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.29 on epoch=119
03/11/2022 04:39:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
03/11/2022 04:39:22 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.43529411764705883 on epoch=124
03/11/2022 04:39:22 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.43529411764705883 on epoch=124, global_step=250
03/11/2022 04:39:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
03/11/2022 04:39:27 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
03/11/2022 04:39:29 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
03/11/2022 04:39:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
03/11/2022 04:39:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=149
03/11/2022 04:39:34 - INFO - __main__ - Global step 300 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 04:39:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=154
03/11/2022 04:39:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
03/11/2022 04:39:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.29 on epoch=164
03/11/2022 04:39:43 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
03/11/2022 04:39:45 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=174
03/11/2022 04:39:46 - INFO - __main__ - Global step 350 Train loss 0.27 Classification-F1 0.4231177094379639 on epoch=174
03/11/2022 04:39:48 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
03/11/2022 04:39:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=184
03/11/2022 04:39:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
03/11/2022 04:39:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
03/11/2022 04:39:57 - INFO - __main__ - Step 400 Global step 400 Train loss 0.33 on epoch=199
03/11/2022 04:39:57 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.5151515151515151 on epoch=199
03/11/2022 04:39:57 - INFO - __main__ - Saving model with best Classification-F1: 0.43529411764705883 -> 0.5151515151515151 on epoch=199, global_step=400
03/11/2022 04:40:00 - INFO - __main__ - Step 410 Global step 410 Train loss 0.24 on epoch=204
03/11/2022 04:40:02 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
03/11/2022 04:40:04 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=214
03/11/2022 04:40:06 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=219
03/11/2022 04:40:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.29 on epoch=224
03/11/2022 04:40:09 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.3816425120772947 on epoch=224
03/11/2022 04:40:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
03/11/2022 04:40:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=234
03/11/2022 04:40:16 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
03/11/2022 04:40:18 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
03/11/2022 04:40:20 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
03/11/2022 04:40:21 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=249
03/11/2022 04:40:23 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
03/11/2022 04:40:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
03/11/2022 04:40:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
03/11/2022 04:40:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
03/11/2022 04:40:32 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
03/11/2022 04:40:32 - INFO - __main__ - Global step 550 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=274
03/11/2022 04:40:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
03/11/2022 04:40:37 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
03/11/2022 04:40:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
03/11/2022 04:40:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
03/11/2022 04:40:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
03/11/2022 04:40:44 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.5607843137254902 on epoch=299
03/11/2022 04:40:44 - INFO - __main__ - Saving model with best Classification-F1: 0.5151515151515151 -> 0.5607843137254902 on epoch=299, global_step=600
03/11/2022 04:40:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=304
03/11/2022 04:40:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=309
03/11/2022 04:40:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=314
03/11/2022 04:40:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
03/11/2022 04:40:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=324
03/11/2022 04:40:56 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.36374269005847953 on epoch=324
03/11/2022 04:40:58 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=329
03/11/2022 04:41:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
03/11/2022 04:41:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.20 on epoch=339
03/11/2022 04:41:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=344
03/11/2022 04:41:07 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=349
03/11/2022 04:41:07 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.3816425120772947 on epoch=349
03/11/2022 04:41:10 - INFO - __main__ - Step 710 Global step 710 Train loss 0.17 on epoch=354
03/11/2022 04:41:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
03/11/2022 04:41:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=364
03/11/2022 04:41:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
03/11/2022 04:41:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=374
03/11/2022 04:41:19 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.3992490613266583 on epoch=374
03/11/2022 04:41:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.20 on epoch=379
03/11/2022 04:41:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
03/11/2022 04:41:26 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=389
03/11/2022 04:41:28 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=394
03/11/2022 04:41:30 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=399
03/11/2022 04:41:31 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.3191489361702127 on epoch=399
03/11/2022 04:41:33 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=404
03/11/2022 04:41:35 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=409
03/11/2022 04:41:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.14 on epoch=414
03/11/2022 04:41:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=419
03/11/2022 04:41:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.15 on epoch=424
03/11/2022 04:41:42 - INFO - __main__ - Global step 850 Train loss 0.16 Classification-F1 0.3191489361702127 on epoch=424
03/11/2022 04:41:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=429
03/11/2022 04:41:47 - INFO - __main__ - Step 870 Global step 870 Train loss 0.12 on epoch=434
03/11/2022 04:41:49 - INFO - __main__ - Step 880 Global step 880 Train loss 0.11 on epoch=439
03/11/2022 04:41:51 - INFO - __main__ - Step 890 Global step 890 Train loss 0.14 on epoch=444
03/11/2022 04:41:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=449
03/11/2022 04:41:54 - INFO - __main__ - Global step 900 Train loss 0.13 Classification-F1 0.39756367663344405 on epoch=449
03/11/2022 04:41:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.10 on epoch=454
03/11/2022 04:41:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.14 on epoch=459
03/11/2022 04:42:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=464
03/11/2022 04:42:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.07 on epoch=469
03/11/2022 04:42:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
03/11/2022 04:42:05 - INFO - __main__ - Global step 950 Train loss 0.09 Classification-F1 0.3191489361702127 on epoch=474
03/11/2022 04:42:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=479
03/11/2022 04:42:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.07 on epoch=484
03/11/2022 04:42:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.05 on epoch=489
03/11/2022 04:42:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.07 on epoch=494
03/11/2022 04:42:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=499
03/11/2022 04:42:17 - INFO - __main__ - Global step 1000 Train loss 0.07 Classification-F1 0.4181818181818182 on epoch=499
03/11/2022 04:42:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
03/11/2022 04:42:22 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
03/11/2022 04:42:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
03/11/2022 04:42:26 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.05 on epoch=519
03/11/2022 04:42:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.04 on epoch=524
03/11/2022 04:42:29 - INFO - __main__ - Global step 1050 Train loss 0.05 Classification-F1 0.5333333333333333 on epoch=524
03/11/2022 04:42:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
03/11/2022 04:42:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
03/11/2022 04:42:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.03 on epoch=539
03/11/2022 04:42:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
03/11/2022 04:42:40 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.04 on epoch=549
03/11/2022 04:42:40 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.3191489361702127 on epoch=549
03/11/2022 04:42:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.04 on epoch=554
03/11/2022 04:42:45 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
03/11/2022 04:42:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.07 on epoch=564
03/11/2022 04:42:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
03/11/2022 04:42:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.04 on epoch=574
03/11/2022 04:42:52 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.5333333333333333 on epoch=574
03/11/2022 04:42:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/11/2022 04:42:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/11/2022 04:42:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/11/2022 04:43:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/11/2022 04:43:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/11/2022 04:43:04 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.5733333333333335 on epoch=599
03/11/2022 04:43:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5733333333333335 on epoch=599, global_step=1200
03/11/2022 04:43:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/11/2022 04:43:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/11/2022 04:43:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/11/2022 04:43:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/11/2022 04:43:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/11/2022 04:43:15 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.49090909090909085 on epoch=624
03/11/2022 04:43:17 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
03/11/2022 04:43:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 04:43:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.01 on epoch=639
03/11/2022 04:43:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
03/11/2022 04:43:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.03 on epoch=649
03/11/2022 04:43:27 - INFO - __main__ - Global step 1300 Train loss 0.02 Classification-F1 0.5151515151515151 on epoch=649
03/11/2022 04:43:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.01 on epoch=654
03/11/2022 04:43:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.01 on epoch=659
03/11/2022 04:43:33 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=664
03/11/2022 04:43:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.01 on epoch=669
03/11/2022 04:43:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 04:43:39 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.46843853820598 on epoch=674
03/11/2022 04:43:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
03/11/2022 04:43:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 04:43:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/11/2022 04:43:47 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 04:43:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/11/2022 04:43:51 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.539313399778516 on epoch=699
03/11/2022 04:43:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/11/2022 04:43:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 04:43:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/11/2022 04:44:00 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 04:44:02 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/11/2022 04:44:03 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.5333333333333333 on epoch=724
03/11/2022 04:44:05 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 04:44:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 04:44:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 04:44:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 04:44:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 04:44:14 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.4909862142099682 on epoch=749
03/11/2022 04:44:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
03/11/2022 04:44:19 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 04:44:21 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 04:44:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 04:44:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 04:44:26 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.464039408866995 on epoch=774
03/11/2022 04:44:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 04:44:31 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 04:44:33 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 04:44:35 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/11/2022 04:44:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 04:44:38 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.5307917888563051 on epoch=799
03/11/2022 04:44:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
03/11/2022 04:44:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 04:44:45 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/11/2022 04:44:47 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 04:44:49 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 04:44:50 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.6000000000000001 on epoch=824
03/11/2022 04:44:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5733333333333335 -> 0.6000000000000001 on epoch=824, global_step=1650
03/11/2022 04:44:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 04:44:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
03/11/2022 04:44:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 04:44:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 04:45:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 04:45:03 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5844155844155844 on epoch=849
03/11/2022 04:45:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 04:45:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/11/2022 04:45:09 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 04:45:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 04:45:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 04:45:15 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.5465587044534412 on epoch=874
03/11/2022 04:45:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 04:45:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 04:45:22 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 04:45:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 04:45:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 04:45:27 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.5733333333333335 on epoch=899
03/11/2022 04:45:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 04:45:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 04:45:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 04:45:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 04:45:38 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 04:45:39 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=924
03/11/2022 04:45:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 04:45:44 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
03/11/2022 04:45:46 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 04:45:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 04:45:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 04:45:51 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=949
03/11/2022 04:45:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/11/2022 04:45:55 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 04:45:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.01 on epoch=964
03/11/2022 04:46:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 04:46:02 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 04:46:03 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=974
03/11/2022 04:46:05 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
03/11/2022 04:46:07 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 04:46:09 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 04:46:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 04:46:14 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 04:46:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:46:15 - INFO - __main__ - Printing 3 examples
03/11/2022 04:46:15 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/11/2022 04:46:15 - INFO - __main__ - ['hate']
03/11/2022 04:46:15 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/11/2022 04:46:15 - INFO - __main__ - ['hate']
03/11/2022 04:46:15 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/11/2022 04:46:15 - INFO - __main__ - ['hate']
03/11/2022 04:46:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 04:46:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:46:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:46:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:46:15 - INFO - __main__ - Printing 3 examples
03/11/2022 04:46:15 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/11/2022 04:46:15 - INFO - __main__ - ['hate']
03/11/2022 04:46:15 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/11/2022 04:46:15 - INFO - __main__ - ['hate']
03/11/2022 04:46:15 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/11/2022 04:46:15 - INFO - __main__ - ['hate']
03/11/2022 04:46:15 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:46:15 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:46:15 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:46:15 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5195195195195195 on epoch=999
03/11/2022 04:46:15 - INFO - __main__ - save last model!
03/11/2022 04:46:15 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 04:46:15 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 04:46:15 - INFO - __main__ - Printing 3 examples
03/11/2022 04:46:15 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 04:46:15 - INFO - __main__ - ['hate']
03/11/2022 04:46:15 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 04:46:15 - INFO - __main__ - ['non-irony']
03/11/2022 04:46:15 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 04:46:15 - INFO - __main__ - ['hate']
03/11/2022 04:46:15 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:46:16 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:46:17 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 04:46:29 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:46:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:46:30 - INFO - __main__ - Starting training!
03/11/2022 04:46:54 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_13_0.3_8_predictions.txt
03/11/2022 04:46:54 - INFO - __main__ - Classification-F1 on test data: 0.5577
03/11/2022 04:46:54 - INFO - __main__ - prefix=tweet_eval-irony_16_13, lr=0.3, bsz=8, dev_performance=0.6000000000000001, test_performance=0.5576694976614722
03/11/2022 04:46:54 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_13, lr=0.2, bsz=8 ...
03/11/2022 04:46:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:46:55 - INFO - __main__ - Printing 3 examples
03/11/2022 04:46:55 - INFO - __main__ -  [tweet_eval-irony] working a double on 2 hours of sleep here we go let's get it
03/11/2022 04:46:55 - INFO - __main__ - ['hate']
03/11/2022 04:46:55 - INFO - __main__ -  [tweet_eval-irony] well today is gonna be a great day 👌
03/11/2022 04:46:55 - INFO - __main__ - ['hate']
03/11/2022 04:46:55 - INFO - __main__ -  [tweet_eval-irony] A great day is one in which it's pouring rain and you walk out of your apartment without your car or apartment keys.
03/11/2022 04:46:55 - INFO - __main__ - ['hate']
03/11/2022 04:46:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:46:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:46:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:46:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:46:55 - INFO - __main__ - Printing 3 examples
03/11/2022 04:46:55 - INFO - __main__ -  [tweet_eval-irony] Can't wait for my boyfriend to come home and kick my butt into shape....  🙈😂 @user
03/11/2022 04:46:55 - INFO - __main__ - ['hate']
03/11/2022 04:46:55 - INFO - __main__ -  [tweet_eval-irony] Not felt this I'll since I was in kos! Eugh a great end to a great week......  #beenshite #killme
03/11/2022 04:46:55 - INFO - __main__ - ['hate']
03/11/2022 04:46:55 - INFO - __main__ -  [tweet_eval-irony] @user One could hope.
03/11/2022 04:46:55 - INFO - __main__ - ['hate']
03/11/2022 04:46:55 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:46:55 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:46:55 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:47:09 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:47:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:47:10 - INFO - __main__ - Starting training!
03/11/2022 04:47:13 - INFO - __main__ - Step 10 Global step 10 Train loss 3.06 on epoch=4
03/11/2022 04:47:15 - INFO - __main__ - Step 20 Global step 20 Train loss 1.56 on epoch=9
03/11/2022 04:47:17 - INFO - __main__ - Step 30 Global step 30 Train loss 0.87 on epoch=14
03/11/2022 04:47:20 - INFO - __main__ - Step 40 Global step 40 Train loss 0.65 on epoch=19
03/11/2022 04:47:22 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=24
03/11/2022 04:47:23 - INFO - __main__ - Global step 50 Train loss 1.34 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 04:47:23 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 04:47:25 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=29
03/11/2022 04:47:27 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=34
03/11/2022 04:47:29 - INFO - __main__ - Step 80 Global step 80 Train loss 0.44 on epoch=39
03/11/2022 04:47:31 - INFO - __main__ - Step 90 Global step 90 Train loss 0.36 on epoch=44
03/11/2022 04:47:34 - INFO - __main__ - Step 100 Global step 100 Train loss 0.37 on epoch=49
03/11/2022 04:47:34 - INFO - __main__ - Global step 100 Train loss 0.43 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 04:47:37 - INFO - __main__ - Step 110 Global step 110 Train loss 0.34 on epoch=54
03/11/2022 04:47:39 - INFO - __main__ - Step 120 Global step 120 Train loss 0.37 on epoch=59
03/11/2022 04:47:41 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=64
03/11/2022 04:47:43 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
03/11/2022 04:47:45 - INFO - __main__ - Step 150 Global step 150 Train loss 0.36 on epoch=74
03/11/2022 04:47:46 - INFO - __main__ - Global step 150 Train loss 0.35 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 04:47:48 - INFO - __main__ - Step 160 Global step 160 Train loss 0.33 on epoch=79
03/11/2022 04:47:50 - INFO - __main__ - Step 170 Global step 170 Train loss 0.36 on epoch=84
03/11/2022 04:47:53 - INFO - __main__ - Step 180 Global step 180 Train loss 0.35 on epoch=89
03/11/2022 04:47:55 - INFO - __main__ - Step 190 Global step 190 Train loss 0.33 on epoch=94
03/11/2022 04:47:57 - INFO - __main__ - Step 200 Global step 200 Train loss 0.33 on epoch=99
03/11/2022 04:47:58 - INFO - __main__ - Global step 200 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 04:48:00 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
03/11/2022 04:48:02 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
03/11/2022 04:48:04 - INFO - __main__ - Step 230 Global step 230 Train loss 0.28 on epoch=114
03/11/2022 04:48:07 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
03/11/2022 04:48:09 - INFO - __main__ - Step 250 Global step 250 Train loss 0.29 on epoch=124
03/11/2022 04:48:10 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.3454545454545454 on epoch=124
03/11/2022 04:48:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3454545454545454 on epoch=124, global_step=250
03/11/2022 04:48:12 - INFO - __main__ - Step 260 Global step 260 Train loss 0.29 on epoch=129
03/11/2022 04:48:14 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
03/11/2022 04:48:16 - INFO - __main__ - Step 280 Global step 280 Train loss 0.32 on epoch=139
03/11/2022 04:48:19 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
03/11/2022 04:48:21 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
03/11/2022 04:48:22 - INFO - __main__ - Global step 300 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 04:48:24 - INFO - __main__ - Step 310 Global step 310 Train loss 0.27 on epoch=154
03/11/2022 04:48:26 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
03/11/2022 04:48:28 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=164
03/11/2022 04:48:30 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=169
03/11/2022 04:48:32 - INFO - __main__ - Step 350 Global step 350 Train loss 0.27 on epoch=174
03/11/2022 04:48:33 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 04:48:35 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
03/11/2022 04:48:38 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
03/11/2022 04:48:40 - INFO - __main__ - Step 380 Global step 380 Train loss 0.29 on epoch=189
03/11/2022 04:48:42 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=194
03/11/2022 04:48:44 - INFO - __main__ - Step 400 Global step 400 Train loss 0.24 on epoch=199
03/11/2022 04:48:45 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=199
03/11/2022 04:48:47 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 04:48:49 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
03/11/2022 04:48:51 - INFO - __main__ - Step 430 Global step 430 Train loss 0.26 on epoch=214
03/11/2022 04:48:53 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
03/11/2022 04:48:56 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
03/11/2022 04:48:56 - INFO - __main__ - Global step 450 Train loss 0.25 Classification-F1 0.3043478260869565 on epoch=224
03/11/2022 04:48:59 - INFO - __main__ - Step 460 Global step 460 Train loss 0.27 on epoch=229
03/11/2022 04:49:01 - INFO - __main__ - Step 470 Global step 470 Train loss 0.30 on epoch=234
03/11/2022 04:49:03 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
03/11/2022 04:49:05 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
03/11/2022 04:49:07 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
03/11/2022 04:49:08 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=249
03/11/2022 04:49:10 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
03/11/2022 04:49:12 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
03/11/2022 04:49:15 - INFO - __main__ - Step 530 Global step 530 Train loss 0.30 on epoch=264
03/11/2022 04:49:17 - INFO - __main__ - Step 540 Global step 540 Train loss 0.29 on epoch=269
03/11/2022 04:49:19 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
03/11/2022 04:49:20 - INFO - __main__ - Global step 550 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=274
03/11/2022 04:49:22 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
03/11/2022 04:49:24 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=284
03/11/2022 04:49:26 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
03/11/2022 04:49:28 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
03/11/2022 04:49:31 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
03/11/2022 04:49:31 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.3454545454545454 on epoch=299
03/11/2022 04:49:34 - INFO - __main__ - Step 610 Global step 610 Train loss 0.19 on epoch=304
03/11/2022 04:49:36 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=309
03/11/2022 04:49:38 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=314
03/11/2022 04:49:40 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=319
03/11/2022 04:49:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.18 on epoch=324
03/11/2022 04:49:43 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.4231177094379639 on epoch=324
03/11/2022 04:49:43 - INFO - __main__ - Saving model with best Classification-F1: 0.3454545454545454 -> 0.4231177094379639 on epoch=324, global_step=650
03/11/2022 04:49:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=329
03/11/2022 04:49:48 - INFO - __main__ - Step 670 Global step 670 Train loss 0.22 on epoch=334
03/11/2022 04:49:50 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=339
03/11/2022 04:49:52 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=344
03/11/2022 04:49:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.18 on epoch=349
03/11/2022 04:49:55 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.37662337662337664 on epoch=349
03/11/2022 04:49:57 - INFO - __main__ - Step 710 Global step 710 Train loss 0.19 on epoch=354
03/11/2022 04:49:59 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=359
03/11/2022 04:50:01 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
03/11/2022 04:50:03 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=369
03/11/2022 04:50:06 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=374
03/11/2022 04:50:06 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.4458874458874459 on epoch=374
03/11/2022 04:50:06 - INFO - __main__ - Saving model with best Classification-F1: 0.4231177094379639 -> 0.4458874458874459 on epoch=374, global_step=750
03/11/2022 04:50:09 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=379
03/11/2022 04:50:11 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=384
03/11/2022 04:50:13 - INFO - __main__ - Step 780 Global step 780 Train loss 0.19 on epoch=389
03/11/2022 04:50:15 - INFO - __main__ - Step 790 Global step 790 Train loss 0.18 on epoch=394
03/11/2022 04:50:17 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=399
03/11/2022 04:50:18 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.5195195195195195 on epoch=399
03/11/2022 04:50:18 - INFO - __main__ - Saving model with best Classification-F1: 0.4458874458874459 -> 0.5195195195195195 on epoch=399, global_step=800
03/11/2022 04:50:21 - INFO - __main__ - Step 810 Global step 810 Train loss 0.18 on epoch=404
03/11/2022 04:50:23 - INFO - __main__ - Step 820 Global step 820 Train loss 0.17 on epoch=409
03/11/2022 04:50:25 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=414
03/11/2022 04:50:27 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=419
03/11/2022 04:50:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=424
03/11/2022 04:50:30 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.5195195195195195 on epoch=424
03/11/2022 04:50:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=429
03/11/2022 04:50:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=434
03/11/2022 04:50:37 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=439
03/11/2022 04:50:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=444
03/11/2022 04:50:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=449
03/11/2022 04:50:42 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.5835835835835835 on epoch=449
03/11/2022 04:50:42 - INFO - __main__ - Saving model with best Classification-F1: 0.5195195195195195 -> 0.5835835835835835 on epoch=449, global_step=900
03/11/2022 04:50:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=454
03/11/2022 04:50:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.15 on epoch=459
03/11/2022 04:50:48 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=464
03/11/2022 04:50:50 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=469
03/11/2022 04:50:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=474
03/11/2022 04:50:53 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.4920634920634921 on epoch=474
03/11/2022 04:50:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=479
03/11/2022 04:50:58 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=484
03/11/2022 04:51:00 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=489
03/11/2022 04:51:02 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=494
03/11/2022 04:51:04 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.15 on epoch=499
03/11/2022 04:51:05 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.5625 on epoch=499
03/11/2022 04:51:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=504
03/11/2022 04:51:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=509
03/11/2022 04:51:12 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=514
03/11/2022 04:51:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=519
03/11/2022 04:51:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=524
03/11/2022 04:51:17 - INFO - __main__ - Global step 1050 Train loss 0.16 Classification-F1 0.41700404858299595 on epoch=524
03/11/2022 04:51:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.15 on epoch=529
03/11/2022 04:51:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=534
03/11/2022 04:51:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.14 on epoch=539
03/11/2022 04:51:25 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=544
03/11/2022 04:51:28 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.17 on epoch=549
03/11/2022 04:51:29 - INFO - __main__ - Global step 1100 Train loss 0.15 Classification-F1 0.4980392156862745 on epoch=549
03/11/2022 04:51:31 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=554
03/11/2022 04:51:33 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=559
03/11/2022 04:51:35 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=564
03/11/2022 04:51:37 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.15 on epoch=569
03/11/2022 04:51:39 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=574
03/11/2022 04:51:40 - INFO - __main__ - Global step 1150 Train loss 0.13 Classification-F1 0.4920634920634921 on epoch=574
03/11/2022 04:51:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.15 on epoch=579
03/11/2022 04:51:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=584
03/11/2022 04:51:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
03/11/2022 04:51:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.13 on epoch=594
03/11/2022 04:51:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=599
03/11/2022 04:51:52 - INFO - __main__ - Global step 1200 Train loss 0.13 Classification-F1 0.39999999999999997 on epoch=599
03/11/2022 04:51:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=604
03/11/2022 04:51:57 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.10 on epoch=609
03/11/2022 04:51:59 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=614
03/11/2022 04:52:01 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.10 on epoch=619
03/11/2022 04:52:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.14 on epoch=624
03/11/2022 04:52:04 - INFO - __main__ - Global step 1250 Train loss 0.12 Classification-F1 0.5607843137254902 on epoch=624
03/11/2022 04:52:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=629
03/11/2022 04:52:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.11 on epoch=634
03/11/2022 04:52:10 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.12 on epoch=639
03/11/2022 04:52:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=644
03/11/2022 04:52:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=649
03/11/2022 04:52:16 - INFO - __main__ - Global step 1300 Train loss 0.11 Classification-F1 0.464039408866995 on epoch=649
03/11/2022 04:52:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=654
03/11/2022 04:52:20 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.10 on epoch=659
03/11/2022 04:52:22 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=664
03/11/2022 04:52:24 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=669
03/11/2022 04:52:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.09 on epoch=674
03/11/2022 04:52:27 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.464039408866995 on epoch=674
03/11/2022 04:52:30 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
03/11/2022 04:52:32 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
03/11/2022 04:52:34 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=689
03/11/2022 04:52:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.07 on epoch=694
03/11/2022 04:52:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.08 on epoch=699
03/11/2022 04:52:39 - INFO - __main__ - Global step 1400 Train loss 0.07 Classification-F1 0.39756367663344405 on epoch=699
03/11/2022 04:52:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.12 on epoch=704
03/11/2022 04:52:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=709
03/11/2022 04:52:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=714
03/11/2022 04:52:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.08 on epoch=719
03/11/2022 04:52:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.04 on epoch=724
03/11/2022 04:52:51 - INFO - __main__ - Global step 1450 Train loss 0.08 Classification-F1 0.39139139139139134 on epoch=724
03/11/2022 04:52:53 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=729
03/11/2022 04:52:55 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.09 on epoch=734
03/11/2022 04:52:57 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
03/11/2022 04:52:59 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=744
03/11/2022 04:53:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
03/11/2022 04:53:02 - INFO - __main__ - Global step 1500 Train loss 0.06 Classification-F1 0.3764102564102564 on epoch=749
03/11/2022 04:53:05 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
03/11/2022 04:53:07 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.04 on epoch=759
03/11/2022 04:53:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
03/11/2022 04:53:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.04 on epoch=769
03/11/2022 04:53:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=774
03/11/2022 04:53:14 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.3650793650793651 on epoch=774
03/11/2022 04:53:16 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.03 on epoch=779
03/11/2022 04:53:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.02 on epoch=784
03/11/2022 04:53:21 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.08 on epoch=789
03/11/2022 04:53:23 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
03/11/2022 04:53:25 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.05 on epoch=799
03/11/2022 04:53:26 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.4009852216748768 on epoch=799
03/11/2022 04:53:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=804
03/11/2022 04:53:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=809
03/11/2022 04:53:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.02 on epoch=814
03/11/2022 04:53:35 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.07 on epoch=819
03/11/2022 04:53:37 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
03/11/2022 04:53:38 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.4909862142099682 on epoch=824
03/11/2022 04:53:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=829
03/11/2022 04:53:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
03/11/2022 04:53:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.08 on epoch=839
03/11/2022 04:53:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/11/2022 04:53:48 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=849
03/11/2022 04:53:49 - INFO - __main__ - Global step 1700 Train loss 0.05 Classification-F1 0.3650793650793651 on epoch=849
03/11/2022 04:53:51 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/11/2022 04:53:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.02 on epoch=859
03/11/2022 04:53:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
03/11/2022 04:53:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=869
03/11/2022 04:54:00 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
03/11/2022 04:54:01 - INFO - __main__ - Global step 1750 Train loss 0.03 Classification-F1 0.25 on epoch=874
03/11/2022 04:54:03 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
03/11/2022 04:54:05 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/11/2022 04:54:07 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
03/11/2022 04:54:10 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.01 on epoch=894
03/11/2022 04:54:12 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 04:54:13 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.34310850439882695 on epoch=899
03/11/2022 04:54:15 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 04:54:17 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/11/2022 04:54:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
03/11/2022 04:54:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.03 on epoch=919
03/11/2022 04:54:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.02 on epoch=924
03/11/2022 04:54:24 - INFO - __main__ - Global step 1850 Train loss 0.02 Classification-F1 0.4375 on epoch=924
03/11/2022 04:54:26 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 04:54:29 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/11/2022 04:54:31 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/11/2022 04:54:33 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 04:54:35 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
03/11/2022 04:54:36 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=949
03/11/2022 04:54:38 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.04 on epoch=954
03/11/2022 04:54:40 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.02 on epoch=959
03/11/2022 04:54:43 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
03/11/2022 04:54:45 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.01 on epoch=969
03/11/2022 04:54:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/11/2022 04:54:48 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.3650793650793651 on epoch=974
03/11/2022 04:54:50 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
03/11/2022 04:54:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 04:54:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
03/11/2022 04:54:56 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.01 on epoch=994
03/11/2022 04:54:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 04:54:59 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.39139139139139134 on epoch=999
03/11/2022 04:54:59 - INFO - __main__ - save last model!
03/11/2022 04:54:59 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 04:54:59 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 04:54:59 - INFO - __main__ - Printing 3 examples
03/11/2022 04:54:59 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 04:54:59 - INFO - __main__ - ['hate']
03/11/2022 04:54:59 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 04:54:59 - INFO - __main__ - ['non-irony']
03/11/2022 04:54:59 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 04:54:59 - INFO - __main__ - ['hate']
03/11/2022 04:54:59 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:55:00 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:55:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:55:00 - INFO - __main__ - Printing 3 examples
03/11/2022 04:55:00 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/11/2022 04:55:00 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:00 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/11/2022 04:55:00 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:00 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/11/2022 04:55:00 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 04:55:00 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:55:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:55:00 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:55:00 - INFO - __main__ - Printing 3 examples
03/11/2022 04:55:00 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/11/2022 04:55:00 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:00 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/11/2022 04:55:00 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:00 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/11/2022 04:55:00 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:00 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:55:00 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:55:00 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:55:01 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 04:55:13 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:55:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:55:13 - INFO - __main__ - Starting training!
03/11/2022 04:55:24 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_13_0.2_8_predictions.txt
03/11/2022 04:55:24 - INFO - __main__ - Classification-F1 on test data: 0.5654
03/11/2022 04:55:24 - INFO - __main__ - prefix=tweet_eval-irony_16_13, lr=0.2, bsz=8, dev_performance=0.5835835835835835, test_performance=0.5654273029966703
03/11/2022 04:55:24 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_21, lr=0.5, bsz=8 ...
03/11/2022 04:55:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:55:25 - INFO - __main__ - Printing 3 examples
03/11/2022 04:55:25 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/11/2022 04:55:25 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:25 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/11/2022 04:55:25 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:25 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/11/2022 04:55:25 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:25 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 04:55:25 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:55:25 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 04:55:25 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 04:55:25 - INFO - __main__ - Printing 3 examples
03/11/2022 04:55:25 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/11/2022 04:55:25 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:25 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/11/2022 04:55:25 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:25 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/11/2022 04:55:25 - INFO - __main__ - ['non-irony']
03/11/2022 04:55:25 - INFO - __main__ - Tokenizing Input ...
03/11/2022 04:55:25 - INFO - __main__ - Tokenizing Output ...
03/11/2022 04:55:25 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 04:55:38 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 04:55:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 04:55:38 - INFO - __main__ - Starting training!
03/11/2022 04:55:41 - INFO - __main__ - Step 10 Global step 10 Train loss 2.38 on epoch=4
03/11/2022 04:55:43 - INFO - __main__ - Step 20 Global step 20 Train loss 0.61 on epoch=9
03/11/2022 04:55:46 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=14
03/11/2022 04:55:48 - INFO - __main__ - Step 40 Global step 40 Train loss 0.36 on epoch=19
03/11/2022 04:55:50 - INFO - __main__ - Step 50 Global step 50 Train loss 0.38 on epoch=24
03/11/2022 04:55:51 - INFO - __main__ - Global step 50 Train loss 0.84 Classification-F1 0.36374269005847953 on epoch=24
03/11/2022 04:55:52 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.36374269005847953 on epoch=24, global_step=50
03/11/2022 04:55:54 - INFO - __main__ - Step 60 Global step 60 Train loss 0.35 on epoch=29
03/11/2022 04:55:56 - INFO - __main__ - Step 70 Global step 70 Train loss 0.33 on epoch=34
03/11/2022 04:55:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.26 on epoch=39
03/11/2022 04:56:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=44
03/11/2022 04:56:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.26 on epoch=49
03/11/2022 04:56:03 - INFO - __main__ - Global step 100 Train loss 0.30 Classification-F1 0.3992490613266583 on epoch=49
03/11/2022 04:56:03 - INFO - __main__ - Saving model with best Classification-F1: 0.36374269005847953 -> 0.3992490613266583 on epoch=49, global_step=100
03/11/2022 04:56:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.28 on epoch=54
03/11/2022 04:56:08 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=59
03/11/2022 04:56:10 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
03/11/2022 04:56:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=69
03/11/2022 04:56:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
03/11/2022 04:56:15 - INFO - __main__ - Global step 150 Train loss 0.28 Classification-F1 0.3552492046659597 on epoch=74
03/11/2022 04:56:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.28 on epoch=79
03/11/2022 04:56:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
03/11/2022 04:56:22 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/11/2022 04:56:24 - INFO - __main__ - Step 190 Global step 190 Train loss 0.27 on epoch=94
03/11/2022 04:56:26 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=99
03/11/2022 04:56:27 - INFO - __main__ - Global step 200 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 04:56:29 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 04:56:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.23 on epoch=109
03/11/2022 04:56:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=114
03/11/2022 04:56:36 - INFO - __main__ - Step 240 Global step 240 Train loss 0.23 on epoch=119
03/11/2022 04:56:38 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=124
03/11/2022 04:56:39 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.4920634920634921 on epoch=124
03/11/2022 04:56:39 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4920634920634921 on epoch=124, global_step=250
03/11/2022 04:56:41 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
03/11/2022 04:56:43 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=134
03/11/2022 04:56:45 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
03/11/2022 04:56:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
03/11/2022 04:56:50 - INFO - __main__ - Step 300 Global step 300 Train loss 0.21 on epoch=149
03/11/2022 04:56:50 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.4181818181818182 on epoch=149
03/11/2022 04:56:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
03/11/2022 04:56:55 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 04:56:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
03/11/2022 04:56:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.22 on epoch=169
03/11/2022 04:57:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
03/11/2022 04:57:02 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 04:57:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=179
03/11/2022 04:57:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
03/11/2022 04:57:09 - INFO - __main__ - Step 380 Global step 380 Train loss 0.23 on epoch=189
03/11/2022 04:57:11 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
03/11/2022 04:57:13 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
03/11/2022 04:57:14 - INFO - __main__ - Global step 400 Train loss 0.23 Classification-F1 0.4385964912280702 on epoch=199
03/11/2022 04:57:16 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=204
03/11/2022 04:57:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
03/11/2022 04:57:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
03/11/2022 04:57:23 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
03/11/2022 04:57:25 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
03/11/2022 04:57:26 - INFO - __main__ - Global step 450 Train loss 0.22 Classification-F1 0.3552492046659597 on epoch=224
03/11/2022 04:57:28 - INFO - __main__ - Step 460 Global step 460 Train loss 0.21 on epoch=229
03/11/2022 04:57:30 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=234
03/11/2022 04:57:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
03/11/2022 04:57:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
03/11/2022 04:57:37 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
03/11/2022 04:57:37 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.4231177094379639 on epoch=249
03/11/2022 04:57:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
03/11/2022 04:57:42 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
03/11/2022 04:57:44 - INFO - __main__ - Step 530 Global step 530 Train loss 0.19 on epoch=264
03/11/2022 04:57:46 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
03/11/2022 04:57:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=274
03/11/2022 04:57:49 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.3266888150609081 on epoch=274
03/11/2022 04:57:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.22 on epoch=279
03/11/2022 04:57:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.18 on epoch=284
03/11/2022 04:57:56 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=289
03/11/2022 04:57:58 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=294
03/11/2022 04:58:00 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
03/11/2022 04:58:01 - INFO - __main__ - Global step 600 Train loss 0.18 Classification-F1 0.5270935960591133 on epoch=299
03/11/2022 04:58:01 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.5270935960591133 on epoch=299, global_step=600
03/11/2022 04:58:03 - INFO - __main__ - Step 610 Global step 610 Train loss 0.20 on epoch=304
03/11/2022 04:58:05 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=309
03/11/2022 04:58:07 - INFO - __main__ - Step 630 Global step 630 Train loss 0.16 on epoch=314
03/11/2022 04:58:10 - INFO - __main__ - Step 640 Global step 640 Train loss 0.15 on epoch=319
03/11/2022 04:58:12 - INFO - __main__ - Step 650 Global step 650 Train loss 0.11 on epoch=324
03/11/2022 04:58:13 - INFO - __main__ - Global step 650 Train loss 0.16 Classification-F1 0.5933528836754642 on epoch=324
03/11/2022 04:58:13 - INFO - __main__ - Saving model with best Classification-F1: 0.5270935960591133 -> 0.5933528836754642 on epoch=324, global_step=650
03/11/2022 04:58:15 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
03/11/2022 04:58:17 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=334
03/11/2022 04:58:19 - INFO - __main__ - Step 680 Global step 680 Train loss 0.14 on epoch=339
03/11/2022 04:58:21 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=344
03/11/2022 04:58:23 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
03/11/2022 04:58:24 - INFO - __main__ - Global step 700 Train loss 0.15 Classification-F1 0.5933528836754642 on epoch=349
03/11/2022 04:58:26 - INFO - __main__ - Step 710 Global step 710 Train loss 0.11 on epoch=354
03/11/2022 04:58:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.05 on epoch=359
03/11/2022 04:58:31 - INFO - __main__ - Step 730 Global step 730 Train loss 0.04 on epoch=364
03/11/2022 04:58:33 - INFO - __main__ - Step 740 Global step 740 Train loss 0.05 on epoch=369
03/11/2022 04:58:35 - INFO - __main__ - Step 750 Global step 750 Train loss 0.05 on epoch=374
03/11/2022 04:58:36 - INFO - __main__ - Global step 750 Train loss 0.06 Classification-F1 0.49090909090909085 on epoch=374
03/11/2022 04:58:38 - INFO - __main__ - Step 760 Global step 760 Train loss 0.03 on epoch=379
03/11/2022 04:58:40 - INFO - __main__ - Step 770 Global step 770 Train loss 0.04 on epoch=384
03/11/2022 04:58:42 - INFO - __main__ - Step 780 Global step 780 Train loss 0.02 on epoch=389
03/11/2022 04:58:45 - INFO - __main__ - Step 790 Global step 790 Train loss 0.04 on epoch=394
03/11/2022 04:58:47 - INFO - __main__ - Step 800 Global step 800 Train loss 0.07 on epoch=399
03/11/2022 04:58:48 - INFO - __main__ - Global step 800 Train loss 0.04 Classification-F1 0.5333333333333333 on epoch=399
03/11/2022 04:58:50 - INFO - __main__ - Step 810 Global step 810 Train loss 0.04 on epoch=404
03/11/2022 04:58:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.04 on epoch=409
03/11/2022 04:58:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.02 on epoch=414
03/11/2022 04:58:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.01 on epoch=419
03/11/2022 04:58:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.03 on epoch=424
03/11/2022 04:58:59 - INFO - __main__ - Global step 850 Train loss 0.03 Classification-F1 0.4666666666666667 on epoch=424
03/11/2022 04:59:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.00 on epoch=429
03/11/2022 04:59:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.06 on epoch=434
03/11/2022 04:59:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.01 on epoch=439
03/11/2022 04:59:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.01 on epoch=444
03/11/2022 04:59:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.01 on epoch=449
03/11/2022 04:59:11 - INFO - __main__ - Global step 900 Train loss 0.02 Classification-F1 0.5555555555555556 on epoch=449
03/11/2022 04:59:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.01 on epoch=454
03/11/2022 04:59:15 - INFO - __main__ - Step 920 Global step 920 Train loss 0.01 on epoch=459
03/11/2022 04:59:17 - INFO - __main__ - Step 930 Global step 930 Train loss 0.01 on epoch=464
03/11/2022 04:59:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
03/11/2022 04:59:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.02 on epoch=474
03/11/2022 04:59:23 - INFO - __main__ - Global step 950 Train loss 0.02 Classification-F1 0.5307917888563051 on epoch=474
03/11/2022 04:59:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.01 on epoch=479
03/11/2022 04:59:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.01 on epoch=484
03/11/2022 04:59:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.00 on epoch=489
03/11/2022 04:59:31 - INFO - __main__ - Step 990 Global step 990 Train loss 0.00 on epoch=494
03/11/2022 04:59:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.01 on epoch=499
03/11/2022 04:59:34 - INFO - __main__ - Global step 1000 Train loss 0.01 Classification-F1 0.625 on epoch=499
03/11/2022 04:59:34 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.625 on epoch=499, global_step=1000
03/11/2022 04:59:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.01 on epoch=504
03/11/2022 04:59:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
03/11/2022 04:59:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.02 on epoch=514
03/11/2022 04:59:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.01 on epoch=519
03/11/2022 04:59:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/11/2022 04:59:46 - INFO - __main__ - Global step 1050 Train loss 0.02 Classification-F1 0.5555555555555556 on epoch=524
03/11/2022 04:59:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.00 on epoch=529
03/11/2022 04:59:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.01 on epoch=534
03/11/2022 04:59:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.00 on epoch=539
03/11/2022 04:59:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/11/2022 04:59:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.00 on epoch=549
03/11/2022 04:59:58 - INFO - __main__ - Global step 1100 Train loss 0.00 Classification-F1 0.6559139784946237 on epoch=549
03/11/2022 04:59:58 - INFO - __main__ - Saving model with best Classification-F1: 0.625 -> 0.6559139784946237 on epoch=549, global_step=1100
03/11/2022 05:00:00 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.00 on epoch=554
03/11/2022 05:00:02 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.00 on epoch=559
03/11/2022 05:00:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
03/11/2022 05:00:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/11/2022 05:00:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.00 on epoch=574
03/11/2022 05:00:10 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.6235294117647059 on epoch=574
03/11/2022 05:00:12 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.00 on epoch=579
03/11/2022 05:00:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/11/2022 05:00:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.00 on epoch=589
03/11/2022 05:00:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/11/2022 05:00:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.00 on epoch=599
03/11/2022 05:00:22 - INFO - __main__ - Global step 1200 Train loss 0.00 Classification-F1 0.6235294117647059 on epoch=599
03/11/2022 05:00:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/11/2022 05:00:26 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/11/2022 05:00:28 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/11/2022 05:00:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.00 on epoch=619
03/11/2022 05:00:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.00 on epoch=624
03/11/2022 05:00:33 - INFO - __main__ - Global step 1250 Train loss 0.00 Classification-F1 0.49090909090909085 on epoch=624
03/11/2022 05:00:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.03 on epoch=629
03/11/2022 05:00:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 05:00:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/11/2022 05:00:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/11/2022 05:00:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.00 on epoch=649
03/11/2022 05:00:45 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.625 on epoch=649
03/11/2022 05:00:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/11/2022 05:00:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/11/2022 05:00:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/11/2022 05:00:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 05:00:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/11/2022 05:00:57 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.6000000000000001 on epoch=674
03/11/2022 05:00:59 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 05:01:01 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 05:01:03 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
03/11/2022 05:01:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 05:01:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/11/2022 05:01:09 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=699
03/11/2022 05:01:11 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/11/2022 05:01:13 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 05:01:15 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 05:01:17 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 05:01:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.00 on epoch=724
03/11/2022 05:01:21 - INFO - __main__ - Global step 1450 Train loss 0.00 Classification-F1 0.6559139784946237 on epoch=724
03/11/2022 05:01:23 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 05:01:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 05:01:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 05:01:29 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 05:01:31 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 05:01:32 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.5607843137254902 on epoch=749
03/11/2022 05:01:34 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.03 on epoch=754
03/11/2022 05:01:37 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 05:01:39 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 05:01:41 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/11/2022 05:01:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 05:01:44 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.6532019704433498 on epoch=774
03/11/2022 05:01:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
03/11/2022 05:01:49 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 05:01:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 05:01:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/11/2022 05:01:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 05:01:56 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.6559139784946237 on epoch=799
03/11/2022 05:01:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 05:02:00 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 05:02:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 05:02:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 05:02:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 05:02:08 - INFO - __main__ - Global step 1650 Train loss 0.00 Classification-F1 0.6559139784946237 on epoch=824
03/11/2022 05:02:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 05:02:12 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/11/2022 05:02:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 05:02:17 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 05:02:19 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 05:02:20 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.6559139784946237 on epoch=849
03/11/2022 05:02:22 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 05:02:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 05:02:26 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 05:02:28 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 05:02:31 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 05:02:31 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.6875 on epoch=874
03/11/2022 05:02:31 - INFO - __main__ - Saving model with best Classification-F1: 0.6559139784946237 -> 0.6875 on epoch=874, global_step=1750
03/11/2022 05:02:34 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 05:02:36 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 05:02:38 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 05:02:40 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 05:02:42 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/11/2022 05:02:43 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.625 on epoch=899
03/11/2022 05:02:45 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 05:02:47 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 05:02:50 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 05:02:52 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 05:02:54 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 05:02:55 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5844155844155844 on epoch=924
03/11/2022 05:02:57 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 05:02:59 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 05:03:01 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 05:03:04 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 05:03:06 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 05:03:07 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=949
03/11/2022 05:03:09 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 05:03:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 05:03:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 05:03:16 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 05:03:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 05:03:19 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.6559139784946237 on epoch=974
03/11/2022 05:03:21 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 05:03:23 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 05:03:25 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.02 on epoch=989
03/11/2022 05:03:27 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 05:03:30 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 05:03:31 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.625 on epoch=999
03/11/2022 05:03:31 - INFO - __main__ - save last model!
03/11/2022 05:03:31 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 05:03:31 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 05:03:31 - INFO - __main__ - Printing 3 examples
03/11/2022 05:03:31 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 05:03:31 - INFO - __main__ - ['hate']
03/11/2022 05:03:31 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 05:03:31 - INFO - __main__ - ['non-irony']
03/11/2022 05:03:31 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 05:03:31 - INFO - __main__ - ['hate']
03/11/2022 05:03:31 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:03:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:03:31 - INFO - __main__ - Printing 3 examples
03/11/2022 05:03:31 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/11/2022 05:03:31 - INFO - __main__ - ['non-irony']
03/11/2022 05:03:31 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/11/2022 05:03:31 - INFO - __main__ - ['non-irony']
03/11/2022 05:03:31 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/11/2022 05:03:31 - INFO - __main__ - ['non-irony']
03/11/2022 05:03:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 05:03:31 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:03:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:03:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:03:31 - INFO - __main__ - Printing 3 examples
03/11/2022 05:03:31 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/11/2022 05:03:31 - INFO - __main__ - ['non-irony']
03/11/2022 05:03:31 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/11/2022 05:03:31 - INFO - __main__ - ['non-irony']
03/11/2022 05:03:31 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/11/2022 05:03:31 - INFO - __main__ - ['non-irony']
03/11/2022 05:03:31 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:03:31 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:03:31 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:03:31 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:03:32 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 05:03:44 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:03:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:03:44 - INFO - __main__ - Starting training!
03/11/2022 05:04:10 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_21_0.5_8_predictions.txt
03/11/2022 05:04:10 - INFO - __main__ - Classification-F1 on test data: 0.4617
03/11/2022 05:04:10 - INFO - __main__ - prefix=tweet_eval-irony_16_21, lr=0.5, bsz=8, dev_performance=0.6875, test_performance=0.46173229935171456
03/11/2022 05:04:10 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_21, lr=0.4, bsz=8 ...
03/11/2022 05:04:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:04:11 - INFO - __main__ - Printing 3 examples
03/11/2022 05:04:11 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/11/2022 05:04:11 - INFO - __main__ - ['non-irony']
03/11/2022 05:04:11 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/11/2022 05:04:11 - INFO - __main__ - ['non-irony']
03/11/2022 05:04:11 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/11/2022 05:04:11 - INFO - __main__ - ['non-irony']
03/11/2022 05:04:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:04:11 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:04:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:04:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:04:11 - INFO - __main__ - Printing 3 examples
03/11/2022 05:04:11 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/11/2022 05:04:11 - INFO - __main__ - ['non-irony']
03/11/2022 05:04:11 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/11/2022 05:04:11 - INFO - __main__ - ['non-irony']
03/11/2022 05:04:11 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/11/2022 05:04:11 - INFO - __main__ - ['non-irony']
03/11/2022 05:04:11 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:04:11 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:04:11 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:04:23 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:04:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:04:24 - INFO - __main__ - Starting training!
03/11/2022 05:04:27 - INFO - __main__ - Step 10 Global step 10 Train loss 2.47 on epoch=4
03/11/2022 05:04:29 - INFO - __main__ - Step 20 Global step 20 Train loss 0.72 on epoch=9
03/11/2022 05:04:31 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=14
03/11/2022 05:04:33 - INFO - __main__ - Step 40 Global step 40 Train loss 0.46 on epoch=19
03/11/2022 05:04:36 - INFO - __main__ - Step 50 Global step 50 Train loss 0.36 on epoch=24
03/11/2022 05:04:37 - INFO - __main__ - Global step 50 Train loss 0.92 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 05:04:37 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 05:04:39 - INFO - __main__ - Step 60 Global step 60 Train loss 0.34 on epoch=29
03/11/2022 05:04:41 - INFO - __main__ - Step 70 Global step 70 Train loss 0.33 on epoch=34
03/11/2022 05:04:43 - INFO - __main__ - Step 80 Global step 80 Train loss 0.33 on epoch=39
03/11/2022 05:04:45 - INFO - __main__ - Step 90 Global step 90 Train loss 0.30 on epoch=44
03/11/2022 05:04:47 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=49
03/11/2022 05:04:48 - INFO - __main__ - Global step 100 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 05:04:50 - INFO - __main__ - Step 110 Global step 110 Train loss 0.32 on epoch=54
03/11/2022 05:04:53 - INFO - __main__ - Step 120 Global step 120 Train loss 0.27 on epoch=59
03/11/2022 05:04:55 - INFO - __main__ - Step 130 Global step 130 Train loss 0.29 on epoch=64
03/11/2022 05:04:57 - INFO - __main__ - Step 140 Global step 140 Train loss 0.33 on epoch=69
03/11/2022 05:04:59 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=74
03/11/2022 05:05:00 - INFO - __main__ - Global step 150 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 05:05:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
03/11/2022 05:05:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=84
03/11/2022 05:05:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/11/2022 05:05:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
03/11/2022 05:05:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.36 on epoch=99
03/11/2022 05:05:12 - INFO - __main__ - Global step 200 Train loss 0.29 Classification-F1 0.3816425120772947 on epoch=99
03/11/2022 05:05:12 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3816425120772947 on epoch=99, global_step=200
03/11/2022 05:05:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.57 on epoch=104
03/11/2022 05:05:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=109
03/11/2022 05:05:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.47 on epoch=114
03/11/2022 05:05:21 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=119
03/11/2022 05:05:23 - INFO - __main__ - Step 250 Global step 250 Train loss 0.23 on epoch=124
03/11/2022 05:05:23 - INFO - __main__ - Global step 250 Train loss 0.37 Classification-F1 0.3992490613266583 on epoch=124
03/11/2022 05:05:23 - INFO - __main__ - Saving model with best Classification-F1: 0.3816425120772947 -> 0.3992490613266583 on epoch=124, global_step=250
03/11/2022 05:05:26 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
03/11/2022 05:05:28 - INFO - __main__ - Step 270 Global step 270 Train loss 0.22 on epoch=134
03/11/2022 05:05:30 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
03/11/2022 05:05:32 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=144
03/11/2022 05:05:34 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
03/11/2022 05:05:35 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 05:05:37 - INFO - __main__ - Step 310 Global step 310 Train loss 0.25 on epoch=154
03/11/2022 05:05:40 - INFO - __main__ - Step 320 Global step 320 Train loss 0.23 on epoch=159
03/11/2022 05:05:42 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=164
03/11/2022 05:05:44 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
03/11/2022 05:05:46 - INFO - __main__ - Step 350 Global step 350 Train loss 0.18 on epoch=174
03/11/2022 05:05:47 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.37662337662337664 on epoch=174
03/11/2022 05:05:49 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
03/11/2022 05:05:51 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
03/11/2022 05:05:54 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 05:05:56 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=194
03/11/2022 05:05:58 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 05:05:59 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.4181818181818182 on epoch=199
03/11/2022 05:05:59 - INFO - __main__ - Saving model with best Classification-F1: 0.3992490613266583 -> 0.4181818181818182 on epoch=199, global_step=400
03/11/2022 05:06:01 - INFO - __main__ - Step 410 Global step 410 Train loss 0.22 on epoch=204
03/11/2022 05:06:03 - INFO - __main__ - Step 420 Global step 420 Train loss 0.26 on epoch=209
03/11/2022 05:06:05 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
03/11/2022 05:06:07 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
03/11/2022 05:06:10 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
03/11/2022 05:06:10 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3816425120772947 on epoch=224
03/11/2022 05:06:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.26 on epoch=229
03/11/2022 05:06:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
03/11/2022 05:06:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.20 on epoch=239
03/11/2022 05:06:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=244
03/11/2022 05:06:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=249
03/11/2022 05:06:22 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.4181818181818182 on epoch=249
03/11/2022 05:06:24 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
03/11/2022 05:06:26 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
03/11/2022 05:06:29 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
03/11/2022 05:06:31 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=269
03/11/2022 05:06:33 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=274
03/11/2022 05:06:34 - INFO - __main__ - Global step 550 Train loss 0.24 Classification-F1 0.3816425120772947 on epoch=274
03/11/2022 05:06:36 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
03/11/2022 05:06:38 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
03/11/2022 05:06:40 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
03/11/2022 05:06:42 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
03/11/2022 05:06:45 - INFO - __main__ - Step 600 Global step 600 Train loss 0.26 on epoch=299
03/11/2022 05:06:45 - INFO - __main__ - Global step 600 Train loss 0.24 Classification-F1 0.46843853820598 on epoch=299
03/11/2022 05:06:45 - INFO - __main__ - Saving model with best Classification-F1: 0.4181818181818182 -> 0.46843853820598 on epoch=299, global_step=600
03/11/2022 05:06:48 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
03/11/2022 05:06:50 - INFO - __main__ - Step 620 Global step 620 Train loss 0.22 on epoch=309
03/11/2022 05:06:52 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=314
03/11/2022 05:06:54 - INFO - __main__ - Step 640 Global step 640 Train loss 0.24 on epoch=319
03/11/2022 05:06:56 - INFO - __main__ - Step 650 Global step 650 Train loss 0.23 on epoch=324
03/11/2022 05:06:57 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.5270935960591133 on epoch=324
03/11/2022 05:06:57 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5270935960591133 on epoch=324, global_step=650
03/11/2022 05:06:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=329
03/11/2022 05:07:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.25 on epoch=334
03/11/2022 05:07:04 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=339
03/11/2022 05:07:06 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=344
03/11/2022 05:07:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=349
03/11/2022 05:07:09 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.4181818181818182 on epoch=349
03/11/2022 05:07:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=354
03/11/2022 05:07:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
03/11/2022 05:07:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=364
03/11/2022 05:07:17 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=369
03/11/2022 05:07:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 05:07:21 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.40566959921798634 on epoch=374
03/11/2022 05:07:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
03/11/2022 05:07:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=384
03/11/2022 05:07:27 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=389
03/11/2022 05:07:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=394
03/11/2022 05:07:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.24 on epoch=399
03/11/2022 05:07:32 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.46843853820598 on epoch=399
03/11/2022 05:07:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
03/11/2022 05:07:37 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=409
03/11/2022 05:07:39 - INFO - __main__ - Step 830 Global step 830 Train loss 0.19 on epoch=414
03/11/2022 05:07:41 - INFO - __main__ - Step 840 Global step 840 Train loss 0.23 on epoch=419
03/11/2022 05:07:43 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
03/11/2022 05:07:44 - INFO - __main__ - Global step 850 Train loss 0.21 Classification-F1 0.4181818181818182 on epoch=424
03/11/2022 05:07:46 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
03/11/2022 05:07:48 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=434
03/11/2022 05:07:51 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=439
03/11/2022 05:07:53 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
03/11/2022 05:07:55 - INFO - __main__ - Step 900 Global step 900 Train loss 0.23 on epoch=449
03/11/2022 05:07:56 - INFO - __main__ - Global step 900 Train loss 0.23 Classification-F1 0.39139139139139134 on epoch=449
03/11/2022 05:07:58 - INFO - __main__ - Step 910 Global step 910 Train loss 0.23 on epoch=454
03/11/2022 05:08:00 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=459
03/11/2022 05:08:02 - INFO - __main__ - Step 930 Global step 930 Train loss 0.24 on epoch=464
03/11/2022 05:08:04 - INFO - __main__ - Step 940 Global step 940 Train loss 0.22 on epoch=469
03/11/2022 05:08:07 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
03/11/2022 05:08:07 - INFO - __main__ - Global step 950 Train loss 0.22 Classification-F1 0.375 on epoch=474
03/11/2022 05:08:10 - INFO - __main__ - Step 960 Global step 960 Train loss 0.22 on epoch=479
03/11/2022 05:08:12 - INFO - __main__ - Step 970 Global step 970 Train loss 0.23 on epoch=484
03/11/2022 05:08:14 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=489
03/11/2022 05:08:16 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=494
03/11/2022 05:08:18 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=499
03/11/2022 05:08:19 - INFO - __main__ - Global step 1000 Train loss 0.24 Classification-F1 0.5555555555555556 on epoch=499
03/11/2022 05:08:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5270935960591133 -> 0.5555555555555556 on epoch=499, global_step=1000
03/11/2022 05:08:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=504
03/11/2022 05:08:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=509
03/11/2022 05:08:26 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=514
03/11/2022 05:08:28 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=519
03/11/2022 05:08:30 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
03/11/2022 05:08:31 - INFO - __main__ - Global step 1050 Train loss 0.21 Classification-F1 0.464039408866995 on epoch=524
03/11/2022 05:08:33 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.19 on epoch=529
03/11/2022 05:08:35 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.22 on epoch=534
03/11/2022 05:08:37 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=539
03/11/2022 05:08:40 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=544
03/11/2022 05:08:42 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=549
03/11/2022 05:08:42 - INFO - __main__ - Global step 1100 Train loss 0.20 Classification-F1 0.43529411764705883 on epoch=549
03/11/2022 05:08:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=554
03/11/2022 05:08:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=559
03/11/2022 05:08:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=564
03/11/2022 05:08:51 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=569
03/11/2022 05:08:53 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=574
03/11/2022 05:08:54 - INFO - __main__ - Global step 1150 Train loss 0.21 Classification-F1 0.3764102564102564 on epoch=574
03/11/2022 05:08:56 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=579
03/11/2022 05:08:59 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.23 on epoch=584
03/11/2022 05:09:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=589
03/11/2022 05:09:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.19 on epoch=594
03/11/2022 05:09:05 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=599
03/11/2022 05:09:06 - INFO - __main__ - Global step 1200 Train loss 0.21 Classification-F1 0.4920634920634921 on epoch=599
03/11/2022 05:09:08 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.24 on epoch=604
03/11/2022 05:09:10 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=609
03/11/2022 05:09:12 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=614
03/11/2022 05:09:15 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=619
03/11/2022 05:09:17 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.18 on epoch=624
03/11/2022 05:09:18 - INFO - __main__ - Global step 1250 Train loss 0.21 Classification-F1 0.5270935960591133 on epoch=624
03/11/2022 05:09:20 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=629
03/11/2022 05:09:22 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=634
03/11/2022 05:09:24 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=639
03/11/2022 05:09:26 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=644
03/11/2022 05:09:28 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.18 on epoch=649
03/11/2022 05:09:29 - INFO - __main__ - Global step 1300 Train loss 0.19 Classification-F1 0.37662337662337664 on epoch=649
03/11/2022 05:09:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=654
03/11/2022 05:09:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.19 on epoch=659
03/11/2022 05:09:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.19 on epoch=664
03/11/2022 05:09:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/11/2022 05:09:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=674
03/11/2022 05:09:41 - INFO - __main__ - Global step 1350 Train loss 0.20 Classification-F1 0.4231177094379639 on epoch=674
03/11/2022 05:09:43 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=679
03/11/2022 05:09:45 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=684
03/11/2022 05:09:48 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.22 on epoch=689
03/11/2022 05:09:50 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
03/11/2022 05:09:52 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=699
03/11/2022 05:09:53 - INFO - __main__ - Global step 1400 Train loss 0.19 Classification-F1 0.4817813765182186 on epoch=699
03/11/2022 05:09:55 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=704
03/11/2022 05:09:57 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.20 on epoch=709
03/11/2022 05:09:59 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.21 on epoch=714
03/11/2022 05:10:01 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.17 on epoch=719
03/11/2022 05:10:04 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=724
03/11/2022 05:10:04 - INFO - __main__ - Global step 1450 Train loss 0.18 Classification-F1 0.6532019704433498 on epoch=724
03/11/2022 05:10:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5555555555555556 -> 0.6532019704433498 on epoch=724, global_step=1450
03/11/2022 05:10:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.21 on epoch=729
03/11/2022 05:10:09 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.17 on epoch=734
03/11/2022 05:10:11 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.20 on epoch=739
03/11/2022 05:10:13 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.20 on epoch=744
03/11/2022 05:10:15 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.17 on epoch=749
03/11/2022 05:10:16 - INFO - __main__ - Global step 1500 Train loss 0.19 Classification-F1 0.4420512820512821 on epoch=749
03/11/2022 05:10:18 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.16 on epoch=754
03/11/2022 05:10:21 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.19 on epoch=759
03/11/2022 05:10:23 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.20 on epoch=764
03/11/2022 05:10:25 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=769
03/11/2022 05:10:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=774
03/11/2022 05:10:28 - INFO - __main__ - Global step 1550 Train loss 0.19 Classification-F1 0.39756367663344405 on epoch=774
03/11/2022 05:10:30 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.17 on epoch=779
03/11/2022 05:10:32 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=784
03/11/2022 05:10:35 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=789
03/11/2022 05:10:37 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=794
03/11/2022 05:10:39 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=799
03/11/2022 05:10:40 - INFO - __main__ - Global step 1600 Train loss 0.16 Classification-F1 0.5901477832512315 on epoch=799
03/11/2022 05:10:42 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=804
03/11/2022 05:10:44 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.18 on epoch=809
03/11/2022 05:10:46 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=814
03/11/2022 05:10:48 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.18 on epoch=819
03/11/2022 05:10:51 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.15 on epoch=824
03/11/2022 05:10:51 - INFO - __main__ - Global step 1650 Train loss 0.18 Classification-F1 0.5588547189819725 on epoch=824
03/11/2022 05:10:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.19 on epoch=829
03/11/2022 05:10:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.17 on epoch=834
03/11/2022 05:10:58 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.14 on epoch=839
03/11/2022 05:11:00 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=844
03/11/2022 05:11:02 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=849
03/11/2022 05:11:03 - INFO - __main__ - Global step 1700 Train loss 0.17 Classification-F1 0.4666666666666667 on epoch=849
03/11/2022 05:11:05 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.15 on epoch=854
03/11/2022 05:11:07 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=859
03/11/2022 05:11:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=864
03/11/2022 05:11:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=869
03/11/2022 05:11:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=874
03/11/2022 05:11:15 - INFO - __main__ - Global step 1750 Train loss 0.15 Classification-F1 0.5195195195195195 on epoch=874
03/11/2022 05:11:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=879
03/11/2022 05:11:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.16 on epoch=884
03/11/2022 05:11:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/11/2022 05:11:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=894
03/11/2022 05:11:26 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.13 on epoch=899
03/11/2022 05:11:26 - INFO - __main__ - Global step 1800 Train loss 0.17 Classification-F1 0.5 on epoch=899
03/11/2022 05:11:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=904
03/11/2022 05:11:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.16 on epoch=909
03/11/2022 05:11:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=914
03/11/2022 05:11:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.14 on epoch=919
03/11/2022 05:11:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.16 on epoch=924
03/11/2022 05:11:38 - INFO - __main__ - Global step 1850 Train loss 0.15 Classification-F1 0.3333333333333333 on epoch=924
03/11/2022 05:11:40 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=929
03/11/2022 05:11:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=934
03/11/2022 05:11:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.12 on epoch=939
03/11/2022 05:11:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=944
03/11/2022 05:11:49 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=949
03/11/2022 05:11:50 - INFO - __main__ - Global step 1900 Train loss 0.11 Classification-F1 0.6000000000000001 on epoch=949
03/11/2022 05:11:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=954
03/11/2022 05:11:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=959
03/11/2022 05:11:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=964
03/11/2022 05:11:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.11 on epoch=969
03/11/2022 05:12:01 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=974
03/11/2022 05:12:02 - INFO - __main__ - Global step 1950 Train loss 0.11 Classification-F1 0.4231177094379639 on epoch=974
03/11/2022 05:12:04 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.11 on epoch=979
03/11/2022 05:12:06 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=984
03/11/2022 05:12:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=989
03/11/2022 05:12:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=994
03/11/2022 05:12:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
03/11/2022 05:12:13 - INFO - __main__ - Global step 2000 Train loss 0.09 Classification-F1 0.39999999999999997 on epoch=999
03/11/2022 05:12:13 - INFO - __main__ - save last model!
03/11/2022 05:12:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 05:12:13 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 05:12:13 - INFO - __main__ - Printing 3 examples
03/11/2022 05:12:13 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 05:12:13 - INFO - __main__ - ['hate']
03/11/2022 05:12:13 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 05:12:13 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:13 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 05:12:13 - INFO - __main__ - ['hate']
03/11/2022 05:12:13 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:12:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:12:14 - INFO - __main__ - Printing 3 examples
03/11/2022 05:12:14 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/11/2022 05:12:14 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:14 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/11/2022 05:12:14 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:14 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/11/2022 05:12:14 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 05:12:14 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:12:14 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:12:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:12:14 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:12:14 - INFO - __main__ - Printing 3 examples
03/11/2022 05:12:14 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/11/2022 05:12:14 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:14 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/11/2022 05:12:14 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:14 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/11/2022 05:12:14 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:14 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:12:14 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:12:14 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:12:15 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 05:12:27 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:12:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:12:27 - INFO - __main__ - Starting training!
03/11/2022 05:12:38 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_21_0.4_8_predictions.txt
03/11/2022 05:12:38 - INFO - __main__ - Classification-F1 on test data: 0.4856
03/11/2022 05:12:38 - INFO - __main__ - prefix=tweet_eval-irony_16_21, lr=0.4, bsz=8, dev_performance=0.6532019704433498, test_performance=0.48561229892822577
03/11/2022 05:12:38 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_21, lr=0.3, bsz=8 ...
03/11/2022 05:12:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:12:39 - INFO - __main__ - Printing 3 examples
03/11/2022 05:12:39 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/11/2022 05:12:39 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:39 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/11/2022 05:12:39 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:39 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/11/2022 05:12:39 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:12:39 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:12:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:12:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:12:39 - INFO - __main__ - Printing 3 examples
03/11/2022 05:12:39 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/11/2022 05:12:39 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:39 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/11/2022 05:12:39 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:39 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/11/2022 05:12:39 - INFO - __main__ - ['non-irony']
03/11/2022 05:12:39 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:12:39 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:12:39 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:12:51 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:12:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:12:52 - INFO - __main__ - Starting training!
03/11/2022 05:12:55 - INFO - __main__ - Step 10 Global step 10 Train loss 2.73 on epoch=4
03/11/2022 05:12:57 - INFO - __main__ - Step 20 Global step 20 Train loss 0.91 on epoch=9
03/11/2022 05:12:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.59 on epoch=14
03/11/2022 05:13:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.38 on epoch=19
03/11/2022 05:13:04 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=24
03/11/2022 05:13:04 - INFO - __main__ - Global step 50 Train loss 1.01 Classification-F1 0.6559139784946237 on epoch=24
03/11/2022 05:13:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.6559139784946237 on epoch=24, global_step=50
03/11/2022 05:13:07 - INFO - __main__ - Step 60 Global step 60 Train loss 0.37 on epoch=29
03/11/2022 05:13:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.31 on epoch=34
03/11/2022 05:13:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.39 on epoch=39
03/11/2022 05:13:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.29 on epoch=44
03/11/2022 05:13:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.28 on epoch=49
03/11/2022 05:13:16 - INFO - __main__ - Global step 100 Train loss 0.33 Classification-F1 0.36374269005847953 on epoch=49
03/11/2022 05:13:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.27 on epoch=54
03/11/2022 05:13:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
03/11/2022 05:13:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
03/11/2022 05:13:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=69
03/11/2022 05:13:27 - INFO - __main__ - Step 150 Global step 150 Train loss 0.28 on epoch=74
03/11/2022 05:13:28 - INFO - __main__ - Global step 150 Train loss 0.29 Classification-F1 0.5195195195195195 on epoch=74
03/11/2022 05:13:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=79
03/11/2022 05:13:32 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
03/11/2022 05:13:34 - INFO - __main__ - Step 180 Global step 180 Train loss 0.23 on epoch=89
03/11/2022 05:13:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.34 on epoch=94
03/11/2022 05:13:39 - INFO - __main__ - Step 200 Global step 200 Train loss 0.32 on epoch=99
03/11/2022 05:13:40 - INFO - __main__ - Global step 200 Train loss 0.30 Classification-F1 0.3992490613266583 on epoch=99
03/11/2022 05:13:42 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
03/11/2022 05:13:44 - INFO - __main__ - Step 220 Global step 220 Train loss 0.24 on epoch=109
03/11/2022 05:13:46 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=114
03/11/2022 05:13:48 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
03/11/2022 05:13:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.21 on epoch=124
03/11/2022 05:13:51 - INFO - __main__ - Global step 250 Train loss 0.27 Classification-F1 0.37662337662337664 on epoch=124
03/11/2022 05:13:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.25 on epoch=129
03/11/2022 05:13:56 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
03/11/2022 05:13:58 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 05:14:00 - INFO - __main__ - Step 290 Global step 290 Train loss 0.26 on epoch=144
03/11/2022 05:14:02 - INFO - __main__ - Step 300 Global step 300 Train loss 0.22 on epoch=149
03/11/2022 05:14:03 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.39999999999999997 on epoch=149
03/11/2022 05:14:05 - INFO - __main__ - Step 310 Global step 310 Train loss 0.23 on epoch=154
03/11/2022 05:14:07 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
03/11/2022 05:14:10 - INFO - __main__ - Step 330 Global step 330 Train loss 0.22 on epoch=164
03/11/2022 05:14:12 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 05:14:14 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
03/11/2022 05:14:15 - INFO - __main__ - Global step 350 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 05:14:17 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
03/11/2022 05:14:19 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 05:14:21 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 05:14:24 - INFO - __main__ - Step 390 Global step 390 Train loss 0.23 on epoch=194
03/11/2022 05:14:26 - INFO - __main__ - Step 400 Global step 400 Train loss 0.23 on epoch=199
03/11/2022 05:14:27 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.40566959921798634 on epoch=199
03/11/2022 05:14:29 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=204
03/11/2022 05:14:31 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
03/11/2022 05:14:33 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=214
03/11/2022 05:14:35 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
03/11/2022 05:14:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
03/11/2022 05:14:38 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3191489361702127 on epoch=224
03/11/2022 05:14:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 05:14:43 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
03/11/2022 05:14:45 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
03/11/2022 05:14:47 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
03/11/2022 05:14:49 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
03/11/2022 05:14:50 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.5607843137254902 on epoch=249
03/11/2022 05:14:52 - INFO - __main__ - Step 510 Global step 510 Train loss 0.24 on epoch=254
03/11/2022 05:14:54 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
03/11/2022 05:14:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
03/11/2022 05:14:59 - INFO - __main__ - Step 540 Global step 540 Train loss 0.21 on epoch=269
03/11/2022 05:15:01 - INFO - __main__ - Step 550 Global step 550 Train loss 0.19 on epoch=274
03/11/2022 05:15:02 - INFO - __main__ - Global step 550 Train loss 0.21 Classification-F1 0.4420512820512821 on epoch=274
03/11/2022 05:15:04 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
03/11/2022 05:15:06 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=284
03/11/2022 05:15:08 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=289
03/11/2022 05:15:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=294
03/11/2022 05:15:13 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
03/11/2022 05:15:13 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.37662337662337664 on epoch=299
03/11/2022 05:15:16 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
03/11/2022 05:15:18 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=309
03/11/2022 05:15:20 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=314
03/11/2022 05:15:22 - INFO - __main__ - Step 640 Global step 640 Train loss 0.20 on epoch=319
03/11/2022 05:15:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.21 on epoch=324
03/11/2022 05:15:25 - INFO - __main__ - Global step 650 Train loss 0.20 Classification-F1 0.4817813765182186 on epoch=324
03/11/2022 05:15:27 - INFO - __main__ - Step 660 Global step 660 Train loss 0.20 on epoch=329
03/11/2022 05:15:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=334
03/11/2022 05:15:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=339
03/11/2022 05:15:34 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=344
03/11/2022 05:15:36 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=349
03/11/2022 05:15:37 - INFO - __main__ - Global step 700 Train loss 0.20 Classification-F1 0.5465587044534412 on epoch=349
03/11/2022 05:15:39 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
03/11/2022 05:15:41 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=359
03/11/2022 05:15:43 - INFO - __main__ - Step 730 Global step 730 Train loss 0.24 on epoch=364
03/11/2022 05:15:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=369
03/11/2022 05:15:48 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=374
03/11/2022 05:15:48 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.5607843137254902 on epoch=374
03/11/2022 05:15:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.18 on epoch=379
03/11/2022 05:15:53 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
03/11/2022 05:15:55 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=389
03/11/2022 05:15:57 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=394
03/11/2022 05:15:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=399
03/11/2022 05:16:00 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.43529411764705883 on epoch=399
03/11/2022 05:16:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=404
03/11/2022 05:16:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.20 on epoch=409
03/11/2022 05:16:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=414
03/11/2022 05:16:09 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=419
03/11/2022 05:16:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.20 on epoch=424
03/11/2022 05:16:12 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.5151515151515151 on epoch=424
03/11/2022 05:16:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.21 on epoch=429
03/11/2022 05:16:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=434
03/11/2022 05:16:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=439
03/11/2022 05:16:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.20 on epoch=444
03/11/2022 05:16:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=449
03/11/2022 05:16:23 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.5195195195195195 on epoch=449
03/11/2022 05:16:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.21 on epoch=454
03/11/2022 05:16:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.19 on epoch=459
03/11/2022 05:16:30 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=464
03/11/2022 05:16:32 - INFO - __main__ - Step 940 Global step 940 Train loss 0.21 on epoch=469
03/11/2022 05:16:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.16 on epoch=474
03/11/2022 05:16:35 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.4666666666666667 on epoch=474
03/11/2022 05:16:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=479
03/11/2022 05:16:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=484
03/11/2022 05:16:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.17 on epoch=489
03/11/2022 05:16:44 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=494
03/11/2022 05:16:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=499
03/11/2022 05:16:47 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.4909862142099682 on epoch=499
03/11/2022 05:16:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=504
03/11/2022 05:16:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=509
03/11/2022 05:16:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.13 on epoch=514
03/11/2022 05:16:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=519
03/11/2022 05:16:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=524
03/11/2022 05:16:58 - INFO - __main__ - Global step 1050 Train loss 0.14 Classification-F1 0.5195195195195195 on epoch=524
03/11/2022 05:17:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.16 on epoch=529
03/11/2022 05:17:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=534
03/11/2022 05:17:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=539
03/11/2022 05:17:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.15 on epoch=544
03/11/2022 05:17:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=549
03/11/2022 05:17:10 - INFO - __main__ - Global step 1100 Train loss 0.14 Classification-F1 0.539313399778516 on epoch=549
03/11/2022 05:17:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.13 on epoch=554
03/11/2022 05:17:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.12 on epoch=559
03/11/2022 05:17:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=564
03/11/2022 05:17:19 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.08 on epoch=569
03/11/2022 05:17:21 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=574
03/11/2022 05:17:21 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.3816425120772947 on epoch=574
03/11/2022 05:17:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
03/11/2022 05:17:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=584
03/11/2022 05:17:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
03/11/2022 05:17:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.05 on epoch=594
03/11/2022 05:17:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.07 on epoch=599
03/11/2022 05:17:33 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.6875 on epoch=599
03/11/2022 05:17:33 - INFO - __main__ - Saving model with best Classification-F1: 0.6559139784946237 -> 0.6875 on epoch=599, global_step=1200
03/11/2022 05:17:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.04 on epoch=604
03/11/2022 05:17:38 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=609
03/11/2022 05:17:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
03/11/2022 05:17:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.03 on epoch=619
03/11/2022 05:17:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
03/11/2022 05:17:45 - INFO - __main__ - Global step 1250 Train loss 0.05 Classification-F1 0.5555555555555556 on epoch=624
03/11/2022 05:17:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.08 on epoch=629
03/11/2022 05:17:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.03 on epoch=634
03/11/2022 05:17:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.02 on epoch=639
03/11/2022 05:17:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.05 on epoch=644
03/11/2022 05:17:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.02 on epoch=649
03/11/2022 05:17:57 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.5465587044534412 on epoch=649
03/11/2022 05:17:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
03/11/2022 05:18:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=659
03/11/2022 05:18:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
03/11/2022 05:18:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
03/11/2022 05:18:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/11/2022 05:18:08 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.46843853820598 on epoch=674
03/11/2022 05:18:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.01 on epoch=679
03/11/2022 05:18:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/11/2022 05:18:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/11/2022 05:18:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/11/2022 05:18:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/11/2022 05:18:20 - INFO - __main__ - Global step 1400 Train loss 0.01 Classification-F1 0.4231177094379639 on epoch=699
03/11/2022 05:18:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
03/11/2022 05:18:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.03 on epoch=709
03/11/2022 05:18:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/11/2022 05:18:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
03/11/2022 05:18:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
03/11/2022 05:18:32 - INFO - __main__ - Global step 1450 Train loss 0.03 Classification-F1 0.5151515151515151 on epoch=724
03/11/2022 05:18:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/11/2022 05:18:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/11/2022 05:18:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 05:18:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/11/2022 05:18:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
03/11/2022 05:18:44 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.3162393162393162 on epoch=749
03/11/2022 05:18:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/11/2022 05:18:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/11/2022 05:18:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/11/2022 05:18:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 05:18:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 05:18:56 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.5465587044534412 on epoch=774
03/11/2022 05:18:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
03/11/2022 05:19:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 05:19:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 05:19:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 05:19:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 05:19:08 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.6190476190476191 on epoch=799
03/11/2022 05:19:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 05:19:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 05:19:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.04 on epoch=814
03/11/2022 05:19:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/11/2022 05:19:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.02 on epoch=824
03/11/2022 05:19:19 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.6389743589743591 on epoch=824
03/11/2022 05:19:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 05:19:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/11/2022 05:19:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 05:19:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.01 on epoch=844
03/11/2022 05:19:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/11/2022 05:19:31 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=849
03/11/2022 05:19:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/11/2022 05:19:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 05:19:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/11/2022 05:19:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
03/11/2022 05:19:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.03 on epoch=874
03/11/2022 05:19:43 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.4420512820512821 on epoch=874
03/11/2022 05:19:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 05:19:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 05:19:50 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 05:19:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 05:19:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
03/11/2022 05:19:55 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.46843853820598 on epoch=899
03/11/2022 05:19:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 05:19:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/11/2022 05:20:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
03/11/2022 05:20:04 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 05:20:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/11/2022 05:20:07 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.3950617283950617 on epoch=924
03/11/2022 05:20:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 05:20:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 05:20:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/11/2022 05:20:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 05:20:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/11/2022 05:20:19 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=949
03/11/2022 05:20:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 05:20:23 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 05:20:25 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 05:20:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 05:20:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/11/2022 05:20:31 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4666666666666667 on epoch=974
03/11/2022 05:20:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 05:20:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 05:20:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 05:20:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 05:20:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 05:20:43 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5901477832512315 on epoch=999
03/11/2022 05:20:43 - INFO - __main__ - save last model!
03/11/2022 05:20:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 05:20:43 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 05:20:43 - INFO - __main__ - Printing 3 examples
03/11/2022 05:20:43 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 05:20:43 - INFO - __main__ - ['hate']
03/11/2022 05:20:43 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 05:20:43 - INFO - __main__ - ['non-irony']
03/11/2022 05:20:43 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 05:20:43 - INFO - __main__ - ['hate']
03/11/2022 05:20:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:20:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:20:43 - INFO - __main__ - Printing 3 examples
03/11/2022 05:20:43 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/11/2022 05:20:43 - INFO - __main__ - ['non-irony']
03/11/2022 05:20:43 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/11/2022 05:20:43 - INFO - __main__ - ['non-irony']
03/11/2022 05:20:43 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/11/2022 05:20:43 - INFO - __main__ - ['non-irony']
03/11/2022 05:20:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 05:20:43 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:20:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:20:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:20:43 - INFO - __main__ - Printing 3 examples
03/11/2022 05:20:43 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/11/2022 05:20:43 - INFO - __main__ - ['non-irony']
03/11/2022 05:20:43 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/11/2022 05:20:43 - INFO - __main__ - ['non-irony']
03/11/2022 05:20:43 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/11/2022 05:20:43 - INFO - __main__ - ['non-irony']
03/11/2022 05:20:43 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:20:43 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:20:43 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:20:44 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:20:45 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 05:20:56 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:20:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:20:57 - INFO - __main__ - Starting training!
03/11/2022 05:21:15 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_21_0.3_8_predictions.txt
03/11/2022 05:21:15 - INFO - __main__ - Classification-F1 on test data: 0.0687
03/11/2022 05:21:15 - INFO - __main__ - prefix=tweet_eval-irony_16_21, lr=0.3, bsz=8, dev_performance=0.6875, test_performance=0.0686724065343219
03/11/2022 05:21:15 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_21, lr=0.2, bsz=8 ...
03/11/2022 05:21:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:21:16 - INFO - __main__ - Printing 3 examples
03/11/2022 05:21:16 - INFO - __main__ -  [tweet_eval-irony] i'm 55 and i've believed for most of my life that God only created 1 race @user @user @user @user
03/11/2022 05:21:16 - INFO - __main__ - ['non-irony']
03/11/2022 05:21:16 - INFO - __main__ -  [tweet_eval-irony] @user lids sucks officially! Will not use again good
03/11/2022 05:21:16 - INFO - __main__ - ['non-irony']
03/11/2022 05:21:16 - INFO - __main__ -  [tweet_eval-irony] I guess I'm dreaming. How else am I able to visit this site. Guess they won't let a prisoner have a phone right?
03/11/2022 05:21:16 - INFO - __main__ - ['non-irony']
03/11/2022 05:21:16 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:21:16 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:21:16 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:21:16 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:21:16 - INFO - __main__ - Printing 3 examples
03/11/2022 05:21:16 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user it was an In progress list of "MN of the year"
03/11/2022 05:21:16 - INFO - __main__ - ['non-irony']
03/11/2022 05:21:16 - INFO - __main__ -  [tweet_eval-irony] @user @user @user @user @user @user mmmm 😂😂😂
03/11/2022 05:21:16 - INFO - __main__ - ['non-irony']
03/11/2022 05:21:16 - INFO - __main__ -  [tweet_eval-irony] Favorite @user if you love Romance Novels. #Love #Drama #Romance
03/11/2022 05:21:16 - INFO - __main__ - ['non-irony']
03/11/2022 05:21:16 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:21:16 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:21:16 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:21:28 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:21:29 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:21:29 - INFO - __main__ - Starting training!
03/11/2022 05:21:32 - INFO - __main__ - Step 10 Global step 10 Train loss 2.90 on epoch=4
03/11/2022 05:21:34 - INFO - __main__ - Step 20 Global step 20 Train loss 1.34 on epoch=9
03/11/2022 05:21:36 - INFO - __main__ - Step 30 Global step 30 Train loss 0.71 on epoch=14
03/11/2022 05:21:39 - INFO - __main__ - Step 40 Global step 40 Train loss 0.54 on epoch=19
03/11/2022 05:21:41 - INFO - __main__ - Step 50 Global step 50 Train loss 0.45 on epoch=24
03/11/2022 05:21:42 - INFO - __main__ - Global step 50 Train loss 1.19 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 05:21:42 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 05:21:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.44 on epoch=29
03/11/2022 05:21:46 - INFO - __main__ - Step 70 Global step 70 Train loss 0.40 on epoch=34
03/11/2022 05:21:48 - INFO - __main__ - Step 80 Global step 80 Train loss 0.40 on epoch=39
03/11/2022 05:21:50 - INFO - __main__ - Step 90 Global step 90 Train loss 0.38 on epoch=44
03/11/2022 05:21:53 - INFO - __main__ - Step 100 Global step 100 Train loss 0.38 on epoch=49
03/11/2022 05:21:53 - INFO - __main__ - Global step 100 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 05:21:56 - INFO - __main__ - Step 110 Global step 110 Train loss 0.35 on epoch=54
03/11/2022 05:21:58 - INFO - __main__ - Step 120 Global step 120 Train loss 0.34 on epoch=59
03/11/2022 05:22:00 - INFO - __main__ - Step 130 Global step 130 Train loss 0.39 on epoch=64
03/11/2022 05:22:02 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
03/11/2022 05:22:04 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
03/11/2022 05:22:05 - INFO - __main__ - Global step 150 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 05:22:07 - INFO - __main__ - Step 160 Global step 160 Train loss 0.31 on epoch=79
03/11/2022 05:22:09 - INFO - __main__ - Step 170 Global step 170 Train loss 0.28 on epoch=84
03/11/2022 05:22:12 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=89
03/11/2022 05:22:14 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
03/11/2022 05:22:16 - INFO - __main__ - Step 200 Global step 200 Train loss 0.24 on epoch=99
03/11/2022 05:22:17 - INFO - __main__ - Global step 200 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 05:22:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
03/11/2022 05:22:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
03/11/2022 05:22:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
03/11/2022 05:22:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.25 on epoch=119
03/11/2022 05:22:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
03/11/2022 05:22:28 - INFO - __main__ - Global step 250 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 05:22:31 - INFO - __main__ - Step 260 Global step 260 Train loss 0.32 on epoch=129
03/11/2022 05:22:33 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 05:22:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.21 on epoch=139
03/11/2022 05:22:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.29 on epoch=144
03/11/2022 05:22:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=149
03/11/2022 05:22:40 - INFO - __main__ - Global step 300 Train loss 0.27 Classification-F1 0.5076923076923077 on epoch=149
03/11/2022 05:22:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.5076923076923077 on epoch=149, global_step=300
03/11/2022 05:22:42 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=154
03/11/2022 05:22:44 - INFO - __main__ - Step 320 Global step 320 Train loss 0.29 on epoch=159
03/11/2022 05:22:47 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
03/11/2022 05:22:49 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 05:22:51 - INFO - __main__ - Step 350 Global step 350 Train loss 0.28 on epoch=174
03/11/2022 05:22:52 - INFO - __main__ - Global step 350 Train loss 0.27 Classification-F1 0.39999999999999997 on epoch=174
03/11/2022 05:22:54 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=179
03/11/2022 05:22:56 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
03/11/2022 05:22:58 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=189
03/11/2022 05:23:01 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
03/11/2022 05:23:03 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=199
03/11/2022 05:23:04 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.37662337662337664 on epoch=199
03/11/2022 05:23:06 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 05:23:08 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
03/11/2022 05:23:10 - INFO - __main__ - Step 430 Global step 430 Train loss 0.22 on epoch=214
03/11/2022 05:23:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
03/11/2022 05:23:15 - INFO - __main__ - Step 450 Global step 450 Train loss 0.21 on epoch=224
03/11/2022 05:23:15 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3992490613266583 on epoch=224
03/11/2022 05:23:17 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
03/11/2022 05:23:20 - INFO - __main__ - Step 470 Global step 470 Train loss 0.25 on epoch=234
03/11/2022 05:23:22 - INFO - __main__ - Step 480 Global step 480 Train loss 0.21 on epoch=239
03/11/2022 05:23:24 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=244
03/11/2022 05:23:26 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
03/11/2022 05:23:27 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3992490613266583 on epoch=249
03/11/2022 05:23:29 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
03/11/2022 05:23:31 - INFO - __main__ - Step 520 Global step 520 Train loss 0.25 on epoch=259
03/11/2022 05:23:33 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=264
03/11/2022 05:23:36 - INFO - __main__ - Step 540 Global step 540 Train loss 0.19 on epoch=269
03/11/2022 05:23:38 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=274
03/11/2022 05:23:39 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.3992490613266583 on epoch=274
03/11/2022 05:23:41 - INFO - __main__ - Step 560 Global step 560 Train loss 0.19 on epoch=279
03/11/2022 05:23:43 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=284
03/11/2022 05:23:45 - INFO - __main__ - Step 580 Global step 580 Train loss 0.21 on epoch=289
03/11/2022 05:23:47 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=294
03/11/2022 05:23:49 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
03/11/2022 05:23:50 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.4909862142099682 on epoch=299
03/11/2022 05:23:52 - INFO - __main__ - Step 610 Global step 610 Train loss 0.22 on epoch=304
03/11/2022 05:23:55 - INFO - __main__ - Step 620 Global step 620 Train loss 0.20 on epoch=309
03/11/2022 05:23:57 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
03/11/2022 05:23:59 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=319
03/11/2022 05:24:01 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=324
03/11/2022 05:24:02 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.36374269005847953 on epoch=324
03/11/2022 05:24:04 - INFO - __main__ - Step 660 Global step 660 Train loss 0.24 on epoch=329
03/11/2022 05:24:06 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
03/11/2022 05:24:08 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=339
03/11/2022 05:24:11 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=344
03/11/2022 05:24:13 - INFO - __main__ - Step 700 Global step 700 Train loss 0.24 on epoch=349
03/11/2022 05:24:14 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.3992490613266583 on epoch=349
03/11/2022 05:24:16 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=354
03/11/2022 05:24:18 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=359
03/11/2022 05:24:20 - INFO - __main__ - Step 730 Global step 730 Train loss 0.22 on epoch=364
03/11/2022 05:24:22 - INFO - __main__ - Step 740 Global step 740 Train loss 0.21 on epoch=369
03/11/2022 05:24:24 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=374
03/11/2022 05:24:25 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.5195195195195195 on epoch=374
03/11/2022 05:24:25 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5195195195195195 on epoch=374, global_step=750
03/11/2022 05:24:27 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
03/11/2022 05:24:30 - INFO - __main__ - Step 770 Global step 770 Train loss 0.23 on epoch=384
03/11/2022 05:24:32 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=389
03/11/2022 05:24:34 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=394
03/11/2022 05:24:36 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=399
03/11/2022 05:24:37 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.4909862142099682 on epoch=399
03/11/2022 05:24:39 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
03/11/2022 05:24:41 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=409
03/11/2022 05:24:43 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=414
03/11/2022 05:24:46 - INFO - __main__ - Step 840 Global step 840 Train loss 0.22 on epoch=419
03/11/2022 05:24:48 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=424
03/11/2022 05:24:49 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.36374269005847953 on epoch=424
03/11/2022 05:24:51 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
03/11/2022 05:24:53 - INFO - __main__ - Step 870 Global step 870 Train loss 0.22 on epoch=434
03/11/2022 05:24:55 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
03/11/2022 05:24:57 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=444
03/11/2022 05:24:59 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=449
03/11/2022 05:25:00 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.4817813765182186 on epoch=449
03/11/2022 05:25:02 - INFO - __main__ - Step 910 Global step 910 Train loss 0.25 on epoch=454
03/11/2022 05:25:05 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=459
03/11/2022 05:25:07 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
03/11/2022 05:25:09 - INFO - __main__ - Step 940 Global step 940 Train loss 0.23 on epoch=469
03/11/2022 05:25:11 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
03/11/2022 05:25:12 - INFO - __main__ - Global step 950 Train loss 0.21 Classification-F1 0.625 on epoch=474
03/11/2022 05:25:12 - INFO - __main__ - Saving model with best Classification-F1: 0.5195195195195195 -> 0.625 on epoch=474, global_step=950
03/11/2022 05:25:14 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=479
03/11/2022 05:25:16 - INFO - __main__ - Step 970 Global step 970 Train loss 0.18 on epoch=484
03/11/2022 05:25:18 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=489
03/11/2022 05:25:21 - INFO - __main__ - Step 990 Global step 990 Train loss 0.22 on epoch=494
03/11/2022 05:25:23 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.19 on epoch=499
03/11/2022 05:25:24 - INFO - __main__ - Global step 1000 Train loss 0.19 Classification-F1 0.3454545454545454 on epoch=499
03/11/2022 05:25:26 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=504
03/11/2022 05:25:28 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=509
03/11/2022 05:25:30 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.22 on epoch=514
03/11/2022 05:25:32 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=519
03/11/2022 05:25:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=524
03/11/2022 05:25:35 - INFO - __main__ - Global step 1050 Train loss 0.19 Classification-F1 0.4420512820512821 on epoch=524
03/11/2022 05:25:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=529
03/11/2022 05:25:40 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.17 on epoch=534
03/11/2022 05:25:42 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=539
03/11/2022 05:25:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=544
03/11/2022 05:25:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=549
03/11/2022 05:25:47 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.5465587044534412 on epoch=549
03/11/2022 05:25:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.20 on epoch=554
03/11/2022 05:25:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.18 on epoch=559
03/11/2022 05:25:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=564
03/11/2022 05:25:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.18 on epoch=569
03/11/2022 05:25:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=574
03/11/2022 05:25:59 - INFO - __main__ - Global step 1150 Train loss 0.20 Classification-F1 0.5625 on epoch=574
03/11/2022 05:26:01 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=579
03/11/2022 05:26:03 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=584
03/11/2022 05:26:05 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.16 on epoch=589
03/11/2022 05:26:07 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=594
03/11/2022 05:26:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.14 on epoch=599
03/11/2022 05:26:10 - INFO - __main__ - Global step 1200 Train loss 0.16 Classification-F1 0.5465587044534412 on epoch=599
03/11/2022 05:26:13 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.18 on epoch=604
03/11/2022 05:26:15 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.14 on epoch=609
03/11/2022 05:26:17 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.14 on epoch=614
03/11/2022 05:26:19 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=619
03/11/2022 05:26:21 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.15 on epoch=624
03/11/2022 05:26:22 - INFO - __main__ - Global step 1250 Train loss 0.15 Classification-F1 0.3816425120772947 on epoch=624
03/11/2022 05:26:24 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.16 on epoch=629
03/11/2022 05:26:26 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.12 on epoch=634
03/11/2022 05:26:29 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=639
03/11/2022 05:26:31 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.11 on epoch=644
03/11/2022 05:26:33 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.12 on epoch=649
03/11/2022 05:26:34 - INFO - __main__ - Global step 1300 Train loss 0.13 Classification-F1 0.5555555555555556 on epoch=649
03/11/2022 05:26:36 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.10 on epoch=654
03/11/2022 05:26:38 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.09 on epoch=659
03/11/2022 05:26:40 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
03/11/2022 05:26:43 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.12 on epoch=669
03/11/2022 05:26:45 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=674
03/11/2022 05:26:46 - INFO - __main__ - Global step 1350 Train loss 0.09 Classification-F1 0.5076923076923077 on epoch=674
03/11/2022 05:26:48 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.09 on epoch=679
03/11/2022 05:26:50 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
03/11/2022 05:26:52 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
03/11/2022 05:26:54 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
03/11/2022 05:26:57 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.04 on epoch=699
03/11/2022 05:26:57 - INFO - __main__ - Global step 1400 Train loss 0.05 Classification-F1 0.464039408866995 on epoch=699
03/11/2022 05:26:59 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.03 on epoch=704
03/11/2022 05:27:02 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.04 on epoch=709
03/11/2022 05:27:04 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
03/11/2022 05:27:06 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.03 on epoch=719
03/11/2022 05:27:08 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.06 on epoch=724
03/11/2022 05:27:09 - INFO - __main__ - Global step 1450 Train loss 0.04 Classification-F1 0.5607843137254902 on epoch=724
03/11/2022 05:27:11 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.06 on epoch=729
03/11/2022 05:27:13 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=734
03/11/2022 05:27:16 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.03 on epoch=739
03/11/2022 05:27:18 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.02 on epoch=744
03/11/2022 05:27:20 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
03/11/2022 05:27:21 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.4909862142099682 on epoch=749
03/11/2022 05:27:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
03/11/2022 05:27:26 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/11/2022 05:27:28 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
03/11/2022 05:27:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.03 on epoch=769
03/11/2022 05:27:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
03/11/2022 05:27:33 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=774
03/11/2022 05:27:36 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
03/11/2022 05:27:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
03/11/2022 05:27:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
03/11/2022 05:27:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.01 on epoch=794
03/11/2022 05:27:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/11/2022 05:27:46 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.5076923076923077 on epoch=799
03/11/2022 05:27:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 05:27:50 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
03/11/2022 05:27:52 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.03 on epoch=814
03/11/2022 05:27:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.02 on epoch=819
03/11/2022 05:27:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.04 on epoch=824
03/11/2022 05:27:58 - INFO - __main__ - Global step 1650 Train loss 0.02 Classification-F1 0.30838530838530837 on epoch=824
03/11/2022 05:28:00 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/11/2022 05:28:02 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/11/2022 05:28:04 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/11/2022 05:28:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.02 on epoch=844
03/11/2022 05:28:09 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/11/2022 05:28:10 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=849
03/11/2022 05:28:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/11/2022 05:28:14 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.01 on epoch=859
03/11/2022 05:28:16 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.02 on epoch=864
03/11/2022 05:28:19 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 05:28:21 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/11/2022 05:28:22 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.4920634920634921 on epoch=874
03/11/2022 05:28:24 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/11/2022 05:28:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.01 on epoch=884
03/11/2022 05:28:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.01 on epoch=889
03/11/2022 05:28:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 05:28:33 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.01 on epoch=899
03/11/2022 05:28:34 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=899
03/11/2022 05:28:36 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/11/2022 05:28:38 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/11/2022 05:28:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 05:28:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/11/2022 05:28:44 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/11/2022 05:28:45 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=924
03/11/2022 05:28:47 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 05:28:50 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 05:28:52 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/11/2022 05:28:54 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 05:28:56 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 05:28:58 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5076923076923077 on epoch=949
03/11/2022 05:29:00 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
03/11/2022 05:29:02 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/11/2022 05:29:04 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
03/11/2022 05:29:06 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 05:29:08 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
03/11/2022 05:29:10 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=974
03/11/2022 05:29:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 05:29:14 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/11/2022 05:29:16 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.01 on epoch=989
03/11/2022 05:29:19 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 05:29:21 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.01 on epoch=999
03/11/2022 05:29:22 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=999
03/11/2022 05:29:22 - INFO - __main__ - save last model!
03/11/2022 05:29:22 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 05:29:22 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 05:29:22 - INFO - __main__ - Printing 3 examples
03/11/2022 05:29:22 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 05:29:22 - INFO - __main__ - ['hate']
03/11/2022 05:29:22 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 05:29:22 - INFO - __main__ - ['non-irony']
03/11/2022 05:29:22 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 05:29:22 - INFO - __main__ - ['hate']
03/11/2022 05:29:22 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:29:22 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:29:22 - INFO - __main__ - Printing 3 examples
03/11/2022 05:29:22 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/11/2022 05:29:22 - INFO - __main__ - ['hate']
03/11/2022 05:29:22 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/11/2022 05:29:22 - INFO - __main__ - ['hate']
03/11/2022 05:29:22 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/11/2022 05:29:22 - INFO - __main__ - ['hate']
03/11/2022 05:29:22 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 05:29:22 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:29:23 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:29:23 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:29:23 - INFO - __main__ - Printing 3 examples
03/11/2022 05:29:23 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/11/2022 05:29:23 - INFO - __main__ - ['hate']
03/11/2022 05:29:23 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/11/2022 05:29:23 - INFO - __main__ - ['hate']
03/11/2022 05:29:23 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/11/2022 05:29:23 - INFO - __main__ - ['hate']
03/11/2022 05:29:23 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:29:23 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:29:23 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:29:23 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:29:24 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 05:29:35 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:29:36 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:29:36 - INFO - __main__ - Starting training!
03/11/2022 05:30:09 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_21_0.2_8_predictions.txt
03/11/2022 05:30:09 - INFO - __main__ - Classification-F1 on test data: 0.2389
03/11/2022 05:30:09 - INFO - __main__ - prefix=tweet_eval-irony_16_21, lr=0.2, bsz=8, dev_performance=0.625, test_performance=0.23887309955972943
03/11/2022 05:30:09 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_42, lr=0.5, bsz=8 ...
03/11/2022 05:30:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:30:10 - INFO - __main__ - Printing 3 examples
03/11/2022 05:30:10 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/11/2022 05:30:10 - INFO - __main__ - ['hate']
03/11/2022 05:30:10 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/11/2022 05:30:10 - INFO - __main__ - ['hate']
03/11/2022 05:30:10 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/11/2022 05:30:10 - INFO - __main__ - ['hate']
03/11/2022 05:30:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:30:10 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:30:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:30:10 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:30:10 - INFO - __main__ - Printing 3 examples
03/11/2022 05:30:10 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/11/2022 05:30:10 - INFO - __main__ - ['hate']
03/11/2022 05:30:10 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/11/2022 05:30:10 - INFO - __main__ - ['hate']
03/11/2022 05:30:10 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/11/2022 05:30:10 - INFO - __main__ - ['hate']
03/11/2022 05:30:10 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:30:10 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:30:10 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:30:22 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:30:23 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:30:23 - INFO - __main__ - Starting training!
03/11/2022 05:30:26 - INFO - __main__ - Step 10 Global step 10 Train loss 2.27 on epoch=4
03/11/2022 05:30:28 - INFO - __main__ - Step 20 Global step 20 Train loss 0.67 on epoch=9
03/11/2022 05:30:30 - INFO - __main__ - Step 30 Global step 30 Train loss 0.47 on epoch=14
03/11/2022 05:30:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.50 on epoch=19
03/11/2022 05:30:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.42 on epoch=24
03/11/2022 05:30:35 - INFO - __main__ - Global step 50 Train loss 0.86 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 05:30:35 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 05:30:37 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=29
03/11/2022 05:30:40 - INFO - __main__ - Step 70 Global step 70 Train loss 0.33 on epoch=34
03/11/2022 05:30:42 - INFO - __main__ - Step 80 Global step 80 Train loss 0.32 on epoch=39
03/11/2022 05:30:44 - INFO - __main__ - Step 90 Global step 90 Train loss 0.39 on epoch=44
03/11/2022 05:30:46 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=49
03/11/2022 05:30:47 - INFO - __main__ - Global step 100 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 05:30:49 - INFO - __main__ - Step 110 Global step 110 Train loss 0.32 on epoch=54
03/11/2022 05:30:51 - INFO - __main__ - Step 120 Global step 120 Train loss 0.32 on epoch=59
03/11/2022 05:30:53 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
03/11/2022 05:30:56 - INFO - __main__ - Step 140 Global step 140 Train loss 0.30 on epoch=69
03/11/2022 05:30:58 - INFO - __main__ - Step 150 Global step 150 Train loss 0.30 on epoch=74
03/11/2022 05:30:59 - INFO - __main__ - Global step 150 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 05:31:01 - INFO - __main__ - Step 160 Global step 160 Train loss 0.27 on epoch=79
03/11/2022 05:31:03 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
03/11/2022 05:31:05 - INFO - __main__ - Step 180 Global step 180 Train loss 0.26 on epoch=89
03/11/2022 05:31:07 - INFO - __main__ - Step 190 Global step 190 Train loss 0.36 on epoch=94
03/11/2022 05:31:09 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/11/2022 05:31:10 - INFO - __main__ - Global step 200 Train loss 0.29 Classification-F1 0.4375 on epoch=99
03/11/2022 05:31:10 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.4375 on epoch=99, global_step=200
03/11/2022 05:31:12 - INFO - __main__ - Step 210 Global step 210 Train loss 0.26 on epoch=104
03/11/2022 05:31:15 - INFO - __main__ - Step 220 Global step 220 Train loss 0.28 on epoch=109
03/11/2022 05:31:17 - INFO - __main__ - Step 230 Global step 230 Train loss 0.24 on epoch=114
03/11/2022 05:31:19 - INFO - __main__ - Step 240 Global step 240 Train loss 0.27 on epoch=119
03/11/2022 05:31:21 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
03/11/2022 05:31:22 - INFO - __main__ - Global step 250 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 05:31:24 - INFO - __main__ - Step 260 Global step 260 Train loss 0.27 on epoch=129
03/11/2022 05:31:26 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 05:31:28 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 05:31:31 - INFO - __main__ - Step 290 Global step 290 Train loss 0.32 on epoch=144
03/11/2022 05:31:33 - INFO - __main__ - Step 300 Global step 300 Train loss 0.29 on epoch=149
03/11/2022 05:31:33 - INFO - __main__ - Global step 300 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 05:31:36 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
03/11/2022 05:31:38 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
03/11/2022 05:31:40 - INFO - __main__ - Step 330 Global step 330 Train loss 0.30 on epoch=164
03/11/2022 05:31:42 - INFO - __main__ - Step 340 Global step 340 Train loss 0.28 on epoch=169
03/11/2022 05:31:44 - INFO - __main__ - Step 350 Global step 350 Train loss 0.29 on epoch=174
03/11/2022 05:31:45 - INFO - __main__ - Global step 350 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 05:31:47 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
03/11/2022 05:31:50 - INFO - __main__ - Step 370 Global step 370 Train loss 0.26 on epoch=184
03/11/2022 05:31:52 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 05:31:54 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
03/11/2022 05:31:56 - INFO - __main__ - Step 400 Global step 400 Train loss 0.27 on epoch=199
03/11/2022 05:31:57 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=199
03/11/2022 05:31:59 - INFO - __main__ - Step 410 Global step 410 Train loss 0.20 on epoch=204
03/11/2022 05:32:01 - INFO - __main__ - Step 420 Global step 420 Train loss 0.25 on epoch=209
03/11/2022 05:32:03 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
03/11/2022 05:32:05 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
03/11/2022 05:32:08 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
03/11/2022 05:32:08 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=224
03/11/2022 05:32:11 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
03/11/2022 05:32:13 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 05:32:15 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 05:32:17 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
03/11/2022 05:32:19 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
03/11/2022 05:32:20 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.5465587044534412 on epoch=249
03/11/2022 05:32:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4375 -> 0.5465587044534412 on epoch=249, global_step=500
03/11/2022 05:32:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=254
03/11/2022 05:32:25 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
03/11/2022 05:32:27 - INFO - __main__ - Step 530 Global step 530 Train loss 0.22 on epoch=264
03/11/2022 05:32:29 - INFO - __main__ - Step 540 Global step 540 Train loss 0.24 on epoch=269
03/11/2022 05:32:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.23 on epoch=274
03/11/2022 05:32:32 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.39139139139139134 on epoch=274
03/11/2022 05:32:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.23 on epoch=279
03/11/2022 05:32:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
03/11/2022 05:32:38 - INFO - __main__ - Step 580 Global step 580 Train loss 0.24 on epoch=289
03/11/2022 05:32:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.20 on epoch=294
03/11/2022 05:32:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
03/11/2022 05:32:43 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=299
03/11/2022 05:32:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.29 on epoch=304
03/11/2022 05:32:48 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
03/11/2022 05:32:50 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
03/11/2022 05:32:52 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=319
03/11/2022 05:32:54 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=324
03/11/2022 05:32:55 - INFO - __main__ - Global step 650 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=324
03/11/2022 05:32:57 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=329
03/11/2022 05:33:00 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=334
03/11/2022 05:33:02 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=339
03/11/2022 05:33:04 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
03/11/2022 05:33:06 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=349
03/11/2022 05:33:07 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.3333333333333333 on epoch=349
03/11/2022 05:33:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=354
03/11/2022 05:33:11 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=359
03/11/2022 05:33:13 - INFO - __main__ - Step 730 Global step 730 Train loss 0.19 on epoch=364
03/11/2022 05:33:15 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=369
03/11/2022 05:33:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.27 on epoch=374
03/11/2022 05:33:18 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.5607843137254902 on epoch=374
03/11/2022 05:33:18 - INFO - __main__ - Saving model with best Classification-F1: 0.5465587044534412 -> 0.5607843137254902 on epoch=374, global_step=750
03/11/2022 05:33:21 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=379
03/11/2022 05:33:23 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
03/11/2022 05:33:25 - INFO - __main__ - Step 780 Global step 780 Train loss 0.21 on epoch=389
03/11/2022 05:33:27 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=394
03/11/2022 05:33:29 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=399
03/11/2022 05:33:30 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.4920634920634921 on epoch=399
03/11/2022 05:33:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.22 on epoch=404
03/11/2022 05:33:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=409
03/11/2022 05:33:37 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
03/11/2022 05:33:39 - INFO - __main__ - Step 840 Global step 840 Train loss 0.19 on epoch=419
03/11/2022 05:33:41 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=424
03/11/2022 05:33:42 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.5076923076923077 on epoch=424
03/11/2022 05:33:44 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=429
03/11/2022 05:33:46 - INFO - __main__ - Step 870 Global step 870 Train loss 0.16 on epoch=434
03/11/2022 05:33:48 - INFO - __main__ - Step 880 Global step 880 Train loss 0.16 on epoch=439
03/11/2022 05:33:50 - INFO - __main__ - Step 890 Global step 890 Train loss 0.16 on epoch=444
03/11/2022 05:33:53 - INFO - __main__ - Step 900 Global step 900 Train loss 0.14 on epoch=449
03/11/2022 05:33:53 - INFO - __main__ - Global step 900 Train loss 0.16 Classification-F1 0.4554554554554554 on epoch=449
03/11/2022 05:33:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.14 on epoch=454
03/11/2022 05:33:58 - INFO - __main__ - Step 920 Global step 920 Train loss 0.13 on epoch=459
03/11/2022 05:34:00 - INFO - __main__ - Step 930 Global step 930 Train loss 0.12 on epoch=464
03/11/2022 05:34:02 - INFO - __main__ - Step 940 Global step 940 Train loss 0.09 on epoch=469
03/11/2022 05:34:04 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=474
03/11/2022 05:34:05 - INFO - __main__ - Global step 950 Train loss 0.11 Classification-F1 0.5270935960591133 on epoch=474
03/11/2022 05:34:07 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=479
03/11/2022 05:34:09 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=484
03/11/2022 05:34:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=489
03/11/2022 05:34:14 - INFO - __main__ - Step 990 Global step 990 Train loss 0.12 on epoch=494
03/11/2022 05:34:16 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=499
03/11/2022 05:34:17 - INFO - __main__ - Global step 1000 Train loss 0.11 Classification-F1 0.5625 on epoch=499
03/11/2022 05:34:17 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5625 on epoch=499, global_step=1000
03/11/2022 05:34:19 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=504
03/11/2022 05:34:21 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=509
03/11/2022 05:34:23 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.05 on epoch=514
03/11/2022 05:34:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=519
03/11/2022 05:34:28 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.07 on epoch=524
03/11/2022 05:34:28 - INFO - __main__ - Global step 1050 Train loss 0.08 Classification-F1 0.5270935960591133 on epoch=524
03/11/2022 05:34:31 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.03 on epoch=529
03/11/2022 05:34:33 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=534
03/11/2022 05:34:35 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.06 on epoch=539
03/11/2022 05:34:37 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=544
03/11/2022 05:34:39 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.11 on epoch=549
03/11/2022 05:34:40 - INFO - __main__ - Global step 1100 Train loss 0.08 Classification-F1 0.5307917888563051 on epoch=549
03/11/2022 05:34:42 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
03/11/2022 05:34:44 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.13 on epoch=559
03/11/2022 05:34:47 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.04 on epoch=564
03/11/2022 05:34:49 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.05 on epoch=569
03/11/2022 05:34:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
03/11/2022 05:34:52 - INFO - __main__ - Global step 1150 Train loss 0.06 Classification-F1 0.3797979797979798 on epoch=574
03/11/2022 05:34:54 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/11/2022 05:34:56 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.08 on epoch=584
03/11/2022 05:34:58 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
03/11/2022 05:35:01 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/11/2022 05:35:03 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.02 on epoch=599
03/11/2022 05:35:04 - INFO - __main__ - Global step 1200 Train loss 0.03 Classification-F1 0.5933528836754642 on epoch=599
03/11/2022 05:35:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5625 -> 0.5933528836754642 on epoch=599, global_step=1200
03/11/2022 05:35:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.03 on epoch=604
03/11/2022 05:35:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
03/11/2022 05:35:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=614
03/11/2022 05:35:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.07 on epoch=619
03/11/2022 05:35:15 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=624
03/11/2022 05:35:15 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.3333333333333333 on epoch=624
03/11/2022 05:35:18 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.05 on epoch=629
03/11/2022 05:35:20 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
03/11/2022 05:35:22 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
03/11/2022 05:35:24 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=644
03/11/2022 05:35:26 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
03/11/2022 05:35:27 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.4980392156862745 on epoch=649
03/11/2022 05:35:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=654
03/11/2022 05:35:31 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.04 on epoch=659
03/11/2022 05:35:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.05 on epoch=664
03/11/2022 05:35:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
03/11/2022 05:35:38 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/11/2022 05:35:39 - INFO - __main__ - Global step 1350 Train loss 0.04 Classification-F1 0.5 on epoch=674
03/11/2022 05:35:41 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=679
03/11/2022 05:35:43 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
03/11/2022 05:35:45 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/11/2022 05:35:48 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
03/11/2022 05:35:50 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.07 on epoch=699
03/11/2022 05:35:51 - INFO - __main__ - Global step 1400 Train loss 0.06 Classification-F1 0.5607843137254902 on epoch=699
03/11/2022 05:35:53 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/11/2022 05:35:55 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.07 on epoch=709
03/11/2022 05:35:57 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
03/11/2022 05:35:59 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
03/11/2022 05:36:01 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.05 on epoch=724
03/11/2022 05:36:02 - INFO - __main__ - Global step 1450 Train loss 0.05 Classification-F1 0.4920634920634921 on epoch=724
03/11/2022 05:36:04 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/11/2022 05:36:07 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.06 on epoch=734
03/11/2022 05:36:09 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=739
03/11/2022 05:36:11 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
03/11/2022 05:36:13 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.03 on epoch=749
03/11/2022 05:36:14 - INFO - __main__ - Global step 1500 Train loss 0.04 Classification-F1 0.3552492046659597 on epoch=749
03/11/2022 05:36:16 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.06 on epoch=754
03/11/2022 05:36:18 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=759
03/11/2022 05:36:20 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.03 on epoch=764
03/11/2022 05:36:23 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/11/2022 05:36:25 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.03 on epoch=774
03/11/2022 05:36:26 - INFO - __main__ - Global step 1550 Train loss 0.04 Classification-F1 0.4554554554554554 on epoch=774
03/11/2022 05:36:28 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
03/11/2022 05:36:30 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
03/11/2022 05:36:32 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.02 on epoch=789
03/11/2022 05:36:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
03/11/2022 05:36:37 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=799
03/11/2022 05:36:37 - INFO - __main__ - Global step 1600 Train loss 0.04 Classification-F1 0.4554554554554554 on epoch=799
03/11/2022 05:36:39 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 05:36:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.06 on epoch=809
03/11/2022 05:36:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=814
03/11/2022 05:36:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.05 on epoch=819
03/11/2022 05:36:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=824
03/11/2022 05:36:49 - INFO - __main__ - Global step 1650 Train loss 0.05 Classification-F1 0.5555555555555556 on epoch=824
03/11/2022 05:36:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.02 on epoch=829
03/11/2022 05:36:53 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.04 on epoch=834
03/11/2022 05:36:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.03 on epoch=839
03/11/2022 05:36:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.05 on epoch=844
03/11/2022 05:37:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.02 on epoch=849
03/11/2022 05:37:01 - INFO - __main__ - Global step 1700 Train loss 0.03 Classification-F1 0.6862745098039216 on epoch=849
03/11/2022 05:37:01 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6862745098039216 on epoch=849, global_step=1700
03/11/2022 05:37:03 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.01 on epoch=854
03/11/2022 05:37:05 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.07 on epoch=859
03/11/2022 05:37:07 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.05 on epoch=864
03/11/2022 05:37:10 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.08 on epoch=869
03/11/2022 05:37:12 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/11/2022 05:37:12 - INFO - __main__ - Global step 1750 Train loss 0.05 Classification-F1 0.6235294117647059 on epoch=874
03/11/2022 05:37:15 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.04 on epoch=879
03/11/2022 05:37:17 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
03/11/2022 05:37:19 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 05:37:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.02 on epoch=894
03/11/2022 05:37:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 05:37:24 - INFO - __main__ - Global step 1800 Train loss 0.02 Classification-F1 0.6559139784946237 on epoch=899
03/11/2022 05:37:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.01 on epoch=904
03/11/2022 05:37:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/11/2022 05:37:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
03/11/2022 05:37:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/11/2022 05:37:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.01 on epoch=924
03/11/2022 05:37:36 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.625 on epoch=924
03/11/2022 05:37:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
03/11/2022 05:37:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.01 on epoch=934
03/11/2022 05:37:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.03 on epoch=939
03/11/2022 05:37:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.05 on epoch=944
03/11/2022 05:37:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.01 on epoch=949
03/11/2022 05:37:48 - INFO - __main__ - Global step 1900 Train loss 0.03 Classification-F1 0.6862745098039216 on epoch=949
03/11/2022 05:37:50 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.02 on epoch=954
03/11/2022 05:37:52 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/11/2022 05:37:54 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 05:37:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
03/11/2022 05:37:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 05:37:59 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5933528836754642 on epoch=974
03/11/2022 05:38:02 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.01 on epoch=979
03/11/2022 05:38:04 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 05:38:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 05:38:08 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 05:38:10 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=999
03/11/2022 05:38:11 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.6476476476476476 on epoch=999
03/11/2022 05:38:11 - INFO - __main__ - save last model!
03/11/2022 05:38:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 05:38:11 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 05:38:11 - INFO - __main__ - Printing 3 examples
03/11/2022 05:38:11 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 05:38:11 - INFO - __main__ - ['hate']
03/11/2022 05:38:11 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 05:38:11 - INFO - __main__ - ['non-irony']
03/11/2022 05:38:11 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 05:38:11 - INFO - __main__ - ['hate']
03/11/2022 05:38:11 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:38:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:38:11 - INFO - __main__ - Printing 3 examples
03/11/2022 05:38:11 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/11/2022 05:38:11 - INFO - __main__ - ['hate']
03/11/2022 05:38:11 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/11/2022 05:38:11 - INFO - __main__ - ['hate']
03/11/2022 05:38:11 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/11/2022 05:38:11 - INFO - __main__ - ['hate']
03/11/2022 05:38:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 05:38:11 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:38:12 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:38:12 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:38:12 - INFO - __main__ - Printing 3 examples
03/11/2022 05:38:12 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/11/2022 05:38:12 - INFO - __main__ - ['hate']
03/11/2022 05:38:12 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/11/2022 05:38:12 - INFO - __main__ - ['hate']
03/11/2022 05:38:12 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/11/2022 05:38:12 - INFO - __main__ - ['hate']
03/11/2022 05:38:12 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:38:12 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:38:12 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:38:12 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:38:13 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 05:38:24 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:38:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:38:25 - INFO - __main__ - Starting training!
03/11/2022 05:38:36 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_42_0.5_8_predictions.txt
03/11/2022 05:38:36 - INFO - __main__ - Classification-F1 on test data: 0.4862
03/11/2022 05:38:36 - INFO - __main__ - prefix=tweet_eval-irony_16_42, lr=0.5, bsz=8, dev_performance=0.6862745098039216, test_performance=0.48618160296992413
03/11/2022 05:38:36 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_42, lr=0.4, bsz=8 ...
03/11/2022 05:38:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:38:37 - INFO - __main__ - Printing 3 examples
03/11/2022 05:38:37 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/11/2022 05:38:37 - INFO - __main__ - ['hate']
03/11/2022 05:38:37 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/11/2022 05:38:37 - INFO - __main__ - ['hate']
03/11/2022 05:38:37 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/11/2022 05:38:37 - INFO - __main__ - ['hate']
03/11/2022 05:38:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:38:37 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:38:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:38:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:38:37 - INFO - __main__ - Printing 3 examples
03/11/2022 05:38:37 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/11/2022 05:38:37 - INFO - __main__ - ['hate']
03/11/2022 05:38:37 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/11/2022 05:38:37 - INFO - __main__ - ['hate']
03/11/2022 05:38:37 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/11/2022 05:38:37 - INFO - __main__ - ['hate']
03/11/2022 05:38:37 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:38:37 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:38:37 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:38:49 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:38:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:38:50 - INFO - __main__ - Starting training!
03/11/2022 05:38:54 - INFO - __main__ - Step 10 Global step 10 Train loss 2.35 on epoch=4
03/11/2022 05:38:56 - INFO - __main__ - Step 20 Global step 20 Train loss 0.78 on epoch=9
03/11/2022 05:38:58 - INFO - __main__ - Step 30 Global step 30 Train loss 0.61 on epoch=14
03/11/2022 05:39:00 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=19
03/11/2022 05:39:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.44 on epoch=24
03/11/2022 05:39:03 - INFO - __main__ - Global step 50 Train loss 0.94 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 05:39:03 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 05:39:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.39 on epoch=29
03/11/2022 05:39:08 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=34
03/11/2022 05:39:10 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=39
03/11/2022 05:39:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=44
03/11/2022 05:39:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.31 on epoch=49
03/11/2022 05:39:15 - INFO - __main__ - Global step 100 Train loss 0.36 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 05:39:17 - INFO - __main__ - Step 110 Global step 110 Train loss 0.32 on epoch=54
03/11/2022 05:39:20 - INFO - __main__ - Step 120 Global step 120 Train loss 0.30 on epoch=59
03/11/2022 05:39:22 - INFO - __main__ - Step 130 Global step 130 Train loss 0.33 on epoch=64
03/11/2022 05:39:24 - INFO - __main__ - Step 140 Global step 140 Train loss 0.35 on epoch=69
03/11/2022 05:39:26 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
03/11/2022 05:39:27 - INFO - __main__ - Global step 150 Train loss 0.33 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 05:39:29 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=79
03/11/2022 05:39:31 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
03/11/2022 05:39:33 - INFO - __main__ - Step 180 Global step 180 Train loss 0.24 on epoch=89
03/11/2022 05:39:36 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
03/11/2022 05:39:38 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/11/2022 05:39:39 - INFO - __main__ - Global step 200 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 05:39:41 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
03/11/2022 05:39:43 - INFO - __main__ - Step 220 Global step 220 Train loss 0.29 on epoch=109
03/11/2022 05:39:45 - INFO - __main__ - Step 230 Global step 230 Train loss 0.27 on epoch=114
03/11/2022 05:39:47 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=119
03/11/2022 05:39:50 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
03/11/2022 05:39:50 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.46843853820598 on epoch=124
03/11/2022 05:39:50 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.46843853820598 on epoch=124, global_step=250
03/11/2022 05:39:53 - INFO - __main__ - Step 260 Global step 260 Train loss 0.30 on epoch=129
03/11/2022 05:39:55 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
03/11/2022 05:39:57 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
03/11/2022 05:39:59 - INFO - __main__ - Step 290 Global step 290 Train loss 0.24 on epoch=144
03/11/2022 05:40:01 - INFO - __main__ - Step 300 Global step 300 Train loss 0.25 on epoch=149
03/11/2022 05:40:02 - INFO - __main__ - Global step 300 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 05:40:04 - INFO - __main__ - Step 310 Global step 310 Train loss 0.28 on epoch=154
03/11/2022 05:40:06 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 05:40:09 - INFO - __main__ - Step 330 Global step 330 Train loss 0.27 on epoch=164
03/11/2022 05:40:11 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
03/11/2022 05:40:13 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
03/11/2022 05:40:14 - INFO - __main__ - Global step 350 Train loss 0.27 Classification-F1 0.3191489361702127 on epoch=174
03/11/2022 05:40:16 - INFO - __main__ - Step 360 Global step 360 Train loss 0.29 on epoch=179
03/11/2022 05:40:18 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
03/11/2022 05:40:20 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
03/11/2022 05:40:22 - INFO - __main__ - Step 390 Global step 390 Train loss 0.21 on epoch=194
03/11/2022 05:40:25 - INFO - __main__ - Step 400 Global step 400 Train loss 0.28 on epoch=199
03/11/2022 05:40:26 - INFO - __main__ - Global step 400 Train loss 0.26 Classification-F1 0.4682306940371457 on epoch=199
03/11/2022 05:40:28 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 05:40:30 - INFO - __main__ - Step 420 Global step 420 Train loss 0.28 on epoch=209
03/11/2022 05:40:32 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
03/11/2022 05:40:34 - INFO - __main__ - Step 440 Global step 440 Train loss 0.25 on epoch=219
03/11/2022 05:40:37 - INFO - __main__ - Step 450 Global step 450 Train loss 0.25 on epoch=224
03/11/2022 05:40:37 - INFO - __main__ - Global step 450 Train loss 0.26 Classification-F1 0.5607843137254902 on epoch=224
03/11/2022 05:40:37 - INFO - __main__ - Saving model with best Classification-F1: 0.46843853820598 -> 0.5607843137254902 on epoch=224, global_step=450
03/11/2022 05:40:40 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
03/11/2022 05:40:42 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 05:40:44 - INFO - __main__ - Step 480 Global step 480 Train loss 0.24 on epoch=239
03/11/2022 05:40:46 - INFO - __main__ - Step 490 Global step 490 Train loss 0.26 on epoch=244
03/11/2022 05:40:48 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
03/11/2022 05:40:49 - INFO - __main__ - Global step 500 Train loss 0.24 Classification-F1 0.3333333333333333 on epoch=249
03/11/2022 05:40:51 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=254
03/11/2022 05:40:53 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
03/11/2022 05:40:56 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
03/11/2022 05:40:58 - INFO - __main__ - Step 540 Global step 540 Train loss 0.23 on epoch=269
03/11/2022 05:41:00 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
03/11/2022 05:41:01 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=274
03/11/2022 05:41:03 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=279
03/11/2022 05:41:05 - INFO - __main__ - Step 570 Global step 570 Train loss 0.25 on epoch=284
03/11/2022 05:41:07 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/11/2022 05:41:10 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=294
03/11/2022 05:41:12 - INFO - __main__ - Step 600 Global step 600 Train loss 0.24 on epoch=299
03/11/2022 05:41:13 - INFO - __main__ - Global step 600 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=299
03/11/2022 05:41:15 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=304
03/11/2022 05:41:17 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=309
03/11/2022 05:41:19 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=314
03/11/2022 05:41:21 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=319
03/11/2022 05:41:24 - INFO - __main__ - Step 650 Global step 650 Train loss 0.24 on epoch=324
03/11/2022 05:41:24 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=324
03/11/2022 05:41:26 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=329
03/11/2022 05:41:29 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=334
03/11/2022 05:41:31 - INFO - __main__ - Step 680 Global step 680 Train loss 0.23 on epoch=339
03/11/2022 05:41:33 - INFO - __main__ - Step 690 Global step 690 Train loss 0.21 on epoch=344
03/11/2022 05:41:35 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
03/11/2022 05:41:36 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.3992490613266583 on epoch=349
03/11/2022 05:41:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
03/11/2022 05:41:40 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=359
03/11/2022 05:41:42 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=364
03/11/2022 05:41:45 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
03/11/2022 05:41:47 - INFO - __main__ - Step 750 Global step 750 Train loss 0.25 on epoch=374
03/11/2022 05:41:48 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.4589371980676329 on epoch=374
03/11/2022 05:41:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=379
03/11/2022 05:41:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.22 on epoch=384
03/11/2022 05:41:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.18 on epoch=389
03/11/2022 05:41:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.21 on epoch=394
03/11/2022 05:41:58 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=399
03/11/2022 05:41:59 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.5465587044534412 on epoch=399
03/11/2022 05:42:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.16 on epoch=404
03/11/2022 05:42:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=409
03/11/2022 05:42:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=414
03/11/2022 05:42:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.16 on epoch=419
03/11/2022 05:42:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.23 on epoch=424
03/11/2022 05:42:11 - INFO - __main__ - Global step 850 Train loss 0.18 Classification-F1 0.5195195195195195 on epoch=424
03/11/2022 05:42:13 - INFO - __main__ - Step 860 Global step 860 Train loss 0.19 on epoch=429
03/11/2022 05:42:15 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=434
03/11/2022 05:42:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=439
03/11/2022 05:42:20 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=444
03/11/2022 05:42:22 - INFO - __main__ - Step 900 Global step 900 Train loss 0.15 on epoch=449
03/11/2022 05:42:23 - INFO - __main__ - Global step 900 Train loss 0.17 Classification-F1 0.46843853820598 on epoch=449
03/11/2022 05:42:25 - INFO - __main__ - Step 910 Global step 910 Train loss 0.16 on epoch=454
03/11/2022 05:42:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.09 on epoch=459
03/11/2022 05:42:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.09 on epoch=464
03/11/2022 05:42:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.11 on epoch=469
03/11/2022 05:42:34 - INFO - __main__ - Step 950 Global step 950 Train loss 0.15 on epoch=474
03/11/2022 05:42:34 - INFO - __main__ - Global step 950 Train loss 0.12 Classification-F1 0.37662337662337664 on epoch=474
03/11/2022 05:42:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=479
03/11/2022 05:42:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.10 on epoch=484
03/11/2022 05:42:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.13 on epoch=489
03/11/2022 05:42:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.06 on epoch=494
03/11/2022 05:42:45 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.09 on epoch=499
03/11/2022 05:42:46 - INFO - __main__ - Global step 1000 Train loss 0.10 Classification-F1 0.4231177094379639 on epoch=499
03/11/2022 05:42:48 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.04 on epoch=504
03/11/2022 05:42:50 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.07 on epoch=509
03/11/2022 05:42:53 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.04 on epoch=514
03/11/2022 05:42:55 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
03/11/2022 05:42:57 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/11/2022 05:42:58 - INFO - __main__ - Global step 1050 Train loss 0.04 Classification-F1 0.4009852216748768 on epoch=524
03/11/2022 05:43:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.06 on epoch=529
03/11/2022 05:43:02 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.02 on epoch=534
03/11/2022 05:43:04 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.04 on epoch=539
03/11/2022 05:43:06 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.00 on epoch=544
03/11/2022 05:43:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.01 on epoch=549
03/11/2022 05:43:09 - INFO - __main__ - Global step 1100 Train loss 0.03 Classification-F1 0.2901234567901234 on epoch=549
03/11/2022 05:43:12 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.03 on epoch=554
03/11/2022 05:43:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/11/2022 05:43:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.00 on epoch=564
03/11/2022 05:43:18 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.00 on epoch=569
03/11/2022 05:43:20 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/11/2022 05:43:21 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=574
03/11/2022 05:43:23 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/11/2022 05:43:25 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/11/2022 05:43:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.03 on epoch=589
03/11/2022 05:43:30 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/11/2022 05:43:32 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/11/2022 05:43:33 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.41700404858299595 on epoch=599
03/11/2022 05:43:35 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/11/2022 05:43:37 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/11/2022 05:43:40 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.00 on epoch=614
03/11/2022 05:43:42 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/11/2022 05:43:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.02 on epoch=624
03/11/2022 05:43:45 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.21978021978021978 on epoch=624
03/11/2022 05:43:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/11/2022 05:43:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 05:43:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/11/2022 05:43:53 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/11/2022 05:43:56 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
03/11/2022 05:43:56 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.27934517589690006 on epoch=649
03/11/2022 05:43:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.02 on epoch=654
03/11/2022 05:44:01 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/11/2022 05:44:03 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/11/2022 05:44:05 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 05:44:07 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 05:44:08 - INFO - __main__ - Global step 1350 Train loss 0.01 Classification-F1 0.4375 on epoch=674
03/11/2022 05:44:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 05:44:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 05:44:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.00 on epoch=689
03/11/2022 05:44:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 05:44:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/11/2022 05:44:20 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.4458874458874459 on epoch=699
03/11/2022 05:44:22 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.01 on epoch=704
03/11/2022 05:44:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.00 on epoch=709
03/11/2022 05:44:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.01 on epoch=714
03/11/2022 05:44:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 05:44:31 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/11/2022 05:44:32 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.3440860215053764 on epoch=724
03/11/2022 05:44:34 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 05:44:36 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 05:44:38 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 05:44:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 05:44:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 05:44:43 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.29971988795518206 on epoch=749
03/11/2022 05:44:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
03/11/2022 05:44:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 05:44:50 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 05:44:52 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 05:44:54 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 05:44:55 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.3373737373737374 on epoch=774
03/11/2022 05:44:57 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 05:44:59 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 05:45:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 05:45:04 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 05:45:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 05:45:07 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=799
03/11/2022 05:45:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 05:45:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 05:45:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 05:45:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 05:45:18 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
03/11/2022 05:45:19 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.30158730158730157 on epoch=824
03/11/2022 05:45:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 05:45:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.02 on epoch=834
03/11/2022 05:45:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 05:45:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 05:45:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 05:45:31 - INFO - __main__ - Global step 1700 Train loss 0.00 Classification-F1 0.37254901960784315 on epoch=849
03/11/2022 05:45:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 05:45:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 05:45:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/11/2022 05:45:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 05:45:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 05:45:43 - INFO - __main__ - Global step 1750 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=874
03/11/2022 05:45:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 05:45:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 05:45:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 05:45:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 05:45:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 05:45:54 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.464039408866995 on epoch=899
03/11/2022 05:45:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 05:45:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.01 on epoch=909
03/11/2022 05:46:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 05:46:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 05:46:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 05:46:06 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.4920634920634921 on epoch=924
03/11/2022 05:46:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 05:46:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 05:46:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 05:46:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 05:46:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 05:46:18 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=949
03/11/2022 05:46:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 05:46:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 05:46:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 05:46:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 05:46:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/11/2022 05:46:31 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=974
03/11/2022 05:46:33 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 05:46:35 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/11/2022 05:46:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 05:46:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 05:46:42 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 05:46:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:46:43 - INFO - __main__ - Printing 3 examples
03/11/2022 05:46:43 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/11/2022 05:46:43 - INFO - __main__ - ['hate']
03/11/2022 05:46:43 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/11/2022 05:46:43 - INFO - __main__ - ['hate']
03/11/2022 05:46:43 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/11/2022 05:46:43 - INFO - __main__ - ['hate']
03/11/2022 05:46:43 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 05:46:43 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:46:43 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:46:43 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:46:43 - INFO - __main__ - Printing 3 examples
03/11/2022 05:46:43 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/11/2022 05:46:43 - INFO - __main__ - ['hate']
03/11/2022 05:46:43 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/11/2022 05:46:43 - INFO - __main__ - ['hate']
03/11/2022 05:46:43 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/11/2022 05:46:43 - INFO - __main__ - ['hate']
03/11/2022 05:46:43 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:46:43 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:46:43 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:46:43 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4682306940371457 on epoch=999
03/11/2022 05:46:43 - INFO - __main__ - save last model!
03/11/2022 05:46:43 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 05:46:43 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 05:46:43 - INFO - __main__ - Printing 3 examples
03/11/2022 05:46:43 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 05:46:43 - INFO - __main__ - ['hate']
03/11/2022 05:46:43 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 05:46:43 - INFO - __main__ - ['non-irony']
03/11/2022 05:46:43 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 05:46:43 - INFO - __main__ - ['hate']
03/11/2022 05:46:43 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:46:43 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:46:44 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 05:46:57 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:46:58 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:46:58 - INFO - __main__ - Starting training!
03/11/2022 05:47:34 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_42_0.4_8_predictions.txt
03/11/2022 05:47:34 - INFO - __main__ - Classification-F1 on test data: 0.1354
03/11/2022 05:47:34 - INFO - __main__ - prefix=tweet_eval-irony_16_42, lr=0.4, bsz=8, dev_performance=0.5607843137254902, test_performance=0.1353687662638577
03/11/2022 05:47:34 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_42, lr=0.3, bsz=8 ...
03/11/2022 05:47:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:47:35 - INFO - __main__ - Printing 3 examples
03/11/2022 05:47:35 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/11/2022 05:47:35 - INFO - __main__ - ['hate']
03/11/2022 05:47:35 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/11/2022 05:47:35 - INFO - __main__ - ['hate']
03/11/2022 05:47:35 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/11/2022 05:47:35 - INFO - __main__ - ['hate']
03/11/2022 05:47:35 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:47:35 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:47:35 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:47:35 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:47:35 - INFO - __main__ - Printing 3 examples
03/11/2022 05:47:35 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/11/2022 05:47:35 - INFO - __main__ - ['hate']
03/11/2022 05:47:35 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/11/2022 05:47:35 - INFO - __main__ - ['hate']
03/11/2022 05:47:35 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/11/2022 05:47:35 - INFO - __main__ - ['hate']
03/11/2022 05:47:35 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:47:35 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:47:35 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:47:49 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:47:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:47:50 - INFO - __main__ - Starting training!
03/11/2022 05:47:54 - INFO - __main__ - Step 10 Global step 10 Train loss 2.81 on epoch=4
03/11/2022 05:47:56 - INFO - __main__ - Step 20 Global step 20 Train loss 1.11 on epoch=9
03/11/2022 05:47:59 - INFO - __main__ - Step 30 Global step 30 Train loss 0.63 on epoch=14
03/11/2022 05:48:01 - INFO - __main__ - Step 40 Global step 40 Train loss 0.62 on epoch=19
03/11/2022 05:48:03 - INFO - __main__ - Step 50 Global step 50 Train loss 0.48 on epoch=24
03/11/2022 05:48:04 - INFO - __main__ - Global step 50 Train loss 1.13 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 05:48:04 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 05:48:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.42 on epoch=29
03/11/2022 05:48:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.43 on epoch=34
03/11/2022 05:48:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.38 on epoch=39
03/11/2022 05:48:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.40 on epoch=44
03/11/2022 05:48:15 - INFO - __main__ - Step 100 Global step 100 Train loss 0.35 on epoch=49
03/11/2022 05:48:16 - INFO - __main__ - Global step 100 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 05:48:18 - INFO - __main__ - Step 110 Global step 110 Train loss 0.35 on epoch=54
03/11/2022 05:48:21 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=59
03/11/2022 05:48:23 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
03/11/2022 05:48:25 - INFO - __main__ - Step 140 Global step 140 Train loss 0.37 on epoch=69
03/11/2022 05:48:28 - INFO - __main__ - Step 150 Global step 150 Train loss 0.33 on epoch=74
03/11/2022 05:48:28 - INFO - __main__ - Global step 150 Train loss 0.35 Classification-F1 0.3650793650793651 on epoch=74
03/11/2022 05:48:28 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.3650793650793651 on epoch=74, global_step=150
03/11/2022 05:48:31 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=79
03/11/2022 05:48:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
03/11/2022 05:48:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.29 on epoch=89
03/11/2022 05:48:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
03/11/2022 05:48:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.34 on epoch=99
03/11/2022 05:48:41 - INFO - __main__ - Global step 200 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 05:48:43 - INFO - __main__ - Step 210 Global step 210 Train loss 0.27 on epoch=104
03/11/2022 05:48:45 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=109
03/11/2022 05:48:47 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
03/11/2022 05:48:50 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=119
03/11/2022 05:48:52 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
03/11/2022 05:48:53 - INFO - __main__ - Global step 250 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 05:48:55 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
03/11/2022 05:48:57 - INFO - __main__ - Step 270 Global step 270 Train loss 0.32 on epoch=134
03/11/2022 05:48:59 - INFO - __main__ - Step 280 Global step 280 Train loss 0.30 on epoch=139
03/11/2022 05:49:01 - INFO - __main__ - Step 290 Global step 290 Train loss 0.28 on epoch=144
03/11/2022 05:49:04 - INFO - __main__ - Step 300 Global step 300 Train loss 0.28 on epoch=149
03/11/2022 05:49:04 - INFO - __main__ - Global step 300 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 05:49:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=154
03/11/2022 05:49:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.28 on epoch=159
03/11/2022 05:49:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.28 on epoch=164
03/11/2022 05:49:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.26 on epoch=169
03/11/2022 05:49:16 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
03/11/2022 05:49:16 - INFO - __main__ - Global step 350 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 05:49:18 - INFO - __main__ - Step 360 Global step 360 Train loss 0.28 on epoch=179
03/11/2022 05:49:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.24 on epoch=184
03/11/2022 05:49:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.27 on epoch=189
03/11/2022 05:49:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.31 on epoch=194
03/11/2022 05:49:27 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
03/11/2022 05:49:28 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=199
03/11/2022 05:49:30 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=204
03/11/2022 05:49:32 - INFO - __main__ - Step 420 Global step 420 Train loss 0.31 on epoch=209
03/11/2022 05:49:34 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
03/11/2022 05:49:37 - INFO - __main__ - Step 440 Global step 440 Train loss 0.22 on epoch=219
03/11/2022 05:49:39 - INFO - __main__ - Step 450 Global step 450 Train loss 0.27 on epoch=224
03/11/2022 05:49:39 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.5333333333333333 on epoch=224
03/11/2022 05:49:40 - INFO - __main__ - Saving model with best Classification-F1: 0.3650793650793651 -> 0.5333333333333333 on epoch=224, global_step=450
03/11/2022 05:49:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.29 on epoch=229
03/11/2022 05:49:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 05:49:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
03/11/2022 05:49:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.28 on epoch=244
03/11/2022 05:49:50 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
03/11/2022 05:49:51 - INFO - __main__ - Global step 500 Train loss 0.26 Classification-F1 0.4980392156862745 on epoch=249
03/11/2022 05:49:53 - INFO - __main__ - Step 510 Global step 510 Train loss 0.25 on epoch=254
03/11/2022 05:49:55 - INFO - __main__ - Step 520 Global step 520 Train loss 0.30 on epoch=259
03/11/2022 05:49:58 - INFO - __main__ - Step 530 Global step 530 Train loss 0.23 on epoch=264
03/11/2022 05:50:00 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
03/11/2022 05:50:02 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=274
03/11/2022 05:50:03 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.49090909090909085 on epoch=274
03/11/2022 05:50:05 - INFO - __main__ - Step 560 Global step 560 Train loss 0.27 on epoch=279
03/11/2022 05:50:07 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
03/11/2022 05:50:09 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/11/2022 05:50:11 - INFO - __main__ - Step 590 Global step 590 Train loss 0.26 on epoch=294
03/11/2022 05:50:14 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
03/11/2022 05:50:14 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.4285714285714286 on epoch=299
03/11/2022 05:50:17 - INFO - __main__ - Step 610 Global step 610 Train loss 0.27 on epoch=304
03/11/2022 05:50:19 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=309
03/11/2022 05:50:21 - INFO - __main__ - Step 630 Global step 630 Train loss 0.25 on epoch=314
03/11/2022 05:50:23 - INFO - __main__ - Step 640 Global step 640 Train loss 0.25 on epoch=319
03/11/2022 05:50:25 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=324
03/11/2022 05:50:26 - INFO - __main__ - Global step 650 Train loss 0.24 Classification-F1 0.3043478260869565 on epoch=324
03/11/2022 05:50:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.22 on epoch=329
03/11/2022 05:50:31 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=334
03/11/2022 05:50:33 - INFO - __main__ - Step 680 Global step 680 Train loss 0.25 on epoch=339
03/11/2022 05:50:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.23 on epoch=344
03/11/2022 05:50:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.26 on epoch=349
03/11/2022 05:50:38 - INFO - __main__ - Global step 700 Train loss 0.23 Classification-F1 0.39756367663344405 on epoch=349
03/11/2022 05:50:40 - INFO - __main__ - Step 710 Global step 710 Train loss 0.22 on epoch=354
03/11/2022 05:50:42 - INFO - __main__ - Step 720 Global step 720 Train loss 0.26 on epoch=359
03/11/2022 05:50:44 - INFO - __main__ - Step 730 Global step 730 Train loss 0.23 on epoch=364
03/11/2022 05:50:46 - INFO - __main__ - Step 740 Global step 740 Train loss 0.22 on epoch=369
03/11/2022 05:50:49 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
03/11/2022 05:50:49 - INFO - __main__ - Global step 750 Train loss 0.23 Classification-F1 0.3333333333333333 on epoch=374
03/11/2022 05:50:52 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
03/11/2022 05:50:54 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=384
03/11/2022 05:50:56 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
03/11/2022 05:50:58 - INFO - __main__ - Step 790 Global step 790 Train loss 0.17 on epoch=394
03/11/2022 05:51:00 - INFO - __main__ - Step 800 Global step 800 Train loss 0.22 on epoch=399
03/11/2022 05:51:01 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.5307917888563051 on epoch=399
03/11/2022 05:51:03 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
03/11/2022 05:51:05 - INFO - __main__ - Step 820 Global step 820 Train loss 0.25 on epoch=409
03/11/2022 05:51:07 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=414
03/11/2022 05:51:10 - INFO - __main__ - Step 840 Global step 840 Train loss 0.35 on epoch=419
03/11/2022 05:51:12 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=424
03/11/2022 05:51:13 - INFO - __main__ - Global step 850 Train loss 0.27 Classification-F1 0.3992490613266583 on epoch=424
03/11/2022 05:51:15 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=429
03/11/2022 05:51:17 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=434
03/11/2022 05:51:19 - INFO - __main__ - Step 880 Global step 880 Train loss 0.24 on epoch=439
03/11/2022 05:51:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.23 on epoch=444
03/11/2022 05:51:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=449
03/11/2022 05:51:24 - INFO - __main__ - Global step 900 Train loss 0.21 Classification-F1 0.3816425120772947 on epoch=449
03/11/2022 05:51:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=454
03/11/2022 05:51:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=459
03/11/2022 05:51:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=464
03/11/2022 05:51:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=469
03/11/2022 05:51:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.18 on epoch=474
03/11/2022 05:51:36 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.5270935960591133 on epoch=474
03/11/2022 05:51:38 - INFO - __main__ - Step 960 Global step 960 Train loss 0.18 on epoch=479
03/11/2022 05:51:40 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=484
03/11/2022 05:51:42 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=489
03/11/2022 05:51:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=494
03/11/2022 05:51:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=499
03/11/2022 05:51:48 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.46843853820598 on epoch=499
03/11/2022 05:51:50 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=504
03/11/2022 05:51:52 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.20 on epoch=509
03/11/2022 05:51:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=514
03/11/2022 05:51:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.18 on epoch=519
03/11/2022 05:51:59 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.10 on epoch=524
03/11/2022 05:51:59 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.4458874458874459 on epoch=524
03/11/2022 05:52:02 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.13 on epoch=529
03/11/2022 05:52:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.15 on epoch=534
03/11/2022 05:52:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.11 on epoch=539
03/11/2022 05:52:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=544
03/11/2022 05:52:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=549
03/11/2022 05:52:11 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.5076923076923077 on epoch=549
03/11/2022 05:52:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.11 on epoch=554
03/11/2022 05:52:15 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=559
03/11/2022 05:52:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.13 on epoch=564
03/11/2022 05:52:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.04 on epoch=569
03/11/2022 05:52:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.11 on epoch=574
03/11/2022 05:52:23 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.4666666666666667 on epoch=574
03/11/2022 05:52:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
03/11/2022 05:52:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=584
03/11/2022 05:52:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
03/11/2022 05:52:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=594
03/11/2022 05:52:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=599
03/11/2022 05:52:34 - INFO - __main__ - Global step 1200 Train loss 0.08 Classification-F1 0.4682306940371457 on epoch=599
03/11/2022 05:52:36 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.05 on epoch=604
03/11/2022 05:52:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.11 on epoch=609
03/11/2022 05:52:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.09 on epoch=614
03/11/2022 05:52:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
03/11/2022 05:52:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
03/11/2022 05:52:46 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.5270935960591133 on epoch=624
03/11/2022 05:52:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
03/11/2022 05:52:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
03/11/2022 05:52:52 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
03/11/2022 05:52:55 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.02 on epoch=644
03/11/2022 05:52:57 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.06 on epoch=649
03/11/2022 05:52:58 - INFO - __main__ - Global step 1300 Train loss 0.04 Classification-F1 0.5270935960591133 on epoch=649
03/11/2022 05:53:00 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.06 on epoch=654
03/11/2022 05:53:02 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.03 on epoch=659
03/11/2022 05:53:04 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.02 on epoch=664
03/11/2022 05:53:06 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.04 on epoch=669
03/11/2022 05:53:08 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.02 on epoch=674
03/11/2022 05:53:09 - INFO - __main__ - Global step 1350 Train loss 0.03 Classification-F1 0.5333333333333333 on epoch=674
03/11/2022 05:53:11 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
03/11/2022 05:53:14 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.04 on epoch=684
03/11/2022 05:53:16 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.04 on epoch=689
03/11/2022 05:53:18 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.05 on epoch=694
03/11/2022 05:53:20 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.02 on epoch=699
03/11/2022 05:53:21 - INFO - __main__ - Global step 1400 Train loss 0.04 Classification-F1 0.5607843137254902 on epoch=699
03/11/2022 05:53:21 - INFO - __main__ - Saving model with best Classification-F1: 0.5333333333333333 -> 0.5607843137254902 on epoch=699, global_step=1400
03/11/2022 05:53:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/11/2022 05:53:25 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/11/2022 05:53:27 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.03 on epoch=714
03/11/2022 05:53:30 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.04 on epoch=719
03/11/2022 05:53:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.01 on epoch=724
03/11/2022 05:53:33 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.5607843137254902 on epoch=724
03/11/2022 05:53:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.05 on epoch=729
03/11/2022 05:53:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/11/2022 05:53:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
03/11/2022 05:53:41 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.04 on epoch=744
03/11/2022 05:53:43 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
03/11/2022 05:53:44 - INFO - __main__ - Global step 1500 Train loss 0.03 Classification-F1 0.4980392156862745 on epoch=749
03/11/2022 05:53:46 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/11/2022 05:53:49 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/11/2022 05:53:51 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.01 on epoch=764
03/11/2022 05:53:53 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 05:53:55 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=774
03/11/2022 05:53:56 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=774
03/11/2022 05:53:58 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.01 on epoch=779
03/11/2022 05:54:00 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.04 on epoch=784
03/11/2022 05:54:02 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.01 on epoch=789
03/11/2022 05:54:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
03/11/2022 05:54:07 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.02 on epoch=799
03/11/2022 05:54:08 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.4980392156862745 on epoch=799
03/11/2022 05:54:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
03/11/2022 05:54:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
03/11/2022 05:54:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 05:54:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/11/2022 05:54:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/11/2022 05:54:19 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=824
03/11/2022 05:54:19 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5901477832512315 on epoch=824, global_step=1650
03/11/2022 05:54:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/11/2022 05:54:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.01 on epoch=834
03/11/2022 05:54:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/11/2022 05:54:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 05:54:30 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/11/2022 05:54:31 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.4666666666666667 on epoch=849
03/11/2022 05:54:33 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
03/11/2022 05:54:35 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
03/11/2022 05:54:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.01 on epoch=864
03/11/2022 05:54:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.01 on epoch=869
03/11/2022 05:54:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 05:54:43 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=874
03/11/2022 05:54:45 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.03 on epoch=879
03/11/2022 05:54:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 05:54:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
03/11/2022 05:54:52 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 05:54:54 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 05:54:54 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=899
03/11/2022 05:54:55 - INFO - __main__ - Saving model with best Classification-F1: 0.5901477832512315 -> 0.5933528836754642 on epoch=899, global_step=1800
03/11/2022 05:54:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 05:54:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 05:55:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.01 on epoch=914
03/11/2022 05:55:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 05:55:05 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 05:55:06 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.5555555555555556 on epoch=924
03/11/2022 05:55:08 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/11/2022 05:55:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 05:55:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 05:55:15 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 05:55:17 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 05:55:18 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.5 on epoch=949
03/11/2022 05:55:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/11/2022 05:55:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.01 on epoch=959
03/11/2022 05:55:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.03 on epoch=964
03/11/2022 05:55:27 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 05:55:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.02 on epoch=974
03/11/2022 05:55:29 - INFO - __main__ - Global step 1950 Train loss 0.02 Classification-F1 0.5 on epoch=974
03/11/2022 05:55:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 05:55:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 05:55:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 05:55:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 05:55:40 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 05:55:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:55:42 - INFO - __main__ - Printing 3 examples
03/11/2022 05:55:42 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/11/2022 05:55:42 - INFO - __main__ - ['hate']
03/11/2022 05:55:42 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/11/2022 05:55:42 - INFO - __main__ - ['hate']
03/11/2022 05:55:42 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/11/2022 05:55:42 - INFO - __main__ - ['hate']
03/11/2022 05:55:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 05:55:42 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:55:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:55:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:55:42 - INFO - __main__ - Printing 3 examples
03/11/2022 05:55:42 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/11/2022 05:55:42 - INFO - __main__ - ['hate']
03/11/2022 05:55:42 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/11/2022 05:55:42 - INFO - __main__ - ['hate']
03/11/2022 05:55:42 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/11/2022 05:55:42 - INFO - __main__ - ['hate']
03/11/2022 05:55:42 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:55:42 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:55:42 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:55:42 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5 on epoch=999
03/11/2022 05:55:42 - INFO - __main__ - save last model!
03/11/2022 05:55:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 05:55:42 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 05:55:42 - INFO - __main__ - Printing 3 examples
03/11/2022 05:55:42 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 05:55:42 - INFO - __main__ - ['hate']
03/11/2022 05:55:42 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 05:55:42 - INFO - __main__ - ['non-irony']
03/11/2022 05:55:42 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 05:55:42 - INFO - __main__ - ['hate']
03/11/2022 05:55:42 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:55:42 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:55:43 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 05:55:54 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:55:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:55:55 - INFO - __main__ - Starting training!
03/11/2022 05:56:23 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_42_0.3_8_predictions.txt
03/11/2022 05:56:23 - INFO - __main__ - Classification-F1 on test data: 0.5126
03/11/2022 05:56:24 - INFO - __main__ - prefix=tweet_eval-irony_16_42, lr=0.3, bsz=8, dev_performance=0.5933528836754642, test_performance=0.5126080395115927
03/11/2022 05:56:24 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_42, lr=0.2, bsz=8 ...
03/11/2022 05:56:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:56:24 - INFO - __main__ - Printing 3 examples
03/11/2022 05:56:24 - INFO - __main__ -  [tweet_eval-irony] Just watched how Pretzels were made. #interesinglife  #whatamidoingwithmylife #longesthashtagnotneededbutYOLO
03/11/2022 05:56:24 - INFO - __main__ - ['hate']
03/11/2022 05:56:24 - INFO - __main__ -  [tweet_eval-irony] I love being on hold. Especially when the music is so great.
03/11/2022 05:56:24 - INFO - __main__ - ['hate']
03/11/2022 05:56:24 - INFO - __main__ -  [tweet_eval-irony] I'm so glad I'm sick today. | #bullcrap
03/11/2022 05:56:24 - INFO - __main__ - ['hate']
03/11/2022 05:56:24 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 05:56:24 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:56:24 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 05:56:24 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 05:56:24 - INFO - __main__ - Printing 3 examples
03/11/2022 05:56:24 - INFO - __main__ -  [tweet_eval-irony] Work Christmas Eve and Christmas Day.. Can't wait!
03/11/2022 05:56:24 - INFO - __main__ - ['hate']
03/11/2022 05:56:24 - INFO - __main__ -  [tweet_eval-irony] There's nothing like almost rear-ending someone because they slam on brakes for no reason to get you alert and ready for an exam!!
03/11/2022 05:56:24 - INFO - __main__ - ['hate']
03/11/2022 05:56:24 - INFO - __main__ -  [tweet_eval-irony] We'll that was a great way to start off my morning 😀
03/11/2022 05:56:24 - INFO - __main__ - ['hate']
03/11/2022 05:56:24 - INFO - __main__ - Tokenizing Input ...
03/11/2022 05:56:24 - INFO - __main__ - Tokenizing Output ...
03/11/2022 05:56:25 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 05:56:37 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 05:56:38 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 05:56:38 - INFO - __main__ - Starting training!
03/11/2022 05:56:41 - INFO - __main__ - Step 10 Global step 10 Train loss 3.17 on epoch=4
03/11/2022 05:56:43 - INFO - __main__ - Step 20 Global step 20 Train loss 1.60 on epoch=9
03/11/2022 05:56:45 - INFO - __main__ - Step 30 Global step 30 Train loss 0.90 on epoch=14
03/11/2022 05:56:47 - INFO - __main__ - Step 40 Global step 40 Train loss 0.60 on epoch=19
03/11/2022 05:56:49 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=24
03/11/2022 05:56:51 - INFO - __main__ - Global step 50 Train loss 1.36 Classification-F1 0.3333333333333333 on epoch=24
03/11/2022 05:56:51 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3333333333333333 on epoch=24, global_step=50
03/11/2022 05:56:53 - INFO - __main__ - Step 60 Global step 60 Train loss 0.41 on epoch=29
03/11/2022 05:56:55 - INFO - __main__ - Step 70 Global step 70 Train loss 0.44 on epoch=34
03/11/2022 05:56:58 - INFO - __main__ - Step 80 Global step 80 Train loss 0.42 on epoch=39
03/11/2022 05:57:00 - INFO - __main__ - Step 90 Global step 90 Train loss 0.36 on epoch=44
03/11/2022 05:57:02 - INFO - __main__ - Step 100 Global step 100 Train loss 0.37 on epoch=49
03/11/2022 05:57:03 - INFO - __main__ - Global step 100 Train loss 0.40 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 05:57:05 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
03/11/2022 05:57:07 - INFO - __main__ - Step 120 Global step 120 Train loss 0.33 on epoch=59
03/11/2022 05:57:09 - INFO - __main__ - Step 130 Global step 130 Train loss 0.37 on epoch=64
03/11/2022 05:57:12 - INFO - __main__ - Step 140 Global step 140 Train loss 0.31 on epoch=69
03/11/2022 05:57:14 - INFO - __main__ - Step 150 Global step 150 Train loss 0.34 on epoch=74
03/11/2022 05:57:15 - INFO - __main__ - Global step 150 Train loss 0.34 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 05:57:17 - INFO - __main__ - Step 160 Global step 160 Train loss 0.32 on epoch=79
03/11/2022 05:57:19 - INFO - __main__ - Step 170 Global step 170 Train loss 0.32 on epoch=84
03/11/2022 05:57:21 - INFO - __main__ - Step 180 Global step 180 Train loss 0.36 on epoch=89
03/11/2022 05:57:23 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
03/11/2022 05:57:25 - INFO - __main__ - Step 200 Global step 200 Train loss 0.27 on epoch=99
03/11/2022 05:57:26 - INFO - __main__ - Global step 200 Train loss 0.31 Classification-F1 0.3333333333333333 on epoch=99
03/11/2022 05:57:28 - INFO - __main__ - Step 210 Global step 210 Train loss 0.29 on epoch=104
03/11/2022 05:57:31 - INFO - __main__ - Step 220 Global step 220 Train loss 0.31 on epoch=109
03/11/2022 05:57:33 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
03/11/2022 05:57:35 - INFO - __main__ - Step 240 Global step 240 Train loss 0.33 on epoch=119
03/11/2022 05:57:37 - INFO - __main__ - Step 250 Global step 250 Train loss 0.27 on epoch=124
03/11/2022 05:57:38 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 05:57:40 - INFO - __main__ - Step 260 Global step 260 Train loss 0.24 on epoch=129
03/11/2022 05:57:42 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
03/11/2022 05:57:44 - INFO - __main__ - Step 280 Global step 280 Train loss 0.28 on epoch=139
03/11/2022 05:57:47 - INFO - __main__ - Step 290 Global step 290 Train loss 0.30 on epoch=144
03/11/2022 05:57:49 - INFO - __main__ - Step 300 Global step 300 Train loss 0.31 on epoch=149
03/11/2022 05:57:50 - INFO - __main__ - Global step 300 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 05:57:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.29 on epoch=154
03/11/2022 05:57:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
03/11/2022 05:57:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=164
03/11/2022 05:57:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.24 on epoch=169
03/11/2022 05:58:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
03/11/2022 05:58:01 - INFO - __main__ - Global step 350 Train loss 0.28 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 05:58:04 - INFO - __main__ - Step 360 Global step 360 Train loss 0.31 on epoch=179
03/11/2022 05:58:06 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
03/11/2022 05:58:08 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
03/11/2022 05:58:10 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
03/11/2022 05:58:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 05:58:13 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.464039408866995 on epoch=199
03/11/2022 05:58:13 - INFO - __main__ - Saving model with best Classification-F1: 0.3333333333333333 -> 0.464039408866995 on epoch=199, global_step=400
03/11/2022 05:58:15 - INFO - __main__ - Step 410 Global step 410 Train loss 0.28 on epoch=204
03/11/2022 05:58:18 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
03/11/2022 05:58:20 - INFO - __main__ - Step 430 Global step 430 Train loss 0.27 on epoch=214
03/11/2022 05:58:22 - INFO - __main__ - Step 440 Global step 440 Train loss 0.27 on epoch=219
03/11/2022 05:58:24 - INFO - __main__ - Step 450 Global step 450 Train loss 0.26 on epoch=224
03/11/2022 05:58:25 - INFO - __main__ - Global step 450 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=224
03/11/2022 05:58:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.25 on epoch=229
03/11/2022 05:58:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 05:58:32 - INFO - __main__ - Step 480 Global step 480 Train loss 0.27 on epoch=239
03/11/2022 05:58:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
03/11/2022 05:58:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.24 on epoch=249
03/11/2022 05:58:37 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=249
03/11/2022 05:58:39 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=254
03/11/2022 05:58:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
03/11/2022 05:58:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
03/11/2022 05:58:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.28 on epoch=269
03/11/2022 05:58:48 - INFO - __main__ - Step 550 Global step 550 Train loss 0.25 on epoch=274
03/11/2022 05:58:48 - INFO - __main__ - Global step 550 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=274
03/11/2022 05:58:51 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=279
03/11/2022 05:58:53 - INFO - __main__ - Step 570 Global step 570 Train loss 0.28 on epoch=284
03/11/2022 05:58:55 - INFO - __main__ - Step 580 Global step 580 Train loss 0.22 on epoch=289
03/11/2022 05:58:57 - INFO - __main__ - Step 590 Global step 590 Train loss 0.22 on epoch=294
03/11/2022 05:58:59 - INFO - __main__ - Step 600 Global step 600 Train loss 0.25 on epoch=299
03/11/2022 05:59:00 - INFO - __main__ - Global step 600 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=299
03/11/2022 05:59:02 - INFO - __main__ - Step 610 Global step 610 Train loss 0.30 on epoch=304
03/11/2022 05:59:04 - INFO - __main__ - Step 620 Global step 620 Train loss 0.26 on epoch=309
03/11/2022 05:59:06 - INFO - __main__ - Step 630 Global step 630 Train loss 0.20 on epoch=314
03/11/2022 05:59:09 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
03/11/2022 05:59:11 - INFO - __main__ - Step 650 Global step 650 Train loss 0.28 on epoch=324
03/11/2022 05:59:12 - INFO - __main__ - Global step 650 Train loss 0.25 Classification-F1 0.3333333333333333 on epoch=324
03/11/2022 05:59:14 - INFO - __main__ - Step 660 Global step 660 Train loss 0.26 on epoch=329
03/11/2022 05:59:16 - INFO - __main__ - Step 670 Global step 670 Train loss 0.24 on epoch=334
03/11/2022 05:59:18 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=339
03/11/2022 05:59:20 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
03/11/2022 05:59:22 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
03/11/2022 05:59:23 - INFO - __main__ - Global step 700 Train loss 0.25 Classification-F1 0.5076923076923077 on epoch=349
03/11/2022 05:59:23 - INFO - __main__ - Saving model with best Classification-F1: 0.464039408866995 -> 0.5076923076923077 on epoch=349, global_step=700
03/11/2022 05:59:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=354
03/11/2022 05:59:28 - INFO - __main__ - Step 720 Global step 720 Train loss 0.25 on epoch=359
03/11/2022 05:59:30 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=364
03/11/2022 05:59:32 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
03/11/2022 05:59:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=374
03/11/2022 05:59:35 - INFO - __main__ - Global step 750 Train loss 0.24 Classification-F1 0.49090909090909085 on epoch=374
03/11/2022 05:59:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.25 on epoch=379
03/11/2022 05:59:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.24 on epoch=384
03/11/2022 05:59:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.22 on epoch=389
03/11/2022 05:59:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=394
03/11/2022 05:59:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.19 on epoch=399
03/11/2022 05:59:47 - INFO - __main__ - Global step 800 Train loss 0.22 Classification-F1 0.5636363636363637 on epoch=399
03/11/2022 05:59:47 - INFO - __main__ - Saving model with best Classification-F1: 0.5076923076923077 -> 0.5636363636363637 on epoch=399, global_step=800
03/11/2022 05:59:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.27 on epoch=404
03/11/2022 05:59:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.22 on epoch=409
03/11/2022 05:59:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.21 on epoch=414
03/11/2022 05:59:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=419
03/11/2022 05:59:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.21 on epoch=424
03/11/2022 05:59:58 - INFO - __main__ - Global step 850 Train loss 0.22 Classification-F1 0.4920634920634921 on epoch=424
03/11/2022 06:00:00 - INFO - __main__ - Step 860 Global step 860 Train loss 0.20 on epoch=429
03/11/2022 06:00:03 - INFO - __main__ - Step 870 Global step 870 Train loss 0.17 on epoch=434
03/11/2022 06:00:05 - INFO - __main__ - Step 880 Global step 880 Train loss 0.19 on epoch=439
03/11/2022 06:00:07 - INFO - __main__ - Step 890 Global step 890 Train loss 0.22 on epoch=444
03/11/2022 06:00:09 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=449
03/11/2022 06:00:10 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.3333333333333333 on epoch=449
03/11/2022 06:00:12 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=454
03/11/2022 06:00:14 - INFO - __main__ - Step 920 Global step 920 Train loss 0.20 on epoch=459
03/11/2022 06:00:16 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=464
03/11/2022 06:00:19 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=469
03/11/2022 06:00:21 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
03/11/2022 06:00:22 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.40566959921798634 on epoch=474
03/11/2022 06:00:24 - INFO - __main__ - Step 960 Global step 960 Train loss 0.20 on epoch=479
03/11/2022 06:00:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.20 on epoch=484
03/11/2022 06:00:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=489
03/11/2022 06:00:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.15 on epoch=494
03/11/2022 06:00:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=499
03/11/2022 06:00:33 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.4682306940371457 on epoch=499
03/11/2022 06:00:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.13 on epoch=504
03/11/2022 06:00:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=509
03/11/2022 06:00:40 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.15 on epoch=514
03/11/2022 06:00:42 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=519
03/11/2022 06:00:44 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.14 on epoch=524
03/11/2022 06:00:45 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.5270935960591133 on epoch=524
03/11/2022 06:00:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=529
03/11/2022 06:00:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.10 on epoch=534
03/11/2022 06:00:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.16 on epoch=539
03/11/2022 06:00:54 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=544
03/11/2022 06:00:56 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.14 on epoch=549
03/11/2022 06:00:57 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.4980392156862745 on epoch=549
03/11/2022 06:00:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.08 on epoch=554
03/11/2022 06:01:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=559
03/11/2022 06:01:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=564
03/11/2022 06:01:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=569
03/11/2022 06:01:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=574
03/11/2022 06:01:08 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.5933528836754642 on epoch=574
03/11/2022 06:01:08 - INFO - __main__ - Saving model with best Classification-F1: 0.5636363636363637 -> 0.5933528836754642 on epoch=574, global_step=1150
03/11/2022 06:01:10 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.06 on epoch=579
03/11/2022 06:01:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
03/11/2022 06:01:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.09 on epoch=589
03/11/2022 06:01:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.10 on epoch=594
03/11/2022 06:01:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=599
03/11/2022 06:01:20 - INFO - __main__ - Global step 1200 Train loss 0.07 Classification-F1 0.464039408866995 on epoch=599
03/11/2022 06:01:22 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.07 on epoch=604
03/11/2022 06:01:24 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
03/11/2022 06:01:26 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.04 on epoch=614
03/11/2022 06:01:29 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
03/11/2022 06:01:31 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
03/11/2022 06:01:32 - INFO - __main__ - Global step 1250 Train loss 0.06 Classification-F1 0.5076923076923077 on epoch=624
03/11/2022 06:01:34 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.06 on epoch=629
03/11/2022 06:01:36 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=634
03/11/2022 06:01:38 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
03/11/2022 06:01:40 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.03 on epoch=644
03/11/2022 06:01:43 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
03/11/2022 06:01:43 - INFO - __main__ - Global step 1300 Train loss 0.06 Classification-F1 0.5270935960591133 on epoch=649
03/11/2022 06:01:46 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
03/11/2022 06:01:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
03/11/2022 06:01:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.03 on epoch=664
03/11/2022 06:01:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
03/11/2022 06:01:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/11/2022 06:01:55 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.5555555555555556 on epoch=674
03/11/2022 06:01:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.02 on epoch=679
03/11/2022 06:01:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.01 on epoch=684
03/11/2022 06:02:02 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.02 on epoch=689
03/11/2022 06:02:04 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.01 on epoch=694
03/11/2022 06:02:06 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.03 on epoch=699
03/11/2022 06:02:07 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.5465587044534412 on epoch=699
03/11/2022 06:02:09 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.04 on epoch=704
03/11/2022 06:02:11 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.02 on epoch=709
03/11/2022 06:02:13 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.02 on epoch=714
03/11/2022 06:02:15 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.01 on epoch=719
03/11/2022 06:02:18 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.03 on epoch=724
03/11/2022 06:02:18 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.5195195195195195 on epoch=724
03/11/2022 06:02:21 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.01 on epoch=729
03/11/2022 06:02:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.02 on epoch=734
03/11/2022 06:02:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 06:02:27 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.01 on epoch=744
03/11/2022 06:02:29 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.01 on epoch=749
03/11/2022 06:02:30 - INFO - __main__ - Global step 1500 Train loss 0.01 Classification-F1 0.5844155844155844 on epoch=749
03/11/2022 06:02:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.02 on epoch=754
03/11/2022 06:02:34 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.01 on epoch=759
03/11/2022 06:02:37 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 06:02:39 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.02 on epoch=769
03/11/2022 06:02:41 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.01 on epoch=774
03/11/2022 06:02:42 - INFO - __main__ - Global step 1550 Train loss 0.01 Classification-F1 0.5835835835835835 on epoch=774
03/11/2022 06:02:44 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 06:02:46 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 06:02:48 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.04 on epoch=789
03/11/2022 06:02:51 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.03 on epoch=794
03/11/2022 06:02:53 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.01 on epoch=799
03/11/2022 06:02:54 - INFO - __main__ - Global step 1600 Train loss 0.02 Classification-F1 0.6113360323886641 on epoch=799
03/11/2022 06:02:54 - INFO - __main__ - Saving model with best Classification-F1: 0.5933528836754642 -> 0.6113360323886641 on epoch=799, global_step=1600
03/11/2022 06:02:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 06:02:58 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.01 on epoch=809
03/11/2022 06:03:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.01 on epoch=814
03/11/2022 06:03:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/11/2022 06:03:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.01 on epoch=824
03/11/2022 06:03:05 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=824
03/11/2022 06:03:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/11/2022 06:03:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.03 on epoch=834
03/11/2022 06:03:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/11/2022 06:03:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 06:03:16 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.01 on epoch=849
03/11/2022 06:03:17 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5270935960591133 on epoch=849
03/11/2022 06:03:19 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
03/11/2022 06:03:21 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 06:03:24 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 06:03:26 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.02 on epoch=869
03/11/2022 06:03:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 06:03:29 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5307917888563051 on epoch=874
03/11/2022 06:03:31 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/11/2022 06:03:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 06:03:35 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.02 on epoch=889
03/11/2022 06:03:37 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 06:03:40 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
03/11/2022 06:03:40 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=899
03/11/2022 06:03:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
03/11/2022 06:03:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 06:03:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 06:03:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 06:03:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 06:03:52 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.5555555555555556 on epoch=924
03/11/2022 06:03:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.01 on epoch=929
03/11/2022 06:03:56 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.02 on epoch=934
03/11/2022 06:03:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.01 on epoch=939
03/11/2022 06:04:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 06:04:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 06:04:04 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5933528836754642 on epoch=949
03/11/2022 06:04:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 06:04:08 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 06:04:10 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 06:04:12 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.02 on epoch=969
03/11/2022 06:04:15 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 06:04:15 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6190476190476191 on epoch=974
03/11/2022 06:04:15 - INFO - __main__ - Saving model with best Classification-F1: 0.6113360323886641 -> 0.6190476190476191 on epoch=974, global_step=1950
03/11/2022 06:04:18 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.02 on epoch=979
03/11/2022 06:04:20 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/11/2022 06:04:22 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 06:04:24 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 06:04:26 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 06:04:27 - INFO - __main__ - Global step 2000 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=999
03/11/2022 06:04:27 - INFO - __main__ - save last model!
03/11/2022 06:04:27 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 06:04:27 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 06:04:27 - INFO - __main__ - Printing 3 examples
03/11/2022 06:04:27 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 06:04:27 - INFO - __main__ - ['hate']
03/11/2022 06:04:27 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 06:04:27 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:27 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 06:04:27 - INFO - __main__ - ['hate']
03/11/2022 06:04:27 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 06:04:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:04:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:04:28 - INFO - __main__ - Printing 3 examples
03/11/2022 06:04:28 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 06:04:28 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:28 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 06:04:28 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:28 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 06:04:28 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:28 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 06:04:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:04:28 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 06:04:28 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:04:28 - INFO - __main__ - Printing 3 examples
03/11/2022 06:04:28 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 06:04:28 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:28 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 06:04:28 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:28 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 06:04:28 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:28 - INFO - __main__ - Tokenizing Input ...
03/11/2022 06:04:28 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:04:28 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 06:04:29 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 06:04:40 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 06:04:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 06:04:41 - INFO - __main__ - Starting training!
03/11/2022 06:04:53 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_42_0.2_8_predictions.txt
03/11/2022 06:04:53 - INFO - __main__ - Classification-F1 on test data: 0.2363
03/11/2022 06:04:53 - INFO - __main__ - prefix=tweet_eval-irony_16_42, lr=0.2, bsz=8, dev_performance=0.6190476190476191, test_performance=0.23627912288716307
03/11/2022 06:04:53 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_87, lr=0.5, bsz=8 ...
03/11/2022 06:04:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:04:54 - INFO - __main__ - Printing 3 examples
03/11/2022 06:04:54 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 06:04:54 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:54 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 06:04:54 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:54 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 06:04:54 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:54 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 06:04:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:04:54 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 06:04:54 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:04:54 - INFO - __main__ - Printing 3 examples
03/11/2022 06:04:54 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 06:04:54 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:54 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 06:04:54 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:54 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 06:04:54 - INFO - __main__ - ['non-irony']
03/11/2022 06:04:54 - INFO - __main__ - Tokenizing Input ...
03/11/2022 06:04:54 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:04:54 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 06:05:07 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 06:05:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 06:05:07 - INFO - __main__ - Starting training!
03/11/2022 06:05:10 - INFO - __main__ - Step 10 Global step 10 Train loss 2.09 on epoch=4
03/11/2022 06:05:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.59 on epoch=9
03/11/2022 06:05:14 - INFO - __main__ - Step 30 Global step 30 Train loss 0.42 on epoch=14
03/11/2022 06:05:17 - INFO - __main__ - Step 40 Global step 40 Train loss 0.44 on epoch=19
03/11/2022 06:05:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.34 on epoch=24
03/11/2022 06:05:20 - INFO - __main__ - Global step 50 Train loss 0.78 Classification-F1 0.5134502923976608 on epoch=24
03/11/2022 06:05:20 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.5134502923976608 on epoch=24, global_step=50
03/11/2022 06:05:22 - INFO - __main__ - Step 60 Global step 60 Train loss 0.28 on epoch=29
03/11/2022 06:05:24 - INFO - __main__ - Step 70 Global step 70 Train loss 0.30 on epoch=34
03/11/2022 06:05:26 - INFO - __main__ - Step 80 Global step 80 Train loss 0.30 on epoch=39
03/11/2022 06:05:29 - INFO - __main__ - Step 90 Global step 90 Train loss 0.31 on epoch=44
03/11/2022 06:05:31 - INFO - __main__ - Step 100 Global step 100 Train loss 0.42 on epoch=49
03/11/2022 06:05:31 - INFO - __main__ - Global step 100 Train loss 0.32 Classification-F1 0.3333333333333333 on epoch=49
03/11/2022 06:05:34 - INFO - __main__ - Step 110 Global step 110 Train loss 0.38 on epoch=54
03/11/2022 06:05:36 - INFO - __main__ - Step 120 Global step 120 Train loss 0.25 on epoch=59
03/11/2022 06:05:38 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
03/11/2022 06:05:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.32 on epoch=69
03/11/2022 06:05:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.26 on epoch=74
03/11/2022 06:05:43 - INFO - __main__ - Global step 150 Train loss 0.30 Classification-F1 0.3191489361702127 on epoch=74
03/11/2022 06:05:45 - INFO - __main__ - Step 160 Global step 160 Train loss 0.31 on epoch=79
03/11/2022 06:05:47 - INFO - __main__ - Step 170 Global step 170 Train loss 0.30 on epoch=84
03/11/2022 06:05:50 - INFO - __main__ - Step 180 Global step 180 Train loss 0.30 on epoch=89
03/11/2022 06:05:52 - INFO - __main__ - Step 190 Global step 190 Train loss 0.23 on epoch=94
03/11/2022 06:05:54 - INFO - __main__ - Step 200 Global step 200 Train loss 0.29 on epoch=99
03/11/2022 06:05:55 - INFO - __main__ - Global step 200 Train loss 0.29 Classification-F1 0.3454545454545454 on epoch=99
03/11/2022 06:05:57 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
03/11/2022 06:05:59 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 06:06:01 - INFO - __main__ - Step 230 Global step 230 Train loss 0.26 on epoch=114
03/11/2022 06:06:03 - INFO - __main__ - Step 240 Global step 240 Train loss 0.26 on epoch=119
03/11/2022 06:06:06 - INFO - __main__ - Step 250 Global step 250 Train loss 0.28 on epoch=124
03/11/2022 06:06:06 - INFO - __main__ - Global step 250 Train loss 0.27 Classification-F1 0.41700404858299595 on epoch=124
03/11/2022 06:06:08 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
03/11/2022 06:06:11 - INFO - __main__ - Step 270 Global step 270 Train loss 0.26 on epoch=134
03/11/2022 06:06:13 - INFO - __main__ - Step 280 Global step 280 Train loss 0.23 on epoch=139
03/11/2022 06:06:15 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
03/11/2022 06:06:17 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=149
03/11/2022 06:06:18 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.4420512820512821 on epoch=149
03/11/2022 06:06:20 - INFO - __main__ - Step 310 Global step 310 Train loss 0.21 on epoch=154
03/11/2022 06:06:22 - INFO - __main__ - Step 320 Global step 320 Train loss 0.26 on epoch=159
03/11/2022 06:06:24 - INFO - __main__ - Step 330 Global step 330 Train loss 0.26 on epoch=164
03/11/2022 06:06:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
03/11/2022 06:06:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.26 on epoch=174
03/11/2022 06:06:30 - INFO - __main__ - Global step 350 Train loss 0.25 Classification-F1 0.37662337662337664 on epoch=174
03/11/2022 06:06:32 - INFO - __main__ - Step 360 Global step 360 Train loss 0.25 on epoch=179
03/11/2022 06:06:34 - INFO - __main__ - Step 370 Global step 370 Train loss 0.25 on epoch=184
03/11/2022 06:06:36 - INFO - __main__ - Step 380 Global step 380 Train loss 0.24 on epoch=189
03/11/2022 06:06:38 - INFO - __main__ - Step 390 Global step 390 Train loss 0.25 on epoch=194
03/11/2022 06:06:40 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
03/11/2022 06:06:41 - INFO - __main__ - Global step 400 Train loss 0.25 Classification-F1 0.5195195195195195 on epoch=199
03/11/2022 06:06:41 - INFO - __main__ - Saving model with best Classification-F1: 0.5134502923976608 -> 0.5195195195195195 on epoch=199, global_step=400
03/11/2022 06:06:43 - INFO - __main__ - Step 410 Global step 410 Train loss 0.25 on epoch=204
03/11/2022 06:06:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
03/11/2022 06:06:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
03/11/2022 06:06:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=219
03/11/2022 06:06:52 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
03/11/2022 06:06:53 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.4980392156862745 on epoch=224
03/11/2022 06:06:55 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 06:06:57 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 06:06:59 - INFO - __main__ - Step 480 Global step 480 Train loss 0.25 on epoch=239
03/11/2022 06:07:01 - INFO - __main__ - Step 490 Global step 490 Train loss 0.25 on epoch=244
03/11/2022 06:07:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.27 on epoch=249
03/11/2022 06:07:04 - INFO - __main__ - Global step 500 Train loss 0.25 Classification-F1 0.6862745098039216 on epoch=249
03/11/2022 06:07:04 - INFO - __main__ - Saving model with best Classification-F1: 0.5195195195195195 -> 0.6862745098039216 on epoch=249, global_step=500
03/11/2022 06:07:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
03/11/2022 06:07:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.20 on epoch=259
03/11/2022 06:07:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
03/11/2022 06:07:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=269
03/11/2022 06:07:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.22 on epoch=274
03/11/2022 06:07:16 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.4909862142099682 on epoch=274
03/11/2022 06:07:18 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
03/11/2022 06:07:20 - INFO - __main__ - Step 570 Global step 570 Train loss 0.21 on epoch=284
03/11/2022 06:07:22 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=289
03/11/2022 06:07:25 - INFO - __main__ - Step 590 Global step 590 Train loss 0.25 on epoch=294
03/11/2022 06:07:27 - INFO - __main__ - Step 600 Global step 600 Train loss 0.20 on epoch=299
03/11/2022 06:07:28 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.5270935960591133 on epoch=299
03/11/2022 06:07:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=304
03/11/2022 06:07:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
03/11/2022 06:07:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.24 on epoch=314
03/11/2022 06:07:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.21 on epoch=319
03/11/2022 06:07:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=324
03/11/2022 06:07:39 - INFO - __main__ - Global step 650 Train loss 0.22 Classification-F1 0.3191489361702127 on epoch=324
03/11/2022 06:07:41 - INFO - __main__ - Step 660 Global step 660 Train loss 0.25 on epoch=329
03/11/2022 06:07:43 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
03/11/2022 06:07:45 - INFO - __main__ - Step 680 Global step 680 Train loss 0.18 on epoch=339
03/11/2022 06:07:48 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=344
03/11/2022 06:07:50 - INFO - __main__ - Step 700 Global step 700 Train loss 0.20 on epoch=349
03/11/2022 06:07:51 - INFO - __main__ - Global step 700 Train loss 0.21 Classification-F1 0.4666666666666667 on epoch=349
03/11/2022 06:07:53 - INFO - __main__ - Step 710 Global step 710 Train loss 0.26 on epoch=354
03/11/2022 06:07:55 - INFO - __main__ - Step 720 Global step 720 Train loss 0.21 on epoch=359
03/11/2022 06:07:57 - INFO - __main__ - Step 730 Global step 730 Train loss 0.21 on epoch=364
03/11/2022 06:07:59 - INFO - __main__ - Step 740 Global step 740 Train loss 0.23 on epoch=369
03/11/2022 06:08:01 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=374
03/11/2022 06:08:02 - INFO - __main__ - Global step 750 Train loss 0.22 Classification-F1 0.5076923076923077 on epoch=374
03/11/2022 06:08:04 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=379
03/11/2022 06:08:06 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=384
03/11/2022 06:08:08 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=389
03/11/2022 06:08:11 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=394
03/11/2022 06:08:13 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=399
03/11/2022 06:08:13 - INFO - __main__ - Global step 800 Train loss 0.20 Classification-F1 0.4458874458874459 on epoch=399
03/11/2022 06:08:16 - INFO - __main__ - Step 810 Global step 810 Train loss 0.19 on epoch=404
03/11/2022 06:08:18 - INFO - __main__ - Step 820 Global step 820 Train loss 0.21 on epoch=409
03/11/2022 06:08:20 - INFO - __main__ - Step 830 Global step 830 Train loss 0.16 on epoch=414
03/11/2022 06:08:22 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=419
03/11/2022 06:08:24 - INFO - __main__ - Step 850 Global step 850 Train loss 0.19 on epoch=424
03/11/2022 06:08:25 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.5076923076923077 on epoch=424
03/11/2022 06:08:27 - INFO - __main__ - Step 860 Global step 860 Train loss 0.18 on epoch=429
03/11/2022 06:08:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.23 on epoch=434
03/11/2022 06:08:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.17 on epoch=439
03/11/2022 06:08:34 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=444
03/11/2022 06:08:36 - INFO - __main__ - Step 900 Global step 900 Train loss 0.20 on epoch=449
03/11/2022 06:08:36 - INFO - __main__ - Global step 900 Train loss 0.19 Classification-F1 0.5555555555555556 on epoch=449
03/11/2022 06:08:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.20 on epoch=454
03/11/2022 06:08:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.18 on epoch=459
03/11/2022 06:08:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=464
03/11/2022 06:08:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.18 on epoch=469
03/11/2022 06:08:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
03/11/2022 06:08:48 - INFO - __main__ - Global step 950 Train loss 0.19 Classification-F1 0.5076923076923077 on epoch=474
03/11/2022 06:08:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.19 on epoch=479
03/11/2022 06:08:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.16 on epoch=484
03/11/2022 06:08:55 - INFO - __main__ - Step 980 Global step 980 Train loss 0.18 on epoch=489
03/11/2022 06:08:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.17 on epoch=494
03/11/2022 06:08:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.13 on epoch=499
03/11/2022 06:09:00 - INFO - __main__ - Global step 1000 Train loss 0.17 Classification-F1 0.5607843137254902 on epoch=499
03/11/2022 06:09:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.14 on epoch=504
03/11/2022 06:09:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.13 on epoch=509
03/11/2022 06:09:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.12 on epoch=514
03/11/2022 06:09:08 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.12 on epoch=519
03/11/2022 06:09:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.09 on epoch=524
03/11/2022 06:09:11 - INFO - __main__ - Global step 1050 Train loss 0.12 Classification-F1 0.5555555555555556 on epoch=524
03/11/2022 06:09:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.12 on epoch=529
03/11/2022 06:09:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=534
03/11/2022 06:09:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=539
03/11/2022 06:09:20 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=544
03/11/2022 06:09:22 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=549
03/11/2022 06:09:23 - INFO - __main__ - Global step 1100 Train loss 0.09 Classification-F1 0.46843853820598 on epoch=549
03/11/2022 06:09:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.05 on epoch=554
03/11/2022 06:09:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.03 on epoch=559
03/11/2022 06:09:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.03 on epoch=564
03/11/2022 06:09:31 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.06 on epoch=569
03/11/2022 06:09:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.02 on epoch=574
03/11/2022 06:09:34 - INFO - __main__ - Global step 1150 Train loss 0.04 Classification-F1 0.6389743589743591 on epoch=574
03/11/2022 06:09:36 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.01 on epoch=579
03/11/2022 06:09:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.01 on epoch=584
03/11/2022 06:09:41 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.04 on epoch=589
03/11/2022 06:09:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.01 on epoch=594
03/11/2022 06:09:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.01 on epoch=599
03/11/2022 06:09:46 - INFO - __main__ - Global step 1200 Train loss 0.02 Classification-F1 0.6113360323886641 on epoch=599
03/11/2022 06:09:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.01 on epoch=604
03/11/2022 06:09:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.01 on epoch=609
03/11/2022 06:09:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.03 on epoch=614
03/11/2022 06:09:54 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.04 on epoch=619
03/11/2022 06:09:57 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/11/2022 06:09:57 - INFO - __main__ - Global step 1250 Train loss 0.02 Classification-F1 0.746031746031746 on epoch=624
03/11/2022 06:09:57 - INFO - __main__ - Saving model with best Classification-F1: 0.6862745098039216 -> 0.746031746031746 on epoch=624, global_step=1250
03/11/2022 06:10:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.01 on epoch=629
03/11/2022 06:10:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 06:10:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.00 on epoch=639
03/11/2022 06:10:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.01 on epoch=644
03/11/2022 06:10:08 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/11/2022 06:10:09 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.6235294117647059 on epoch=649
03/11/2022 06:10:11 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.03 on epoch=654
03/11/2022 06:10:13 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.02 on epoch=659
03/11/2022 06:10:15 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
03/11/2022 06:10:18 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 06:10:20 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.00 on epoch=674
03/11/2022 06:10:21 - INFO - __main__ - Global step 1350 Train loss 0.02 Classification-F1 0.6532019704433498 on epoch=674
03/11/2022 06:10:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 06:10:25 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 06:10:27 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/11/2022 06:10:29 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.00 on epoch=694
03/11/2022 06:10:31 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.00 on epoch=699
03/11/2022 06:10:32 - INFO - __main__ - Global step 1400 Train loss 0.00 Classification-F1 0.6113360323886641 on epoch=699
03/11/2022 06:10:34 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.00 on epoch=704
03/11/2022 06:10:36 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/11/2022 06:10:38 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 06:10:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
03/11/2022 06:10:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
03/11/2022 06:10:44 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.6532019704433498 on epoch=724
03/11/2022 06:10:46 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 06:10:48 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 06:10:50 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 06:10:53 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 06:10:55 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 06:10:56 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.6532019704433498 on epoch=749
03/11/2022 06:10:59 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.00 on epoch=754
03/11/2022 06:11:01 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.02 on epoch=759
03/11/2022 06:11:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.02 on epoch=764
03/11/2022 06:11:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
03/11/2022 06:11:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 06:11:08 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.6532019704433498 on epoch=774
03/11/2022 06:11:10 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 06:11:12 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.00 on epoch=784
03/11/2022 06:11:14 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 06:11:17 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 06:11:19 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 06:11:20 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.6532019704433498 on epoch=799
03/11/2022 06:11:23 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.00 on epoch=804
03/11/2022 06:11:25 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.00 on epoch=809
03/11/2022 06:11:27 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 06:11:29 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 06:11:31 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.03 on epoch=824
03/11/2022 06:11:33 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.625 on epoch=824
03/11/2022 06:11:35 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 06:11:37 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 06:11:40 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.00 on epoch=839
03/11/2022 06:11:42 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 06:11:44 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.05 on epoch=849
03/11/2022 06:11:45 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.6532019704433498 on epoch=849
03/11/2022 06:11:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.03 on epoch=854
03/11/2022 06:11:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 06:11:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 06:11:54 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 06:11:56 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 06:11:57 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.6825396825396826 on epoch=874
03/11/2022 06:11:59 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 06:12:02 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 06:12:04 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 06:12:06 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 06:12:08 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 06:12:09 - INFO - __main__ - Global step 1800 Train loss 0.00 Classification-F1 0.6761133603238867 on epoch=899
03/11/2022 06:12:12 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 06:12:14 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 06:12:16 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 06:12:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 06:12:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
03/11/2022 06:12:22 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.6532019704433498 on epoch=924
03/11/2022 06:12:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 06:12:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 06:12:28 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
03/11/2022 06:12:30 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 06:12:33 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 06:12:33 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.5134502923976608 on epoch=949
03/11/2022 06:12:36 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
03/11/2022 06:12:38 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 06:12:40 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 06:12:42 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 06:12:44 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 06:12:46 - INFO - __main__ - Global step 1950 Train loss 0.01 Classification-F1 0.6761133603238867 on epoch=974
03/11/2022 06:12:48 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 06:12:50 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 06:12:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 06:12:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 06:12:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 06:12:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:12:58 - INFO - __main__ - Printing 3 examples
03/11/2022 06:12:58 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 06:12:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:12:58 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 06:12:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:12:58 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 06:12:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:12:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 06:12:58 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:12:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 06:12:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:12:58 - INFO - __main__ - Printing 3 examples
03/11/2022 06:12:58 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 06:12:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:12:58 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 06:12:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:12:58 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 06:12:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:12:58 - INFO - __main__ - Tokenizing Input ...
03/11/2022 06:12:58 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:12:58 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 06:12:58 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.6825396825396826 on epoch=999
03/11/2022 06:12:58 - INFO - __main__ - save last model!
03/11/2022 06:12:58 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 06:12:58 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 06:12:58 - INFO - __main__ - Printing 3 examples
03/11/2022 06:12:58 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 06:12:58 - INFO - __main__ - ['hate']
03/11/2022 06:12:58 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 06:12:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:12:58 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 06:12:58 - INFO - __main__ - ['hate']
03/11/2022 06:12:58 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 06:12:58 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:12:59 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 06:13:12 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 06:13:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 06:13:13 - INFO - __main__ - Starting training!
03/11/2022 06:13:57 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_87_0.5_8_predictions.txt
03/11/2022 06:13:57 - INFO - __main__ - Classification-F1 on test data: 0.5158
03/11/2022 06:13:57 - INFO - __main__ - prefix=tweet_eval-irony_16_87, lr=0.5, bsz=8, dev_performance=0.746031746031746, test_performance=0.5158420403795799
03/11/2022 06:13:57 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_87, lr=0.4, bsz=8 ...
03/11/2022 06:13:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:13:58 - INFO - __main__ - Printing 3 examples
03/11/2022 06:13:58 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 06:13:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:13:58 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 06:13:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:13:58 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 06:13:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:13:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 06:13:58 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:13:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 06:13:58 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:13:58 - INFO - __main__ - Printing 3 examples
03/11/2022 06:13:58 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 06:13:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:13:58 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 06:13:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:13:58 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 06:13:58 - INFO - __main__ - ['non-irony']
03/11/2022 06:13:58 - INFO - __main__ - Tokenizing Input ...
03/11/2022 06:13:58 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:13:58 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 06:14:12 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 06:14:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 06:14:13 - INFO - __main__ - Starting training!
03/11/2022 06:14:16 - INFO - __main__ - Step 10 Global step 10 Train loss 2.48 on epoch=4
03/11/2022 06:14:18 - INFO - __main__ - Step 20 Global step 20 Train loss 0.74 on epoch=9
03/11/2022 06:14:20 - INFO - __main__ - Step 30 Global step 30 Train loss 0.50 on epoch=14
03/11/2022 06:14:22 - INFO - __main__ - Step 40 Global step 40 Train loss 0.40 on epoch=19
03/11/2022 06:14:25 - INFO - __main__ - Step 50 Global step 50 Train loss 0.39 on epoch=24
03/11/2022 06:14:26 - INFO - __main__ - Global step 50 Train loss 0.90 Classification-F1 0.3043478260869565 on epoch=24
03/11/2022 06:14:26 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.3043478260869565 on epoch=24, global_step=50
03/11/2022 06:14:28 - INFO - __main__ - Step 60 Global step 60 Train loss 0.33 on epoch=29
03/11/2022 06:14:30 - INFO - __main__ - Step 70 Global step 70 Train loss 0.29 on epoch=34
03/11/2022 06:14:32 - INFO - __main__ - Step 80 Global step 80 Train loss 0.36 on epoch=39
03/11/2022 06:14:35 - INFO - __main__ - Step 90 Global step 90 Train loss 0.28 on epoch=44
03/11/2022 06:14:37 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=49
03/11/2022 06:14:38 - INFO - __main__ - Global step 100 Train loss 0.32 Classification-F1 0.5607843137254902 on epoch=49
03/11/2022 06:14:38 - INFO - __main__ - Saving model with best Classification-F1: 0.3043478260869565 -> 0.5607843137254902 on epoch=49, global_step=100
03/11/2022 06:14:40 - INFO - __main__ - Step 110 Global step 110 Train loss 0.37 on epoch=54
03/11/2022 06:14:42 - INFO - __main__ - Step 120 Global step 120 Train loss 0.29 on epoch=59
03/11/2022 06:14:44 - INFO - __main__ - Step 130 Global step 130 Train loss 0.27 on epoch=64
03/11/2022 06:14:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.27 on epoch=69
03/11/2022 06:14:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.32 on epoch=74
03/11/2022 06:14:50 - INFO - __main__ - Global step 150 Train loss 0.30 Classification-F1 0.6000000000000001 on epoch=74
03/11/2022 06:14:50 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.6000000000000001 on epoch=74, global_step=150
03/11/2022 06:14:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=79
03/11/2022 06:14:54 - INFO - __main__ - Step 170 Global step 170 Train loss 0.26 on epoch=84
03/11/2022 06:14:56 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
03/11/2022 06:14:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
03/11/2022 06:15:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.25 on epoch=99
03/11/2022 06:15:02 - INFO - __main__ - Global step 200 Train loss 0.27 Classification-F1 0.4980392156862745 on epoch=99
03/11/2022 06:15:04 - INFO - __main__ - Step 210 Global step 210 Train loss 0.25 on epoch=104
03/11/2022 06:15:06 - INFO - __main__ - Step 220 Global step 220 Train loss 0.22 on epoch=109
03/11/2022 06:15:08 - INFO - __main__ - Step 230 Global step 230 Train loss 0.23 on epoch=114
03/11/2022 06:15:10 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
03/11/2022 06:15:13 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=124
03/11/2022 06:15:13 - INFO - __main__ - Global step 250 Train loss 0.24 Classification-F1 0.49090909090909085 on epoch=124
03/11/2022 06:15:16 - INFO - __main__ - Step 260 Global step 260 Train loss 0.26 on epoch=129
03/11/2022 06:15:18 - INFO - __main__ - Step 270 Global step 270 Train loss 0.25 on epoch=134
03/11/2022 06:15:20 - INFO - __main__ - Step 280 Global step 280 Train loss 0.25 on epoch=139
03/11/2022 06:15:22 - INFO - __main__ - Step 290 Global step 290 Train loss 0.27 on epoch=144
03/11/2022 06:15:25 - INFO - __main__ - Step 300 Global step 300 Train loss 0.17 on epoch=149
03/11/2022 06:15:25 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.4920634920634921 on epoch=149
03/11/2022 06:15:28 - INFO - __main__ - Step 310 Global step 310 Train loss 0.20 on epoch=154
03/11/2022 06:15:30 - INFO - __main__ - Step 320 Global step 320 Train loss 0.21 on epoch=159
03/11/2022 06:15:32 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=164
03/11/2022 06:15:34 - INFO - __main__ - Step 340 Global step 340 Train loss 0.25 on epoch=169
03/11/2022 06:15:36 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
03/11/2022 06:15:37 - INFO - __main__ - Global step 350 Train loss 0.22 Classification-F1 0.4909862142099682 on epoch=174
03/11/2022 06:15:39 - INFO - __main__ - Step 360 Global step 360 Train loss 0.24 on epoch=179
03/11/2022 06:15:42 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
03/11/2022 06:15:44 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
03/11/2022 06:15:46 - INFO - __main__ - Step 390 Global step 390 Train loss 0.22 on epoch=194
03/11/2022 06:15:48 - INFO - __main__ - Step 400 Global step 400 Train loss 0.25 on epoch=199
03/11/2022 06:15:49 - INFO - __main__ - Global step 400 Train loss 0.22 Classification-F1 0.4666666666666667 on epoch=199
03/11/2022 06:15:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=204
03/11/2022 06:15:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.22 on epoch=209
03/11/2022 06:15:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
03/11/2022 06:15:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.23 on epoch=219
03/11/2022 06:16:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
03/11/2022 06:16:01 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.39999999999999997 on epoch=224
03/11/2022 06:16:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.22 on epoch=229
03/11/2022 06:16:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.22 on epoch=234
03/11/2022 06:16:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.19 on epoch=239
03/11/2022 06:16:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.21 on epoch=244
03/11/2022 06:16:12 - INFO - __main__ - Step 500 Global step 500 Train loss 0.18 on epoch=249
03/11/2022 06:16:13 - INFO - __main__ - Global step 500 Train loss 0.20 Classification-F1 0.4980392156862745 on epoch=249
03/11/2022 06:16:15 - INFO - __main__ - Step 510 Global step 510 Train loss 0.19 on epoch=254
03/11/2022 06:16:17 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=259
03/11/2022 06:16:20 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
03/11/2022 06:16:22 - INFO - __main__ - Step 540 Global step 540 Train loss 0.20 on epoch=269
03/11/2022 06:16:24 - INFO - __main__ - Step 550 Global step 550 Train loss 0.20 on epoch=274
03/11/2022 06:16:25 - INFO - __main__ - Global step 550 Train loss 0.20 Classification-F1 0.5195195195195195 on epoch=274
03/11/2022 06:16:27 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=279
03/11/2022 06:16:29 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=284
03/11/2022 06:16:31 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=289
03/11/2022 06:16:34 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=294
03/11/2022 06:16:36 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=299
03/11/2022 06:16:37 - INFO - __main__ - Global step 600 Train loss 0.20 Classification-F1 0.4920634920634921 on epoch=299
03/11/2022 06:16:39 - INFO - __main__ - Step 610 Global step 610 Train loss 0.17 on epoch=304
03/11/2022 06:16:41 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
03/11/2022 06:16:43 - INFO - __main__ - Step 630 Global step 630 Train loss 0.17 on epoch=314
03/11/2022 06:16:45 - INFO - __main__ - Step 640 Global step 640 Train loss 0.18 on epoch=319
03/11/2022 06:16:48 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=324
03/11/2022 06:16:48 - INFO - __main__ - Global step 650 Train loss 0.18 Classification-F1 0.4009852216748768 on epoch=324
03/11/2022 06:16:51 - INFO - __main__ - Step 660 Global step 660 Train loss 0.19 on epoch=329
03/11/2022 06:16:53 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=334
03/11/2022 06:16:55 - INFO - __main__ - Step 680 Global step 680 Train loss 0.19 on epoch=339
03/11/2022 06:16:57 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=344
03/11/2022 06:17:00 - INFO - __main__ - Step 700 Global step 700 Train loss 0.16 on epoch=349
03/11/2022 06:17:00 - INFO - __main__ - Global step 700 Train loss 0.18 Classification-F1 0.4682306940371457 on epoch=349
03/11/2022 06:17:03 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=354
03/11/2022 06:17:05 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=359
03/11/2022 06:17:07 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
03/11/2022 06:17:09 - INFO - __main__ - Step 740 Global step 740 Train loss 0.17 on epoch=369
03/11/2022 06:17:11 - INFO - __main__ - Step 750 Global step 750 Train loss 0.13 on epoch=374
03/11/2022 06:17:12 - INFO - __main__ - Global step 750 Train loss 0.15 Classification-F1 0.3764102564102564 on epoch=374
03/11/2022 06:17:14 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=379
03/11/2022 06:17:16 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=384
03/11/2022 06:17:19 - INFO - __main__ - Step 780 Global step 780 Train loss 0.14 on epoch=389
03/11/2022 06:17:21 - INFO - __main__ - Step 790 Global step 790 Train loss 0.12 on epoch=394
03/11/2022 06:17:23 - INFO - __main__ - Step 800 Global step 800 Train loss 0.12 on epoch=399
03/11/2022 06:17:24 - INFO - __main__ - Global step 800 Train loss 0.13 Classification-F1 0.4009852216748768 on epoch=399
03/11/2022 06:17:26 - INFO - __main__ - Step 810 Global step 810 Train loss 0.10 on epoch=404
03/11/2022 06:17:28 - INFO - __main__ - Step 820 Global step 820 Train loss 0.10 on epoch=409
03/11/2022 06:17:30 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=414
03/11/2022 06:17:33 - INFO - __main__ - Step 840 Global step 840 Train loss 0.12 on epoch=419
03/11/2022 06:17:35 - INFO - __main__ - Step 850 Global step 850 Train loss 0.06 on epoch=424
03/11/2022 06:17:36 - INFO - __main__ - Global step 850 Train loss 0.10 Classification-F1 0.3650793650793651 on epoch=424
03/11/2022 06:17:38 - INFO - __main__ - Step 860 Global step 860 Train loss 0.06 on epoch=429
03/11/2022 06:17:40 - INFO - __main__ - Step 870 Global step 870 Train loss 0.08 on epoch=434
03/11/2022 06:17:42 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=439
03/11/2022 06:17:44 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=444
03/11/2022 06:17:47 - INFO - __main__ - Step 900 Global step 900 Train loss 0.04 on epoch=449
03/11/2022 06:17:47 - INFO - __main__ - Global step 900 Train loss 0.07 Classification-F1 0.4817813765182186 on epoch=449
03/11/2022 06:17:50 - INFO - __main__ - Step 910 Global step 910 Train loss 0.05 on epoch=454
03/11/2022 06:17:52 - INFO - __main__ - Step 920 Global step 920 Train loss 0.07 on epoch=459
03/11/2022 06:17:54 - INFO - __main__ - Step 930 Global step 930 Train loss 0.05 on epoch=464
03/11/2022 06:17:56 - INFO - __main__ - Step 940 Global step 940 Train loss 0.04 on epoch=469
03/11/2022 06:17:58 - INFO - __main__ - Step 950 Global step 950 Train loss 0.05 on epoch=474
03/11/2022 06:17:59 - INFO - __main__ - Global step 950 Train loss 0.05 Classification-F1 0.5195195195195195 on epoch=474
03/11/2022 06:18:01 - INFO - __main__ - Step 960 Global step 960 Train loss 0.04 on epoch=479
03/11/2022 06:18:04 - INFO - __main__ - Step 970 Global step 970 Train loss 0.04 on epoch=484
03/11/2022 06:18:06 - INFO - __main__ - Step 980 Global step 980 Train loss 0.02 on epoch=489
03/11/2022 06:18:08 - INFO - __main__ - Step 990 Global step 990 Train loss 0.04 on epoch=494
03/11/2022 06:18:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.04 on epoch=499
03/11/2022 06:18:11 - INFO - __main__ - Global step 1000 Train loss 0.04 Classification-F1 0.29964912280701755 on epoch=499
03/11/2022 06:18:13 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.03 on epoch=504
03/11/2022 06:18:15 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.02 on epoch=509
03/11/2022 06:18:17 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.07 on epoch=514
03/11/2022 06:18:20 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.02 on epoch=519
03/11/2022 06:18:22 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.02 on epoch=524
03/11/2022 06:18:23 - INFO - __main__ - Global step 1050 Train loss 0.03 Classification-F1 0.5151515151515151 on epoch=524
03/11/2022 06:18:25 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.02 on epoch=529
03/11/2022 06:18:27 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.00 on epoch=534
03/11/2022 06:18:29 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.05 on epoch=539
03/11/2022 06:18:31 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.02 on epoch=544
03/11/2022 06:18:34 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.03 on epoch=549
03/11/2022 06:18:34 - INFO - __main__ - Global step 1100 Train loss 0.02 Classification-F1 0.3010752688172043 on epoch=549
03/11/2022 06:18:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.01 on epoch=554
03/11/2022 06:18:39 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.01 on epoch=559
03/11/2022 06:18:41 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.01 on epoch=564
03/11/2022 06:18:43 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.01 on epoch=569
03/11/2022 06:18:45 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.01 on epoch=574
03/11/2022 06:18:46 - INFO - __main__ - Global step 1150 Train loss 0.01 Classification-F1 0.2904761904761905 on epoch=574
03/11/2022 06:18:48 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.02 on epoch=579
03/11/2022 06:18:51 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.00 on epoch=584
03/11/2022 06:18:53 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.01 on epoch=589
03/11/2022 06:18:55 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.00 on epoch=594
03/11/2022 06:18:57 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.03 on epoch=599
03/11/2022 06:18:58 - INFO - __main__ - Global step 1200 Train loss 0.01 Classification-F1 0.2827442827442827 on epoch=599
03/11/2022 06:19:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.00 on epoch=604
03/11/2022 06:19:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.00 on epoch=609
03/11/2022 06:19:05 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.01 on epoch=614
03/11/2022 06:19:07 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.02 on epoch=619
03/11/2022 06:19:09 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.01 on epoch=624
03/11/2022 06:19:11 - INFO - __main__ - Global step 1250 Train loss 0.01 Classification-F1 0.4554554554554554 on epoch=624
03/11/2022 06:19:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.02 on epoch=629
03/11/2022 06:19:16 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.00 on epoch=634
03/11/2022 06:19:18 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.03 on epoch=639
03/11/2022 06:19:20 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.00 on epoch=644
03/11/2022 06:19:22 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.01 on epoch=649
03/11/2022 06:19:24 - INFO - __main__ - Global step 1300 Train loss 0.01 Classification-F1 0.4385964912280702 on epoch=649
03/11/2022 06:19:27 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.00 on epoch=654
03/11/2022 06:19:29 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.00 on epoch=659
03/11/2022 06:19:31 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.00 on epoch=664
03/11/2022 06:19:33 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.00 on epoch=669
03/11/2022 06:19:35 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.01 on epoch=674
03/11/2022 06:19:37 - INFO - __main__ - Global step 1350 Train loss 0.00 Classification-F1 0.36111111111111116 on epoch=674
03/11/2022 06:19:39 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.00 on epoch=679
03/11/2022 06:19:42 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.00 on epoch=684
03/11/2022 06:19:44 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.03 on epoch=689
03/11/2022 06:19:46 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
03/11/2022 06:19:48 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/11/2022 06:19:49 - INFO - __main__ - Global step 1400 Train loss 0.02 Classification-F1 0.30864197530864196 on epoch=699
03/11/2022 06:19:52 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/11/2022 06:19:54 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/11/2022 06:19:56 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.00 on epoch=714
03/11/2022 06:19:58 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.00 on epoch=719
03/11/2022 06:20:00 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
03/11/2022 06:20:01 - INFO - __main__ - Global step 1450 Train loss 0.01 Classification-F1 0.2739583333333333 on epoch=724
03/11/2022 06:20:03 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.00 on epoch=729
03/11/2022 06:20:06 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.00 on epoch=734
03/11/2022 06:20:08 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.00 on epoch=739
03/11/2022 06:20:10 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.00 on epoch=744
03/11/2022 06:20:12 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.00 on epoch=749
03/11/2022 06:20:13 - INFO - __main__ - Global step 1500 Train loss 0.00 Classification-F1 0.282051282051282 on epoch=749
03/11/2022 06:20:15 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/11/2022 06:20:17 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.00 on epoch=759
03/11/2022 06:20:19 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.00 on epoch=764
03/11/2022 06:20:22 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.00 on epoch=769
03/11/2022 06:20:24 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.00 on epoch=774
03/11/2022 06:20:25 - INFO - __main__ - Global step 1550 Train loss 0.00 Classification-F1 0.2554385964912281 on epoch=774
03/11/2022 06:20:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 06:20:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
03/11/2022 06:20:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 06:20:34 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.00 on epoch=794
03/11/2022 06:20:36 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 06:20:38 - INFO - __main__ - Global step 1600 Train loss 0.00 Classification-F1 0.30486486486486486 on epoch=799
03/11/2022 06:20:40 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.02 on epoch=804
03/11/2022 06:20:42 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=809
03/11/2022 06:20:44 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 06:20:46 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.00 on epoch=819
03/11/2022 06:20:48 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 06:20:50 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.43529411764705883 on epoch=824
03/11/2022 06:20:52 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.00 on epoch=829
03/11/2022 06:20:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 06:20:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.01 on epoch=839
03/11/2022 06:20:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 06:21:01 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=849
03/11/2022 06:21:02 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.39139139139139134 on epoch=849
03/11/2022 06:21:04 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 06:21:06 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.03 on epoch=859
03/11/2022 06:21:08 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.00 on epoch=864
03/11/2022 06:21:11 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 06:21:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.00 on epoch=874
03/11/2022 06:21:14 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=874
03/11/2022 06:21:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.00 on epoch=879
03/11/2022 06:21:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.00 on epoch=884
03/11/2022 06:21:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.00 on epoch=889
03/11/2022 06:21:23 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 06:21:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.02 on epoch=899
03/11/2022 06:21:27 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.4375 on epoch=899
03/11/2022 06:21:29 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 06:21:31 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 06:21:33 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.00 on epoch=914
03/11/2022 06:21:35 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.00 on epoch=919
03/11/2022 06:21:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 06:21:39 - INFO - __main__ - Global step 1850 Train loss 0.00 Classification-F1 0.43529411764705883 on epoch=924
03/11/2022 06:21:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 06:21:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 06:21:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.00 on epoch=939
03/11/2022 06:21:47 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 06:21:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 06:21:52 - INFO - __main__ - Global step 1900 Train loss 0.00 Classification-F1 0.4554554554554554 on epoch=949
03/11/2022 06:21:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.00 on epoch=954
03/11/2022 06:21:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 06:21:58 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 06:22:00 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 06:22:03 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 06:22:05 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.4420512820512821 on epoch=974
03/11/2022 06:22:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 06:22:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.00 on epoch=984
03/11/2022 06:22:11 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 06:22:13 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 06:22:15 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 06:22:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:22:17 - INFO - __main__ - Printing 3 examples
03/11/2022 06:22:17 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 06:22:17 - INFO - __main__ - ['non-irony']
03/11/2022 06:22:17 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 06:22:17 - INFO - __main__ - ['non-irony']
03/11/2022 06:22:17 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 06:22:17 - INFO - __main__ - ['non-irony']
03/11/2022 06:22:17 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 06:22:17 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:22:17 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 06:22:17 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:22:17 - INFO - __main__ - Printing 3 examples
03/11/2022 06:22:17 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 06:22:17 - INFO - __main__ - ['non-irony']
03/11/2022 06:22:17 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 06:22:17 - INFO - __main__ - ['non-irony']
03/11/2022 06:22:17 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 06:22:17 - INFO - __main__ - ['non-irony']
03/11/2022 06:22:17 - INFO - __main__ - Tokenizing Input ...
03/11/2022 06:22:17 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:22:17 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 06:22:17 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.4817813765182186 on epoch=999
03/11/2022 06:22:17 - INFO - __main__ - save last model!
03/11/2022 06:22:17 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 06:22:17 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 06:22:17 - INFO - __main__ - Printing 3 examples
03/11/2022 06:22:17 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 06:22:17 - INFO - __main__ - ['hate']
03/11/2022 06:22:17 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 06:22:17 - INFO - __main__ - ['non-irony']
03/11/2022 06:22:17 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 06:22:17 - INFO - __main__ - ['hate']
03/11/2022 06:22:17 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 06:22:18 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:22:18 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 06:22:29 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 06:22:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 06:22:30 - INFO - __main__ - Starting training!
03/11/2022 06:23:04 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_87_0.4_8_predictions.txt
03/11/2022 06:23:04 - INFO - __main__ - Classification-F1 on test data: 0.3445
03/11/2022 06:23:05 - INFO - __main__ - prefix=tweet_eval-irony_16_87, lr=0.4, bsz=8, dev_performance=0.6000000000000001, test_performance=0.34445944141147566
03/11/2022 06:23:05 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_87, lr=0.3, bsz=8 ...
03/11/2022 06:23:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:23:06 - INFO - __main__ - Printing 3 examples
03/11/2022 06:23:06 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 06:23:06 - INFO - __main__ - ['non-irony']
03/11/2022 06:23:06 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 06:23:06 - INFO - __main__ - ['non-irony']
03/11/2022 06:23:06 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 06:23:06 - INFO - __main__ - ['non-irony']
03/11/2022 06:23:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 06:23:06 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:23:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 06:23:06 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:23:06 - INFO - __main__ - Printing 3 examples
03/11/2022 06:23:06 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 06:23:06 - INFO - __main__ - ['non-irony']
03/11/2022 06:23:06 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 06:23:06 - INFO - __main__ - ['non-irony']
03/11/2022 06:23:06 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 06:23:06 - INFO - __main__ - ['non-irony']
03/11/2022 06:23:06 - INFO - __main__ - Tokenizing Input ...
03/11/2022 06:23:06 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:23:06 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 06:23:18 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 06:23:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 06:23:19 - INFO - __main__ - Starting training!
03/11/2022 06:23:24 - INFO - __main__ - Step 10 Global step 10 Train loss 2.81 on epoch=4
03/11/2022 06:23:26 - INFO - __main__ - Step 20 Global step 20 Train loss 1.06 on epoch=9
03/11/2022 06:23:28 - INFO - __main__ - Step 30 Global step 30 Train loss 0.65 on epoch=14
03/11/2022 06:23:30 - INFO - __main__ - Step 40 Global step 40 Train loss 0.49 on epoch=19
03/11/2022 06:23:33 - INFO - __main__ - Step 50 Global step 50 Train loss 0.46 on epoch=24
03/11/2022 06:23:33 - INFO - __main__ - Global step 50 Train loss 1.09 Classification-F1 0.39999999999999997 on epoch=24
03/11/2022 06:23:33 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.39999999999999997 on epoch=24, global_step=50
03/11/2022 06:23:35 - INFO - __main__ - Step 60 Global step 60 Train loss 0.37 on epoch=29
03/11/2022 06:23:38 - INFO - __main__ - Step 70 Global step 70 Train loss 0.36 on epoch=34
03/11/2022 06:23:40 - INFO - __main__ - Step 80 Global step 80 Train loss 0.31 on epoch=39
03/11/2022 06:23:42 - INFO - __main__ - Step 90 Global step 90 Train loss 0.34 on epoch=44
03/11/2022 06:23:44 - INFO - __main__ - Step 100 Global step 100 Train loss 0.34 on epoch=49
03/11/2022 06:23:45 - INFO - __main__ - Global step 100 Train loss 0.34 Classification-F1 0.4554554554554554 on epoch=49
03/11/2022 06:23:45 - INFO - __main__ - Saving model with best Classification-F1: 0.39999999999999997 -> 0.4554554554554554 on epoch=49, global_step=100
03/11/2022 06:23:47 - INFO - __main__ - Step 110 Global step 110 Train loss 0.34 on epoch=54
03/11/2022 06:23:49 - INFO - __main__ - Step 120 Global step 120 Train loss 0.31 on epoch=59
03/11/2022 06:23:51 - INFO - __main__ - Step 130 Global step 130 Train loss 0.28 on epoch=64
03/11/2022 06:23:54 - INFO - __main__ - Step 140 Global step 140 Train loss 0.28 on epoch=69
03/11/2022 06:23:56 - INFO - __main__ - Step 150 Global step 150 Train loss 0.27 on epoch=74
03/11/2022 06:23:56 - INFO - __main__ - Global step 150 Train loss 0.30 Classification-F1 0.3333333333333333 on epoch=74
03/11/2022 06:23:59 - INFO - __main__ - Step 160 Global step 160 Train loss 0.29 on epoch=79
03/11/2022 06:24:01 - INFO - __main__ - Step 170 Global step 170 Train loss 0.29 on epoch=84
03/11/2022 06:24:03 - INFO - __main__ - Step 180 Global step 180 Train loss 0.25 on epoch=89
03/11/2022 06:24:05 - INFO - __main__ - Step 190 Global step 190 Train loss 0.28 on epoch=94
03/11/2022 06:24:07 - INFO - __main__ - Step 200 Global step 200 Train loss 0.26 on epoch=99
03/11/2022 06:24:08 - INFO - __main__ - Global step 200 Train loss 0.27 Classification-F1 0.28888888888888886 on epoch=99
03/11/2022 06:24:10 - INFO - __main__ - Step 210 Global step 210 Train loss 0.22 on epoch=104
03/11/2022 06:24:12 - INFO - __main__ - Step 220 Global step 220 Train loss 0.27 on epoch=109
03/11/2022 06:24:15 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=114
03/11/2022 06:24:17 - INFO - __main__ - Step 240 Global step 240 Train loss 0.24 on epoch=119
03/11/2022 06:24:19 - INFO - __main__ - Step 250 Global step 250 Train loss 0.24 on epoch=124
03/11/2022 06:24:20 - INFO - __main__ - Global step 250 Train loss 0.25 Classification-F1 0.4666666666666667 on epoch=124
03/11/2022 06:24:20 - INFO - __main__ - Saving model with best Classification-F1: 0.4554554554554554 -> 0.4666666666666667 on epoch=124, global_step=250
03/11/2022 06:24:22 - INFO - __main__ - Step 260 Global step 260 Train loss 0.23 on epoch=129
03/11/2022 06:24:24 - INFO - __main__ - Step 270 Global step 270 Train loss 0.27 on epoch=134
03/11/2022 06:24:26 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
03/11/2022 06:24:28 - INFO - __main__ - Step 290 Global step 290 Train loss 0.23 on epoch=144
03/11/2022 06:24:31 - INFO - __main__ - Step 300 Global step 300 Train loss 0.24 on epoch=149
03/11/2022 06:24:31 - INFO - __main__ - Global step 300 Train loss 0.24 Classification-F1 0.4920634920634921 on epoch=149
03/11/2022 06:24:31 - INFO - __main__ - Saving model with best Classification-F1: 0.4666666666666667 -> 0.4920634920634921 on epoch=149, global_step=300
03/11/2022 06:24:33 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 06:24:36 - INFO - __main__ - Step 320 Global step 320 Train loss 0.19 on epoch=159
03/11/2022 06:24:38 - INFO - __main__ - Step 330 Global step 330 Train loss 0.25 on epoch=164
03/11/2022 06:24:40 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
03/11/2022 06:24:42 - INFO - __main__ - Step 350 Global step 350 Train loss 0.25 on epoch=174
03/11/2022 06:24:43 - INFO - __main__ - Global step 350 Train loss 0.24 Classification-F1 0.4909862142099682 on epoch=174
03/11/2022 06:24:45 - INFO - __main__ - Step 360 Global step 360 Train loss 0.23 on epoch=179
03/11/2022 06:24:47 - INFO - __main__ - Step 370 Global step 370 Train loss 0.22 on epoch=184
03/11/2022 06:24:49 - INFO - __main__ - Step 380 Global step 380 Train loss 0.25 on epoch=189
03/11/2022 06:24:52 - INFO - __main__ - Step 390 Global step 390 Train loss 0.26 on epoch=194
03/11/2022 06:24:54 - INFO - __main__ - Step 400 Global step 400 Train loss 0.21 on epoch=199
03/11/2022 06:24:55 - INFO - __main__ - Global step 400 Train loss 0.24 Classification-F1 0.4181818181818182 on epoch=199
03/11/2022 06:24:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=204
03/11/2022 06:24:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.27 on epoch=209
03/11/2022 06:25:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.25 on epoch=214
03/11/2022 06:25:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.21 on epoch=219
03/11/2022 06:25:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
03/11/2022 06:25:06 - INFO - __main__ - Global step 450 Train loss 0.24 Classification-F1 0.4920634920634921 on epoch=224
03/11/2022 06:25:08 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
03/11/2022 06:25:10 - INFO - __main__ - Step 470 Global step 470 Train loss 0.24 on epoch=234
03/11/2022 06:25:13 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=239
03/11/2022 06:25:15 - INFO - __main__ - Step 490 Global step 490 Train loss 0.22 on epoch=244
03/11/2022 06:25:17 - INFO - __main__ - Step 500 Global step 500 Train loss 0.25 on epoch=249
03/11/2022 06:25:18 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.4375 on epoch=249
03/11/2022 06:25:20 - INFO - __main__ - Step 510 Global step 510 Train loss 0.21 on epoch=254
03/11/2022 06:25:22 - INFO - __main__ - Step 520 Global step 520 Train loss 0.24 on epoch=259
03/11/2022 06:25:24 - INFO - __main__ - Step 530 Global step 530 Train loss 0.21 on epoch=264
03/11/2022 06:25:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.18 on epoch=269
03/11/2022 06:25:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.24 on epoch=274
03/11/2022 06:25:29 - INFO - __main__ - Global step 550 Train loss 0.22 Classification-F1 0.4980392156862745 on epoch=274
03/11/2022 06:25:29 - INFO - __main__ - Saving model with best Classification-F1: 0.4920634920634921 -> 0.4980392156862745 on epoch=274, global_step=550
03/11/2022 06:25:31 - INFO - __main__ - Step 560 Global step 560 Train loss 0.24 on epoch=279
03/11/2022 06:25:33 - INFO - __main__ - Step 570 Global step 570 Train loss 0.19 on epoch=284
03/11/2022 06:25:36 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
03/11/2022 06:25:38 - INFO - __main__ - Step 590 Global step 590 Train loss 0.21 on epoch=294
03/11/2022 06:25:40 - INFO - __main__ - Step 600 Global step 600 Train loss 0.19 on epoch=299
03/11/2022 06:25:41 - INFO - __main__ - Global step 600 Train loss 0.21 Classification-F1 0.4666666666666667 on epoch=299
03/11/2022 06:25:43 - INFO - __main__ - Step 610 Global step 610 Train loss 0.21 on epoch=304
03/11/2022 06:25:45 - INFO - __main__ - Step 620 Global step 620 Train loss 0.21 on epoch=309
03/11/2022 06:25:47 - INFO - __main__ - Step 630 Global step 630 Train loss 0.21 on epoch=314
03/11/2022 06:25:49 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=319
03/11/2022 06:25:51 - INFO - __main__ - Step 650 Global step 650 Train loss 0.19 on epoch=324
03/11/2022 06:25:52 - INFO - __main__ - Global step 650 Train loss 0.21 Classification-F1 0.625 on epoch=324
03/11/2022 06:25:52 - INFO - __main__ - Saving model with best Classification-F1: 0.4980392156862745 -> 0.625 on epoch=324, global_step=650
03/11/2022 06:25:54 - INFO - __main__ - Step 660 Global step 660 Train loss 0.18 on epoch=329
03/11/2022 06:25:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.19 on epoch=334
03/11/2022 06:25:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.16 on epoch=339
03/11/2022 06:26:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.19 on epoch=344
03/11/2022 06:26:03 - INFO - __main__ - Step 700 Global step 700 Train loss 0.17 on epoch=349
03/11/2022 06:26:04 - INFO - __main__ - Global step 700 Train loss 0.18 Classification-F1 0.3333333333333333 on epoch=349
03/11/2022 06:26:06 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
03/11/2022 06:26:08 - INFO - __main__ - Step 720 Global step 720 Train loss 0.22 on epoch=359
03/11/2022 06:26:10 - INFO - __main__ - Step 730 Global step 730 Train loss 0.18 on epoch=364
03/11/2022 06:26:12 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=369
03/11/2022 06:26:15 - INFO - __main__ - Step 750 Global step 750 Train loss 0.20 on epoch=374
03/11/2022 06:26:15 - INFO - __main__ - Global step 750 Train loss 0.20 Classification-F1 0.4920634920634921 on epoch=374
03/11/2022 06:26:18 - INFO - __main__ - Step 760 Global step 760 Train loss 0.17 on epoch=379
03/11/2022 06:26:20 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
03/11/2022 06:26:22 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=389
03/11/2022 06:26:24 - INFO - __main__ - Step 790 Global step 790 Train loss 0.20 on epoch=394
03/11/2022 06:26:26 - INFO - __main__ - Step 800 Global step 800 Train loss 0.18 on epoch=399
03/11/2022 06:26:27 - INFO - __main__ - Global step 800 Train loss 0.19 Classification-F1 0.5195195195195195 on epoch=399
03/11/2022 06:26:29 - INFO - __main__ - Step 810 Global step 810 Train loss 0.20 on epoch=404
03/11/2022 06:26:31 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=409
03/11/2022 06:26:33 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=414
03/11/2022 06:26:35 - INFO - __main__ - Step 840 Global step 840 Train loss 0.20 on epoch=419
03/11/2022 06:26:38 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=424
03/11/2022 06:26:38 - INFO - __main__ - Global step 850 Train loss 0.19 Classification-F1 0.464039408866995 on epoch=424
03/11/2022 06:26:40 - INFO - __main__ - Step 860 Global step 860 Train loss 0.17 on epoch=429
03/11/2022 06:26:43 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=434
03/11/2022 06:26:45 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=439
03/11/2022 06:26:47 - INFO - __main__ - Step 890 Global step 890 Train loss 0.17 on epoch=444
03/11/2022 06:26:49 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=449
03/11/2022 06:26:50 - INFO - __main__ - Global step 900 Train loss 0.18 Classification-F1 0.4682306940371457 on epoch=449
03/11/2022 06:26:52 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=454
03/11/2022 06:26:54 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=459
03/11/2022 06:26:56 - INFO - __main__ - Step 930 Global step 930 Train loss 0.17 on epoch=464
03/11/2022 06:26:58 - INFO - __main__ - Step 940 Global step 940 Train loss 0.17 on epoch=469
03/11/2022 06:27:01 - INFO - __main__ - Step 950 Global step 950 Train loss 0.14 on epoch=474
03/11/2022 06:27:01 - INFO - __main__ - Global step 950 Train loss 0.16 Classification-F1 0.464039408866995 on epoch=474
03/11/2022 06:27:04 - INFO - __main__ - Step 960 Global step 960 Train loss 0.15 on epoch=479
03/11/2022 06:27:06 - INFO - __main__ - Step 970 Global step 970 Train loss 0.15 on epoch=484
03/11/2022 06:27:08 - INFO - __main__ - Step 980 Global step 980 Train loss 0.14 on epoch=489
03/11/2022 06:27:10 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=494
03/11/2022 06:27:12 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
03/11/2022 06:27:13 - INFO - __main__ - Global step 1000 Train loss 0.16 Classification-F1 0.5333333333333333 on epoch=499
03/11/2022 06:27:15 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.17 on epoch=504
03/11/2022 06:27:17 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.17 on epoch=509
03/11/2022 06:27:19 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=514
03/11/2022 06:27:22 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.11 on epoch=519
03/11/2022 06:27:24 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.16 on epoch=524
03/11/2022 06:27:24 - INFO - __main__ - Global step 1050 Train loss 0.15 Classification-F1 0.4009852216748768 on epoch=524
03/11/2022 06:27:27 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.11 on epoch=529
03/11/2022 06:27:29 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.12 on epoch=534
03/11/2022 06:27:31 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.13 on epoch=539
03/11/2022 06:27:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.12 on epoch=544
03/11/2022 06:27:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.12 on epoch=549
03/11/2022 06:27:36 - INFO - __main__ - Global step 1100 Train loss 0.12 Classification-F1 0.5270935960591133 on epoch=549
03/11/2022 06:27:38 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.12 on epoch=554
03/11/2022 06:27:40 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.11 on epoch=559
03/11/2022 06:27:43 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.10 on epoch=564
03/11/2022 06:27:45 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.11 on epoch=569
03/11/2022 06:27:47 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.08 on epoch=574
03/11/2022 06:27:48 - INFO - __main__ - Global step 1150 Train loss 0.10 Classification-F1 0.5076923076923077 on epoch=574
03/11/2022 06:27:50 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.11 on epoch=579
03/11/2022 06:27:52 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.12 on epoch=584
03/11/2022 06:27:54 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
03/11/2022 06:27:56 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=594
03/11/2022 06:27:58 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.10 on epoch=599
03/11/2022 06:27:59 - INFO - __main__ - Global step 1200 Train loss 0.10 Classification-F1 0.4554554554554554 on epoch=599
03/11/2022 06:28:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=604
03/11/2022 06:28:03 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.06 on epoch=609
03/11/2022 06:28:06 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.06 on epoch=614
03/11/2022 06:28:08 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.05 on epoch=619
03/11/2022 06:28:10 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
03/11/2022 06:28:11 - INFO - __main__ - Global step 1250 Train loss 0.07 Classification-F1 0.5607843137254902 on epoch=624
03/11/2022 06:28:13 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.04 on epoch=629
03/11/2022 06:28:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.04 on epoch=634
03/11/2022 06:28:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.04 on epoch=639
03/11/2022 06:28:19 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.07 on epoch=644
03/11/2022 06:28:21 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.04 on epoch=649
03/11/2022 06:28:22 - INFO - __main__ - Global step 1300 Train loss 0.05 Classification-F1 0.4181818181818182 on epoch=649
03/11/2022 06:28:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.04 on epoch=654
03/11/2022 06:28:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.07 on epoch=659
03/11/2022 06:28:29 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.04 on epoch=664
03/11/2022 06:28:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.03 on epoch=669
03/11/2022 06:28:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=674
03/11/2022 06:28:34 - INFO - __main__ - Global step 1350 Train loss 0.05 Classification-F1 0.4817813765182186 on epoch=674
03/11/2022 06:28:36 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
03/11/2022 06:28:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.02 on epoch=684
03/11/2022 06:28:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.01 on epoch=689
03/11/2022 06:28:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.04 on epoch=694
03/11/2022 06:28:45 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.01 on epoch=699
03/11/2022 06:28:45 - INFO - __main__ - Global step 1400 Train loss 0.03 Classification-F1 0.4682306940371457 on epoch=699
03/11/2022 06:28:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.02 on epoch=704
03/11/2022 06:28:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.01 on epoch=709
03/11/2022 06:28:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
03/11/2022 06:28:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.02 on epoch=719
03/11/2022 06:28:56 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.02 on epoch=724
03/11/2022 06:28:57 - INFO - __main__ - Global step 1450 Train loss 0.02 Classification-F1 0.4817813765182186 on epoch=724
03/11/2022 06:28:59 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.02 on epoch=729
03/11/2022 06:29:01 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.01 on epoch=734
03/11/2022 06:29:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.02 on epoch=739
03/11/2022 06:29:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.03 on epoch=744
03/11/2022 06:29:08 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.02 on epoch=749
03/11/2022 06:29:09 - INFO - __main__ - Global step 1500 Train loss 0.02 Classification-F1 0.5134502923976608 on epoch=749
03/11/2022 06:29:11 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.01 on epoch=754
03/11/2022 06:29:13 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.03 on epoch=759
03/11/2022 06:29:15 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.04 on epoch=764
03/11/2022 06:29:17 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.01 on epoch=769
03/11/2022 06:29:19 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.02 on epoch=774
03/11/2022 06:29:20 - INFO - __main__ - Global step 1550 Train loss 0.02 Classification-F1 0.5270935960591133 on epoch=774
03/11/2022 06:29:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.00 on epoch=779
03/11/2022 06:29:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.01 on epoch=784
03/11/2022 06:29:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.00 on epoch=789
03/11/2022 06:29:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.02 on epoch=794
03/11/2022 06:29:31 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.00 on epoch=799
03/11/2022 06:29:32 - INFO - __main__ - Global step 1600 Train loss 0.01 Classification-F1 0.4682306940371457 on epoch=799
03/11/2022 06:29:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.01 on epoch=804
03/11/2022 06:29:37 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.02 on epoch=809
03/11/2022 06:29:39 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.00 on epoch=814
03/11/2022 06:29:41 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.01 on epoch=819
03/11/2022 06:29:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.00 on epoch=824
03/11/2022 06:29:45 - INFO - __main__ - Global step 1650 Train loss 0.01 Classification-F1 0.4666666666666667 on epoch=824
03/11/2022 06:29:47 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.01 on epoch=829
03/11/2022 06:29:50 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.00 on epoch=834
03/11/2022 06:29:52 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.02 on epoch=839
03/11/2022 06:29:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.00 on epoch=844
03/11/2022 06:29:56 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.00 on epoch=849
03/11/2022 06:29:57 - INFO - __main__ - Global step 1700 Train loss 0.01 Classification-F1 0.5901477832512315 on epoch=849
03/11/2022 06:29:59 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.00 on epoch=854
03/11/2022 06:30:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.00 on epoch=859
03/11/2022 06:30:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.03 on epoch=864
03/11/2022 06:30:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.00 on epoch=869
03/11/2022 06:30:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.01 on epoch=874
03/11/2022 06:30:08 - INFO - __main__ - Global step 1750 Train loss 0.01 Classification-F1 0.5195195195195195 on epoch=874
03/11/2022 06:30:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.01 on epoch=879
03/11/2022 06:30:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.02 on epoch=884
03/11/2022 06:30:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.03 on epoch=889
03/11/2022 06:30:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.00 on epoch=894
03/11/2022 06:30:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.00 on epoch=899
03/11/2022 06:30:20 - INFO - __main__ - Global step 1800 Train loss 0.01 Classification-F1 0.4817813765182186 on epoch=899
03/11/2022 06:30:22 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.00 on epoch=904
03/11/2022 06:30:24 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.00 on epoch=909
03/11/2022 06:30:26 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.02 on epoch=914
03/11/2022 06:30:28 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.01 on epoch=919
03/11/2022 06:30:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.00 on epoch=924
03/11/2022 06:30:33 - INFO - __main__ - Global step 1850 Train loss 0.01 Classification-F1 0.31705426356589145 on epoch=924
03/11/2022 06:30:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.00 on epoch=929
03/11/2022 06:30:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.00 on epoch=934
03/11/2022 06:30:39 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.02 on epoch=939
03/11/2022 06:30:41 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.00 on epoch=944
03/11/2022 06:30:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.00 on epoch=949
03/11/2022 06:30:44 - INFO - __main__ - Global step 1900 Train loss 0.01 Classification-F1 0.4666666666666667 on epoch=949
03/11/2022 06:30:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.01 on epoch=954
03/11/2022 06:30:49 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.00 on epoch=959
03/11/2022 06:30:51 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.00 on epoch=964
03/11/2022 06:30:53 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.00 on epoch=969
03/11/2022 06:30:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.00 on epoch=974
03/11/2022 06:30:56 - INFO - __main__ - Global step 1950 Train loss 0.00 Classification-F1 0.625 on epoch=974
03/11/2022 06:30:59 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.00 on epoch=979
03/11/2022 06:31:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/11/2022 06:31:03 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.00 on epoch=989
03/11/2022 06:31:05 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.00 on epoch=994
03/11/2022 06:31:07 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.00 on epoch=999
03/11/2022 06:31:08 - INFO - __main__ - Global step 2000 Train loss 0.00 Classification-F1 0.5625 on epoch=999
03/11/2022 06:31:08 - INFO - __main__ - save last model!
03/11/2022 06:31:08 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 06:31:08 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 06:31:08 - INFO - __main__ - Printing 3 examples
03/11/2022 06:31:08 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 06:31:08 - INFO - __main__ - ['hate']
03/11/2022 06:31:08 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 06:31:08 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:08 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 06:31:08 - INFO - __main__ - ['hate']
03/11/2022 06:31:08 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 06:31:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:31:08 - INFO - __main__ - Printing 3 examples
03/11/2022 06:31:08 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 06:31:08 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:08 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 06:31:08 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:08 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 06:31:08 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/11/2022 06:31:08 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:31:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 06:31:08 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:31:08 - INFO - __main__ - Printing 3 examples
03/11/2022 06:31:08 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 06:31:08 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:08 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 06:31:08 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:08 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 06:31:08 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:08 - INFO - __main__ - Tokenizing Input ...
03/11/2022 06:31:08 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:31:08 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:31:08 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 06:31:09 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 06:31:23 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 06:31:24 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 06:31:24 - INFO - __main__ - Starting training!
03/11/2022 06:31:40 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_87_0.3_8_predictions.txt
03/11/2022 06:31:40 - INFO - __main__ - Classification-F1 on test data: 0.5442
03/11/2022 06:31:41 - INFO - __main__ - prefix=tweet_eval-irony_16_87, lr=0.3, bsz=8, dev_performance=0.625, test_performance=0.544164749179513
03/11/2022 06:31:41 - INFO - __main__ - Running ... prefix=tweet_eval-irony_16_87, lr=0.2, bsz=8 ...
03/11/2022 06:31:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:31:42 - INFO - __main__ - Printing 3 examples
03/11/2022 06:31:42 - INFO - __main__ -  [tweet_eval-irony] is the name of the game.
03/11/2022 06:31:42 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:42 - INFO - __main__ -  [tweet_eval-irony] #All #Cute #Dude #Girlygirl #I039m #Into #Is |Please RT:
03/11/2022 06:31:42 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:42 - INFO - __main__ -  [tweet_eval-irony] Let's go CAVS!!! #cleveland #cavs #cavaliers #nba @ Quicken Loans Arena
03/11/2022 06:31:42 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 06:31:42 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:31:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/11/2022 06:31:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/11/2022 06:31:42 - INFO - __main__ - Printing 3 examples
03/11/2022 06:31:42 - INFO - __main__ -  [tweet_eval-irony] #nuffsaid  #stupidity #hadenough lols 😂😃
03/11/2022 06:31:42 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:42 - INFO - __main__ -  [tweet_eval-irony] People are loving this Funk. Let's keep the Tweet going on.
03/11/2022 06:31:42 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:42 - INFO - __main__ -  [tweet_eval-irony] The funky taste of chocolate with the Chunky Monkey all just in a cup .|| George of the Jungle,Cup and a Lick
03/11/2022 06:31:42 - INFO - __main__ - ['non-irony']
03/11/2022 06:31:42 - INFO - __main__ - Tokenizing Input ...
03/11/2022 06:31:42 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:31:42 - INFO - __main__ - Loaded 32 examples from dev data
03/11/2022 06:31:56 - INFO - __main__ - load prompt embedding from ckpt
03/11/2022 06:31:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/11/2022 06:31:57 - INFO - __main__ - Starting training!
03/11/2022 06:31:59 - INFO - __main__ - Step 10 Global step 10 Train loss 3.10 on epoch=4
03/11/2022 06:32:02 - INFO - __main__ - Step 20 Global step 20 Train loss 1.56 on epoch=9
03/11/2022 06:32:04 - INFO - __main__ - Step 30 Global step 30 Train loss 0.82 on epoch=14
03/11/2022 06:32:06 - INFO - __main__ - Step 40 Global step 40 Train loss 0.70 on epoch=19
03/11/2022 06:32:09 - INFO - __main__ - Step 50 Global step 50 Train loss 0.52 on epoch=24
03/11/2022 06:32:09 - INFO - __main__ - Global step 50 Train loss 1.34 Classification-F1 0.41700404858299595 on epoch=24
03/11/2022 06:32:09 - INFO - __main__ - Saving model with best Classification-F1: -1.0 -> 0.41700404858299595 on epoch=24, global_step=50
03/11/2022 06:32:12 - INFO - __main__ - Step 60 Global step 60 Train loss 0.49 on epoch=29
03/11/2022 06:32:14 - INFO - __main__ - Step 70 Global step 70 Train loss 0.42 on epoch=34
03/11/2022 06:32:16 - INFO - __main__ - Step 80 Global step 80 Train loss 0.37 on epoch=39
03/11/2022 06:32:18 - INFO - __main__ - Step 90 Global step 90 Train loss 0.33 on epoch=44
03/11/2022 06:32:21 - INFO - __main__ - Step 100 Global step 100 Train loss 0.33 on epoch=49
03/11/2022 06:32:22 - INFO - __main__ - Global step 100 Train loss 0.39 Classification-F1 0.3992490613266583 on epoch=49
03/11/2022 06:32:24 - INFO - __main__ - Step 110 Global step 110 Train loss 0.31 on epoch=54
03/11/2022 06:32:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.36 on epoch=59
03/11/2022 06:32:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.32 on epoch=64
03/11/2022 06:32:31 - INFO - __main__ - Step 140 Global step 140 Train loss 0.29 on epoch=69
03/11/2022 06:32:33 - INFO - __main__ - Step 150 Global step 150 Train loss 0.31 on epoch=74
03/11/2022 06:32:34 - INFO - __main__ - Global step 150 Train loss 0.32 Classification-F1 0.4231177094379639 on epoch=74
03/11/2022 06:32:34 - INFO - __main__ - Saving model with best Classification-F1: 0.41700404858299595 -> 0.4231177094379639 on epoch=74, global_step=150
03/11/2022 06:32:36 - INFO - __main__ - Step 160 Global step 160 Train loss 0.30 on epoch=79
03/11/2022 06:32:38 - INFO - __main__ - Step 170 Global step 170 Train loss 0.33 on epoch=84
03/11/2022 06:32:40 - INFO - __main__ - Step 180 Global step 180 Train loss 0.31 on epoch=89
03/11/2022 06:32:43 - INFO - __main__ - Step 190 Global step 190 Train loss 0.29 on epoch=94
03/11/2022 06:32:45 - INFO - __main__ - Step 200 Global step 200 Train loss 0.30 on epoch=99
03/11/2022 06:32:46 - INFO - __main__ - Global step 200 Train loss 0.31 Classification-F1 0.4666666666666667 on epoch=99
03/11/2022 06:32:46 - INFO - __main__ - Saving model with best Classification-F1: 0.4231177094379639 -> 0.4666666666666667 on epoch=99, global_step=200
03/11/2022 06:32:48 - INFO - __main__ - Step 210 Global step 210 Train loss 0.31 on epoch=104
03/11/2022 06:32:50 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
03/11/2022 06:32:53 - INFO - __main__ - Step 230 Global step 230 Train loss 0.29 on epoch=114
03/11/2022 06:32:55 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=119
03/11/2022 06:32:57 - INFO - __main__ - Step 250 Global step 250 Train loss 0.26 on epoch=124
03/11/2022 06:32:58 - INFO - __main__ - Global step 250 Train loss 0.29 Classification-F1 0.3333333333333333 on epoch=124
03/11/2022 06:33:00 - INFO - __main__ - Step 260 Global step 260 Train loss 0.21 on epoch=129
03/11/2022 06:33:03 - INFO - __main__ - Step 270 Global step 270 Train loss 0.23 on epoch=134
03/11/2022 06:33:05 - INFO - __main__ - Step 280 Global step 280 Train loss 0.24 on epoch=139
03/11/2022 06:33:07 - INFO - __main__ - Step 290 Global step 290 Train loss 0.34 on epoch=144
03/11/2022 06:33:09 - INFO - __main__ - Step 300 Global step 300 Train loss 0.27 on epoch=149
03/11/2022 06:33:10 - INFO - __main__ - Global step 300 Train loss 0.26 Classification-F1 0.3333333333333333 on epoch=149
03/11/2022 06:33:12 - INFO - __main__ - Step 310 Global step 310 Train loss 0.24 on epoch=154
03/11/2022 06:33:15 - INFO - __main__ - Step 320 Global step 320 Train loss 0.27 on epoch=159
03/11/2022 06:33:17 - INFO - __main__ - Step 330 Global step 330 Train loss 0.32 on epoch=164
03/11/2022 06:33:19 - INFO - __main__ - Step 340 Global step 340 Train loss 0.27 on epoch=169
03/11/2022 06:33:21 - INFO - __main__ - Step 350 Global step 350 Train loss 0.23 on epoch=174
03/11/2022 06:33:22 - INFO - __main__ - Global step 350 Train loss 0.27 Classification-F1 0.3333333333333333 on epoch=174
03/11/2022 06:33:24 - INFO - __main__ - Step 360 Global step 360 Train loss 0.26 on epoch=179
03/11/2022 06:33:27 - INFO - __main__ - Step 370 Global step 370 Train loss 0.27 on epoch=184
03/11/2022 06:33:29 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
03/11/2022 06:33:31 - INFO - __main__ - Step 390 Global step 390 Train loss 0.28 on epoch=194
03/11/2022 06:33:34 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
03/11/2022 06:33:34 - INFO - __main__ - Global step 400 Train loss 0.27 Classification-F1 0.39756367663344405 on epoch=199
03/11/2022 06:33:37 - INFO - __main__ - Step 410 Global step 410 Train loss 0.23 on epoch=204
03/11/2022 06:33:39 - INFO - __main__ - Step 420 Global step 420 Train loss 0.23 on epoch=209
03/11/2022 06:33:41 - INFO - __main__ - Step 430 Global step 430 Train loss 0.23 on epoch=214
03/11/2022 06:33:43 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
03/11/2022 06:33:46 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
03/11/2022 06:33:46 - INFO - __main__ - Global step 450 Train loss 0.23 Classification-F1 0.3454545454545454 on epoch=224
03/11/2022 06:33:49 - INFO - __main__ - Step 460 Global step 460 Train loss 0.23 on epoch=229
03/11/2022 06:33:51 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
03/11/2022 06:33:53 - INFO - __main__ - Step 480 Global step 480 Train loss 0.23 on epoch=239
03/11/2022 06:33:55 - INFO - __main__ - Step 490 Global step 490 Train loss 0.24 on epoch=244
03/11/2022 06:33:58 - INFO - __main__ - Step 500 Global step 500 Train loss 0.23 on epoch=249
03/11/2022 06:33:59 - INFO - __main__ - Global step 500 Train loss 0.23 Classification-F1 0.4385964912280702 on epoch=249
03/11/2022 06:34:01 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=254
03/11/2022 06:34:03 - INFO - __main__ - Step 520 Global step 520 Train loss 0.23 on epoch=259
03/11/2022 06:34:05 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=264
03/11/2022 06:34:08 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=269
03/11/2022 06:34:10 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
03/11/2022 06:34:11 - INFO - __main__ - Global step 550 Train loss 0.23 Classification-F1 0.4420512820512821 on epoch=274
03/11/2022 06:34:13 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=279
03/11/2022 06:34:15 - INFO - __main__ - Step 570 Global step 570 Train loss 0.23 on epoch=284
03/11/2022 06:34:17 - INFO - __main__ - Step 580 Global step 580 Train loss 0.25 on epoch=289
03/11/2022 06:34:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.24 on epoch=294
03/11/2022 06:34:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=299
03/11/2022 06:34:23 - INFO - __main__ - Global step 600 Train loss 0.22 Classification-F1 0.3764102564102564 on epoch=299
03/11/2022 06:34:25 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=304
03/11/2022 06:34:27 - INFO - __main__ - Step 620 Global step 620 Train loss 0.24 on epoch=309
03/11/2022 06:34:30 - INFO - __main__ - Step 630 Global step 630 Train loss 0.22 on epoch=314
03/11/2022 06:34:32 - INFO - __main__ - Step 640 Global step 640 Train loss 0.23 on epoch=319
03/11/2022 06:34:34 - INFO - __main__ - Step 650 Global step 650 Train loss 0.20 on epoch=324
03/11/2022 06:34:35 - INFO - __main__ - Global step 650 Train loss 0.23 Classification-F1 0.464039408866995 on epoch=324
03/11/2022 06:34:37 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=329
03/11/2022 06:34:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.23 on epoch=334
03/11/2022 06:34:42 - INFO - __main__ - Step 680 Global step 680 Train loss 0.22 on epoch=339
03/11/2022 06:34:44 - INFO - __main__ - Step 690 Global step 690 Train loss 0.22 on epoch=344
03/11/2022 06:34:46 - INFO - __main__ - Step 700 Global step 700 Train loss 0.22 on epoch=349
03/11/2022 06:34:47 - INFO - __main__ - Global step 700 Train loss 0.22 Classification-F1 0.4554554554554554 on epoch=349
03/11/2022 06:34:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.20 on epoch=354
03/11/2022 06:34:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.23 on epoch=359
03/11/2022 06:34:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=364
03/11/2022 06:34:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=369
03/11/2022 06:34:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.19 on epoch=374
03/11/2022 06:34:59 - INFO - __main__ - Global step 750 Train loss 0.21 Classification-F1 0.4682306940371457 on epoch=374
03/11/2022 06:34:59 - INFO - __main__ - Saving model with best Classification-F1: 0.4666666666666667 -> 0.4682306940371457 on epoch=374, global_step=750
03/11/2022 06:35:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.19 on epoch=379
03/11/2022 06:35:04 - INFO - __main__ - Step 770 Global step 770 Train loss 0.20 on epoch=384
03/11/2022 06:35:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=389
03/11/2022 06:35:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.22 on epoch=394
03/11/2022 06:35:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.21 on epoch=399
03/11/2022 06:35:11 - INFO - __main__ - Global step 800 Train loss 0.21 Classification-F1 0.4231177094379639 on epoch=399
03/11/2022 06:35:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.21 on epoch=404
03/11/2022 06:35:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.19 on epoch=409
03/11/2022 06:35:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.20 on epoch=414
03/11/2022 06:35:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.18 on epoch=419
03/11/2022 06:35:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.24 on epoch=424
03/11/2022 06:35:23 - INFO - __main__ - Global step 850 Train loss 0.20 Classification-F1 0.49090909090909085 on epoch=424
03/11/2022 06:35:23 - INFO - __main__ - Saving model with best Classification-F1: 0.4682306940371457 -> 0.49090909090909085 on epoch=424, global_step=850
03/11/2022 06:35:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.23 on epoch=429
03/11/2022 06:35:28 - INFO - __main__ - Step 870 Global step 870 Train loss 0.19 on epoch=434
03/11/2022 06:35:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.23 on epoch=439
03/11/2022 06:35:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.19 on epoch=444
03/11/2022 06:35:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.18 on epoch=449
03/11/2022 06:35:35 - INFO - __main__ - Global step 900 Train loss 0.20 Classification-F1 0.4420512820512821 on epoch=449
03/11/2022 06:35:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.18 on epoch=454
03/11/2022 06:35:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=459
03/11/2022 06:35:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.19 on epoch=464
03/11/2022 06:35:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=469
03/11/2022 06:35:47 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
03/11/2022 06:35:48 - INFO - __main__ - Global step 950 Train loss 0.18 Classification-F1 0.4980392156862745 on epoch=474
03/11/2022 06:35:48 - INFO - __main__ - Saving model with best Classification-F1: 0.49090909090909085 -> 0.4980392156862745 on epoch=474, global_step=950
03/11/2022 06:35:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=479
03/11/2022 06:35:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.19 on epoch=484
03/11/2022 06:35:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.19 on epoch=489
03/11/2022 06:35:57 - INFO - __main__ - Step 990 Global step 990 Train loss 0.19 on epoch=494
03/11/2022 06:35:59 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.17 on epoch=499
03/11/2022 06:36:00 - INFO - __main__ - Global step 1000 Train loss 0.18 Classification-F1 0.3333333333333333 on epoch=499
03/11/2022 06:36:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=504
03/11/2022 06:36:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=509
03/11/2022 06:36:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.16 on epoch=514
03/11/2022 06:36:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.16 on epoch=519
03/11/2022 06:36:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.17 on epoch=524
03/11/2022 06:36:12 - INFO - __main__ - Global step 1050 Train loss 0.18 Classification-F1 0.4909862142099682 on epoch=524
03/11/2022 06:36:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.17 on epoch=529
03/11/2022 06:36:16 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=534
03/11/2022 06:36:18 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.19 on epoch=539
03/11/2022 06:36:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.16 on epoch=544
03/11/2022 06:36:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=549
03/11/2022 06:36:24 - INFO - __main__ - Global step 1100 Train loss 0.18 Classification-F1 0.37662337662337664 on epoch=549
03/11/2022 06:36:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=554
03/11/2022 06:36:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.16 on epoch=559
03/11/2022 06:36:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.15 on epoch=564
03/11/2022 06:36:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=569
03/11/2022 06:36:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=574
03/11/2022 06:36:36 - INFO - __main__ - Global step 1150 Train loss 0.16 Classification-F1 0.5270935960591133 on epoch=574
03/11/2022 06:36:36 - INFO - __main__ - Saving model with best Classification-F1: 0.4980392156862745 -> 0.5270935960591133 on epoch=574, global_step=1150
03/11/2022 06:36:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=579
03/11/2022 06:36:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.16 on epoch=584
03/11/2022 06:36:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=589
03/11/2022 06:36:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=594
03/11/2022 06:36:47 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.15 on epoch=599
03/11/2022 06:36:48 - INFO - __main__ - Global step 1200 Train loss 0.17 Classification-F1 0.5607843137254902 on epoch=599
03/11/2022 06:36:48 - INFO - __main__ - Saving model with best Classification-F1: 0.5270935960591133 -> 0.5607843137254902 on epoch=599, global_step=1200
03/11/2022 06:36:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=604
03/11/2022 06:36:53 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.18 on epoch=609
03/11/2022 06:36:55 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.19 on epoch=614
03/11/2022 06:36:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=619
03/11/2022 06:36:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=624
03/11/2022 06:37:00 - INFO - __main__ - Global step 1250 Train loss 0.17 Classification-F1 0.5076923076923077 on epoch=624
03/11/2022 06:37:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.14 on epoch=629
03/11/2022 06:37:05 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=634
03/11/2022 06:37:07 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=639
03/11/2022 06:37:09 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=644
03/11/2022 06:37:12 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=649
03/11/2022 06:37:12 - INFO - __main__ - Global step 1300 Train loss 0.15 Classification-F1 0.5307917888563051 on epoch=649
03/11/2022 06:37:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.14 on epoch=654
03/11/2022 06:37:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.18 on epoch=659
03/11/2022 06:37:19 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=664
03/11/2022 06:37:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.17 on epoch=669
03/11/2022 06:37:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.13 on epoch=674
03/11/2022 06:37:24 - INFO - __main__ - Global step 1350 Train loss 0.15 Classification-F1 0.5195195195195195 on epoch=674
03/11/2022 06:37:27 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.15 on epoch=679
03/11/2022 06:37:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.12 on epoch=684
03/11/2022 06:37:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.13 on epoch=689
03/11/2022 06:37:34 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=694
03/11/2022 06:37:36 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=699
03/11/2022 06:37:37 - INFO - __main__ - Global step 1400 Train loss 0.13 Classification-F1 0.5901477832512315 on epoch=699
03/11/2022 06:37:37 - INFO - __main__ - Saving model with best Classification-F1: 0.5607843137254902 -> 0.5901477832512315 on epoch=699, global_step=1400
03/11/2022 06:37:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.14 on epoch=704
03/11/2022 06:37:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.14 on epoch=709
03/11/2022 06:37:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.11 on epoch=714
03/11/2022 06:37:46 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=719
03/11/2022 06:37:48 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=724
03/11/2022 06:37:49 - INFO - __main__ - Global step 1450 Train loss 0.12 Classification-F1 0.5835835835835835 on epoch=724
03/11/2022 06:37:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.11 on epoch=729
03/11/2022 06:37:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=734
03/11/2022 06:37:56 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.08 on epoch=739
03/11/2022 06:37:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.11 on epoch=744
03/11/2022 06:38:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.11 on epoch=749
03/11/2022 06:38:01 - INFO - __main__ - Global step 1500 Train loss 0.11 Classification-F1 0.5307917888563051 on epoch=749
03/11/2022 06:38:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.12 on epoch=754
03/11/2022 06:38:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.08 on epoch=759
03/11/2022 06:38:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.10 on epoch=764
03/11/2022 06:38:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.10 on epoch=769
03/11/2022 06:38:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.09 on epoch=774
03/11/2022 06:38:13 - INFO - __main__ - Global step 1550 Train loss 0.10 Classification-F1 0.5151515151515151 on epoch=774
03/11/2022 06:38:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.08 on epoch=779
03/11/2022 06:38:18 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=784
03/11/2022 06:38:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=789
03/11/2022 06:38:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.06 on epoch=794
03/11/2022 06:38:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.06 on epoch=799
03/11/2022 06:38:25 - INFO - __main__ - Global step 1600 Train loss 0.08 Classification-F1 0.3816425120772947 on epoch=799
03/11/2022 06:38:27 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.09 on epoch=804
03/11/2022 06:38:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=809
03/11/2022 06:38:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.08 on epoch=814
03/11/2022 06:38:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=819
03/11/2022 06:38:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=824
03/11/2022 06:38:37 - INFO - __main__ - Global step 1650 Train loss 0.09 Classification-F1 0.49090909090909085 on epoch=824
03/11/2022 06:38:39 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.07 on epoch=829
03/11/2022 06:38:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.11 on epoch=834
03/11/2022 06:38:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.07 on epoch=839
03/11/2022 06:38:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.06 on epoch=844
03/11/2022 06:38:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.06 on epoch=849
03/11/2022 06:38:49 - INFO - __main__ - Global step 1700 Train loss 0.07 Classification-F1 0.3319088319088319 on epoch=849
03/11/2022 06:38:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
03/11/2022 06:38:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=859
03/11/2022 06:38:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.08 on epoch=864
03/11/2022 06:38:58 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.09 on epoch=869
03/11/2022 06:39:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.05 on epoch=874
03/11/2022 06:39:01 - INFO - __main__ - Global step 1750 Train loss 0.07 Classification-F1 0.2837209302325581 on epoch=874
03/11/2022 06:39:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=879
03/11/2022 06:39:06 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.03 on epoch=884
03/11/2022 06:39:08 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=889
03/11/2022 06:39:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.03 on epoch=894
03/11/2022 06:39:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=899
03/11/2022 06:39:14 - INFO - __main__ - Global step 1800 Train loss 0.04 Classification-F1 0.29744816586921846 on epoch=899
03/11/2022 06:39:16 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.02 on epoch=904
03/11/2022 06:39:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=909
03/11/2022 06:39:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.04 on epoch=914
03/11/2022 06:39:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=919
03/11/2022 06:39:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
03/11/2022 06:39:26 - INFO - __main__ - Global step 1850 Train loss 0.04 Classification-F1 0.30757575757575756 on epoch=924
03/11/2022 06:39:28 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.05 on epoch=929
03/11/2022 06:39:30 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.03 on epoch=934
03/11/2022 06:39:32 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
03/11/2022 06:39:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.04 on epoch=944
03/11/2022 06:39:37 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.03 on epoch=949
03/11/2022 06:39:38 - INFO - __main__ - Global step 1900 Train loss 0.04 Classification-F1 0.5901477832512315 on epoch=949
03/11/2022 06:39:40 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=954
03/11/2022 06:39:42 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
03/11/2022 06:39:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.02 on epoch=964
03/11/2022 06:39:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
03/11/2022 06:39:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.01 on epoch=974
03/11/2022 06:39:50 - INFO - __main__ - Global step 1950 Train loss 0.03 Classification-F1 0.4909862142099682 on epoch=974
03/11/2022 06:39:52 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.04 on epoch=979
03/11/2022 06:39:54 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.01 on epoch=984
03/11/2022 06:39:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=989
03/11/2022 06:39:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.02 on epoch=994
03/11/2022 06:40:01 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=999
03/11/2022 06:40:02 - INFO - __main__ - Global step 2000 Train loss 0.04 Classification-F1 0.5151515151515151 on epoch=999
03/11/2022 06:40:02 - INFO - __main__ - save last model!
03/11/2022 06:40:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/11/2022 06:40:02 - INFO - __main__ - Start tokenizing ... 955 instances
03/11/2022 06:40:02 - INFO - __main__ - Printing 3 examples
03/11/2022 06:40:02 - INFO - __main__ -  [tweet_eval-irony] #NBA players #NY support protests of #police killings/sure #money donations for #college WB coming
03/11/2022 06:40:02 - INFO - __main__ - ['hate']
03/11/2022 06:40:02 - INFO - __main__ -  [tweet_eval-irony] A new year about to start|So many people came so many went|But always wat's gone is better.
03/11/2022 06:40:02 - INFO - __main__ - ['non-irony']
03/11/2022 06:40:02 - INFO - __main__ -  [tweet_eval-irony] Obama's $1,176,120.90 in Taxpayer Funded Costs to Attend Political Fundraisers in Los Angeles, San Francisco
03/11/2022 06:40:02 - INFO - __main__ - ['hate']
03/11/2022 06:40:02 - INFO - __main__ - Tokenizing Input ...
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/11/2022 06:40:02 - INFO - __main__ - Tokenizing Output ...
03/11/2022 06:40:03 - INFO - __main__ - Loaded 955 examples from test data
03/11/2022 06:40:26 - INFO - __main__ - Saved prediction in models/T5-large-fomaml-random-3e-5-2-5000-5e-1/singletask-tweet_eval-irony/tweet_eval-irony_16_87_0.2_8_predictions.txt
03/11/2022 06:40:26 - INFO - __main__ - Classification-F1 on test data: 0.2251
03/11/2022 06:40:27 - INFO - __main__ - prefix=tweet_eval-irony_16_87, lr=0.2, bsz=8, dev_performance=0.5901477832512315, test_performance=0.2250882409408429
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003364086151123047 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15773", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 10605, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15774", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 10605, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 10605, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15803): No such process
Task: quoref, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta_1.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29588
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_mst2hbqj/none_ttuzanc9
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_mst2hbqj/none_ttuzanc9/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_mst2hbqj/none_ttuzanc9/attempt_0/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15810) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_mst2hbqj/none_ttuzanc9/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_mst2hbqj/none_ttuzanc9/attempt_1/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15812) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_mst2hbqj/none_ttuzanc9/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_mst2hbqj/none_ttuzanc9/attempt_2/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15814) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_mst2hbqj/none_ttuzanc9/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_mst2hbqj/none_ttuzanc9/attempt_3/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15816) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.000415802001953125 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15816", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15817", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 15816 (local_rank 0) FAILED (exitcode 2)
Error msg: Process failed with exitcode 2
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    singletask_from_meta_1.py FAILED   
=======================================
Root Cause:
[0]:
  time: 2022-03-11_06:40:51
  rank: 0 (local_rank: 0)
  exitcode: 2 (pid: 15816)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
=======================================
Other Failures:
[1]:
  time: 2022-03-11_06:40:51
  rank: 1 (local_rank: 1)
  exitcode: 2 (pid: 15817)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15819): No such process
Task: amazon_polarity, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta_1.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29588
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_v4o2wp8q/none_vvm541ul
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_v4o2wp8q/none_vvm541ul/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_v4o2wp8q/none_vvm541ul/attempt_0/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15826) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_v4o2wp8q/none_vvm541ul/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_v4o2wp8q/none_vvm541ul/attempt_1/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15828) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_v4o2wp8q/none_vvm541ul/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_v4o2wp8q/none_vvm541ul/attempt_2/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15830) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_v4o2wp8q/none_vvm541ul/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_v4o2wp8q/none_vvm541ul/attempt_3/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15832) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0005230903625488281 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15832", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15833", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 15832 (local_rank 0) FAILED (exitcode 2)
Error msg: Process failed with exitcode 2
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    singletask_from_meta_1.py FAILED   
=======================================
Root Cause:
[0]:
  time: 2022-03-11_06:41:12
  rank: 0 (local_rank: 0)
  exitcode: 2 (pid: 15832)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
=======================================
Other Failures:
[1]:
  time: 2022-03-11_06:41:12
  rank: 1 (local_rank: 1)
  exitcode: 2 (pid: 15833)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15835): No such process
Task: blimp-sentential_negation_npi_licensor_present, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta_1.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29588
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_6gibsqk3/none_vcqovrcw
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_6gibsqk3/none_vcqovrcw/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_6gibsqk3/none_vcqovrcw/attempt_0/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15842) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_6gibsqk3/none_vcqovrcw/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_6gibsqk3/none_vcqovrcw/attempt_1/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15844) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_6gibsqk3/none_vcqovrcw/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_6gibsqk3/none_vcqovrcw/attempt_2/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15846) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_6gibsqk3/none_vcqovrcw/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_6gibsqk3/none_vcqovrcw/attempt_3/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15848) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003936290740966797 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15848", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15849", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 15848 (local_rank 0) FAILED (exitcode 2)
Error msg: Process failed with exitcode 2
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    singletask_from_meta_1.py FAILED   
=======================================
Root Cause:
[0]:
  time: 2022-03-11_06:41:34
  rank: 0 (local_rank: 0)
  exitcode: 2 (pid: 15848)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
=======================================
Other Failures:
[1]:
  time: 2022-03-11_06:41:34
  rank: 1 (local_rank: 1)
  exitcode: 2 (pid: 15849)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15851): No such process
Task: tweet_eval-irony, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta_1.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29588
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_59erzeoo/none_1jvqer7r
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_59erzeoo/none_1jvqer7r/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_59erzeoo/none_1jvqer7r/attempt_0/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15858) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_59erzeoo/none_1jvqer7r/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_59erzeoo/none_1jvqer7r/attempt_1/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15860) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_59erzeoo/none_1jvqer7r/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_59erzeoo/none_1jvqer7r/attempt_2/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15862) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_59erzeoo/none_1jvqer7r/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_59erzeoo/none_1jvqer7r/attempt_3/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15864) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00044155120849609375 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15864", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15865", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 15864 (local_rank 0) FAILED (exitcode 2)
Error msg: Process failed with exitcode 2
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    singletask_from_meta_1.py FAILED   
=======================================
Root Cause:
[0]:
  time: 2022-03-11_06:41:55
  rank: 0 (local_rank: 0)
  exitcode: 2 (pid: 15864)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
=======================================
Other Failures:
[1]:
  time: 2022-03-11_06:41:55
  rank: 1 (local_rank: 1)
  exitcode: 2 (pid: 15865)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15867): No such process
Task: quoref, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta_1.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29588
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_hav68fkt/none_5d5euw99
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_hav68fkt/none_5d5euw99/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_hav68fkt/none_5d5euw99/attempt_0/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15874) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_hav68fkt/none_5d5euw99/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_hav68fkt/none_5d5euw99/attempt_1/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15876) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_hav68fkt/none_5d5euw99/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_hav68fkt/none_5d5euw99/attempt_2/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15878) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_hav68fkt/none_5d5euw99/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_hav68fkt/none_5d5euw99/attempt_3/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15880) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003497600555419922 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15880", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15881", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 15880 (local_rank 0) FAILED (exitcode 2)
Error msg: Process failed with exitcode 2
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    singletask_from_meta_1.py FAILED   
=======================================
Root Cause:
[0]:
  time: 2022-03-11_06:42:16
  rank: 0 (local_rank: 0)
  exitcode: 2 (pid: 15880)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
=======================================
Other Failures:
[1]:
  time: 2022-03-11_06:42:16
  rank: 1 (local_rank: 1)
  exitcode: 2 (pid: 15881)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15883): No such process
Task: amazon_polarity, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta_1.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29588
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_npdffbw2/none_lgpgvv9a
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_npdffbw2/none_lgpgvv9a/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_npdffbw2/none_lgpgvv9a/attempt_0/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15890) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_npdffbw2/none_lgpgvv9a/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_npdffbw2/none_lgpgvv9a/attempt_1/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15892) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_npdffbw2/none_lgpgvv9a/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_npdffbw2/none_lgpgvv9a/attempt_2/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15894) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_npdffbw2/none_lgpgvv9a/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_npdffbw2/none_lgpgvv9a/attempt_3/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15896) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003924369812011719 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15896", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15897", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 15896 (local_rank 0) FAILED (exitcode 2)
Error msg: Process failed with exitcode 2
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    singletask_from_meta_1.py FAILED   
=======================================
Root Cause:
[0]:
  time: 2022-03-11_06:42:38
  rank: 0 (local_rank: 0)
  exitcode: 2 (pid: 15896)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
=======================================
Other Failures:
[1]:
  time: 2022-03-11_06:42:38
  rank: 1 (local_rank: 1)
  exitcode: 2 (pid: 15897)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15899): No such process
Task: blimp-sentential_negation_npi_licensor_present, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta_1.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29588
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_93fh7zxj/none_wlgpuiew
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_93fh7zxj/none_wlgpuiew/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_93fh7zxj/none_wlgpuiew/attempt_0/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15906) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_93fh7zxj/none_wlgpuiew/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_93fh7zxj/none_wlgpuiew/attempt_1/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15908) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_93fh7zxj/none_wlgpuiew/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_93fh7zxj/none_wlgpuiew/attempt_2/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15910) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_93fh7zxj/none_wlgpuiew/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_93fh7zxj/none_wlgpuiew/attempt_3/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15912) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003643035888671875 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15912", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15913", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 15912 (local_rank 0) FAILED (exitcode 2)
Error msg: Process failed with exitcode 2
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    singletask_from_meta_1.py FAILED   
=======================================
Root Cause:
[0]:
  time: 2022-03-11_06:42:59
  rank: 0 (local_rank: 0)
  exitcode: 2 (pid: 15912)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
=======================================
Other Failures:
[1]:
  time: 2022-03-11_06:42:59
  rank: 1 (local_rank: 1)
  exitcode: 2 (pid: 15913)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15915): No such process
Task: tweet_eval-irony, Checkpoint: models/upstream-maml-random-3e-5-4-5000-5e-1/last-model.pt, Identifier: T5-large-maml-3e-5-4-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta_1.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29588
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_qumnfxmh/none_hsae2m1w
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_qumnfxmh/none_hsae2m1w/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_qumnfxmh/none_hsae2m1w/attempt_0/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15922) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_qumnfxmh/none_hsae2m1w/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_qumnfxmh/none_hsae2m1w/attempt_1/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15924) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_qumnfxmh/none_hsae2m1w/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_qumnfxmh/none_hsae2m1w/attempt_2/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15926) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29588
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_qumnfxmh/none_hsae2m1w/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_qumnfxmh/none_hsae2m1w/attempt_3/1/error.json
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
/opt/conda/envs/meta/bin/python: can't open file '/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_fomaml/singletask_from_meta_1.py': [Errno 2] No such file or directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 2) local_rank: 0 (pid: 15928) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00037169456481933594 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "15928", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "15929", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 20, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 15928 (local_rank 0) FAILED (exitcode 2)
Error msg: Process failed with exitcode 2
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
    singletask_from_meta_1.py FAILED   
=======================================
Root Cause:
[0]:
  time: 2022-03-11_06:43:20
  rank: 0 (local_rank: 0)
  exitcode: 2 (pid: 15928)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
=======================================
Other Failures:
[1]:
  time: 2022-03-11_06:43:20
  rank: 1 (local_rank: 1)
  exitcode: 2 (pid: 15929)
  error_file: <N/A>
  msg: "Process failed with exitcode 2"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (15931): No such process
