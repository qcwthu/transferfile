nohup: ignoring input
t5base para reptile downstream
Task: glue-mrpc, Checkpoint: models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt, Identifier: T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10
Output directory () already exists and is not empty.
06/21/2022 14:57:46 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/21/2022 14:57:46 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc
06/21/2022 14:57:46 - INFO - __main__ - Namespace(task_dir='data/glue-mrpc/', task_name='glue-mrpc', identifier='T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-reptile-nopara2para-3e-5-2-5000-5e-1-10-t5base/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/base/pytorch_model.bin', model='google/t5-v1_1-base', prompt_number=100, cuda='6,7')
06/21/2022 14:57:46 - INFO - __main__ - models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc
06/21/2022 14:57:46 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 1
06/21/2022 14:57:46 - INFO - root - Added key: store_based_barrier_key:1 to store for rank: 0
06/21/2022 14:57:46 - INFO - __main__ - args.device: cuda:0
06/21/2022 14:57:46 - INFO - __main__ - Using 2 gpus
06/21/2022 14:57:46 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/21/2022 14:57:46 - INFO - __main__ - args.device: cuda:1
06/21/2022 14:57:46 - INFO - __main__ - Using 2 gpus
06/21/2022 14:57:46 - INFO - __main__ - Fine-tuning the following samples: ['glue-mrpc_16_100', 'glue-mrpc_16_13', 'glue-mrpc_16_21', 'glue-mrpc_16_42', 'glue-mrpc_16_87']
06/21/2022 14:57:52 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.5, bsz=8 ...
06/21/2022 14:57:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 14:57:53 - INFO - __main__ - Printing 3 examples
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 14:57:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 14:57:53 - INFO - __main__ - Printing 3 examples
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 14:57:53 - INFO - __main__ - Tokenizing Output ...
06/21/2022 14:57:53 - INFO - __main__ - Tokenizing Output ...
06/21/2022 14:57:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 14:57:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 14:57:53 - INFO - __main__ - Printing 3 examples
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ - Tokenizing Input ...
06/21/2022 14:57:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 14:57:53 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 14:57:53 - INFO - __main__ - Printing 3 examples
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/21/2022 14:57:53 - INFO - __main__ - ['not_equivalent']
06/21/2022 14:57:53 - INFO - __main__ - Tokenizing Input ...
06/21/2022 14:57:53 - INFO - __main__ - Tokenizing Output ...
06/21/2022 14:57:53 - INFO - __main__ - Tokenizing Output ...
06/21/2022 14:57:53 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 14:57:53 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 14:57:59 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 14:58:00 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 14:58:00 - INFO - __main__ - Starting training!
06/21/2022 14:58:00 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 14:58:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 14:58:06 - INFO - __main__ - Starting training!
06/21/2022 14:58:09 - INFO - __main__ - Step 10 Global step 10 Train loss 6.53 on epoch=4
06/21/2022 14:58:10 - INFO - __main__ - Step 20 Global step 20 Train loss 6.53 on epoch=9
06/21/2022 14:58:11 - INFO - __main__ - Step 30 Global step 30 Train loss 6.58 on epoch=14
06/21/2022 14:58:13 - INFO - __main__ - Step 40 Global step 40 Train loss 6.47 on epoch=19
06/21/2022 14:58:14 - INFO - __main__ - Step 50 Global step 50 Train loss 6.45 on epoch=24
06/21/2022 14:58:18 - INFO - __main__ - Global step 50 Train loss 6.51 ACC 0.0 on epoch=24
06/21/2022 14:58:18 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 14:58:20 - INFO - __main__ - Step 60 Global step 60 Train loss 6.49 on epoch=29
06/21/2022 14:58:21 - INFO - __main__ - Step 70 Global step 70 Train loss 6.43 on epoch=34
06/21/2022 14:58:22 - INFO - __main__ - Step 80 Global step 80 Train loss 6.40 on epoch=39
06/21/2022 14:58:24 - INFO - __main__ - Step 90 Global step 90 Train loss 6.34 on epoch=44
06/21/2022 14:58:25 - INFO - __main__ - Step 100 Global step 100 Train loss 6.27 on epoch=49
06/21/2022 14:58:30 - INFO - __main__ - Global step 100 Train loss 6.39 ACC 0.0 on epoch=49
06/21/2022 14:58:31 - INFO - __main__ - Step 110 Global step 110 Train loss 6.19 on epoch=54
06/21/2022 14:58:32 - INFO - __main__ - Step 120 Global step 120 Train loss 6.05 on epoch=59
06/21/2022 14:58:34 - INFO - __main__ - Step 130 Global step 130 Train loss 6.05 on epoch=64
06/21/2022 14:58:35 - INFO - __main__ - Step 140 Global step 140 Train loss 5.93 on epoch=69
06/21/2022 14:58:36 - INFO - __main__ - Step 150 Global step 150 Train loss 5.93 on epoch=74
06/21/2022 14:58:40 - INFO - __main__ - Global step 150 Train loss 6.03 ACC 0.0 on epoch=74
06/21/2022 14:58:41 - INFO - __main__ - Step 160 Global step 160 Train loss 5.86 on epoch=79
06/21/2022 14:58:43 - INFO - __main__ - Step 170 Global step 170 Train loss 5.85 on epoch=84
06/21/2022 14:58:44 - INFO - __main__ - Step 180 Global step 180 Train loss 5.81 on epoch=89
06/21/2022 14:58:45 - INFO - __main__ - Step 190 Global step 190 Train loss 5.77 on epoch=94
06/21/2022 14:58:47 - INFO - __main__ - Step 200 Global step 200 Train loss 5.70 on epoch=99
06/21/2022 14:58:55 - INFO - __main__ - Global step 200 Train loss 5.80 ACC 0.0 on epoch=99
06/21/2022 14:58:57 - INFO - __main__ - Step 210 Global step 210 Train loss 5.75 on epoch=104
06/21/2022 14:58:58 - INFO - __main__ - Step 220 Global step 220 Train loss 5.73 on epoch=109
06/21/2022 14:58:59 - INFO - __main__ - Step 230 Global step 230 Train loss 5.78 on epoch=114
06/21/2022 14:59:01 - INFO - __main__ - Step 240 Global step 240 Train loss 5.71 on epoch=119
06/21/2022 14:59:02 - INFO - __main__ - Step 250 Global step 250 Train loss 5.61 on epoch=124
06/21/2022 14:59:09 - INFO - __main__ - Global step 250 Train loss 5.72 ACC 0.0 on epoch=124
06/21/2022 14:59:10 - INFO - __main__ - Step 260 Global step 260 Train loss 5.52 on epoch=129
06/21/2022 14:59:11 - INFO - __main__ - Step 270 Global step 270 Train loss 5.49 on epoch=134
06/21/2022 14:59:12 - INFO - __main__ - Step 280 Global step 280 Train loss 5.44 on epoch=139
06/21/2022 14:59:14 - INFO - __main__ - Step 290 Global step 290 Train loss 5.43 on epoch=144
06/21/2022 14:59:15 - INFO - __main__ - Step 300 Global step 300 Train loss 5.39 on epoch=149
06/21/2022 14:59:28 - INFO - __main__ - Global step 300 Train loss 5.45 ACC 0.0 on epoch=149
06/21/2022 14:59:29 - INFO - __main__ - Step 310 Global step 310 Train loss 5.48 on epoch=154
06/21/2022 14:59:31 - INFO - __main__ - Step 320 Global step 320 Train loss 5.49 on epoch=159
06/21/2022 14:59:32 - INFO - __main__ - Step 330 Global step 330 Train loss 5.51 on epoch=164
06/21/2022 14:59:33 - INFO - __main__ - Step 340 Global step 340 Train loss 5.48 on epoch=169
06/21/2022 14:59:35 - INFO - __main__ - Step 350 Global step 350 Train loss 5.37 on epoch=174
06/21/2022 14:59:39 - INFO - __main__ - Global step 350 Train loss 5.46 ACC 0.0 on epoch=174
06/21/2022 14:59:40 - INFO - __main__ - Step 360 Global step 360 Train loss 5.32 on epoch=179
06/21/2022 14:59:41 - INFO - __main__ - Step 370 Global step 370 Train loss 5.22 on epoch=184
06/21/2022 14:59:43 - INFO - __main__ - Step 380 Global step 380 Train loss 5.24 on epoch=189
06/21/2022 14:59:44 - INFO - __main__ - Step 390 Global step 390 Train loss 5.26 on epoch=194
06/21/2022 14:59:45 - INFO - __main__ - Step 400 Global step 400 Train loss 5.16 on epoch=199
06/21/2022 14:59:53 - INFO - __main__ - Global step 400 Train loss 5.24 ACC 0.0 on epoch=199
06/21/2022 14:59:54 - INFO - __main__ - Step 410 Global step 410 Train loss 5.10 on epoch=204
06/21/2022 14:59:56 - INFO - __main__ - Step 420 Global step 420 Train loss 4.96 on epoch=209
06/21/2022 14:59:57 - INFO - __main__ - Step 430 Global step 430 Train loss 4.89 on epoch=214
06/21/2022 14:59:58 - INFO - __main__ - Step 440 Global step 440 Train loss 5.08 on epoch=219
06/21/2022 15:00:00 - INFO - __main__ - Step 450 Global step 450 Train loss 4.93 on epoch=224
06/21/2022 15:00:08 - INFO - __main__ - Global step 450 Train loss 4.99 ACC 0.0 on epoch=224
06/21/2022 15:00:09 - INFO - __main__ - Step 460 Global step 460 Train loss 4.89 on epoch=229
06/21/2022 15:00:10 - INFO - __main__ - Step 470 Global step 470 Train loss 4.81 on epoch=234
06/21/2022 15:00:12 - INFO - __main__ - Step 480 Global step 480 Train loss 4.82 on epoch=239
06/21/2022 15:00:13 - INFO - __main__ - Step 490 Global step 490 Train loss 4.75 on epoch=244
06/21/2022 15:00:14 - INFO - __main__ - Step 500 Global step 500 Train loss 4.66 on epoch=249
06/21/2022 15:00:27 - INFO - __main__ - Global step 500 Train loss 4.79 ACC 0.0 on epoch=249
06/21/2022 15:00:29 - INFO - __main__ - Step 510 Global step 510 Train loss 4.59 on epoch=254
06/21/2022 15:00:30 - INFO - __main__ - Step 520 Global step 520 Train loss 4.57 on epoch=259
06/21/2022 15:00:31 - INFO - __main__ - Step 530 Global step 530 Train loss 4.52 on epoch=264
06/21/2022 15:00:33 - INFO - __main__ - Step 540 Global step 540 Train loss 4.41 on epoch=269
06/21/2022 15:00:34 - INFO - __main__ - Step 550 Global step 550 Train loss 4.27 on epoch=274
06/21/2022 15:00:36 - INFO - __main__ - Global step 550 Train loss 4.47 ACC 0.0 on epoch=274
06/21/2022 15:00:37 - INFO - __main__ - Step 560 Global step 560 Train loss 4.27 on epoch=279
06/21/2022 15:00:38 - INFO - __main__ - Step 570 Global step 570 Train loss 4.19 on epoch=284
06/21/2022 15:00:40 - INFO - __main__ - Step 580 Global step 580 Train loss 4.23 on epoch=289
06/21/2022 15:00:41 - INFO - __main__ - Step 590 Global step 590 Train loss 4.02 on epoch=294
06/21/2022 15:00:42 - INFO - __main__ - Step 600 Global step 600 Train loss 3.96 on epoch=299
06/21/2022 15:00:48 - INFO - __main__ - Global step 600 Train loss 4.13 ACC 0.0 on epoch=299
06/21/2022 15:00:49 - INFO - __main__ - Step 610 Global step 610 Train loss 3.83 on epoch=304
06/21/2022 15:00:50 - INFO - __main__ - Step 620 Global step 620 Train loss 3.77 on epoch=309
06/21/2022 15:00:52 - INFO - __main__ - Step 630 Global step 630 Train loss 3.57 on epoch=314
06/21/2022 15:00:53 - INFO - __main__ - Step 640 Global step 640 Train loss 3.51 on epoch=319
06/21/2022 15:00:54 - INFO - __main__ - Step 650 Global step 650 Train loss 3.43 on epoch=324
06/21/2022 15:00:58 - INFO - __main__ - Global step 650 Train loss 3.62 ACC 0.1875 on epoch=324
06/21/2022 15:00:58 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.1875 on epoch=324, global_step=650
06/21/2022 15:01:00 - INFO - __main__ - Step 660 Global step 660 Train loss 3.43 on epoch=329
06/21/2022 15:01:01 - INFO - __main__ - Step 670 Global step 670 Train loss 3.28 on epoch=334
06/21/2022 15:01:03 - INFO - __main__ - Step 680 Global step 680 Train loss 3.26 on epoch=339
06/21/2022 15:01:04 - INFO - __main__ - Step 690 Global step 690 Train loss 3.09 on epoch=344
06/21/2022 15:01:05 - INFO - __main__ - Step 700 Global step 700 Train loss 2.99 on epoch=349
06/21/2022 15:01:08 - INFO - __main__ - Global step 700 Train loss 3.21 ACC 0.5 on epoch=349
06/21/2022 15:01:08 - INFO - __main__ - Saving model with best ACC: 0.1875 -> 0.5 on epoch=349, global_step=700
06/21/2022 15:01:09 - INFO - __main__ - Step 710 Global step 710 Train loss 2.89 on epoch=354
06/21/2022 15:01:10 - INFO - __main__ - Step 720 Global step 720 Train loss 2.90 on epoch=359
06/21/2022 15:01:11 - INFO - __main__ - Step 730 Global step 730 Train loss 2.76 on epoch=364
06/21/2022 15:01:13 - INFO - __main__ - Step 740 Global step 740 Train loss 2.71 on epoch=369
06/21/2022 15:01:14 - INFO - __main__ - Step 750 Global step 750 Train loss 2.65 on epoch=374
06/21/2022 15:01:16 - INFO - __main__ - Global step 750 Train loss 2.78 ACC 0.5 on epoch=374
06/21/2022 15:01:18 - INFO - __main__ - Step 760 Global step 760 Train loss 2.47 on epoch=379
06/21/2022 15:01:19 - INFO - __main__ - Step 770 Global step 770 Train loss 2.47 on epoch=384
06/21/2022 15:01:20 - INFO - __main__ - Step 780 Global step 780 Train loss 2.30 on epoch=389
06/21/2022 15:01:21 - INFO - __main__ - Step 790 Global step 790 Train loss 2.22 on epoch=394
06/21/2022 15:01:22 - INFO - __main__ - Step 800 Global step 800 Train loss 2.20 on epoch=399
06/21/2022 15:01:32 - INFO - __main__ - Global step 800 Train loss 2.33 ACC 0.46875 on epoch=399
06/21/2022 15:01:33 - INFO - __main__ - Step 810 Global step 810 Train loss 2.16 on epoch=404
06/21/2022 15:01:34 - INFO - __main__ - Step 820 Global step 820 Train loss 1.96 on epoch=409
06/21/2022 15:01:35 - INFO - __main__ - Step 830 Global step 830 Train loss 1.85 on epoch=414
06/21/2022 15:01:37 - INFO - __main__ - Step 840 Global step 840 Train loss 1.84 on epoch=419
06/21/2022 15:01:38 - INFO - __main__ - Step 850 Global step 850 Train loss 1.82 on epoch=424
06/21/2022 15:01:41 - INFO - __main__ - Global step 850 Train loss 1.93 ACC 0.46875 on epoch=424
06/21/2022 15:01:42 - INFO - __main__ - Step 860 Global step 860 Train loss 1.66 on epoch=429
06/21/2022 15:01:43 - INFO - __main__ - Step 870 Global step 870 Train loss 1.60 on epoch=434
06/21/2022 15:01:44 - INFO - __main__ - Step 880 Global step 880 Train loss 1.60 on epoch=439
06/21/2022 15:01:46 - INFO - __main__ - Step 890 Global step 890 Train loss 1.44 on epoch=444
06/21/2022 15:01:47 - INFO - __main__ - Step 900 Global step 900 Train loss 1.43 on epoch=449
06/21/2022 15:01:49 - INFO - __main__ - Global step 900 Train loss 1.55 ACC 0.46875 on epoch=449
06/21/2022 15:01:51 - INFO - __main__ - Step 910 Global step 910 Train loss 1.33 on epoch=454
06/21/2022 15:01:52 - INFO - __main__ - Step 920 Global step 920 Train loss 1.35 on epoch=459
06/21/2022 15:01:53 - INFO - __main__ - Step 930 Global step 930 Train loss 1.31 on epoch=464
06/21/2022 15:01:54 - INFO - __main__ - Step 940 Global step 940 Train loss 1.19 on epoch=469
06/21/2022 15:01:56 - INFO - __main__ - Step 950 Global step 950 Train loss 1.19 on epoch=474
06/21/2022 15:02:01 - INFO - __main__ - Global step 950 Train loss 1.27 ACC 0.40625 on epoch=474
06/21/2022 15:02:02 - INFO - __main__ - Step 960 Global step 960 Train loss 1.16 on epoch=479
06/21/2022 15:02:03 - INFO - __main__ - Step 970 Global step 970 Train loss 1.11 on epoch=484
06/21/2022 15:02:04 - INFO - __main__ - Step 980 Global step 980 Train loss 1.04 on epoch=489
06/21/2022 15:02:06 - INFO - __main__ - Step 990 Global step 990 Train loss 0.96 on epoch=494
06/21/2022 15:02:07 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.93 on epoch=499
06/21/2022 15:02:09 - INFO - __main__ - Global step 1000 Train loss 1.04 ACC 0.4375 on epoch=499
06/21/2022 15:02:10 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.90 on epoch=504
06/21/2022 15:02:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.85 on epoch=509
06/21/2022 15:02:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.86 on epoch=514
06/21/2022 15:02:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.86 on epoch=519
06/21/2022 15:02:15 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.83 on epoch=524
06/21/2022 15:02:17 - INFO - __main__ - Global step 1050 Train loss 0.86 ACC 0.5 on epoch=524
06/21/2022 15:02:18 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.75 on epoch=529
06/21/2022 15:02:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.79 on epoch=534
06/21/2022 15:02:20 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.77 on epoch=539
06/21/2022 15:02:22 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.74 on epoch=544
06/21/2022 15:02:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.74 on epoch=549
06/21/2022 15:02:25 - INFO - __main__ - Global step 1100 Train loss 0.76 ACC 0.5625 on epoch=549
06/21/2022 15:02:25 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=549, global_step=1100
06/21/2022 15:02:26 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.67 on epoch=554
06/21/2022 15:02:28 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.65 on epoch=559
06/21/2022 15:02:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.68 on epoch=564
06/21/2022 15:02:30 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.65 on epoch=569
06/21/2022 15:02:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.66 on epoch=574
06/21/2022 15:02:33 - INFO - __main__ - Global step 1150 Train loss 0.66 ACC 0.5 on epoch=574
06/21/2022 15:02:34 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.55 on epoch=579
06/21/2022 15:02:35 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=584
06/21/2022 15:02:36 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.61 on epoch=589
06/21/2022 15:02:38 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.61 on epoch=594
06/21/2022 15:02:39 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.54 on epoch=599
06/21/2022 15:02:41 - INFO - __main__ - Global step 1200 Train loss 0.58 ACC 0.40625 on epoch=599
06/21/2022 15:02:42 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.58 on epoch=604
06/21/2022 15:02:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.51 on epoch=609
06/21/2022 15:02:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
06/21/2022 15:02:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.60 on epoch=619
06/21/2022 15:02:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.60 on epoch=624
06/21/2022 15:02:49 - INFO - __main__ - Global step 1250 Train loss 0.56 ACC 0.5 on epoch=624
06/21/2022 15:02:50 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.49 on epoch=629
06/21/2022 15:02:51 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.47 on epoch=634
06/21/2022 15:02:53 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.48 on epoch=639
06/21/2022 15:02:54 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.49 on epoch=644
06/21/2022 15:02:55 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.46 on epoch=649
06/21/2022 15:02:57 - INFO - __main__ - Global step 1300 Train loss 0.48 ACC 0.375 on epoch=649
06/21/2022 15:02:59 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/21/2022 15:03:00 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.53 on epoch=659
06/21/2022 15:03:01 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.52 on epoch=664
06/21/2022 15:03:03 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
06/21/2022 15:03:04 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.45 on epoch=674
06/21/2022 15:03:06 - INFO - __main__ - Global step 1350 Train loss 0.49 ACC 0.5 on epoch=674
06/21/2022 15:03:07 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.47 on epoch=679
06/21/2022 15:03:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
06/21/2022 15:03:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.42 on epoch=689
06/21/2022 15:03:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.48 on epoch=694
06/21/2022 15:03:13 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.46 on epoch=699
06/21/2022 15:03:14 - INFO - __main__ - Global step 1400 Train loss 0.46 ACC 0.375 on epoch=699
06/21/2022 15:03:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.44 on epoch=704
06/21/2022 15:03:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.45 on epoch=709
06/21/2022 15:03:18 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.40 on epoch=714
06/21/2022 15:03:20 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.37 on epoch=719
06/21/2022 15:03:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.41 on epoch=724
06/21/2022 15:03:23 - INFO - __main__ - Global step 1450 Train loss 0.41 ACC 0.46875 on epoch=724
06/21/2022 15:03:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
06/21/2022 15:03:25 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.47 on epoch=734
06/21/2022 15:03:27 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.34 on epoch=739
06/21/2022 15:03:28 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.39 on epoch=744
06/21/2022 15:03:30 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.46 on epoch=749
06/21/2022 15:03:31 - INFO - __main__ - Global step 1500 Train loss 0.41 ACC 0.53125 on epoch=749
06/21/2022 15:03:32 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.40 on epoch=754
06/21/2022 15:03:33 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.38 on epoch=759
06/21/2022 15:03:35 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.40 on epoch=764
06/21/2022 15:03:36 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.39 on epoch=769
06/21/2022 15:03:38 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.41 on epoch=774
06/21/2022 15:03:38 - INFO - __main__ - Global step 1550 Train loss 0.40 ACC 0.25 on epoch=774
06/21/2022 15:03:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.48 on epoch=779
06/21/2022 15:03:41 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.43 on epoch=784
06/21/2022 15:03:42 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
06/21/2022 15:03:44 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.36 on epoch=794
06/21/2022 15:03:45 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.37 on epoch=799
06/21/2022 15:03:46 - INFO - __main__ - Global step 1600 Train loss 0.41 ACC 0.46875 on epoch=799
06/21/2022 15:03:47 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.45 on epoch=804
06/21/2022 15:03:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.43 on epoch=809
06/21/2022 15:03:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.48 on epoch=814
06/21/2022 15:03:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=819
06/21/2022 15:03:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=824
06/21/2022 15:03:53 - INFO - __main__ - Global step 1650 Train loss 0.44 ACC 0.5 on epoch=824
06/21/2022 15:03:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.39 on epoch=829
06/21/2022 15:03:56 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.40 on epoch=834
06/21/2022 15:03:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
06/21/2022 15:03:59 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.42 on epoch=844
06/21/2022 15:04:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.35 on epoch=849
06/21/2022 15:04:00 - INFO - __main__ - Global step 1700 Train loss 0.40 ACC 0.5 on epoch=849
06/21/2022 15:04:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.37 on epoch=854
06/21/2022 15:04:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.37 on epoch=859
06/21/2022 15:04:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.36 on epoch=864
06/21/2022 15:04:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.44 on epoch=869
06/21/2022 15:04:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
06/21/2022 15:04:08 - INFO - __main__ - Global step 1750 Train loss 0.38 ACC 0.5 on epoch=874
06/21/2022 15:04:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.36 on epoch=879
06/21/2022 15:04:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
06/21/2022 15:04:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.34 on epoch=889
06/21/2022 15:04:13 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.39 on epoch=894
06/21/2022 15:04:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
06/21/2022 15:04:15 - INFO - __main__ - Global step 1800 Train loss 0.38 ACC 0.5 on epoch=899
06/21/2022 15:04:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.35 on epoch=904
06/21/2022 15:04:18 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.34 on epoch=909
06/21/2022 15:04:19 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
06/21/2022 15:04:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.33 on epoch=919
06/21/2022 15:04:22 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
06/21/2022 15:04:22 - INFO - __main__ - Global step 1850 Train loss 0.35 ACC 0.4375 on epoch=924
06/21/2022 15:04:24 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/21/2022 15:04:25 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.41 on epoch=934
06/21/2022 15:04:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.31 on epoch=939
06/21/2022 15:04:28 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.36 on epoch=944
06/21/2022 15:04:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
06/21/2022 15:04:30 - INFO - __main__ - Global step 1900 Train loss 0.36 ACC 0.59375 on epoch=949
06/21/2022 15:04:30 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=949, global_step=1900
06/21/2022 15:04:31 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.32 on epoch=954
06/21/2022 15:04:33 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.36 on epoch=959
06/21/2022 15:04:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.38 on epoch=964
06/21/2022 15:04:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.31 on epoch=969
06/21/2022 15:04:37 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.33 on epoch=974
06/21/2022 15:04:38 - INFO - __main__ - Global step 1950 Train loss 0.34 ACC 0.4375 on epoch=974
06/21/2022 15:04:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.34 on epoch=979
06/21/2022 15:04:40 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.33 on epoch=984
06/21/2022 15:04:42 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.35 on epoch=989
06/21/2022 15:04:43 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
06/21/2022 15:04:44 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/21/2022 15:04:45 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.4375 on epoch=999
06/21/2022 15:04:45 - INFO - __main__ - save last model!
06/21/2022 15:04:45 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 15:04:45 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 15:04:45 - INFO - __main__ - Printing 3 examples
06/21/2022 15:04:45 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 15:04:45 - INFO - __main__ - ['equivalent']
06/21/2022 15:04:45 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 15:04:45 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:45 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 15:04:45 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:45 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:04:45 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:04:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:04:46 - INFO - __main__ - Printing 3 examples
06/21/2022 15:04:46 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/21/2022 15:04:46 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:46 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/21/2022 15:04:46 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:46 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/21/2022 15:04:46 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:46 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 15:04:46 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:04:46 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:04:46 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:04:46 - INFO - __main__ - Printing 3 examples
06/21/2022 15:04:46 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/21/2022 15:04:46 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:46 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/21/2022 15:04:46 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:46 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/21/2022 15:04:46 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:46 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:04:46 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:04:46 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:04:46 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 15:04:52 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:04:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:04:52 - INFO - __main__ - Starting training!
06/21/2022 15:04:55 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.5_8_predictions.txt
06/21/2022 15:04:55 - INFO - __main__ - ACC on test data: 0.3382
06/21/2022 15:04:55 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.5, bsz=8, dev_performance=0.59375, test_performance=0.3382352941176471
06/21/2022 15:04:55 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.4, bsz=8 ...
06/21/2022 15:04:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:04:56 - INFO - __main__ - Printing 3 examples
06/21/2022 15:04:56 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/21/2022 15:04:56 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/21/2022 15:04:56 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:56 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/21/2022 15:04:56 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 15:04:56 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:04:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:04:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:04:56 - INFO - __main__ - Printing 3 examples
06/21/2022 15:04:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/21/2022 15:04:56 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:56 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/21/2022 15:04:56 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/21/2022 15:04:56 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:04:56 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:04:56 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:04:56 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:05:03 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:05:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:05:03 - INFO - __main__ - Starting training!
06/21/2022 15:05:04 - INFO - __main__ - Step 10 Global step 10 Train loss 6.50 on epoch=4
06/21/2022 15:05:06 - INFO - __main__ - Step 20 Global step 20 Train loss 6.55 on epoch=9
06/21/2022 15:05:07 - INFO - __main__ - Step 30 Global step 30 Train loss 6.53 on epoch=14
06/21/2022 15:05:09 - INFO - __main__ - Step 40 Global step 40 Train loss 6.52 on epoch=19
06/21/2022 15:05:10 - INFO - __main__ - Step 50 Global step 50 Train loss 6.47 on epoch=24
06/21/2022 15:05:14 - INFO - __main__ - Global step 50 Train loss 6.51 ACC 0.0 on epoch=24
06/21/2022 15:05:14 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 15:05:16 - INFO - __main__ - Step 60 Global step 60 Train loss 6.50 on epoch=29
06/21/2022 15:05:17 - INFO - __main__ - Step 70 Global step 70 Train loss 6.41 on epoch=34
06/21/2022 15:05:18 - INFO - __main__ - Step 80 Global step 80 Train loss 6.38 on epoch=39
06/21/2022 15:05:20 - INFO - __main__ - Step 90 Global step 90 Train loss 6.37 on epoch=44
06/21/2022 15:05:21 - INFO - __main__ - Step 100 Global step 100 Train loss 6.32 on epoch=49
06/21/2022 15:05:24 - INFO - __main__ - Global step 100 Train loss 6.40 ACC 0.0 on epoch=49
06/21/2022 15:05:25 - INFO - __main__ - Step 110 Global step 110 Train loss 6.19 on epoch=54
06/21/2022 15:05:27 - INFO - __main__ - Step 120 Global step 120 Train loss 6.16 on epoch=59
06/21/2022 15:05:28 - INFO - __main__ - Step 130 Global step 130 Train loss 6.18 on epoch=64
06/21/2022 15:05:30 - INFO - __main__ - Step 140 Global step 140 Train loss 6.09 on epoch=69
06/21/2022 15:05:31 - INFO - __main__ - Step 150 Global step 150 Train loss 6.06 on epoch=74
06/21/2022 15:05:38 - INFO - __main__ - Global step 150 Train loss 6.14 ACC 0.0 on epoch=74
06/21/2022 15:05:39 - INFO - __main__ - Step 160 Global step 160 Train loss 6.02 on epoch=79
06/21/2022 15:05:41 - INFO - __main__ - Step 170 Global step 170 Train loss 6.09 on epoch=84
06/21/2022 15:05:42 - INFO - __main__ - Step 180 Global step 180 Train loss 5.93 on epoch=89
06/21/2022 15:05:43 - INFO - __main__ - Step 190 Global step 190 Train loss 5.92 on epoch=94
06/21/2022 15:05:45 - INFO - __main__ - Step 200 Global step 200 Train loss 5.84 on epoch=99
06/21/2022 15:05:54 - INFO - __main__ - Global step 200 Train loss 5.96 ACC 0.0 on epoch=99
06/21/2022 15:05:55 - INFO - __main__ - Step 210 Global step 210 Train loss 5.75 on epoch=104
06/21/2022 15:05:57 - INFO - __main__ - Step 220 Global step 220 Train loss 5.74 on epoch=109
06/21/2022 15:05:58 - INFO - __main__ - Step 230 Global step 230 Train loss 5.83 on epoch=114
06/21/2022 15:05:59 - INFO - __main__ - Step 240 Global step 240 Train loss 5.75 on epoch=119
06/21/2022 15:06:01 - INFO - __main__ - Step 250 Global step 250 Train loss 5.71 on epoch=124
06/21/2022 15:06:08 - INFO - __main__ - Global step 250 Train loss 5.76 ACC 0.0 on epoch=124
06/21/2022 15:06:10 - INFO - __main__ - Step 260 Global step 260 Train loss 5.63 on epoch=129
06/21/2022 15:06:11 - INFO - __main__ - Step 270 Global step 270 Train loss 5.55 on epoch=134
06/21/2022 15:06:12 - INFO - __main__ - Step 280 Global step 280 Train loss 5.64 on epoch=139
06/21/2022 15:06:14 - INFO - __main__ - Step 290 Global step 290 Train loss 5.48 on epoch=144
06/21/2022 15:06:15 - INFO - __main__ - Step 300 Global step 300 Train loss 5.44 on epoch=149
06/21/2022 15:06:17 - INFO - __main__ - Global step 300 Train loss 5.55 ACC 0.0 on epoch=149
06/21/2022 15:06:18 - INFO - __main__ - Step 310 Global step 310 Train loss 5.44 on epoch=154
06/21/2022 15:06:19 - INFO - __main__ - Step 320 Global step 320 Train loss 5.38 on epoch=159
06/21/2022 15:06:21 - INFO - __main__ - Step 330 Global step 330 Train loss 5.34 on epoch=164
06/21/2022 15:06:22 - INFO - __main__ - Step 340 Global step 340 Train loss 5.27 on epoch=169
06/21/2022 15:06:23 - INFO - __main__ - Step 350 Global step 350 Train loss 5.18 on epoch=174
06/21/2022 15:06:25 - INFO - __main__ - Global step 350 Train loss 5.32 ACC 0.0 on epoch=174
06/21/2022 15:06:27 - INFO - __main__ - Step 360 Global step 360 Train loss 5.11 on epoch=179
06/21/2022 15:06:28 - INFO - __main__ - Step 370 Global step 370 Train loss 5.07 on epoch=184
06/21/2022 15:06:29 - INFO - __main__ - Step 380 Global step 380 Train loss 5.06 on epoch=189
06/21/2022 15:06:31 - INFO - __main__ - Step 390 Global step 390 Train loss 5.09 on epoch=194
06/21/2022 15:06:32 - INFO - __main__ - Step 400 Global step 400 Train loss 5.02 on epoch=199
06/21/2022 15:06:33 - INFO - __main__ - Global step 400 Train loss 5.07 ACC 0.0 on epoch=199
06/21/2022 15:06:35 - INFO - __main__ - Step 410 Global step 410 Train loss 4.87 on epoch=204
06/21/2022 15:06:36 - INFO - __main__ - Step 420 Global step 420 Train loss 4.97 on epoch=209
06/21/2022 15:06:37 - INFO - __main__ - Step 430 Global step 430 Train loss 4.81 on epoch=214
06/21/2022 15:06:39 - INFO - __main__ - Step 440 Global step 440 Train loss 4.75 on epoch=219
06/21/2022 15:06:40 - INFO - __main__ - Step 450 Global step 450 Train loss 4.75 on epoch=224
06/21/2022 15:06:41 - INFO - __main__ - Global step 450 Train loss 4.83 ACC 0.0 on epoch=224
06/21/2022 15:06:42 - INFO - __main__ - Step 460 Global step 460 Train loss 4.81 on epoch=229
06/21/2022 15:06:44 - INFO - __main__ - Step 470 Global step 470 Train loss 4.64 on epoch=234
06/21/2022 15:06:45 - INFO - __main__ - Step 480 Global step 480 Train loss 4.57 on epoch=239
06/21/2022 15:06:47 - INFO - __main__ - Step 490 Global step 490 Train loss 4.56 on epoch=244
06/21/2022 15:06:48 - INFO - __main__ - Step 500 Global step 500 Train loss 4.48 on epoch=249
06/21/2022 15:06:49 - INFO - __main__ - Global step 500 Train loss 4.61 ACC 0.0 on epoch=249
06/21/2022 15:06:51 - INFO - __main__ - Step 510 Global step 510 Train loss 4.55 on epoch=254
06/21/2022 15:06:52 - INFO - __main__ - Step 520 Global step 520 Train loss 4.45 on epoch=259
06/21/2022 15:06:53 - INFO - __main__ - Step 530 Global step 530 Train loss 4.32 on epoch=264
06/21/2022 15:06:55 - INFO - __main__ - Step 540 Global step 540 Train loss 4.30 on epoch=269
06/21/2022 15:06:56 - INFO - __main__ - Step 550 Global step 550 Train loss 4.28 on epoch=274
06/21/2022 15:06:57 - INFO - __main__ - Global step 550 Train loss 4.38 ACC 0.0 on epoch=274
06/21/2022 15:06:59 - INFO - __main__ - Step 560 Global step 560 Train loss 4.25 on epoch=279
06/21/2022 15:07:00 - INFO - __main__ - Step 570 Global step 570 Train loss 4.09 on epoch=284
06/21/2022 15:07:02 - INFO - __main__ - Step 580 Global step 580 Train loss 4.14 on epoch=289
06/21/2022 15:07:03 - INFO - __main__ - Step 590 Global step 590 Train loss 3.95 on epoch=294
06/21/2022 15:07:04 - INFO - __main__ - Step 600 Global step 600 Train loss 3.86 on epoch=299
06/21/2022 15:07:06 - INFO - __main__ - Global step 600 Train loss 4.06 ACC 0.0 on epoch=299
06/21/2022 15:07:08 - INFO - __main__ - Step 610 Global step 610 Train loss 3.86 on epoch=304
06/21/2022 15:07:09 - INFO - __main__ - Step 620 Global step 620 Train loss 3.77 on epoch=309
06/21/2022 15:07:10 - INFO - __main__ - Step 630 Global step 630 Train loss 3.69 on epoch=314
06/21/2022 15:07:12 - INFO - __main__ - Step 640 Global step 640 Train loss 3.61 on epoch=319
06/21/2022 15:07:13 - INFO - __main__ - Step 650 Global step 650 Train loss 3.49 on epoch=324
06/21/2022 15:07:16 - INFO - __main__ - Global step 650 Train loss 3.68 ACC 0.0 on epoch=324
06/21/2022 15:07:17 - INFO - __main__ - Step 660 Global step 660 Train loss 3.48 on epoch=329
06/21/2022 15:07:18 - INFO - __main__ - Step 670 Global step 670 Train loss 3.48 on epoch=334
06/21/2022 15:07:20 - INFO - __main__ - Step 680 Global step 680 Train loss 3.33 on epoch=339
06/21/2022 15:07:21 - INFO - __main__ - Step 690 Global step 690 Train loss 3.23 on epoch=344
06/21/2022 15:07:23 - INFO - __main__ - Step 700 Global step 700 Train loss 3.19 on epoch=349
06/21/2022 15:07:25 - INFO - __main__ - Global step 700 Train loss 3.34 ACC 0.0 on epoch=349
06/21/2022 15:07:27 - INFO - __main__ - Step 710 Global step 710 Train loss 3.15 on epoch=354
06/21/2022 15:07:28 - INFO - __main__ - Step 720 Global step 720 Train loss 2.98 on epoch=359
06/21/2022 15:07:30 - INFO - __main__ - Step 730 Global step 730 Train loss 3.02 on epoch=364
06/21/2022 15:07:31 - INFO - __main__ - Step 740 Global step 740 Train loss 2.96 on epoch=369
06/21/2022 15:07:32 - INFO - __main__ - Step 750 Global step 750 Train loss 2.93 on epoch=374
06/21/2022 15:07:35 - INFO - __main__ - Global step 750 Train loss 3.01 ACC 0.03125 on epoch=374
06/21/2022 15:07:35 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=374, global_step=750
06/21/2022 15:07:36 - INFO - __main__ - Step 760 Global step 760 Train loss 2.77 on epoch=379
06/21/2022 15:07:37 - INFO - __main__ - Step 770 Global step 770 Train loss 2.76 on epoch=384
06/21/2022 15:07:39 - INFO - __main__ - Step 780 Global step 780 Train loss 2.64 on epoch=389
06/21/2022 15:07:40 - INFO - __main__ - Step 790 Global step 790 Train loss 2.67 on epoch=394
06/21/2022 15:07:41 - INFO - __main__ - Step 800 Global step 800 Train loss 2.55 on epoch=399
06/21/2022 15:07:48 - INFO - __main__ - Global step 800 Train loss 2.68 ACC 0.25 on epoch=399
06/21/2022 15:07:48 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.25 on epoch=399, global_step=800
06/21/2022 15:07:49 - INFO - __main__ - Step 810 Global step 810 Train loss 2.46 on epoch=404
06/21/2022 15:07:51 - INFO - __main__ - Step 820 Global step 820 Train loss 2.47 on epoch=409
06/21/2022 15:07:52 - INFO - __main__ - Step 830 Global step 830 Train loss 2.31 on epoch=414
06/21/2022 15:07:53 - INFO - __main__ - Step 840 Global step 840 Train loss 2.40 on epoch=419
06/21/2022 15:07:55 - INFO - __main__ - Step 850 Global step 850 Train loss 2.28 on epoch=424
06/21/2022 15:07:57 - INFO - __main__ - Global step 850 Train loss 2.38 ACC 0.40625 on epoch=424
06/21/2022 15:07:57 - INFO - __main__ - Saving model with best ACC: 0.25 -> 0.40625 on epoch=424, global_step=850
06/21/2022 15:07:58 - INFO - __main__ - Step 860 Global step 860 Train loss 2.16 on epoch=429
06/21/2022 15:07:59 - INFO - __main__ - Step 870 Global step 870 Train loss 2.18 on epoch=434
06/21/2022 15:08:01 - INFO - __main__ - Step 880 Global step 880 Train loss 2.12 on epoch=439
06/21/2022 15:08:02 - INFO - __main__ - Step 890 Global step 890 Train loss 2.06 on epoch=444
06/21/2022 15:08:03 - INFO - __main__ - Step 900 Global step 900 Train loss 1.99 on epoch=449
06/21/2022 15:08:05 - INFO - __main__ - Global step 900 Train loss 2.10 ACC 0.5625 on epoch=449
06/21/2022 15:08:05 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5625 on epoch=449, global_step=900
06/21/2022 15:08:06 - INFO - __main__ - Step 910 Global step 910 Train loss 1.84 on epoch=454
06/21/2022 15:08:07 - INFO - __main__ - Step 920 Global step 920 Train loss 1.91 on epoch=459
06/21/2022 15:08:09 - INFO - __main__ - Step 930 Global step 930 Train loss 1.90 on epoch=464
06/21/2022 15:08:10 - INFO - __main__ - Step 940 Global step 940 Train loss 1.80 on epoch=469
06/21/2022 15:08:11 - INFO - __main__ - Step 950 Global step 950 Train loss 1.72 on epoch=474
06/21/2022 15:08:12 - INFO - __main__ - Global step 950 Train loss 1.84 ACC 0.46875 on epoch=474
06/21/2022 15:08:14 - INFO - __main__ - Step 960 Global step 960 Train loss 1.64 on epoch=479
06/21/2022 15:08:15 - INFO - __main__ - Step 970 Global step 970 Train loss 1.61 on epoch=484
06/21/2022 15:08:16 - INFO - __main__ - Step 980 Global step 980 Train loss 1.49 on epoch=489
06/21/2022 15:08:18 - INFO - __main__ - Step 990 Global step 990 Train loss 1.56 on epoch=494
06/21/2022 15:08:19 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.44 on epoch=499
06/21/2022 15:08:20 - INFO - __main__ - Global step 1000 Train loss 1.55 ACC 0.53125 on epoch=499
06/21/2022 15:08:21 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.32 on epoch=504
06/21/2022 15:08:23 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.25 on epoch=509
06/21/2022 15:08:24 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.24 on epoch=514
06/21/2022 15:08:25 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.26 on epoch=519
06/21/2022 15:08:27 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.15 on epoch=524
06/21/2022 15:08:28 - INFO - __main__ - Global step 1050 Train loss 1.24 ACC 0.5 on epoch=524
06/21/2022 15:08:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.15 on epoch=529
06/21/2022 15:08:30 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.15 on epoch=534
06/21/2022 15:08:32 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.03 on epoch=539
06/21/2022 15:08:33 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.01 on epoch=544
06/21/2022 15:08:35 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.04 on epoch=549
06/21/2022 15:08:35 - INFO - __main__ - Global step 1100 Train loss 1.07 ACC 0.5 on epoch=549
06/21/2022 15:08:37 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.95 on epoch=554
06/21/2022 15:08:38 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.86 on epoch=559
06/21/2022 15:08:39 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.86 on epoch=564
06/21/2022 15:08:41 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.94 on epoch=569
06/21/2022 15:08:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.91 on epoch=574
06/21/2022 15:08:43 - INFO - __main__ - Global step 1150 Train loss 0.90 ACC 0.5 on epoch=574
06/21/2022 15:08:45 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.83 on epoch=579
06/21/2022 15:08:46 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.85 on epoch=584
06/21/2022 15:08:48 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.82 on epoch=589
06/21/2022 15:08:49 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.83 on epoch=594
06/21/2022 15:08:51 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.80 on epoch=599
06/21/2022 15:08:51 - INFO - __main__ - Global step 1200 Train loss 0.83 ACC 0.5 on epoch=599
06/21/2022 15:08:53 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.82 on epoch=604
06/21/2022 15:08:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.76 on epoch=609
06/21/2022 15:08:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.79 on epoch=614
06/21/2022 15:08:57 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.79 on epoch=619
06/21/2022 15:08:59 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.71 on epoch=624
06/21/2022 15:08:59 - INFO - __main__ - Global step 1250 Train loss 0.77 ACC 0.5 on epoch=624
06/21/2022 15:09:01 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.78 on epoch=629
06/21/2022 15:09:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.75 on epoch=634
06/21/2022 15:09:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.75 on epoch=639
06/21/2022 15:09:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.69 on epoch=644
06/21/2022 15:09:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.72 on epoch=649
06/21/2022 15:09:08 - INFO - __main__ - Global step 1300 Train loss 0.74 ACC 0.5 on epoch=649
06/21/2022 15:09:09 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.74 on epoch=654
06/21/2022 15:09:11 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.62 on epoch=659
06/21/2022 15:09:12 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.69 on epoch=664
06/21/2022 15:09:14 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=669
06/21/2022 15:09:15 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.68 on epoch=674
06/21/2022 15:09:16 - INFO - __main__ - Global step 1350 Train loss 0.69 ACC 0.5 on epoch=674
06/21/2022 15:09:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.65 on epoch=679
06/21/2022 15:09:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.68 on epoch=684
06/21/2022 15:09:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.59 on epoch=689
06/21/2022 15:09:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.56 on epoch=694
06/21/2022 15:09:23 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.65 on epoch=699
06/21/2022 15:09:23 - INFO - __main__ - Global step 1400 Train loss 0.63 ACC 0.5 on epoch=699
06/21/2022 15:09:25 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.53 on epoch=704
06/21/2022 15:09:26 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.58 on epoch=709
06/21/2022 15:09:28 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.59 on epoch=714
06/21/2022 15:09:29 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.59 on epoch=719
06/21/2022 15:09:30 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.57 on epoch=724
06/21/2022 15:09:31 - INFO - __main__ - Global step 1450 Train loss 0.57 ACC 0.5 on epoch=724
06/21/2022 15:09:33 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/21/2022 15:09:34 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.53 on epoch=734
06/21/2022 15:09:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.55 on epoch=739
06/21/2022 15:09:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.54 on epoch=744
06/21/2022 15:09:38 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.53 on epoch=749
06/21/2022 15:09:39 - INFO - __main__ - Global step 1500 Train loss 0.54 ACC 0.5 on epoch=749
06/21/2022 15:09:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.52 on epoch=754
06/21/2022 15:09:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.53 on epoch=759
06/21/2022 15:09:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.51 on epoch=764
06/21/2022 15:09:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
06/21/2022 15:09:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.54 on epoch=774
06/21/2022 15:09:47 - INFO - __main__ - Global step 1550 Train loss 0.51 ACC 0.53125 on epoch=774
06/21/2022 15:09:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.49 on epoch=779
06/21/2022 15:09:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=784
06/21/2022 15:09:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.49 on epoch=789
06/21/2022 15:09:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.51 on epoch=794
06/21/2022 15:09:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.47 on epoch=799
06/21/2022 15:09:55 - INFO - __main__ - Global step 1600 Train loss 0.49 ACC 0.5 on epoch=799
06/21/2022 15:09:56 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.47 on epoch=804
06/21/2022 15:09:57 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
06/21/2022 15:09:59 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.44 on epoch=814
06/21/2022 15:10:00 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.47 on epoch=819
06/21/2022 15:10:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=824
06/21/2022 15:10:02 - INFO - __main__ - Global step 1650 Train loss 0.49 ACC 0.53125 on epoch=824
06/21/2022 15:10:04 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.47 on epoch=829
06/21/2022 15:10:05 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.52 on epoch=834
06/21/2022 15:10:07 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.46 on epoch=839
06/21/2022 15:10:08 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.48 on epoch=844
06/21/2022 15:10:10 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.47 on epoch=849
06/21/2022 15:10:10 - INFO - __main__ - Global step 1700 Train loss 0.48 ACC 0.5 on epoch=849
06/21/2022 15:10:12 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.45 on epoch=854
06/21/2022 15:10:13 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.49 on epoch=859
06/21/2022 15:10:15 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.49 on epoch=864
06/21/2022 15:10:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/21/2022 15:10:17 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.45 on epoch=874
06/21/2022 15:10:18 - INFO - __main__ - Global step 1750 Train loss 0.48 ACC 0.5 on epoch=874
06/21/2022 15:10:20 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.44 on epoch=879
06/21/2022 15:10:21 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.46 on epoch=884
06/21/2022 15:10:23 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
06/21/2022 15:10:24 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.46 on epoch=894
06/21/2022 15:10:25 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.47 on epoch=899
06/21/2022 15:10:26 - INFO - __main__ - Global step 1800 Train loss 0.46 ACC 0.5 on epoch=899
06/21/2022 15:10:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.48 on epoch=904
06/21/2022 15:10:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.46 on epoch=909
06/21/2022 15:10:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
06/21/2022 15:10:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.42 on epoch=919
06/21/2022 15:10:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.41 on epoch=924
06/21/2022 15:10:34 - INFO - __main__ - Global step 1850 Train loss 0.43 ACC 0.5 on epoch=924
06/21/2022 15:10:35 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/21/2022 15:10:37 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=934
06/21/2022 15:10:38 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.49 on epoch=939
06/21/2022 15:10:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.47 on epoch=944
06/21/2022 15:10:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.45 on epoch=949
06/21/2022 15:10:42 - INFO - __main__ - Global step 1900 Train loss 0.44 ACC 0.5 on epoch=949
06/21/2022 15:10:43 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.41 on epoch=954
06/21/2022 15:10:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/21/2022 15:10:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.47 on epoch=964
06/21/2022 15:10:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
06/21/2022 15:10:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.47 on epoch=974
06/21/2022 15:10:49 - INFO - __main__ - Global step 1950 Train loss 0.42 ACC 0.5 on epoch=974
06/21/2022 15:10:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.37 on epoch=979
06/21/2022 15:10:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.39 on epoch=984
06/21/2022 15:10:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
06/21/2022 15:10:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.43 on epoch=994
06/21/2022 15:10:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.40 on epoch=999
06/21/2022 15:10:57 - INFO - __main__ - Global step 2000 Train loss 0.41 ACC 0.5 on epoch=999
06/21/2022 15:10:57 - INFO - __main__ - save last model!
06/21/2022 15:10:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 15:10:57 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 15:10:57 - INFO - __main__ - Printing 3 examples
06/21/2022 15:10:57 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 15:10:57 - INFO - __main__ - ['equivalent']
06/21/2022 15:10:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 15:10:57 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:10:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 15:10:57 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:10:57 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:10:57 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:10:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:10:58 - INFO - __main__ - Printing 3 examples
06/21/2022 15:10:58 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/21/2022 15:10:58 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:10:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/21/2022 15:10:58 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:10:58 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/21/2022 15:10:58 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:10:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 15:10:58 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:10:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:10:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:10:58 - INFO - __main__ - Printing 3 examples
06/21/2022 15:10:58 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/21/2022 15:10:58 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:10:58 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/21/2022 15:10:58 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:10:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/21/2022 15:10:58 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:10:58 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:10:58 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:10:58 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 15:10:58 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:11:03 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:11:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:11:03 - INFO - __main__ - Starting training!
06/21/2022 15:11:05 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.4_8_predictions.txt
06/21/2022 15:11:05 - INFO - __main__ - ACC on test data: 0.6814
06/21/2022 15:11:05 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.4, bsz=8, dev_performance=0.5625, test_performance=0.6813725490196079
06/21/2022 15:11:05 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.3, bsz=8 ...
06/21/2022 15:11:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:11:06 - INFO - __main__ - Printing 3 examples
06/21/2022 15:11:06 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/21/2022 15:11:06 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:11:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/21/2022 15:11:06 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:11:06 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/21/2022 15:11:06 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:11:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 15:11:06 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:11:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:11:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:11:06 - INFO - __main__ - Printing 3 examples
06/21/2022 15:11:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/21/2022 15:11:06 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:11:06 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/21/2022 15:11:06 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:11:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/21/2022 15:11:06 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:11:06 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:11:06 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:11:07 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:11:12 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:11:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:11:13 - INFO - __main__ - Starting training!
06/21/2022 15:11:14 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=4
06/21/2022 15:11:16 - INFO - __main__ - Step 20 Global step 20 Train loss 6.60 on epoch=9
06/21/2022 15:11:17 - INFO - __main__ - Step 30 Global step 30 Train loss 6.55 on epoch=14
06/21/2022 15:11:18 - INFO - __main__ - Step 40 Global step 40 Train loss 6.55 on epoch=19
06/21/2022 15:11:20 - INFO - __main__ - Step 50 Global step 50 Train loss 6.43 on epoch=24
06/21/2022 15:11:21 - INFO - __main__ - Global step 50 Train loss 6.54 ACC 0.0 on epoch=24
06/21/2022 15:11:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 15:11:23 - INFO - __main__ - Step 60 Global step 60 Train loss 6.56 on epoch=29
06/21/2022 15:11:24 - INFO - __main__ - Step 70 Global step 70 Train loss 6.51 on epoch=34
06/21/2022 15:11:25 - INFO - __main__ - Step 80 Global step 80 Train loss 6.48 on epoch=39
06/21/2022 15:11:27 - INFO - __main__ - Step 90 Global step 90 Train loss 6.41 on epoch=44
06/21/2022 15:11:28 - INFO - __main__ - Step 100 Global step 100 Train loss 6.43 on epoch=49
06/21/2022 15:11:33 - INFO - __main__ - Global step 100 Train loss 6.48 ACC 0.0 on epoch=49
06/21/2022 15:11:34 - INFO - __main__ - Step 110 Global step 110 Train loss 6.42 on epoch=54
06/21/2022 15:11:36 - INFO - __main__ - Step 120 Global step 120 Train loss 6.41 on epoch=59
06/21/2022 15:11:37 - INFO - __main__ - Step 130 Global step 130 Train loss 6.36 on epoch=64
06/21/2022 15:11:38 - INFO - __main__ - Step 140 Global step 140 Train loss 6.28 on epoch=69
06/21/2022 15:11:40 - INFO - __main__ - Step 150 Global step 150 Train loss 6.26 on epoch=74
06/21/2022 15:11:40 - INFO - __main__ - Global step 150 Train loss 6.35 ACC 0.0 on epoch=74
06/21/2022 15:11:42 - INFO - __main__ - Step 160 Global step 160 Train loss 6.26 on epoch=79
06/21/2022 15:11:43 - INFO - __main__ - Step 170 Global step 170 Train loss 6.22 on epoch=84
06/21/2022 15:11:44 - INFO - __main__ - Step 180 Global step 180 Train loss 6.19 on epoch=89
06/21/2022 15:11:46 - INFO - __main__ - Step 190 Global step 190 Train loss 6.19 on epoch=94
06/21/2022 15:11:47 - INFO - __main__ - Step 200 Global step 200 Train loss 6.24 on epoch=99
06/21/2022 15:11:50 - INFO - __main__ - Global step 200 Train loss 6.22 ACC 0.0 on epoch=99
06/21/2022 15:11:51 - INFO - __main__ - Step 210 Global step 210 Train loss 6.14 on epoch=104
06/21/2022 15:11:52 - INFO - __main__ - Step 220 Global step 220 Train loss 6.07 on epoch=109
06/21/2022 15:11:54 - INFO - __main__ - Step 230 Global step 230 Train loss 6.07 on epoch=114
06/21/2022 15:11:55 - INFO - __main__ - Step 240 Global step 240 Train loss 5.98 on epoch=119
06/21/2022 15:11:56 - INFO - __main__ - Step 250 Global step 250 Train loss 5.93 on epoch=124
06/21/2022 15:12:00 - INFO - __main__ - Global step 250 Train loss 6.04 ACC 0.0 on epoch=124
06/21/2022 15:12:01 - INFO - __main__ - Step 260 Global step 260 Train loss 5.93 on epoch=129
06/21/2022 15:12:02 - INFO - __main__ - Step 270 Global step 270 Train loss 5.91 on epoch=134
06/21/2022 15:12:04 - INFO - __main__ - Step 280 Global step 280 Train loss 5.85 on epoch=139
06/21/2022 15:12:05 - INFO - __main__ - Step 290 Global step 290 Train loss 5.87 on epoch=144
06/21/2022 15:12:07 - INFO - __main__ - Step 300 Global step 300 Train loss 5.72 on epoch=149
06/21/2022 15:12:13 - INFO - __main__ - Global step 300 Train loss 5.86 ACC 0.0 on epoch=149
06/21/2022 15:12:14 - INFO - __main__ - Step 310 Global step 310 Train loss 5.76 on epoch=154
06/21/2022 15:12:16 - INFO - __main__ - Step 320 Global step 320 Train loss 5.75 on epoch=159
06/21/2022 15:12:17 - INFO - __main__ - Step 330 Global step 330 Train loss 5.63 on epoch=164
06/21/2022 15:12:18 - INFO - __main__ - Step 340 Global step 340 Train loss 5.65 on epoch=169
06/21/2022 15:12:20 - INFO - __main__ - Step 350 Global step 350 Train loss 5.44 on epoch=174
06/21/2022 15:12:21 - INFO - __main__ - Global step 350 Train loss 5.65 ACC 0.0 on epoch=174
06/21/2022 15:12:23 - INFO - __main__ - Step 360 Global step 360 Train loss 5.55 on epoch=179
06/21/2022 15:12:24 - INFO - __main__ - Step 370 Global step 370 Train loss 5.40 on epoch=184
06/21/2022 15:12:25 - INFO - __main__ - Step 380 Global step 380 Train loss 5.42 on epoch=189
06/21/2022 15:12:27 - INFO - __main__ - Step 390 Global step 390 Train loss 5.30 on epoch=194
06/21/2022 15:12:28 - INFO - __main__ - Step 400 Global step 400 Train loss 5.28 on epoch=199
06/21/2022 15:12:30 - INFO - __main__ - Global step 400 Train loss 5.39 ACC 0.0 on epoch=199
06/21/2022 15:12:32 - INFO - __main__ - Step 410 Global step 410 Train loss 5.31 on epoch=204
06/21/2022 15:12:33 - INFO - __main__ - Step 420 Global step 420 Train loss 5.18 on epoch=209
06/21/2022 15:12:35 - INFO - __main__ - Step 430 Global step 430 Train loss 5.27 on epoch=214
06/21/2022 15:12:37 - INFO - __main__ - Step 440 Global step 440 Train loss 5.11 on epoch=219
06/21/2022 15:12:38 - INFO - __main__ - Step 450 Global step 450 Train loss 5.10 on epoch=224
06/21/2022 15:12:43 - INFO - __main__ - Global step 450 Train loss 5.19 ACC 0.0 on epoch=224
06/21/2022 15:12:45 - INFO - __main__ - Step 460 Global step 460 Train loss 5.10 on epoch=229
06/21/2022 15:12:46 - INFO - __main__ - Step 470 Global step 470 Train loss 5.01 on epoch=234
06/21/2022 15:12:48 - INFO - __main__ - Step 480 Global step 480 Train loss 5.08 on epoch=239
06/21/2022 15:12:50 - INFO - __main__ - Step 490 Global step 490 Train loss 4.88 on epoch=244
06/21/2022 15:12:51 - INFO - __main__ - Step 500 Global step 500 Train loss 4.85 on epoch=249
06/21/2022 15:12:54 - INFO - __main__ - Global step 500 Train loss 4.99 ACC 0.0 on epoch=249
06/21/2022 15:12:56 - INFO - __main__ - Step 510 Global step 510 Train loss 4.97 on epoch=254
06/21/2022 15:12:57 - INFO - __main__ - Step 520 Global step 520 Train loss 4.85 on epoch=259
06/21/2022 15:12:58 - INFO - __main__ - Step 530 Global step 530 Train loss 4.74 on epoch=264
06/21/2022 15:13:00 - INFO - __main__ - Step 540 Global step 540 Train loss 4.76 on epoch=269
06/21/2022 15:13:01 - INFO - __main__ - Step 550 Global step 550 Train loss 4.77 on epoch=274
06/21/2022 15:13:09 - INFO - __main__ - Global step 550 Train loss 4.82 ACC 0.0 on epoch=274
06/21/2022 15:13:10 - INFO - __main__ - Step 560 Global step 560 Train loss 4.69 on epoch=279
06/21/2022 15:13:12 - INFO - __main__ - Step 570 Global step 570 Train loss 4.64 on epoch=284
06/21/2022 15:13:13 - INFO - __main__ - Step 580 Global step 580 Train loss 4.59 on epoch=289
06/21/2022 15:13:15 - INFO - __main__ - Step 590 Global step 590 Train loss 4.61 on epoch=294
06/21/2022 15:13:16 - INFO - __main__ - Step 600 Global step 600 Train loss 4.56 on epoch=299
06/21/2022 15:13:20 - INFO - __main__ - Global step 600 Train loss 4.62 ACC 0.0 on epoch=299
06/21/2022 15:13:21 - INFO - __main__ - Step 610 Global step 610 Train loss 4.59 on epoch=304
06/21/2022 15:13:22 - INFO - __main__ - Step 620 Global step 620 Train loss 4.40 on epoch=309
06/21/2022 15:13:24 - INFO - __main__ - Step 630 Global step 630 Train loss 4.55 on epoch=314
06/21/2022 15:13:25 - INFO - __main__ - Step 640 Global step 640 Train loss 4.44 on epoch=319
06/21/2022 15:13:27 - INFO - __main__ - Step 650 Global step 650 Train loss 4.34 on epoch=324
06/21/2022 15:13:36 - INFO - __main__ - Global step 650 Train loss 4.46 ACC 0.0 on epoch=324
06/21/2022 15:13:37 - INFO - __main__ - Step 660 Global step 660 Train loss 4.30 on epoch=329
06/21/2022 15:13:39 - INFO - __main__ - Step 670 Global step 670 Train loss 4.32 on epoch=334
06/21/2022 15:13:40 - INFO - __main__ - Step 680 Global step 680 Train loss 4.14 on epoch=339
06/21/2022 15:13:42 - INFO - __main__ - Step 690 Global step 690 Train loss 4.20 on epoch=344
06/21/2022 15:13:43 - INFO - __main__ - Step 700 Global step 700 Train loss 4.12 on epoch=349
06/21/2022 15:13:51 - INFO - __main__ - Global step 700 Train loss 4.22 ACC 0.0 on epoch=349
06/21/2022 15:13:52 - INFO - __main__ - Step 710 Global step 710 Train loss 4.10 on epoch=354
06/21/2022 15:13:54 - INFO - __main__ - Step 720 Global step 720 Train loss 4.09 on epoch=359
06/21/2022 15:13:55 - INFO - __main__ - Step 730 Global step 730 Train loss 4.03 on epoch=364
06/21/2022 15:13:56 - INFO - __main__ - Step 740 Global step 740 Train loss 3.86 on epoch=369
06/21/2022 15:13:58 - INFO - __main__ - Step 750 Global step 750 Train loss 3.89 on epoch=374
06/21/2022 15:14:06 - INFO - __main__ - Global step 750 Train loss 3.99 ACC 0.0 on epoch=374
06/21/2022 15:14:07 - INFO - __main__ - Step 760 Global step 760 Train loss 3.89 on epoch=379
06/21/2022 15:14:08 - INFO - __main__ - Step 770 Global step 770 Train loss 3.69 on epoch=384
06/21/2022 15:14:10 - INFO - __main__ - Step 780 Global step 780 Train loss 3.65 on epoch=389
06/21/2022 15:14:11 - INFO - __main__ - Step 790 Global step 790 Train loss 3.65 on epoch=394
06/21/2022 15:14:12 - INFO - __main__ - Step 800 Global step 800 Train loss 3.47 on epoch=399
06/21/2022 15:14:21 - INFO - __main__ - Global step 800 Train loss 3.67 ACC 0.0 on epoch=399
06/21/2022 15:14:23 - INFO - __main__ - Step 810 Global step 810 Train loss 3.48 on epoch=404
06/21/2022 15:14:24 - INFO - __main__ - Step 820 Global step 820 Train loss 3.37 on epoch=409
06/21/2022 15:14:25 - INFO - __main__ - Step 830 Global step 830 Train loss 3.25 on epoch=414
06/21/2022 15:14:27 - INFO - __main__ - Step 840 Global step 840 Train loss 3.09 on epoch=419
06/21/2022 15:14:28 - INFO - __main__ - Step 850 Global step 850 Train loss 3.18 on epoch=424
06/21/2022 15:14:39 - INFO - __main__ - Global step 850 Train loss 3.27 ACC 0.0 on epoch=424
06/21/2022 15:14:40 - INFO - __main__ - Step 860 Global step 860 Train loss 3.15 on epoch=429
06/21/2022 15:14:41 - INFO - __main__ - Step 870 Global step 870 Train loss 2.92 on epoch=434
06/21/2022 15:14:43 - INFO - __main__ - Step 880 Global step 880 Train loss 3.06 on epoch=439
06/21/2022 15:14:44 - INFO - __main__ - Step 890 Global step 890 Train loss 2.97 on epoch=444
06/21/2022 15:14:45 - INFO - __main__ - Step 900 Global step 900 Train loss 2.94 on epoch=449
06/21/2022 15:14:57 - INFO - __main__ - Global step 900 Train loss 3.01 ACC 0.0 on epoch=449
06/21/2022 15:14:58 - INFO - __main__ - Step 910 Global step 910 Train loss 2.84 on epoch=454
06/21/2022 15:15:00 - INFO - __main__ - Step 920 Global step 920 Train loss 2.78 on epoch=459
06/21/2022 15:15:01 - INFO - __main__ - Step 930 Global step 930 Train loss 2.65 on epoch=464
06/21/2022 15:15:02 - INFO - __main__ - Step 940 Global step 940 Train loss 2.70 on epoch=469
06/21/2022 15:15:04 - INFO - __main__ - Step 950 Global step 950 Train loss 2.53 on epoch=474
06/21/2022 15:15:14 - INFO - __main__ - Global step 950 Train loss 2.70 ACC 0.0625 on epoch=474
06/21/2022 15:15:14 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.0625 on epoch=474, global_step=950
06/21/2022 15:15:16 - INFO - __main__ - Step 960 Global step 960 Train loss 2.48 on epoch=479
06/21/2022 15:15:17 - INFO - __main__ - Step 970 Global step 970 Train loss 2.56 on epoch=484
06/21/2022 15:15:19 - INFO - __main__ - Step 980 Global step 980 Train loss 2.38 on epoch=489
06/21/2022 15:15:20 - INFO - __main__ - Step 990 Global step 990 Train loss 2.35 on epoch=494
06/21/2022 15:15:22 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.42 on epoch=499
06/21/2022 15:15:29 - INFO - __main__ - Global step 1000 Train loss 2.44 ACC 0.3125 on epoch=499
06/21/2022 15:15:29 - INFO - __main__ - Saving model with best ACC: 0.0625 -> 0.3125 on epoch=499, global_step=1000
06/21/2022 15:15:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.47 on epoch=504
06/21/2022 15:15:31 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.34 on epoch=509
06/21/2022 15:15:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.16 on epoch=514
06/21/2022 15:15:34 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.14 on epoch=519
06/21/2022 15:15:35 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.18 on epoch=524
06/21/2022 15:15:36 - INFO - __main__ - Global step 1050 Train loss 2.26 ACC 0.40625 on epoch=524
06/21/2022 15:15:37 - INFO - __main__ - Saving model with best ACC: 0.3125 -> 0.40625 on epoch=524, global_step=1050
06/21/2022 15:15:38 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.07 on epoch=529
06/21/2022 15:15:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.01 on epoch=534
06/21/2022 15:15:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.92 on epoch=539
06/21/2022 15:15:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.07 on epoch=544
06/21/2022 15:15:43 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.06 on epoch=549
06/21/2022 15:15:44 - INFO - __main__ - Global step 1100 Train loss 2.03 ACC 0.5 on epoch=549
06/21/2022 15:15:44 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.5 on epoch=549, global_step=1100
06/21/2022 15:15:46 - INFO - __main__ - Step 1110 Global step 1110 Train loss 2.02 on epoch=554
06/21/2022 15:15:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.94 on epoch=559
06/21/2022 15:15:48 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.83 on epoch=564
06/21/2022 15:15:50 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.93 on epoch=569
06/21/2022 15:15:51 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.89 on epoch=574
06/21/2022 15:15:52 - INFO - __main__ - Global step 1150 Train loss 1.92 ACC 0.5 on epoch=574
06/21/2022 15:15:53 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.98 on epoch=579
06/21/2022 15:15:54 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.83 on epoch=584
06/21/2022 15:15:56 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.78 on epoch=589
06/21/2022 15:15:57 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.67 on epoch=594
06/21/2022 15:15:59 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.88 on epoch=599
06/21/2022 15:15:59 - INFO - __main__ - Global step 1200 Train loss 1.83 ACC 0.5 on epoch=599
06/21/2022 15:16:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.77 on epoch=604
06/21/2022 15:16:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.76 on epoch=609
06/21/2022 15:16:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.70 on epoch=614
06/21/2022 15:16:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.79 on epoch=619
06/21/2022 15:16:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.48 on epoch=624
06/21/2022 15:16:07 - INFO - __main__ - Global step 1250 Train loss 1.70 ACC 0.5 on epoch=624
06/21/2022 15:16:08 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.68 on epoch=629
06/21/2022 15:16:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.47 on epoch=634
06/21/2022 15:16:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.62 on epoch=639
06/21/2022 15:16:12 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.51 on epoch=644
06/21/2022 15:16:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.47 on epoch=649
06/21/2022 15:16:14 - INFO - __main__ - Global step 1300 Train loss 1.55 ACC 0.5 on epoch=649
06/21/2022 15:16:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.43 on epoch=654
06/21/2022 15:16:17 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.43 on epoch=659
06/21/2022 15:16:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.48 on epoch=664
06/21/2022 15:16:20 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.42 on epoch=669
06/21/2022 15:16:21 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.44 on epoch=674
06/21/2022 15:16:22 - INFO - __main__ - Global step 1350 Train loss 1.44 ACC 0.5 on epoch=674
06/21/2022 15:16:23 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.42 on epoch=679
06/21/2022 15:16:24 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.43 on epoch=684
06/21/2022 15:16:26 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.47 on epoch=689
06/21/2022 15:16:27 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.24 on epoch=694
06/21/2022 15:16:28 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.32 on epoch=699
06/21/2022 15:16:29 - INFO - __main__ - Global step 1400 Train loss 1.38 ACC 0.5 on epoch=699
06/21/2022 15:16:31 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.36 on epoch=704
06/21/2022 15:16:32 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.24 on epoch=709
06/21/2022 15:16:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.29 on epoch=714
06/21/2022 15:16:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.31 on epoch=719
06/21/2022 15:16:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.20 on epoch=724
06/21/2022 15:16:37 - INFO - __main__ - Global step 1450 Train loss 1.28 ACC 0.5 on epoch=724
06/21/2022 15:16:39 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.16 on epoch=729
06/21/2022 15:16:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.18 on epoch=734
06/21/2022 15:16:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 1.14 on epoch=739
06/21/2022 15:16:43 - INFO - __main__ - Step 1490 Global step 1490 Train loss 1.17 on epoch=744
06/21/2022 15:16:44 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.14 on epoch=749
06/21/2022 15:16:45 - INFO - __main__ - Global step 1500 Train loss 1.16 ACC 0.5 on epoch=749
06/21/2022 15:16:47 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.13 on epoch=754
06/21/2022 15:16:48 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.17 on epoch=759
06/21/2022 15:16:49 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.11 on epoch=764
06/21/2022 15:16:51 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.13 on epoch=769
06/21/2022 15:16:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.05 on epoch=774
06/21/2022 15:16:53 - INFO - __main__ - Global step 1550 Train loss 1.12 ACC 0.5 on epoch=774
06/21/2022 15:16:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.01 on epoch=779
06/21/2022 15:16:56 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.95 on epoch=784
06/21/2022 15:16:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.04 on epoch=789
06/21/2022 15:16:58 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.95 on epoch=794
06/21/2022 15:17:00 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.95 on epoch=799
06/21/2022 15:17:01 - INFO - __main__ - Global step 1600 Train loss 0.98 ACC 0.5 on epoch=799
06/21/2022 15:17:02 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.98 on epoch=804
06/21/2022 15:17:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.92 on epoch=809
06/21/2022 15:17:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.88 on epoch=814
06/21/2022 15:17:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.98 on epoch=819
06/21/2022 15:17:08 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.86 on epoch=824
06/21/2022 15:17:08 - INFO - __main__ - Global step 1650 Train loss 0.92 ACC 0.5 on epoch=824
06/21/2022 15:17:10 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.95 on epoch=829
06/21/2022 15:17:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.90 on epoch=834
06/21/2022 15:17:13 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.88 on epoch=839
06/21/2022 15:17:14 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.83 on epoch=844
06/21/2022 15:17:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.83 on epoch=849
06/21/2022 15:17:16 - INFO - __main__ - Global step 1700 Train loss 0.88 ACC 0.5 on epoch=849
06/21/2022 15:17:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.85 on epoch=854
06/21/2022 15:17:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.85 on epoch=859
06/21/2022 15:17:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.82 on epoch=864
06/21/2022 15:17:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.86 on epoch=869
06/21/2022 15:17:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.82 on epoch=874
06/21/2022 15:17:23 - INFO - __main__ - Global step 1750 Train loss 0.84 ACC 0.5 on epoch=874
06/21/2022 15:17:25 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.79 on epoch=879
06/21/2022 15:17:26 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.76 on epoch=884
06/21/2022 15:17:27 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.76 on epoch=889
06/21/2022 15:17:29 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.83 on epoch=894
06/21/2022 15:17:30 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.75 on epoch=899
06/21/2022 15:17:30 - INFO - __main__ - Global step 1800 Train loss 0.78 ACC 0.5 on epoch=899
06/21/2022 15:17:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.77 on epoch=904
06/21/2022 15:17:33 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.75 on epoch=909
06/21/2022 15:17:34 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.72 on epoch=914
06/21/2022 15:17:36 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.77 on epoch=919
06/21/2022 15:17:37 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.80 on epoch=924
06/21/2022 15:17:38 - INFO - __main__ - Global step 1850 Train loss 0.76 ACC 0.5 on epoch=924
06/21/2022 15:17:39 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.73 on epoch=929
06/21/2022 15:17:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.75 on epoch=934
06/21/2022 15:17:42 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.77 on epoch=939
06/21/2022 15:17:43 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.73 on epoch=944
06/21/2022 15:17:44 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.80 on epoch=949
06/21/2022 15:17:45 - INFO - __main__ - Global step 1900 Train loss 0.75 ACC 0.5 on epoch=949
06/21/2022 15:17:46 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.73 on epoch=954
06/21/2022 15:17:48 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.57 on epoch=959
06/21/2022 15:17:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.65 on epoch=964
06/21/2022 15:17:50 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.71 on epoch=969
06/21/2022 15:17:52 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.61 on epoch=974
06/21/2022 15:17:52 - INFO - __main__ - Global step 1950 Train loss 0.65 ACC 0.5 on epoch=974
06/21/2022 15:17:54 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.56 on epoch=979
06/21/2022 15:17:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.62 on epoch=984
06/21/2022 15:17:56 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.68 on epoch=989
06/21/2022 15:17:58 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.58 on epoch=994
06/21/2022 15:17:59 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.62 on epoch=999
06/21/2022 15:18:00 - INFO - __main__ - Global step 2000 Train loss 0.61 ACC 0.5 on epoch=999
06/21/2022 15:18:00 - INFO - __main__ - save last model!
06/21/2022 15:18:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 15:18:00 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 15:18:00 - INFO - __main__ - Printing 3 examples
06/21/2022 15:18:00 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 15:18:00 - INFO - __main__ - ['equivalent']
06/21/2022 15:18:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 15:18:00 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:00 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 15:18:00 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:00 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:18:00 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:18:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:18:00 - INFO - __main__ - Printing 3 examples
06/21/2022 15:18:00 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/21/2022 15:18:00 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/21/2022 15:18:00 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:00 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/21/2022 15:18:00 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:00 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 15:18:00 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:18:00 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:18:00 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:18:00 - INFO - __main__ - Printing 3 examples
06/21/2022 15:18:00 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/21/2022 15:18:00 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:00 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/21/2022 15:18:00 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:00 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/21/2022 15:18:00 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:00 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:18:00 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:18:00 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:18:01 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 15:18:06 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:18:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:18:07 - INFO - __main__ - Starting training!
06/21/2022 15:18:09 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.3_8_predictions.txt
06/21/2022 15:18:09 - INFO - __main__ - ACC on test data: 0.6838
06/21/2022 15:18:09 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.3, bsz=8, dev_performance=0.5, test_performance=0.6838235294117647
06/21/2022 15:18:09 - INFO - __main__ - Running ... prefix=glue-mrpc_16_100, lr=0.2, bsz=8 ...
06/21/2022 15:18:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:18:10 - INFO - __main__ - Printing 3 examples
06/21/2022 15:18:10 - INFO - __main__ -  [glue-mrpc] sentence 1: In court papers filed Tuesday , Lee asked for an injunction against Viacom 's use of the name , saying he had never given his consent for it to be used . [SEP] sentence 2: In papers filed Tuesday in Manhattan 's state Supreme Court , Lee asked for an injunction against Viacom 's use of the name Spike for TNN .
06/21/2022 15:18:10 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Lt. Scotty Smither , a county firefighter , was struck by lightning . [SEP] sentence 2: A county firefighter , was struck by lightning and was in stable condition at Frankfort Regional Medical Center .
06/21/2022 15:18:10 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:10 - INFO - __main__ -  [glue-mrpc] sentence 1: " These are defining moments for players and organizations , " Anaheim coach Mike Babcock said . [SEP] sentence 2: " There are defining moments for players and organizations where you are measured .
06/21/2022 15:18:10 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 15:18:10 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:18:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:18:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:18:10 - INFO - __main__ - Printing 3 examples
06/21/2022 15:18:10 - INFO - __main__ -  [glue-mrpc] sentence 1: The hall will be home to the Los Angeles Philharmonic and the LA Master Chorale . [SEP] sentence 2: The Los Angeles Master Chorale and the Los Angeles Philharmonic Brass Ensemble also performed .
06/21/2022 15:18:10 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:10 - INFO - __main__ -  [glue-mrpc] sentence 1: She appeared in federal court there Monday and was expected to be transferred to Houston in two weeks . [SEP] sentence 2: Holloway surrendered in Cleveland on Friday and was expected to be transferred to Houston in two weeks .
06/21/2022 15:18:10 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Against the Swiss franc CHF = , the dollar was at 1.3172 francs , down 0.60 percent . [SEP] sentence 2: Against the yen the dollar was down 0.7 percent at 110.73 yen .
06/21/2022 15:18:10 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:18:10 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:18:10 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:18:10 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:18:16 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:18:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:18:16 - INFO - __main__ - Starting training!
06/21/2022 15:18:18 - INFO - __main__ - Step 10 Global step 10 Train loss 6.58 on epoch=4
06/21/2022 15:18:19 - INFO - __main__ - Step 20 Global step 20 Train loss 6.51 on epoch=9
06/21/2022 15:18:21 - INFO - __main__ - Step 30 Global step 30 Train loss 6.44 on epoch=14
06/21/2022 15:18:22 - INFO - __main__ - Step 40 Global step 40 Train loss 6.53 on epoch=19
06/21/2022 15:18:24 - INFO - __main__ - Step 50 Global step 50 Train loss 6.63 on epoch=24
06/21/2022 15:18:25 - INFO - __main__ - Global step 50 Train loss 6.54 ACC 0.0 on epoch=24
06/21/2022 15:18:25 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 15:18:26 - INFO - __main__ - Step 60 Global step 60 Train loss 6.55 on epoch=29
06/21/2022 15:18:28 - INFO - __main__ - Step 70 Global step 70 Train loss 6.51 on epoch=34
06/21/2022 15:18:29 - INFO - __main__ - Step 80 Global step 80 Train loss 6.57 on epoch=39
06/21/2022 15:18:30 - INFO - __main__ - Step 90 Global step 90 Train loss 6.51 on epoch=44
06/21/2022 15:18:32 - INFO - __main__ - Step 100 Global step 100 Train loss 6.51 on epoch=49
06/21/2022 15:18:33 - INFO - __main__ - Global step 100 Train loss 6.53 ACC 0.0 on epoch=49
06/21/2022 15:18:34 - INFO - __main__ - Step 110 Global step 110 Train loss 6.48 on epoch=54
06/21/2022 15:18:36 - INFO - __main__ - Step 120 Global step 120 Train loss 6.46 on epoch=59
06/21/2022 15:18:37 - INFO - __main__ - Step 130 Global step 130 Train loss 6.49 on epoch=64
06/21/2022 15:18:39 - INFO - __main__ - Step 140 Global step 140 Train loss 6.46 on epoch=69
06/21/2022 15:18:40 - INFO - __main__ - Step 150 Global step 150 Train loss 6.46 on epoch=74
06/21/2022 15:18:41 - INFO - __main__ - Global step 150 Train loss 6.47 ACC 0.0 on epoch=74
06/21/2022 15:18:43 - INFO - __main__ - Step 160 Global step 160 Train loss 6.44 on epoch=79
06/21/2022 15:18:44 - INFO - __main__ - Step 170 Global step 170 Train loss 6.40 on epoch=84
06/21/2022 15:18:46 - INFO - __main__ - Step 180 Global step 180 Train loss 6.44 on epoch=89
06/21/2022 15:18:47 - INFO - __main__ - Step 190 Global step 190 Train loss 6.35 on epoch=94
06/21/2022 15:18:48 - INFO - __main__ - Step 200 Global step 200 Train loss 6.35 on epoch=99
06/21/2022 15:18:49 - INFO - __main__ - Global step 200 Train loss 6.39 ACC 0.0 on epoch=99
06/21/2022 15:18:51 - INFO - __main__ - Step 210 Global step 210 Train loss 6.37 on epoch=104
06/21/2022 15:18:52 - INFO - __main__ - Step 220 Global step 220 Train loss 6.38 on epoch=109
06/21/2022 15:18:53 - INFO - __main__ - Step 230 Global step 230 Train loss 6.37 on epoch=114
06/21/2022 15:18:55 - INFO - __main__ - Step 240 Global step 240 Train loss 6.32 on epoch=119
06/21/2022 15:18:56 - INFO - __main__ - Step 250 Global step 250 Train loss 6.29 on epoch=124
06/21/2022 15:19:06 - INFO - __main__ - Global step 250 Train loss 6.35 ACC 0.0 on epoch=124
06/21/2022 15:19:08 - INFO - __main__ - Step 260 Global step 260 Train loss 6.23 on epoch=129
06/21/2022 15:19:09 - INFO - __main__ - Step 270 Global step 270 Train loss 6.26 on epoch=134
06/21/2022 15:19:11 - INFO - __main__ - Step 280 Global step 280 Train loss 6.14 on epoch=139
06/21/2022 15:19:12 - INFO - __main__ - Step 290 Global step 290 Train loss 6.19 on epoch=144
06/21/2022 15:19:14 - INFO - __main__ - Step 300 Global step 300 Train loss 6.09 on epoch=149
06/21/2022 15:19:16 - INFO - __main__ - Global step 300 Train loss 6.18 ACC 0.0 on epoch=149
06/21/2022 15:19:17 - INFO - __main__ - Step 310 Global step 310 Train loss 6.16 on epoch=154
06/21/2022 15:19:19 - INFO - __main__ - Step 320 Global step 320 Train loss 6.10 on epoch=159
06/21/2022 15:19:20 - INFO - __main__ - Step 330 Global step 330 Train loss 6.04 on epoch=164
06/21/2022 15:19:22 - INFO - __main__ - Step 340 Global step 340 Train loss 5.99 on epoch=169
06/21/2022 15:19:23 - INFO - __main__ - Step 350 Global step 350 Train loss 5.96 on epoch=174
06/21/2022 15:19:26 - INFO - __main__ - Global step 350 Train loss 6.05 ACC 0.0 on epoch=174
06/21/2022 15:19:27 - INFO - __main__ - Step 360 Global step 360 Train loss 5.96 on epoch=179
06/21/2022 15:19:29 - INFO - __main__ - Step 370 Global step 370 Train loss 5.99 on epoch=184
06/21/2022 15:19:30 - INFO - __main__ - Step 380 Global step 380 Train loss 5.95 on epoch=189
06/21/2022 15:19:32 - INFO - __main__ - Step 390 Global step 390 Train loss 5.85 on epoch=194
06/21/2022 15:19:33 - INFO - __main__ - Step 400 Global step 400 Train loss 5.78 on epoch=199
06/21/2022 15:19:39 - INFO - __main__ - Global step 400 Train loss 5.91 ACC 0.0 on epoch=199
06/21/2022 15:19:41 - INFO - __main__ - Step 410 Global step 410 Train loss 5.77 on epoch=204
06/21/2022 15:19:42 - INFO - __main__ - Step 420 Global step 420 Train loss 5.86 on epoch=209
06/21/2022 15:19:43 - INFO - __main__ - Step 430 Global step 430 Train loss 5.78 on epoch=214
06/21/2022 15:19:45 - INFO - __main__ - Step 440 Global step 440 Train loss 5.87 on epoch=219
06/21/2022 15:19:46 - INFO - __main__ - Step 450 Global step 450 Train loss 5.79 on epoch=224
06/21/2022 15:19:48 - INFO - __main__ - Global step 450 Train loss 5.81 ACC 0.0 on epoch=224
06/21/2022 15:19:50 - INFO - __main__ - Step 460 Global step 460 Train loss 5.69 on epoch=229
06/21/2022 15:19:51 - INFO - __main__ - Step 470 Global step 470 Train loss 5.64 on epoch=234
06/21/2022 15:19:53 - INFO - __main__ - Step 480 Global step 480 Train loss 5.69 on epoch=239
06/21/2022 15:19:54 - INFO - __main__ - Step 490 Global step 490 Train loss 5.61 on epoch=244
06/21/2022 15:19:55 - INFO - __main__ - Step 500 Global step 500 Train loss 5.66 on epoch=249
06/21/2022 15:20:03 - INFO - __main__ - Global step 500 Train loss 5.66 ACC 0.0 on epoch=249
06/21/2022 15:20:04 - INFO - __main__ - Step 510 Global step 510 Train loss 5.59 on epoch=254
06/21/2022 15:20:05 - INFO - __main__ - Step 520 Global step 520 Train loss 5.60 on epoch=259
06/21/2022 15:20:07 - INFO - __main__ - Step 530 Global step 530 Train loss 5.63 on epoch=264
06/21/2022 15:20:08 - INFO - __main__ - Step 540 Global step 540 Train loss 5.72 on epoch=269
06/21/2022 15:20:10 - INFO - __main__ - Step 550 Global step 550 Train loss 5.79 on epoch=274
06/21/2022 15:20:20 - INFO - __main__ - Global step 550 Train loss 5.67 ACC 0.0 on epoch=274
06/21/2022 15:20:21 - INFO - __main__ - Step 560 Global step 560 Train loss 5.70 on epoch=279
06/21/2022 15:20:23 - INFO - __main__ - Step 570 Global step 570 Train loss 5.68 on epoch=284
06/21/2022 15:20:24 - INFO - __main__ - Step 580 Global step 580 Train loss 5.70 on epoch=289
06/21/2022 15:20:26 - INFO - __main__ - Step 590 Global step 590 Train loss 5.64 on epoch=294
06/21/2022 15:20:27 - INFO - __main__ - Step 600 Global step 600 Train loss 5.52 on epoch=299
06/21/2022 15:20:31 - INFO - __main__ - Global step 600 Train loss 5.65 ACC 0.0 on epoch=299
06/21/2022 15:20:32 - INFO - __main__ - Step 610 Global step 610 Train loss 5.69 on epoch=304
06/21/2022 15:20:34 - INFO - __main__ - Step 620 Global step 620 Train loss 5.72 on epoch=309
06/21/2022 15:20:35 - INFO - __main__ - Step 630 Global step 630 Train loss 5.66 on epoch=314
06/21/2022 15:20:37 - INFO - __main__ - Step 640 Global step 640 Train loss 5.64 on epoch=319
06/21/2022 15:20:38 - INFO - __main__ - Step 650 Global step 650 Train loss 5.48 on epoch=324
06/21/2022 15:20:40 - INFO - __main__ - Global step 650 Train loss 5.64 ACC 0.0 on epoch=324
06/21/2022 15:20:41 - INFO - __main__ - Step 660 Global step 660 Train loss 5.52 on epoch=329
06/21/2022 15:20:43 - INFO - __main__ - Step 670 Global step 670 Train loss 5.61 on epoch=334
06/21/2022 15:20:45 - INFO - __main__ - Step 680 Global step 680 Train loss 5.59 on epoch=339
06/21/2022 15:20:46 - INFO - __main__ - Step 690 Global step 690 Train loss 5.54 on epoch=344
06/21/2022 15:20:48 - INFO - __main__ - Step 700 Global step 700 Train loss 5.49 on epoch=349
06/21/2022 15:20:51 - INFO - __main__ - Global step 700 Train loss 5.55 ACC 0.0 on epoch=349
06/21/2022 15:20:52 - INFO - __main__ - Step 710 Global step 710 Train loss 5.43 on epoch=354
06/21/2022 15:20:54 - INFO - __main__ - Step 720 Global step 720 Train loss 5.49 on epoch=359
06/21/2022 15:20:55 - INFO - __main__ - Step 730 Global step 730 Train loss 5.49 on epoch=364
06/21/2022 15:20:57 - INFO - __main__ - Step 740 Global step 740 Train loss 5.43 on epoch=369
06/21/2022 15:20:58 - INFO - __main__ - Step 750 Global step 750 Train loss 5.40 on epoch=374
06/21/2022 15:21:05 - INFO - __main__ - Global step 750 Train loss 5.45 ACC 0.0 on epoch=374
06/21/2022 15:21:06 - INFO - __main__ - Step 760 Global step 760 Train loss 5.37 on epoch=379
06/21/2022 15:21:08 - INFO - __main__ - Step 770 Global step 770 Train loss 5.38 on epoch=384
06/21/2022 15:21:09 - INFO - __main__ - Step 780 Global step 780 Train loss 5.40 on epoch=389
06/21/2022 15:21:11 - INFO - __main__ - Step 790 Global step 790 Train loss 5.36 on epoch=394
06/21/2022 15:21:12 - INFO - __main__ - Step 800 Global step 800 Train loss 5.34 on epoch=399
06/21/2022 15:21:18 - INFO - __main__ - Global step 800 Train loss 5.37 ACC 0.0 on epoch=399
06/21/2022 15:21:20 - INFO - __main__ - Step 810 Global step 810 Train loss 5.29 on epoch=404
06/21/2022 15:21:21 - INFO - __main__ - Step 820 Global step 820 Train loss 5.30 on epoch=409
06/21/2022 15:21:23 - INFO - __main__ - Step 830 Global step 830 Train loss 5.28 on epoch=414
06/21/2022 15:21:24 - INFO - __main__ - Step 840 Global step 840 Train loss 5.33 on epoch=419
06/21/2022 15:21:26 - INFO - __main__ - Step 850 Global step 850 Train loss 5.33 on epoch=424
06/21/2022 15:21:28 - INFO - __main__ - Global step 850 Train loss 5.31 ACC 0.0 on epoch=424
06/21/2022 15:21:29 - INFO - __main__ - Step 860 Global step 860 Train loss 5.31 on epoch=429
06/21/2022 15:21:31 - INFO - __main__ - Step 870 Global step 870 Train loss 5.41 on epoch=434
06/21/2022 15:21:32 - INFO - __main__ - Step 880 Global step 880 Train loss 5.26 on epoch=439
06/21/2022 15:21:33 - INFO - __main__ - Step 890 Global step 890 Train loss 5.29 on epoch=444
06/21/2022 15:21:35 - INFO - __main__ - Step 900 Global step 900 Train loss 5.20 on epoch=449
06/21/2022 15:21:41 - INFO - __main__ - Global step 900 Train loss 5.29 ACC 0.0 on epoch=449
06/21/2022 15:21:42 - INFO - __main__ - Step 910 Global step 910 Train loss 5.31 on epoch=454
06/21/2022 15:21:43 - INFO - __main__ - Step 920 Global step 920 Train loss 5.25 on epoch=459
06/21/2022 15:21:45 - INFO - __main__ - Step 930 Global step 930 Train loss 5.12 on epoch=464
06/21/2022 15:21:46 - INFO - __main__ - Step 940 Global step 940 Train loss 5.26 on epoch=469
06/21/2022 15:21:48 - INFO - __main__ - Step 950 Global step 950 Train loss 5.15 on epoch=474
06/21/2022 15:21:52 - INFO - __main__ - Global step 950 Train loss 5.22 ACC 0.0 on epoch=474
06/21/2022 15:21:53 - INFO - __main__ - Step 960 Global step 960 Train loss 5.25 on epoch=479
06/21/2022 15:21:55 - INFO - __main__ - Step 970 Global step 970 Train loss 5.15 on epoch=484
06/21/2022 15:21:56 - INFO - __main__ - Step 980 Global step 980 Train loss 5.20 on epoch=489
06/21/2022 15:21:58 - INFO - __main__ - Step 990 Global step 990 Train loss 5.04 on epoch=494
06/21/2022 15:22:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 5.17 on epoch=499
06/21/2022 15:22:05 - INFO - __main__ - Global step 1000 Train loss 5.16 ACC 0.0 on epoch=499
06/21/2022 15:22:06 - INFO - __main__ - Step 1010 Global step 1010 Train loss 5.12 on epoch=504
06/21/2022 15:22:08 - INFO - __main__ - Step 1020 Global step 1020 Train loss 5.10 on epoch=509
06/21/2022 15:22:09 - INFO - __main__ - Step 1030 Global step 1030 Train loss 5.03 on epoch=514
06/21/2022 15:22:10 - INFO - __main__ - Step 1040 Global step 1040 Train loss 5.06 on epoch=519
06/21/2022 15:22:12 - INFO - __main__ - Step 1050 Global step 1050 Train loss 5.05 on epoch=524
06/21/2022 15:22:19 - INFO - __main__ - Global step 1050 Train loss 5.07 ACC 0.0 on epoch=524
06/21/2022 15:22:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.94 on epoch=529
06/21/2022 15:22:22 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.92 on epoch=534
06/21/2022 15:22:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.82 on epoch=539
06/21/2022 15:22:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 5.06 on epoch=544
06/21/2022 15:22:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.96 on epoch=549
06/21/2022 15:22:33 - INFO - __main__ - Global step 1100 Train loss 4.94 ACC 0.0 on epoch=549
06/21/2022 15:22:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 5.01 on epoch=554
06/21/2022 15:22:35 - INFO - __main__ - Step 1120 Global step 1120 Train loss 4.94 on epoch=559
06/21/2022 15:22:37 - INFO - __main__ - Step 1130 Global step 1130 Train loss 4.84 on epoch=564
06/21/2022 15:22:38 - INFO - __main__ - Step 1140 Global step 1140 Train loss 4.85 on epoch=569
06/21/2022 15:22:40 - INFO - __main__ - Step 1150 Global step 1150 Train loss 4.81 on epoch=574
06/21/2022 15:22:46 - INFO - __main__ - Global step 1150 Train loss 4.89 ACC 0.0 on epoch=574
06/21/2022 15:22:47 - INFO - __main__ - Step 1160 Global step 1160 Train loss 4.88 on epoch=579
06/21/2022 15:22:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 4.80 on epoch=584
06/21/2022 15:22:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 4.81 on epoch=589
06/21/2022 15:22:51 - INFO - __main__ - Step 1190 Global step 1190 Train loss 4.80 on epoch=594
06/21/2022 15:22:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 4.69 on epoch=599
06/21/2022 15:22:59 - INFO - __main__ - Global step 1200 Train loss 4.80 ACC 0.0 on epoch=599
06/21/2022 15:23:01 - INFO - __main__ - Step 1210 Global step 1210 Train loss 4.61 on epoch=604
06/21/2022 15:23:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 4.63 on epoch=609
06/21/2022 15:23:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 4.61 on epoch=614
06/21/2022 15:23:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 4.67 on epoch=619
06/21/2022 15:23:06 - INFO - __main__ - Step 1250 Global step 1250 Train loss 4.56 on epoch=624
06/21/2022 15:23:08 - INFO - __main__ - Global step 1250 Train loss 4.62 ACC 0.0 on epoch=624
06/21/2022 15:23:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 4.55 on epoch=629
06/21/2022 15:23:10 - INFO - __main__ - Step 1270 Global step 1270 Train loss 4.38 on epoch=634
06/21/2022 15:23:12 - INFO - __main__ - Step 1280 Global step 1280 Train loss 4.34 on epoch=639
06/21/2022 15:23:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 4.35 on epoch=644
06/21/2022 15:23:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 4.22 on epoch=649
06/21/2022 15:23:17 - INFO - __main__ - Global step 1300 Train loss 4.37 ACC 0.0 on epoch=649
06/21/2022 15:23:18 - INFO - __main__ - Step 1310 Global step 1310 Train loss 4.40 on epoch=654
06/21/2022 15:23:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 4.22 on epoch=659
06/21/2022 15:23:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 4.20 on epoch=664
06/21/2022 15:23:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 4.17 on epoch=669
06/21/2022 15:23:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 4.09 on epoch=674
06/21/2022 15:23:33 - INFO - __main__ - Global step 1350 Train loss 4.22 ACC 0.0 on epoch=674
06/21/2022 15:23:34 - INFO - __main__ - Step 1360 Global step 1360 Train loss 4.10 on epoch=679
06/21/2022 15:23:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 4.13 on epoch=684
06/21/2022 15:23:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 4.02 on epoch=689
06/21/2022 15:23:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 4.06 on epoch=694
06/21/2022 15:23:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 3.98 on epoch=699
06/21/2022 15:23:42 - INFO - __main__ - Global step 1400 Train loss 4.06 ACC 0.0 on epoch=699
06/21/2022 15:23:43 - INFO - __main__ - Step 1410 Global step 1410 Train loss 3.88 on epoch=704
06/21/2022 15:23:45 - INFO - __main__ - Step 1420 Global step 1420 Train loss 3.89 on epoch=709
06/21/2022 15:23:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 3.95 on epoch=714
06/21/2022 15:23:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 3.84 on epoch=719
06/21/2022 15:23:49 - INFO - __main__ - Step 1450 Global step 1450 Train loss 3.83 on epoch=724
06/21/2022 15:23:51 - INFO - __main__ - Global step 1450 Train loss 3.88 ACC 0.0 on epoch=724
06/21/2022 15:23:52 - INFO - __main__ - Step 1460 Global step 1460 Train loss 3.83 on epoch=729
06/21/2022 15:23:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 3.70 on epoch=734
06/21/2022 15:23:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 3.78 on epoch=739
06/21/2022 15:23:56 - INFO - __main__ - Step 1490 Global step 1490 Train loss 3.74 on epoch=744
06/21/2022 15:23:58 - INFO - __main__ - Step 1500 Global step 1500 Train loss 3.67 on epoch=749
06/21/2022 15:23:59 - INFO - __main__ - Global step 1500 Train loss 3.74 ACC 0.0 on epoch=749
06/21/2022 15:24:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 3.63 on epoch=754
06/21/2022 15:24:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 3.75 on epoch=759
06/21/2022 15:24:03 - INFO - __main__ - Step 1530 Global step 1530 Train loss 3.54 on epoch=764
06/21/2022 15:24:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 3.47 on epoch=769
06/21/2022 15:24:06 - INFO - __main__ - Step 1550 Global step 1550 Train loss 3.60 on epoch=774
06/21/2022 15:24:16 - INFO - __main__ - Global step 1550 Train loss 3.60 ACC 0.03125 on epoch=774
06/21/2022 15:24:16 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=774, global_step=1550
06/21/2022 15:24:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 3.60 on epoch=779
06/21/2022 15:24:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.53 on epoch=784
06/21/2022 15:24:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 3.57 on epoch=789
06/21/2022 15:24:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 3.55 on epoch=794
06/21/2022 15:24:23 - INFO - __main__ - Step 1600 Global step 1600 Train loss 3.48 on epoch=799
06/21/2022 15:24:34 - INFO - __main__ - Global step 1600 Train loss 3.55 ACC 0.03125 on epoch=799
06/21/2022 15:24:35 - INFO - __main__ - Step 1610 Global step 1610 Train loss 3.44 on epoch=804
06/21/2022 15:24:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 3.44 on epoch=809
06/21/2022 15:24:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 3.46 on epoch=814
06/21/2022 15:24:39 - INFO - __main__ - Step 1640 Global step 1640 Train loss 3.39 on epoch=819
06/21/2022 15:24:41 - INFO - __main__ - Step 1650 Global step 1650 Train loss 3.31 on epoch=824
06/21/2022 15:24:48 - INFO - __main__ - Global step 1650 Train loss 3.41 ACC 0.0 on epoch=824
06/21/2022 15:24:50 - INFO - __main__ - Step 1660 Global step 1660 Train loss 3.27 on epoch=829
06/21/2022 15:24:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 3.24 on epoch=834
06/21/2022 15:24:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 3.20 on epoch=839
06/21/2022 15:24:54 - INFO - __main__ - Step 1690 Global step 1690 Train loss 3.18 on epoch=844
06/21/2022 15:24:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 3.24 on epoch=849
06/21/2022 15:25:06 - INFO - __main__ - Global step 1700 Train loss 3.23 ACC 0.28125 on epoch=849
06/21/2022 15:25:06 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.28125 on epoch=849, global_step=1700
06/21/2022 15:25:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 3.16 on epoch=854
06/21/2022 15:25:09 - INFO - __main__ - Step 1720 Global step 1720 Train loss 3.12 on epoch=859
06/21/2022 15:25:10 - INFO - __main__ - Step 1730 Global step 1730 Train loss 3.12 on epoch=864
06/21/2022 15:25:12 - INFO - __main__ - Step 1740 Global step 1740 Train loss 3.05 on epoch=869
06/21/2022 15:25:13 - INFO - __main__ - Step 1750 Global step 1750 Train loss 3.01 on epoch=874
06/21/2022 15:25:17 - INFO - __main__ - Global step 1750 Train loss 3.09 ACC 0.4375 on epoch=874
06/21/2022 15:25:17 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.4375 on epoch=874, global_step=1750
06/21/2022 15:25:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 2.92 on epoch=879
06/21/2022 15:25:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 2.88 on epoch=884
06/21/2022 15:25:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 2.85 on epoch=889
06/21/2022 15:25:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 2.85 on epoch=894
06/21/2022 15:25:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 2.74 on epoch=899
06/21/2022 15:25:27 - INFO - __main__ - Global step 1800 Train loss 2.85 ACC 0.5 on epoch=899
06/21/2022 15:25:27 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=899, global_step=1800
06/21/2022 15:25:28 - INFO - __main__ - Step 1810 Global step 1810 Train loss 2.67 on epoch=904
06/21/2022 15:25:29 - INFO - __main__ - Step 1820 Global step 1820 Train loss 2.77 on epoch=909
06/21/2022 15:25:31 - INFO - __main__ - Step 1830 Global step 1830 Train loss 2.70 on epoch=914
06/21/2022 15:25:32 - INFO - __main__ - Step 1840 Global step 1840 Train loss 2.63 on epoch=919
06/21/2022 15:25:33 - INFO - __main__ - Step 1850 Global step 1850 Train loss 2.57 on epoch=924
06/21/2022 15:25:44 - INFO - __main__ - Global step 1850 Train loss 2.67 ACC 0.4375 on epoch=924
06/21/2022 15:25:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 2.54 on epoch=929
06/21/2022 15:25:47 - INFO - __main__ - Step 1870 Global step 1870 Train loss 2.48 on epoch=934
06/21/2022 15:25:48 - INFO - __main__ - Step 1880 Global step 1880 Train loss 2.48 on epoch=939
06/21/2022 15:25:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 2.47 on epoch=944
06/21/2022 15:25:51 - INFO - __main__ - Step 1900 Global step 1900 Train loss 2.26 on epoch=949
06/21/2022 15:25:53 - INFO - __main__ - Global step 1900 Train loss 2.45 ACC 0.5 on epoch=949
06/21/2022 15:25:54 - INFO - __main__ - Step 1910 Global step 1910 Train loss 2.44 on epoch=954
06/21/2022 15:25:56 - INFO - __main__ - Step 1920 Global step 1920 Train loss 2.34 on epoch=959
06/21/2022 15:25:57 - INFO - __main__ - Step 1930 Global step 1930 Train loss 2.24 on epoch=964
06/21/2022 15:25:59 - INFO - __main__ - Step 1940 Global step 1940 Train loss 2.29 on epoch=969
06/21/2022 15:26:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 2.13 on epoch=974
06/21/2022 15:26:02 - INFO - __main__ - Global step 1950 Train loss 2.29 ACC 0.5 on epoch=974
06/21/2022 15:26:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 2.15 on epoch=979
06/21/2022 15:26:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 2.18 on epoch=984
06/21/2022 15:26:06 - INFO - __main__ - Step 1980 Global step 1980 Train loss 2.09 on epoch=989
06/21/2022 15:26:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 2.11 on epoch=994
06/21/2022 15:26:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 2.06 on epoch=999
06/21/2022 15:26:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:26:10 - INFO - __main__ - Printing 3 examples
06/21/2022 15:26:10 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/21/2022 15:26:10 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/21/2022 15:26:10 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/21/2022 15:26:10 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:10 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 15:26:10 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:26:10 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:26:10 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:26:10 - INFO - __main__ - Printing 3 examples
06/21/2022 15:26:10 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/21/2022 15:26:10 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:10 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/21/2022 15:26:10 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:10 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/21/2022 15:26:10 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:10 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:26:10 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:26:10 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:26:10 - INFO - __main__ - Global step 2000 Train loss 2.12 ACC 0.5 on epoch=999
06/21/2022 15:26:10 - INFO - __main__ - save last model!
06/21/2022 15:26:10 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 15:26:10 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 15:26:10 - INFO - __main__ - Printing 3 examples
06/21/2022 15:26:10 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 15:26:10 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:10 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 15:26:10 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:26:10 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 15:26:10 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:26:10 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:26:11 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:26:11 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 15:26:17 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:26:17 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:26:17 - INFO - __main__ - Starting training!
06/21/2022 15:26:41 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_100_0.2_8_predictions.txt
06/21/2022 15:26:41 - INFO - __main__ - ACC on test data: 0.6667
06/21/2022 15:26:41 - INFO - __main__ - prefix=glue-mrpc_16_100, lr=0.2, bsz=8, dev_performance=0.5, test_performance=0.6666666666666666
06/21/2022 15:26:41 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.5, bsz=8 ...
06/21/2022 15:26:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:26:42 - INFO - __main__ - Printing 3 examples
06/21/2022 15:26:42 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/21/2022 15:26:42 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:42 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/21/2022 15:26:42 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:42 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/21/2022 15:26:42 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 15:26:42 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:26:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:26:42 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:26:42 - INFO - __main__ - Printing 3 examples
06/21/2022 15:26:42 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/21/2022 15:26:42 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:42 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/21/2022 15:26:42 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:42 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/21/2022 15:26:42 - INFO - __main__ - ['equivalent']
06/21/2022 15:26:42 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:26:42 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:26:42 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:26:48 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:26:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:26:48 - INFO - __main__ - Starting training!
06/21/2022 15:26:50 - INFO - __main__ - Step 10 Global step 10 Train loss 6.92 on epoch=4
06/21/2022 15:26:51 - INFO - __main__ - Step 20 Global step 20 Train loss 6.90 on epoch=9
06/21/2022 15:26:53 - INFO - __main__ - Step 30 Global step 30 Train loss 6.86 on epoch=14
06/21/2022 15:26:54 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/21/2022 15:26:56 - INFO - __main__ - Step 50 Global step 50 Train loss 6.72 on epoch=24
06/21/2022 15:26:58 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/21/2022 15:26:58 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 15:26:59 - INFO - __main__ - Step 60 Global step 60 Train loss 6.85 on epoch=29
06/21/2022 15:27:00 - INFO - __main__ - Step 70 Global step 70 Train loss 6.74 on epoch=34
06/21/2022 15:27:02 - INFO - __main__ - Step 80 Global step 80 Train loss 6.75 on epoch=39
06/21/2022 15:27:03 - INFO - __main__ - Step 90 Global step 90 Train loss 6.66 on epoch=44
06/21/2022 15:27:04 - INFO - __main__ - Step 100 Global step 100 Train loss 6.73 on epoch=49
06/21/2022 15:27:08 - INFO - __main__ - Global step 100 Train loss 6.75 ACC 0.0 on epoch=49
06/21/2022 15:27:09 - INFO - __main__ - Step 110 Global step 110 Train loss 6.64 on epoch=54
06/21/2022 15:27:10 - INFO - __main__ - Step 120 Global step 120 Train loss 6.63 on epoch=59
06/21/2022 15:27:12 - INFO - __main__ - Step 130 Global step 130 Train loss 6.62 on epoch=64
06/21/2022 15:27:13 - INFO - __main__ - Step 140 Global step 140 Train loss 6.53 on epoch=69
06/21/2022 15:27:14 - INFO - __main__ - Step 150 Global step 150 Train loss 6.67 on epoch=74
06/21/2022 15:27:16 - INFO - __main__ - Global step 150 Train loss 6.62 ACC 0.0 on epoch=74
06/21/2022 15:27:17 - INFO - __main__ - Step 160 Global step 160 Train loss 6.60 on epoch=79
06/21/2022 15:27:19 - INFO - __main__ - Step 170 Global step 170 Train loss 6.50 on epoch=84
06/21/2022 15:27:20 - INFO - __main__ - Step 180 Global step 180 Train loss 6.51 on epoch=89
06/21/2022 15:27:21 - INFO - __main__ - Step 190 Global step 190 Train loss 6.48 on epoch=94
06/21/2022 15:27:23 - INFO - __main__ - Step 200 Global step 200 Train loss 6.40 on epoch=99
06/21/2022 15:27:33 - INFO - __main__ - Global step 200 Train loss 6.50 ACC 0.0 on epoch=99
06/21/2022 15:27:35 - INFO - __main__ - Step 210 Global step 210 Train loss 6.30 on epoch=104
06/21/2022 15:27:36 - INFO - __main__ - Step 220 Global step 220 Train loss 6.32 on epoch=109
06/21/2022 15:27:37 - INFO - __main__ - Step 230 Global step 230 Train loss 6.22 on epoch=114
06/21/2022 15:27:39 - INFO - __main__ - Step 240 Global step 240 Train loss 6.15 on epoch=119
06/21/2022 15:27:40 - INFO - __main__ - Step 250 Global step 250 Train loss 6.07 on epoch=124
06/21/2022 15:27:46 - INFO - __main__ - Global step 250 Train loss 6.21 ACC 0.0 on epoch=124
06/21/2022 15:27:48 - INFO - __main__ - Step 260 Global step 260 Train loss 5.94 on epoch=129
06/21/2022 15:27:49 - INFO - __main__ - Step 270 Global step 270 Train loss 5.86 on epoch=134
06/21/2022 15:27:51 - INFO - __main__ - Step 280 Global step 280 Train loss 5.80 on epoch=139
06/21/2022 15:27:52 - INFO - __main__ - Step 290 Global step 290 Train loss 5.60 on epoch=144
06/21/2022 15:27:53 - INFO - __main__ - Step 300 Global step 300 Train loss 5.36 on epoch=149
06/21/2022 15:27:59 - INFO - __main__ - Global step 300 Train loss 5.71 ACC 0.0 on epoch=149
06/21/2022 15:28:00 - INFO - __main__ - Step 310 Global step 310 Train loss 5.34 on epoch=154
06/21/2022 15:28:02 - INFO - __main__ - Step 320 Global step 320 Train loss 5.25 on epoch=159
06/21/2022 15:28:03 - INFO - __main__ - Step 330 Global step 330 Train loss 5.16 on epoch=164
06/21/2022 15:28:05 - INFO - __main__ - Step 340 Global step 340 Train loss 5.10 on epoch=169
06/21/2022 15:28:06 - INFO - __main__ - Step 350 Global step 350 Train loss 4.91 on epoch=174
06/21/2022 15:28:07 - INFO - __main__ - Global step 350 Train loss 5.15 ACC 0.0 on epoch=174
06/21/2022 15:28:09 - INFO - __main__ - Step 360 Global step 360 Train loss 4.84 on epoch=179
06/21/2022 15:28:10 - INFO - __main__ - Step 370 Global step 370 Train loss 4.69 on epoch=184
06/21/2022 15:28:12 - INFO - __main__ - Step 380 Global step 380 Train loss 4.67 on epoch=189
06/21/2022 15:28:13 - INFO - __main__ - Step 390 Global step 390 Train loss 4.62 on epoch=194
06/21/2022 15:28:14 - INFO - __main__ - Step 400 Global step 400 Train loss 4.53 on epoch=199
06/21/2022 15:28:15 - INFO - __main__ - Global step 400 Train loss 4.67 ACC 0.0 on epoch=199
06/21/2022 15:28:17 - INFO - __main__ - Step 410 Global step 410 Train loss 4.48 on epoch=204
06/21/2022 15:28:18 - INFO - __main__ - Step 420 Global step 420 Train loss 4.41 on epoch=209
06/21/2022 15:28:20 - INFO - __main__ - Step 430 Global step 430 Train loss 4.31 on epoch=214
06/21/2022 15:28:21 - INFO - __main__ - Step 440 Global step 440 Train loss 4.31 on epoch=219
06/21/2022 15:28:22 - INFO - __main__ - Step 450 Global step 450 Train loss 4.13 on epoch=224
06/21/2022 15:28:24 - INFO - __main__ - Global step 450 Train loss 4.33 ACC 0.0 on epoch=224
06/21/2022 15:28:25 - INFO - __main__ - Step 460 Global step 460 Train loss 4.23 on epoch=229
06/21/2022 15:28:26 - INFO - __main__ - Step 470 Global step 470 Train loss 4.15 on epoch=234
06/21/2022 15:28:28 - INFO - __main__ - Step 480 Global step 480 Train loss 3.93 on epoch=239
06/21/2022 15:28:29 - INFO - __main__ - Step 490 Global step 490 Train loss 3.83 on epoch=244
06/21/2022 15:28:30 - INFO - __main__ - Step 500 Global step 500 Train loss 3.91 on epoch=249
06/21/2022 15:28:31 - INFO - __main__ - Global step 500 Train loss 4.01 ACC 0.0 on epoch=249
06/21/2022 15:28:33 - INFO - __main__ - Step 510 Global step 510 Train loss 3.77 on epoch=254
06/21/2022 15:28:34 - INFO - __main__ - Step 520 Global step 520 Train loss 3.62 on epoch=259
06/21/2022 15:28:36 - INFO - __main__ - Step 530 Global step 530 Train loss 3.67 on epoch=264
06/21/2022 15:28:37 - INFO - __main__ - Step 540 Global step 540 Train loss 3.43 on epoch=269
06/21/2022 15:28:38 - INFO - __main__ - Step 550 Global step 550 Train loss 3.33 on epoch=274
06/21/2022 15:28:40 - INFO - __main__ - Global step 550 Train loss 3.56 ACC 0.0 on epoch=274
06/21/2022 15:28:41 - INFO - __main__ - Step 560 Global step 560 Train loss 3.38 on epoch=279
06/21/2022 15:28:43 - INFO - __main__ - Step 570 Global step 570 Train loss 3.47 on epoch=284
06/21/2022 15:28:44 - INFO - __main__ - Step 580 Global step 580 Train loss 3.27 on epoch=289
06/21/2022 15:28:45 - INFO - __main__ - Step 590 Global step 590 Train loss 3.20 on epoch=294
06/21/2022 15:28:47 - INFO - __main__ - Step 600 Global step 600 Train loss 3.15 on epoch=299
06/21/2022 15:28:49 - INFO - __main__ - Global step 600 Train loss 3.29 ACC 0.0 on epoch=299
06/21/2022 15:28:50 - INFO - __main__ - Step 610 Global step 610 Train loss 3.08 on epoch=304
06/21/2022 15:28:52 - INFO - __main__ - Step 620 Global step 620 Train loss 3.01 on epoch=309
06/21/2022 15:28:53 - INFO - __main__ - Step 630 Global step 630 Train loss 3.09 on epoch=314
06/21/2022 15:28:54 - INFO - __main__ - Step 640 Global step 640 Train loss 3.04 on epoch=319
06/21/2022 15:28:56 - INFO - __main__ - Step 650 Global step 650 Train loss 3.01 on epoch=324
06/21/2022 15:28:57 - INFO - __main__ - Global step 650 Train loss 3.05 ACC 0.0 on epoch=324
06/21/2022 15:28:59 - INFO - __main__ - Step 660 Global step 660 Train loss 2.91 on epoch=329
06/21/2022 15:29:00 - INFO - __main__ - Step 670 Global step 670 Train loss 2.77 on epoch=334
06/21/2022 15:29:01 - INFO - __main__ - Step 680 Global step 680 Train loss 2.74 on epoch=339
06/21/2022 15:29:03 - INFO - __main__ - Step 690 Global step 690 Train loss 2.78 on epoch=344
06/21/2022 15:29:04 - INFO - __main__ - Step 700 Global step 700 Train loss 2.71 on epoch=349
06/21/2022 15:29:08 - INFO - __main__ - Global step 700 Train loss 2.78 ACC 0.0 on epoch=349
06/21/2022 15:29:09 - INFO - __main__ - Step 710 Global step 710 Train loss 2.59 on epoch=354
06/21/2022 15:29:10 - INFO - __main__ - Step 720 Global step 720 Train loss 2.54 on epoch=359
06/21/2022 15:29:12 - INFO - __main__ - Step 730 Global step 730 Train loss 2.53 on epoch=364
06/21/2022 15:29:13 - INFO - __main__ - Step 740 Global step 740 Train loss 2.53 on epoch=369
06/21/2022 15:29:14 - INFO - __main__ - Step 750 Global step 750 Train loss 2.46 on epoch=374
06/21/2022 15:29:17 - INFO - __main__ - Global step 750 Train loss 2.53 ACC 0.0 on epoch=374
06/21/2022 15:29:18 - INFO - __main__ - Step 760 Global step 760 Train loss 2.39 on epoch=379
06/21/2022 15:29:19 - INFO - __main__ - Step 770 Global step 770 Train loss 2.27 on epoch=384
06/21/2022 15:29:20 - INFO - __main__ - Step 780 Global step 780 Train loss 2.23 on epoch=389
06/21/2022 15:29:22 - INFO - __main__ - Step 790 Global step 790 Train loss 2.30 on epoch=394
06/21/2022 15:29:23 - INFO - __main__ - Step 800 Global step 800 Train loss 2.15 on epoch=399
06/21/2022 15:29:24 - INFO - __main__ - Global step 800 Train loss 2.27 ACC 0.0 on epoch=399
06/21/2022 15:29:26 - INFO - __main__ - Step 810 Global step 810 Train loss 2.17 on epoch=404
06/21/2022 15:29:27 - INFO - __main__ - Step 820 Global step 820 Train loss 2.14 on epoch=409
06/21/2022 15:29:28 - INFO - __main__ - Step 830 Global step 830 Train loss 2.01 on epoch=414
06/21/2022 15:29:30 - INFO - __main__ - Step 840 Global step 840 Train loss 1.91 on epoch=419
06/21/2022 15:29:31 - INFO - __main__ - Step 850 Global step 850 Train loss 1.87 on epoch=424
06/21/2022 15:29:33 - INFO - __main__ - Global step 850 Train loss 2.02 ACC 0.28125 on epoch=424
06/21/2022 15:29:33 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.28125 on epoch=424, global_step=850
06/21/2022 15:29:34 - INFO - __main__ - Step 860 Global step 860 Train loss 1.85 on epoch=429
06/21/2022 15:29:36 - INFO - __main__ - Step 870 Global step 870 Train loss 1.96 on epoch=434
06/21/2022 15:29:37 - INFO - __main__ - Step 880 Global step 880 Train loss 1.79 on epoch=439
06/21/2022 15:29:38 - INFO - __main__ - Step 890 Global step 890 Train loss 1.84 on epoch=444
06/21/2022 15:29:40 - INFO - __main__ - Step 900 Global step 900 Train loss 1.78 on epoch=449
06/21/2022 15:29:42 - INFO - __main__ - Global step 900 Train loss 1.84 ACC 0.46875 on epoch=449
06/21/2022 15:29:42 - INFO - __main__ - Saving model with best ACC: 0.28125 -> 0.46875 on epoch=449, global_step=900
06/21/2022 15:29:43 - INFO - __main__ - Step 910 Global step 910 Train loss 1.72 on epoch=454
06/21/2022 15:29:45 - INFO - __main__ - Step 920 Global step 920 Train loss 1.71 on epoch=459
06/21/2022 15:29:46 - INFO - __main__ - Step 930 Global step 930 Train loss 1.62 on epoch=464
06/21/2022 15:29:48 - INFO - __main__ - Step 940 Global step 940 Train loss 1.72 on epoch=469
06/21/2022 15:29:49 - INFO - __main__ - Step 950 Global step 950 Train loss 1.57 on epoch=474
06/21/2022 15:29:51 - INFO - __main__ - Global step 950 Train loss 1.67 ACC 0.5 on epoch=474
06/21/2022 15:29:51 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=474, global_step=950
06/21/2022 15:29:53 - INFO - __main__ - Step 960 Global step 960 Train loss 1.53 on epoch=479
06/21/2022 15:29:54 - INFO - __main__ - Step 970 Global step 970 Train loss 1.46 on epoch=484
06/21/2022 15:29:55 - INFO - __main__ - Step 980 Global step 980 Train loss 1.46 on epoch=489
06/21/2022 15:29:57 - INFO - __main__ - Step 990 Global step 990 Train loss 1.50 on epoch=494
06/21/2022 15:29:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.46 on epoch=499
06/21/2022 15:30:00 - INFO - __main__ - Global step 1000 Train loss 1.48 ACC 0.65625 on epoch=499
06/21/2022 15:30:00 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.65625 on epoch=499, global_step=1000
06/21/2022 15:30:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.37 on epoch=504
06/21/2022 15:30:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.40 on epoch=509
06/21/2022 15:30:04 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.38 on epoch=514
06/21/2022 15:30:06 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.46 on epoch=519
06/21/2022 15:30:07 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.28 on epoch=524
06/21/2022 15:30:12 - INFO - __main__ - Global step 1050 Train loss 1.38 ACC 0.4375 on epoch=524
06/21/2022 15:30:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.27 on epoch=529
06/21/2022 15:30:14 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.26 on epoch=534
06/21/2022 15:30:16 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.20 on epoch=539
06/21/2022 15:30:17 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.16 on epoch=544
06/21/2022 15:30:18 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.12 on epoch=549
06/21/2022 15:30:20 - INFO - __main__ - Global step 1100 Train loss 1.20 ACC 0.5 on epoch=549
06/21/2022 15:30:21 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.03 on epoch=554
06/21/2022 15:30:23 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.04 on epoch=559
06/21/2022 15:30:24 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.00 on epoch=564
06/21/2022 15:30:26 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.98 on epoch=569
06/21/2022 15:30:27 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.04 on epoch=574
06/21/2022 15:30:29 - INFO - __main__ - Global step 1150 Train loss 1.02 ACC 0.5 on epoch=574
06/21/2022 15:30:31 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.96 on epoch=579
06/21/2022 15:30:32 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.89 on epoch=584
06/21/2022 15:30:33 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.89 on epoch=589
06/21/2022 15:30:35 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.89 on epoch=594
06/21/2022 15:30:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.86 on epoch=599
06/21/2022 15:30:38 - INFO - __main__ - Global step 1200 Train loss 0.90 ACC 0.5 on epoch=599
06/21/2022 15:30:39 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.73 on epoch=604
06/21/2022 15:30:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.71 on epoch=609
06/21/2022 15:30:42 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.78 on epoch=614
06/21/2022 15:30:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.83 on epoch=619
06/21/2022 15:30:45 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.76 on epoch=624
06/21/2022 15:30:47 - INFO - __main__ - Global step 1250 Train loss 0.76 ACC 0.5 on epoch=624
06/21/2022 15:30:48 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.77 on epoch=629
06/21/2022 15:30:50 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.70 on epoch=634
06/21/2022 15:30:51 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.69 on epoch=639
06/21/2022 15:30:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.68 on epoch=644
06/21/2022 15:30:54 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.65 on epoch=649
06/21/2022 15:30:55 - INFO - __main__ - Global step 1300 Train loss 0.70 ACC 0.5 on epoch=649
06/21/2022 15:30:56 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.52 on epoch=654
06/21/2022 15:30:58 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.59 on epoch=659
06/21/2022 15:30:59 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.64 on epoch=664
06/21/2022 15:31:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.57 on epoch=669
06/21/2022 15:31:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.60 on epoch=674
06/21/2022 15:31:04 - INFO - __main__ - Global step 1350 Train loss 0.58 ACC 0.5 on epoch=674
06/21/2022 15:31:05 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.69 on epoch=679
06/21/2022 15:31:07 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.54 on epoch=684
06/21/2022 15:31:08 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
06/21/2022 15:31:10 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.54 on epoch=694
06/21/2022 15:31:11 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.54 on epoch=699
06/21/2022 15:31:12 - INFO - __main__ - Global step 1400 Train loss 0.56 ACC 0.5 on epoch=699
06/21/2022 15:31:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=704
06/21/2022 15:31:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.44 on epoch=709
06/21/2022 15:31:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.53 on epoch=714
06/21/2022 15:31:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
06/21/2022 15:31:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.49 on epoch=724
06/21/2022 15:31:20 - INFO - __main__ - Global step 1450 Train loss 0.51 ACC 0.5 on epoch=724
06/21/2022 15:31:22 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/21/2022 15:31:23 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.52 on epoch=734
06/21/2022 15:31:25 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.43 on epoch=739
06/21/2022 15:31:26 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.50 on epoch=744
06/21/2022 15:31:27 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=749
06/21/2022 15:31:28 - INFO - __main__ - Global step 1500 Train loss 0.48 ACC 0.5 on epoch=749
06/21/2022 15:31:30 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
06/21/2022 15:31:31 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.46 on epoch=759
06/21/2022 15:31:33 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.47 on epoch=764
06/21/2022 15:31:34 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.43 on epoch=769
06/21/2022 15:31:36 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.35 on epoch=774
06/21/2022 15:31:36 - INFO - __main__ - Global step 1550 Train loss 0.43 ACC 0.5 on epoch=774
06/21/2022 15:31:38 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.47 on epoch=779
06/21/2022 15:31:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.40 on epoch=784
06/21/2022 15:31:41 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.47 on epoch=789
06/21/2022 15:31:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.44 on epoch=794
06/21/2022 15:31:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.42 on epoch=799
06/21/2022 15:31:45 - INFO - __main__ - Global step 1600 Train loss 0.44 ACC 0.5 on epoch=799
06/21/2022 15:31:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.49 on epoch=804
06/21/2022 15:31:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.46 on epoch=809
06/21/2022 15:31:49 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.38 on epoch=814
06/21/2022 15:31:51 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.43 on epoch=819
06/21/2022 15:31:52 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.51 on epoch=824
06/21/2022 15:31:53 - INFO - __main__ - Global step 1650 Train loss 0.45 ACC 0.53125 on epoch=824
06/21/2022 15:31:54 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
06/21/2022 15:31:55 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=834
06/21/2022 15:31:57 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.47 on epoch=839
06/21/2022 15:31:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.43 on epoch=844
06/21/2022 15:32:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.42 on epoch=849
06/21/2022 15:32:00 - INFO - __main__ - Global step 1700 Train loss 0.43 ACC 0.46875 on epoch=849
06/21/2022 15:32:02 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
06/21/2022 15:32:03 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.43 on epoch=859
06/21/2022 15:32:04 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.46 on epoch=864
06/21/2022 15:32:06 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.39 on epoch=869
06/21/2022 15:32:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.36 on epoch=874
06/21/2022 15:32:08 - INFO - __main__ - Global step 1750 Train loss 0.40 ACC 0.5 on epoch=874
06/21/2022 15:32:09 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.47 on epoch=879
06/21/2022 15:32:11 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.41 on epoch=884
06/21/2022 15:32:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.37 on epoch=889
06/21/2022 15:32:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.34 on epoch=894
06/21/2022 15:32:15 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
06/21/2022 15:32:16 - INFO - __main__ - Global step 1800 Train loss 0.39 ACC 0.5 on epoch=899
06/21/2022 15:32:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.39 on epoch=904
06/21/2022 15:32:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.39 on epoch=909
06/21/2022 15:32:20 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.46 on epoch=914
06/21/2022 15:32:21 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.37 on epoch=919
06/21/2022 15:32:23 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
06/21/2022 15:32:23 - INFO - __main__ - Global step 1850 Train loss 0.40 ACC 0.5 on epoch=924
06/21/2022 15:32:25 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.42 on epoch=929
06/21/2022 15:32:26 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.37 on epoch=934
06/21/2022 15:32:27 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
06/21/2022 15:32:29 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.39 on epoch=944
06/21/2022 15:32:30 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
06/21/2022 15:32:31 - INFO - __main__ - Global step 1900 Train loss 0.40 ACC 0.5 on epoch=949
06/21/2022 15:32:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.35 on epoch=954
06/21/2022 15:32:34 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
06/21/2022 15:32:35 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
06/21/2022 15:32:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.35 on epoch=969
06/21/2022 15:32:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.36 on epoch=974
06/21/2022 15:32:39 - INFO - __main__ - Global step 1950 Train loss 0.35 ACC 0.5 on epoch=974
06/21/2022 15:32:40 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.38 on epoch=979
06/21/2022 15:32:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.31 on epoch=984
06/21/2022 15:32:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=989
06/21/2022 15:32:44 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.32 on epoch=994
06/21/2022 15:32:45 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.35 on epoch=999
06/21/2022 15:32:46 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.5 on epoch=999
06/21/2022 15:32:46 - INFO - __main__ - save last model!
06/21/2022 15:32:46 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 15:32:46 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 15:32:46 - INFO - __main__ - Printing 3 examples
06/21/2022 15:32:46 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 15:32:46 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:46 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 15:32:46 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:32:46 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 15:32:46 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:32:46 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:32:46 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:32:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:32:47 - INFO - __main__ - Printing 3 examples
06/21/2022 15:32:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/21/2022 15:32:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/21/2022 15:32:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/21/2022 15:32:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 15:32:47 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:32:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:32:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:32:47 - INFO - __main__ - Printing 3 examples
06/21/2022 15:32:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/21/2022 15:32:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/21/2022 15:32:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:47 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/21/2022 15:32:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:47 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:32:47 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:32:47 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:32:47 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 15:32:53 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:32:53 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:32:54 - INFO - __main__ - Starting training!
06/21/2022 15:32:54 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.5_8_predictions.txt
06/21/2022 15:32:54 - INFO - __main__ - ACC on test data: 0.6863
06/21/2022 15:32:55 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.5, bsz=8, dev_performance=0.65625, test_performance=0.6862745098039216
06/21/2022 15:32:55 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.4, bsz=8 ...
06/21/2022 15:32:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:32:56 - INFO - __main__ - Printing 3 examples
06/21/2022 15:32:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/21/2022 15:32:56 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/21/2022 15:32:56 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/21/2022 15:32:56 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 15:32:56 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:32:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:32:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:32:56 - INFO - __main__ - Printing 3 examples
06/21/2022 15:32:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/21/2022 15:32:56 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/21/2022 15:32:56 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:56 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/21/2022 15:32:56 - INFO - __main__ - ['equivalent']
06/21/2022 15:32:56 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:32:56 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:32:56 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:33:01 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:33:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:33:02 - INFO - __main__ - Starting training!
06/21/2022 15:33:03 - INFO - __main__ - Step 10 Global step 10 Train loss 6.89 on epoch=4
06/21/2022 15:33:05 - INFO - __main__ - Step 20 Global step 20 Train loss 6.90 on epoch=9
06/21/2022 15:33:06 - INFO - __main__ - Step 30 Global step 30 Train loss 6.76 on epoch=14
06/21/2022 15:33:08 - INFO - __main__ - Step 40 Global step 40 Train loss 6.79 on epoch=19
06/21/2022 15:33:09 - INFO - __main__ - Step 50 Global step 50 Train loss 6.89 on epoch=24
06/21/2022 15:33:13 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/21/2022 15:33:13 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 15:33:14 - INFO - __main__ - Step 60 Global step 60 Train loss 6.83 on epoch=29
06/21/2022 15:33:15 - INFO - __main__ - Step 70 Global step 70 Train loss 6.82 on epoch=34
06/21/2022 15:33:17 - INFO - __main__ - Step 80 Global step 80 Train loss 6.85 on epoch=39
06/21/2022 15:33:18 - INFO - __main__ - Step 90 Global step 90 Train loss 6.81 on epoch=44
06/21/2022 15:33:19 - INFO - __main__ - Step 100 Global step 100 Train loss 6.68 on epoch=49
06/21/2022 15:33:22 - INFO - __main__ - Global step 100 Train loss 6.80 ACC 0.0 on epoch=49
06/21/2022 15:33:23 - INFO - __main__ - Step 110 Global step 110 Train loss 6.84 on epoch=54
06/21/2022 15:33:25 - INFO - __main__ - Step 120 Global step 120 Train loss 6.68 on epoch=59
06/21/2022 15:33:26 - INFO - __main__ - Step 130 Global step 130 Train loss 6.72 on epoch=64
06/21/2022 15:33:27 - INFO - __main__ - Step 140 Global step 140 Train loss 6.66 on epoch=69
06/21/2022 15:33:29 - INFO - __main__ - Step 150 Global step 150 Train loss 6.53 on epoch=74
06/21/2022 15:33:32 - INFO - __main__ - Global step 150 Train loss 6.68 ACC 0.0 on epoch=74
06/21/2022 15:33:33 - INFO - __main__ - Step 160 Global step 160 Train loss 6.60 on epoch=79
06/21/2022 15:33:35 - INFO - __main__ - Step 170 Global step 170 Train loss 6.43 on epoch=84
06/21/2022 15:33:36 - INFO - __main__ - Step 180 Global step 180 Train loss 6.47 on epoch=89
06/21/2022 15:33:38 - INFO - __main__ - Step 190 Global step 190 Train loss 6.30 on epoch=94
06/21/2022 15:33:39 - INFO - __main__ - Step 200 Global step 200 Train loss 6.20 on epoch=99
06/21/2022 15:33:46 - INFO - __main__ - Global step 200 Train loss 6.40 ACC 0.0 on epoch=99
06/21/2022 15:33:48 - INFO - __main__ - Step 210 Global step 210 Train loss 6.19 on epoch=104
06/21/2022 15:33:49 - INFO - __main__ - Step 220 Global step 220 Train loss 5.96 on epoch=109
06/21/2022 15:33:50 - INFO - __main__ - Step 230 Global step 230 Train loss 5.93 on epoch=114
06/21/2022 15:33:51 - INFO - __main__ - Step 240 Global step 240 Train loss 5.88 on epoch=119
06/21/2022 15:33:53 - INFO - __main__ - Step 250 Global step 250 Train loss 5.74 on epoch=124
06/21/2022 15:33:58 - INFO - __main__ - Global step 250 Train loss 5.94 ACC 0.0 on epoch=124
06/21/2022 15:33:59 - INFO - __main__ - Step 260 Global step 260 Train loss 5.84 on epoch=129
06/21/2022 15:34:00 - INFO - __main__ - Step 270 Global step 270 Train loss 5.68 on epoch=134
06/21/2022 15:34:02 - INFO - __main__ - Step 280 Global step 280 Train loss 5.64 on epoch=139
06/21/2022 15:34:03 - INFO - __main__ - Step 290 Global step 290 Train loss 5.65 on epoch=144
06/21/2022 15:34:05 - INFO - __main__ - Step 300 Global step 300 Train loss 5.53 on epoch=149
06/21/2022 15:34:15 - INFO - __main__ - Global step 300 Train loss 5.67 ACC 0.0 on epoch=149
06/21/2022 15:34:16 - INFO - __main__ - Step 310 Global step 310 Train loss 5.52 on epoch=154
06/21/2022 15:34:18 - INFO - __main__ - Step 320 Global step 320 Train loss 5.46 on epoch=159
06/21/2022 15:34:19 - INFO - __main__ - Step 330 Global step 330 Train loss 5.26 on epoch=164
06/21/2022 15:34:21 - INFO - __main__ - Step 340 Global step 340 Train loss 5.21 on epoch=169
06/21/2022 15:34:22 - INFO - __main__ - Step 350 Global step 350 Train loss 5.09 on epoch=174
06/21/2022 15:34:24 - INFO - __main__ - Global step 350 Train loss 5.31 ACC 0.0 on epoch=174
06/21/2022 15:34:25 - INFO - __main__ - Step 360 Global step 360 Train loss 5.15 on epoch=179
06/21/2022 15:34:27 - INFO - __main__ - Step 370 Global step 370 Train loss 5.13 on epoch=184
06/21/2022 15:34:28 - INFO - __main__ - Step 380 Global step 380 Train loss 5.00 on epoch=189
06/21/2022 15:34:29 - INFO - __main__ - Step 390 Global step 390 Train loss 4.94 on epoch=194
06/21/2022 15:34:31 - INFO - __main__ - Step 400 Global step 400 Train loss 4.84 on epoch=199
06/21/2022 15:34:33 - INFO - __main__ - Global step 400 Train loss 5.01 ACC 0.0 on epoch=199
06/21/2022 15:34:34 - INFO - __main__ - Step 410 Global step 410 Train loss 4.79 on epoch=204
06/21/2022 15:34:36 - INFO - __main__ - Step 420 Global step 420 Train loss 4.64 on epoch=209
06/21/2022 15:34:37 - INFO - __main__ - Step 430 Global step 430 Train loss 4.54 on epoch=214
06/21/2022 15:34:38 - INFO - __main__ - Step 440 Global step 440 Train loss 4.39 on epoch=219
06/21/2022 15:34:40 - INFO - __main__ - Step 450 Global step 450 Train loss 4.23 on epoch=224
06/21/2022 15:34:41 - INFO - __main__ - Global step 450 Train loss 4.52 ACC 0.0 on epoch=224
06/21/2022 15:34:42 - INFO - __main__ - Step 460 Global step 460 Train loss 4.17 on epoch=229
06/21/2022 15:34:44 - INFO - __main__ - Step 470 Global step 470 Train loss 4.14 on epoch=234
06/21/2022 15:34:45 - INFO - __main__ - Step 480 Global step 480 Train loss 4.05 on epoch=239
06/21/2022 15:34:46 - INFO - __main__ - Step 490 Global step 490 Train loss 3.94 on epoch=244
06/21/2022 15:34:48 - INFO - __main__ - Step 500 Global step 500 Train loss 3.80 on epoch=249
06/21/2022 15:34:51 - INFO - __main__ - Global step 500 Train loss 4.02 ACC 0.0 on epoch=249
06/21/2022 15:34:52 - INFO - __main__ - Step 510 Global step 510 Train loss 3.69 on epoch=254
06/21/2022 15:34:54 - INFO - __main__ - Step 520 Global step 520 Train loss 3.54 on epoch=259
06/21/2022 15:34:55 - INFO - __main__ - Step 530 Global step 530 Train loss 3.54 on epoch=264
06/21/2022 15:34:57 - INFO - __main__ - Step 540 Global step 540 Train loss 3.32 on epoch=269
06/21/2022 15:34:58 - INFO - __main__ - Step 550 Global step 550 Train loss 3.35 on epoch=274
06/21/2022 15:35:05 - INFO - __main__ - Global step 550 Train loss 3.49 ACC 0.0 on epoch=274
06/21/2022 15:35:06 - INFO - __main__ - Step 560 Global step 560 Train loss 3.26 on epoch=279
06/21/2022 15:35:08 - INFO - __main__ - Step 570 Global step 570 Train loss 3.06 on epoch=284
06/21/2022 15:35:09 - INFO - __main__ - Step 580 Global step 580 Train loss 3.02 on epoch=289
06/21/2022 15:35:10 - INFO - __main__ - Step 590 Global step 590 Train loss 2.79 on epoch=294
06/21/2022 15:35:12 - INFO - __main__ - Step 600 Global step 600 Train loss 2.84 on epoch=299
06/21/2022 15:35:14 - INFO - __main__ - Global step 600 Train loss 2.99 ACC 0.09375 on epoch=299
06/21/2022 15:35:14 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=299, global_step=600
06/21/2022 15:35:15 - INFO - __main__ - Step 610 Global step 610 Train loss 2.67 on epoch=304
06/21/2022 15:35:17 - INFO - __main__ - Step 620 Global step 620 Train loss 2.61 on epoch=309
06/21/2022 15:35:18 - INFO - __main__ - Step 630 Global step 630 Train loss 2.50 on epoch=314
06/21/2022 15:35:19 - INFO - __main__ - Step 640 Global step 640 Train loss 2.35 on epoch=319
06/21/2022 15:35:21 - INFO - __main__ - Step 650 Global step 650 Train loss 2.41 on epoch=324
06/21/2022 15:35:23 - INFO - __main__ - Global step 650 Train loss 2.51 ACC 0.5 on epoch=324
06/21/2022 15:35:23 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.5 on epoch=324, global_step=650
06/21/2022 15:35:24 - INFO - __main__ - Step 660 Global step 660 Train loss 2.28 on epoch=329
06/21/2022 15:35:26 - INFO - __main__ - Step 670 Global step 670 Train loss 2.17 on epoch=334
06/21/2022 15:35:27 - INFO - __main__ - Step 680 Global step 680 Train loss 2.20 on epoch=339
06/21/2022 15:35:29 - INFO - __main__ - Step 690 Global step 690 Train loss 2.10 on epoch=344
06/21/2022 15:35:30 - INFO - __main__ - Step 700 Global step 700 Train loss 1.85 on epoch=349
06/21/2022 15:35:37 - INFO - __main__ - Global step 700 Train loss 2.12 ACC 0.5 on epoch=349
06/21/2022 15:35:38 - INFO - __main__ - Step 710 Global step 710 Train loss 1.93 on epoch=354
06/21/2022 15:35:40 - INFO - __main__ - Step 720 Global step 720 Train loss 1.82 on epoch=359
06/21/2022 15:35:41 - INFO - __main__ - Step 730 Global step 730 Train loss 1.70 on epoch=364
06/21/2022 15:35:43 - INFO - __main__ - Step 740 Global step 740 Train loss 1.78 on epoch=369
06/21/2022 15:35:44 - INFO - __main__ - Step 750 Global step 750 Train loss 1.43 on epoch=374
06/21/2022 15:35:49 - INFO - __main__ - Global step 750 Train loss 1.73 ACC 0.5 on epoch=374
06/21/2022 15:35:51 - INFO - __main__ - Step 760 Global step 760 Train loss 1.50 on epoch=379
06/21/2022 15:35:52 - INFO - __main__ - Step 770 Global step 770 Train loss 1.42 on epoch=384
06/21/2022 15:35:53 - INFO - __main__ - Step 780 Global step 780 Train loss 1.40 on epoch=389
06/21/2022 15:35:55 - INFO - __main__ - Step 790 Global step 790 Train loss 1.32 on epoch=394
06/21/2022 15:35:56 - INFO - __main__ - Step 800 Global step 800 Train loss 1.27 on epoch=399
06/21/2022 15:35:58 - INFO - __main__ - Global step 800 Train loss 1.38 ACC 0.53125 on epoch=399
06/21/2022 15:35:58 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=399, global_step=800
06/21/2022 15:35:59 - INFO - __main__ - Step 810 Global step 810 Train loss 1.23 on epoch=404
06/21/2022 15:36:00 - INFO - __main__ - Step 820 Global step 820 Train loss 1.23 on epoch=409
06/21/2022 15:36:02 - INFO - __main__ - Step 830 Global step 830 Train loss 1.11 on epoch=414
06/21/2022 15:36:03 - INFO - __main__ - Step 840 Global step 840 Train loss 1.13 on epoch=419
06/21/2022 15:36:05 - INFO - __main__ - Step 850 Global step 850 Train loss 1.14 on epoch=424
06/21/2022 15:36:06 - INFO - __main__ - Global step 850 Train loss 1.17 ACC 0.5 on epoch=424
06/21/2022 15:36:08 - INFO - __main__ - Step 860 Global step 860 Train loss 1.10 on epoch=429
06/21/2022 15:36:09 - INFO - __main__ - Step 870 Global step 870 Train loss 0.99 on epoch=434
06/21/2022 15:36:10 - INFO - __main__ - Step 880 Global step 880 Train loss 0.97 on epoch=439
06/21/2022 15:36:11 - INFO - __main__ - Step 890 Global step 890 Train loss 0.94 on epoch=444
06/21/2022 15:36:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.90 on epoch=449
06/21/2022 15:36:19 - INFO - __main__ - Global step 900 Train loss 0.98 ACC 0.5625 on epoch=449
06/21/2022 15:36:19 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=449, global_step=900
06/21/2022 15:36:20 - INFO - __main__ - Step 910 Global step 910 Train loss 0.86 on epoch=454
06/21/2022 15:36:21 - INFO - __main__ - Step 920 Global step 920 Train loss 0.77 on epoch=459
06/21/2022 15:36:23 - INFO - __main__ - Step 930 Global step 930 Train loss 0.78 on epoch=464
06/21/2022 15:36:24 - INFO - __main__ - Step 940 Global step 940 Train loss 0.81 on epoch=469
06/21/2022 15:36:25 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=474
06/21/2022 15:36:26 - INFO - __main__ - Global step 950 Train loss 0.81 ACC 0.5 on epoch=474
06/21/2022 15:36:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=479
06/21/2022 15:36:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.70 on epoch=484
06/21/2022 15:36:30 - INFO - __main__ - Step 980 Global step 980 Train loss 0.76 on epoch=489
06/21/2022 15:36:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.66 on epoch=494
06/21/2022 15:36:33 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.64 on epoch=499
06/21/2022 15:36:34 - INFO - __main__ - Global step 1000 Train loss 0.70 ACC 0.5 on epoch=499
06/21/2022 15:36:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.70 on epoch=504
06/21/2022 15:36:36 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.62 on epoch=509
06/21/2022 15:36:38 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.54 on epoch=514
06/21/2022 15:36:39 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.66 on epoch=519
06/21/2022 15:36:41 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.54 on epoch=524
06/21/2022 15:36:41 - INFO - __main__ - Global step 1050 Train loss 0.61 ACC 0.5 on epoch=524
06/21/2022 15:36:43 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.58 on epoch=529
06/21/2022 15:36:44 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.52 on epoch=534
06/21/2022 15:36:45 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.59 on epoch=539
06/21/2022 15:36:47 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.54 on epoch=544
06/21/2022 15:36:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.54 on epoch=549
06/21/2022 15:36:49 - INFO - __main__ - Global step 1100 Train loss 0.56 ACC 0.65625 on epoch=549
06/21/2022 15:36:49 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.65625 on epoch=549, global_step=1100
06/21/2022 15:36:51 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.62 on epoch=554
06/21/2022 15:36:52 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.60 on epoch=559
06/21/2022 15:36:53 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.56 on epoch=564
06/21/2022 15:36:55 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.56 on epoch=569
06/21/2022 15:36:56 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.56 on epoch=574
06/21/2022 15:36:57 - INFO - __main__ - Global step 1150 Train loss 0.58 ACC 0.53125 on epoch=574
06/21/2022 15:36:58 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.44 on epoch=579
06/21/2022 15:37:00 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.51 on epoch=584
06/21/2022 15:37:01 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.51 on epoch=589
06/21/2022 15:37:03 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.52 on epoch=594
06/21/2022 15:37:04 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.56 on epoch=599
06/21/2022 15:37:05 - INFO - __main__ - Global step 1200 Train loss 0.51 ACC 0.53125 on epoch=599
06/21/2022 15:37:06 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.47 on epoch=604
06/21/2022 15:37:08 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.53 on epoch=609
06/21/2022 15:37:09 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.52 on epoch=614
06/21/2022 15:37:11 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.56 on epoch=619
06/21/2022 15:37:12 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.52 on epoch=624
06/21/2022 15:37:13 - INFO - __main__ - Global step 1250 Train loss 0.52 ACC 0.5 on epoch=624
06/21/2022 15:37:14 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.46 on epoch=629
06/21/2022 15:37:15 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.48 on epoch=634
06/21/2022 15:37:17 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=639
06/21/2022 15:37:18 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.52 on epoch=644
06/21/2022 15:37:20 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.47 on epoch=649
06/21/2022 15:37:21 - INFO - __main__ - Global step 1300 Train loss 0.49 ACC 0.5 on epoch=649
06/21/2022 15:37:22 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.50 on epoch=654
06/21/2022 15:37:23 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.50 on epoch=659
06/21/2022 15:37:25 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.46 on epoch=664
06/21/2022 15:37:26 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.45 on epoch=669
06/21/2022 15:37:27 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.43 on epoch=674
06/21/2022 15:37:28 - INFO - __main__ - Global step 1350 Train loss 0.47 ACC 0.5 on epoch=674
06/21/2022 15:37:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.40 on epoch=679
06/21/2022 15:37:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.41 on epoch=684
06/21/2022 15:37:32 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.39 on epoch=689
06/21/2022 15:37:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.38 on epoch=694
06/21/2022 15:37:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.45 on epoch=699
06/21/2022 15:37:35 - INFO - __main__ - Global step 1400 Train loss 0.41 ACC 0.53125 on epoch=699
06/21/2022 15:37:37 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.40 on epoch=704
06/21/2022 15:37:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.39 on epoch=709
06/21/2022 15:37:40 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.41 on epoch=714
06/21/2022 15:37:41 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.36 on epoch=719
06/21/2022 15:37:43 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.42 on epoch=724
06/21/2022 15:37:43 - INFO - __main__ - Global step 1450 Train loss 0.39 ACC 0.5 on epoch=724
06/21/2022 15:37:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.40 on epoch=729
06/21/2022 15:37:46 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.48 on epoch=734
06/21/2022 15:37:48 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.48 on epoch=739
06/21/2022 15:37:50 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=744
06/21/2022 15:37:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.42 on epoch=749
06/21/2022 15:37:52 - INFO - __main__ - Global step 1500 Train loss 0.45 ACC 0.5 on epoch=749
06/21/2022 15:37:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
06/21/2022 15:37:54 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.52 on epoch=759
06/21/2022 15:37:56 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.41 on epoch=764
06/21/2022 15:37:57 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.42 on epoch=769
06/21/2022 15:37:59 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.40 on epoch=774
06/21/2022 15:37:59 - INFO - __main__ - Global step 1550 Train loss 0.44 ACC 0.53125 on epoch=774
06/21/2022 15:38:01 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
06/21/2022 15:38:02 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.38 on epoch=784
06/21/2022 15:38:03 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.38 on epoch=789
06/21/2022 15:38:05 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.39 on epoch=794
06/21/2022 15:38:06 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
06/21/2022 15:38:07 - INFO - __main__ - Global step 1600 Train loss 0.39 ACC 0.34375 on epoch=799
06/21/2022 15:38:08 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.44 on epoch=804
06/21/2022 15:38:09 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.42 on epoch=809
06/21/2022 15:38:11 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.36 on epoch=814
06/21/2022 15:38:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.34 on epoch=819
06/21/2022 15:38:13 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.38 on epoch=824
06/21/2022 15:38:14 - INFO - __main__ - Global step 1650 Train loss 0.39 ACC 0.53125 on epoch=824
06/21/2022 15:38:15 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.40 on epoch=829
06/21/2022 15:38:17 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.45 on epoch=834
06/21/2022 15:38:18 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.43 on epoch=839
06/21/2022 15:38:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.37 on epoch=844
06/21/2022 15:38:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
06/21/2022 15:38:21 - INFO - __main__ - Global step 1700 Train loss 0.41 ACC 0.5 on epoch=849
06/21/2022 15:38:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.41 on epoch=854
06/21/2022 15:38:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.42 on epoch=859
06/21/2022 15:38:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.41 on epoch=864
06/21/2022 15:38:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.45 on epoch=869
06/21/2022 15:38:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.42 on epoch=874
06/21/2022 15:38:29 - INFO - __main__ - Global step 1750 Train loss 0.42 ACC 0.5 on epoch=874
06/21/2022 15:38:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.41 on epoch=879
06/21/2022 15:38:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
06/21/2022 15:38:33 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
06/21/2022 15:38:34 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.37 on epoch=894
06/21/2022 15:38:36 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.40 on epoch=899
06/21/2022 15:38:36 - INFO - __main__ - Global step 1800 Train loss 0.37 ACC 0.5 on epoch=899
06/21/2022 15:38:38 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
06/21/2022 15:38:39 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.36 on epoch=909
06/21/2022 15:38:40 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.32 on epoch=914
06/21/2022 15:38:42 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.44 on epoch=919
06/21/2022 15:38:43 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
06/21/2022 15:38:43 - INFO - __main__ - Global step 1850 Train loss 0.38 ACC 0.4375 on epoch=924
06/21/2022 15:38:45 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.43 on epoch=929
06/21/2022 15:38:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.36 on epoch=934
06/21/2022 15:38:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=939
06/21/2022 15:38:49 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.32 on epoch=944
06/21/2022 15:38:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.36 on epoch=949
06/21/2022 15:38:50 - INFO - __main__ - Global step 1900 Train loss 0.37 ACC 0.4375 on epoch=949
06/21/2022 15:38:52 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
06/21/2022 15:38:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.39 on epoch=959
06/21/2022 15:38:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.35 on epoch=964
06/21/2022 15:38:56 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.39 on epoch=969
06/21/2022 15:38:58 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.34 on epoch=974
06/21/2022 15:38:58 - INFO - __main__ - Global step 1950 Train loss 0.37 ACC 0.4375 on epoch=974
06/21/2022 15:39:00 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.40 on epoch=979
06/21/2022 15:39:01 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.30 on epoch=984
06/21/2022 15:39:02 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.36 on epoch=989
06/21/2022 15:39:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.36 on epoch=994
06/21/2022 15:39:05 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.33 on epoch=999
06/21/2022 15:39:06 - INFO - __main__ - Global step 2000 Train loss 0.35 ACC 0.59375 on epoch=999
06/21/2022 15:39:06 - INFO - __main__ - save last model!
06/21/2022 15:39:06 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 15:39:06 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 15:39:06 - INFO - __main__ - Printing 3 examples
06/21/2022 15:39:06 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 15:39:06 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 15:39:06 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:39:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 15:39:06 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:39:06 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:39:06 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:39:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:39:06 - INFO - __main__ - Printing 3 examples
06/21/2022 15:39:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/21/2022 15:39:06 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/21/2022 15:39:06 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:06 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/21/2022 15:39:06 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:06 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 15:39:06 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:39:06 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:39:06 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:39:06 - INFO - __main__ - Printing 3 examples
06/21/2022 15:39:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/21/2022 15:39:06 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:06 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/21/2022 15:39:06 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:06 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/21/2022 15:39:06 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:06 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:39:06 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:39:06 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:39:06 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 15:39:13 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:39:13 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:39:13 - INFO - __main__ - Starting training!
06/21/2022 15:39:14 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.4_8_predictions.txt
06/21/2022 15:39:14 - INFO - __main__ - ACC on test data: 0.5735
06/21/2022 15:39:14 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.4, bsz=8, dev_performance=0.65625, test_performance=0.5735294117647058
06/21/2022 15:39:14 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.3, bsz=8 ...
06/21/2022 15:39:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:39:15 - INFO - __main__ - Printing 3 examples
06/21/2022 15:39:15 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/21/2022 15:39:15 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/21/2022 15:39:15 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:15 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/21/2022 15:39:15 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 15:39:15 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:39:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:39:15 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:39:15 - INFO - __main__ - Printing 3 examples
06/21/2022 15:39:15 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/21/2022 15:39:15 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:15 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/21/2022 15:39:15 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:15 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/21/2022 15:39:15 - INFO - __main__ - ['equivalent']
06/21/2022 15:39:15 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:39:15 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:39:15 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:39:21 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:39:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:39:21 - INFO - __main__ - Starting training!
06/21/2022 15:39:23 - INFO - __main__ - Step 10 Global step 10 Train loss 6.85 on epoch=4
06/21/2022 15:39:24 - INFO - __main__ - Step 20 Global step 20 Train loss 6.91 on epoch=9
06/21/2022 15:39:25 - INFO - __main__ - Step 30 Global step 30 Train loss 6.90 on epoch=14
06/21/2022 15:39:27 - INFO - __main__ - Step 40 Global step 40 Train loss 6.87 on epoch=19
06/21/2022 15:39:28 - INFO - __main__ - Step 50 Global step 50 Train loss 6.82 on epoch=24
06/21/2022 15:39:32 - INFO - __main__ - Global step 50 Train loss 6.87 ACC 0.0 on epoch=24
06/21/2022 15:39:32 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 15:39:33 - INFO - __main__ - Step 60 Global step 60 Train loss 6.79 on epoch=29
06/21/2022 15:39:34 - INFO - __main__ - Step 70 Global step 70 Train loss 6.87 on epoch=34
06/21/2022 15:39:36 - INFO - __main__ - Step 80 Global step 80 Train loss 6.72 on epoch=39
06/21/2022 15:39:37 - INFO - __main__ - Step 90 Global step 90 Train loss 6.79 on epoch=44
06/21/2022 15:39:38 - INFO - __main__ - Step 100 Global step 100 Train loss 6.74 on epoch=49
06/21/2022 15:39:39 - INFO - __main__ - Global step 100 Train loss 6.78 ACC 0.0 on epoch=49
06/21/2022 15:39:41 - INFO - __main__ - Step 110 Global step 110 Train loss 6.70 on epoch=54
06/21/2022 15:39:42 - INFO - __main__ - Step 120 Global step 120 Train loss 6.70 on epoch=59
06/21/2022 15:39:44 - INFO - __main__ - Step 130 Global step 130 Train loss 6.68 on epoch=64
06/21/2022 15:39:45 - INFO - __main__ - Step 140 Global step 140 Train loss 6.66 on epoch=69
06/21/2022 15:39:47 - INFO - __main__ - Step 150 Global step 150 Train loss 6.55 on epoch=74
06/21/2022 15:39:57 - INFO - __main__ - Global step 150 Train loss 6.66 ACC 0.0 on epoch=74
06/21/2022 15:39:58 - INFO - __main__ - Step 160 Global step 160 Train loss 6.59 on epoch=79
06/21/2022 15:40:00 - INFO - __main__ - Step 170 Global step 170 Train loss 6.54 on epoch=84
06/21/2022 15:40:01 - INFO - __main__ - Step 180 Global step 180 Train loss 6.48 on epoch=89
06/21/2022 15:40:03 - INFO - __main__ - Step 190 Global step 190 Train loss 6.50 on epoch=94
06/21/2022 15:40:04 - INFO - __main__ - Step 200 Global step 200 Train loss 6.35 on epoch=99
06/21/2022 15:40:12 - INFO - __main__ - Global step 200 Train loss 6.49 ACC 0.0 on epoch=99
06/21/2022 15:40:14 - INFO - __main__ - Step 210 Global step 210 Train loss 6.44 on epoch=104
06/21/2022 15:40:15 - INFO - __main__ - Step 220 Global step 220 Train loss 6.34 on epoch=109
06/21/2022 15:40:17 - INFO - __main__ - Step 230 Global step 230 Train loss 6.27 on epoch=114
06/21/2022 15:40:18 - INFO - __main__ - Step 240 Global step 240 Train loss 6.21 on epoch=119
06/21/2022 15:40:19 - INFO - __main__ - Step 250 Global step 250 Train loss 6.20 on epoch=124
06/21/2022 15:40:30 - INFO - __main__ - Global step 250 Train loss 6.29 ACC 0.0 on epoch=124
06/21/2022 15:40:31 - INFO - __main__ - Step 260 Global step 260 Train loss 6.20 on epoch=129
06/21/2022 15:40:33 - INFO - __main__ - Step 270 Global step 270 Train loss 6.17 on epoch=134
06/21/2022 15:40:34 - INFO - __main__ - Step 280 Global step 280 Train loss 6.28 on epoch=139
06/21/2022 15:40:35 - INFO - __main__ - Step 290 Global step 290 Train loss 6.13 on epoch=144
06/21/2022 15:40:37 - INFO - __main__ - Step 300 Global step 300 Train loss 6.17 on epoch=149
06/21/2022 15:40:43 - INFO - __main__ - Global step 300 Train loss 6.19 ACC 0.0 on epoch=149
06/21/2022 15:40:45 - INFO - __main__ - Step 310 Global step 310 Train loss 6.13 on epoch=154
06/21/2022 15:40:46 - INFO - __main__ - Step 320 Global step 320 Train loss 6.08 on epoch=159
06/21/2022 15:40:47 - INFO - __main__ - Step 330 Global step 330 Train loss 6.01 on epoch=164
06/21/2022 15:40:49 - INFO - __main__ - Step 340 Global step 340 Train loss 6.06 on epoch=169
06/21/2022 15:40:50 - INFO - __main__ - Step 350 Global step 350 Train loss 5.99 on epoch=174
06/21/2022 15:40:56 - INFO - __main__ - Global step 350 Train loss 6.05 ACC 0.0 on epoch=174
06/21/2022 15:40:57 - INFO - __main__ - Step 360 Global step 360 Train loss 5.99 on epoch=179
06/21/2022 15:40:59 - INFO - __main__ - Step 370 Global step 370 Train loss 5.99 on epoch=184
06/21/2022 15:41:00 - INFO - __main__ - Step 380 Global step 380 Train loss 5.97 on epoch=189
06/21/2022 15:41:02 - INFO - __main__ - Step 390 Global step 390 Train loss 5.81 on epoch=194
06/21/2022 15:41:03 - INFO - __main__ - Step 400 Global step 400 Train loss 5.89 on epoch=199
06/21/2022 15:41:13 - INFO - __main__ - Global step 400 Train loss 5.93 ACC 0.0 on epoch=199
06/21/2022 15:41:15 - INFO - __main__ - Step 410 Global step 410 Train loss 5.77 on epoch=204
06/21/2022 15:41:16 - INFO - __main__ - Step 420 Global step 420 Train loss 5.74 on epoch=209
06/21/2022 15:41:17 - INFO - __main__ - Step 430 Global step 430 Train loss 5.78 on epoch=214
06/21/2022 15:41:19 - INFO - __main__ - Step 440 Global step 440 Train loss 5.59 on epoch=219
06/21/2022 15:41:21 - INFO - __main__ - Step 450 Global step 450 Train loss 5.58 on epoch=224
06/21/2022 15:41:23 - INFO - __main__ - Global step 450 Train loss 5.69 ACC 0.0 on epoch=224
06/21/2022 15:41:25 - INFO - __main__ - Step 460 Global step 460 Train loss 5.59 on epoch=229
06/21/2022 15:41:27 - INFO - __main__ - Step 470 Global step 470 Train loss 5.53 on epoch=234
06/21/2022 15:41:28 - INFO - __main__ - Step 480 Global step 480 Train loss 5.63 on epoch=239
06/21/2022 15:41:30 - INFO - __main__ - Step 490 Global step 490 Train loss 5.62 on epoch=244
06/21/2022 15:41:32 - INFO - __main__ - Step 500 Global step 500 Train loss 5.62 on epoch=249
06/21/2022 15:41:38 - INFO - __main__ - Global step 500 Train loss 5.60 ACC 0.0 on epoch=249
06/21/2022 15:41:39 - INFO - __main__ - Step 510 Global step 510 Train loss 5.60 on epoch=254
06/21/2022 15:41:41 - INFO - __main__ - Step 520 Global step 520 Train loss 5.41 on epoch=259
06/21/2022 15:41:42 - INFO - __main__ - Step 530 Global step 530 Train loss 5.31 on epoch=264
06/21/2022 15:41:43 - INFO - __main__ - Step 540 Global step 540 Train loss 5.38 on epoch=269
06/21/2022 15:41:45 - INFO - __main__ - Step 550 Global step 550 Train loss 5.35 on epoch=274
06/21/2022 15:41:47 - INFO - __main__ - Global step 550 Train loss 5.41 ACC 0.0 on epoch=274
06/21/2022 15:41:48 - INFO - __main__ - Step 560 Global step 560 Train loss 5.31 on epoch=279
06/21/2022 15:41:50 - INFO - __main__ - Step 570 Global step 570 Train loss 5.41 on epoch=284
06/21/2022 15:41:51 - INFO - __main__ - Step 580 Global step 580 Train loss 5.23 on epoch=289
06/21/2022 15:41:53 - INFO - __main__ - Step 590 Global step 590 Train loss 5.20 on epoch=294
06/21/2022 15:41:54 - INFO - __main__ - Step 600 Global step 600 Train loss 5.11 on epoch=299
06/21/2022 15:41:58 - INFO - __main__ - Global step 600 Train loss 5.25 ACC 0.0 on epoch=299
06/21/2022 15:42:00 - INFO - __main__ - Step 610 Global step 610 Train loss 5.22 on epoch=304
06/21/2022 15:42:01 - INFO - __main__ - Step 620 Global step 620 Train loss 5.14 on epoch=309
06/21/2022 15:42:03 - INFO - __main__ - Step 630 Global step 630 Train loss 5.02 on epoch=314
06/21/2022 15:42:05 - INFO - __main__ - Step 640 Global step 640 Train loss 5.07 on epoch=319
06/21/2022 15:42:06 - INFO - __main__ - Step 650 Global step 650 Train loss 5.01 on epoch=324
06/21/2022 15:42:07 - INFO - __main__ - Global step 650 Train loss 5.09 ACC 0.0 on epoch=324
06/21/2022 15:42:09 - INFO - __main__ - Step 660 Global step 660 Train loss 5.04 on epoch=329
06/21/2022 15:42:10 - INFO - __main__ - Step 670 Global step 670 Train loss 5.04 on epoch=334
06/21/2022 15:42:11 - INFO - __main__ - Step 680 Global step 680 Train loss 5.15 on epoch=339
06/21/2022 15:42:13 - INFO - __main__ - Step 690 Global step 690 Train loss 5.06 on epoch=344
06/21/2022 15:42:14 - INFO - __main__ - Step 700 Global step 700 Train loss 4.95 on epoch=349
06/21/2022 15:42:20 - INFO - __main__ - Global step 700 Train loss 5.05 ACC 0.0 on epoch=349
06/21/2022 15:42:22 - INFO - __main__ - Step 710 Global step 710 Train loss 4.80 on epoch=354
06/21/2022 15:42:23 - INFO - __main__ - Step 720 Global step 720 Train loss 4.78 on epoch=359
06/21/2022 15:42:24 - INFO - __main__ - Step 730 Global step 730 Train loss 4.86 on epoch=364
06/21/2022 15:42:26 - INFO - __main__ - Step 740 Global step 740 Train loss 4.79 on epoch=369
06/21/2022 15:42:27 - INFO - __main__ - Step 750 Global step 750 Train loss 4.72 on epoch=374
06/21/2022 15:42:28 - INFO - __main__ - Global step 750 Train loss 4.79 ACC 0.0 on epoch=374
06/21/2022 15:42:30 - INFO - __main__ - Step 760 Global step 760 Train loss 4.61 on epoch=379
06/21/2022 15:42:31 - INFO - __main__ - Step 770 Global step 770 Train loss 4.76 on epoch=384
06/21/2022 15:42:33 - INFO - __main__ - Step 780 Global step 780 Train loss 4.61 on epoch=389
06/21/2022 15:42:34 - INFO - __main__ - Step 790 Global step 790 Train loss 4.58 on epoch=394
06/21/2022 15:42:36 - INFO - __main__ - Step 800 Global step 800 Train loss 4.46 on epoch=399
06/21/2022 15:42:37 - INFO - __main__ - Global step 800 Train loss 4.60 ACC 0.0 on epoch=399
06/21/2022 15:42:38 - INFO - __main__ - Step 810 Global step 810 Train loss 4.51 on epoch=404
06/21/2022 15:42:39 - INFO - __main__ - Step 820 Global step 820 Train loss 4.35 on epoch=409
06/21/2022 15:42:41 - INFO - __main__ - Step 830 Global step 830 Train loss 4.49 on epoch=414
06/21/2022 15:42:42 - INFO - __main__ - Step 840 Global step 840 Train loss 4.42 on epoch=419
06/21/2022 15:42:43 - INFO - __main__ - Step 850 Global step 850 Train loss 4.34 on epoch=424
06/21/2022 15:42:44 - INFO - __main__ - Global step 850 Train loss 4.42 ACC 0.0 on epoch=424
06/21/2022 15:42:46 - INFO - __main__ - Step 860 Global step 860 Train loss 4.22 on epoch=429
06/21/2022 15:42:47 - INFO - __main__ - Step 870 Global step 870 Train loss 4.26 on epoch=434
06/21/2022 15:42:49 - INFO - __main__ - Step 880 Global step 880 Train loss 4.12 on epoch=439
06/21/2022 15:42:50 - INFO - __main__ - Step 890 Global step 890 Train loss 4.08 on epoch=444
06/21/2022 15:42:51 - INFO - __main__ - Step 900 Global step 900 Train loss 4.08 on epoch=449
06/21/2022 15:42:52 - INFO - __main__ - Global step 900 Train loss 4.15 ACC 0.0 on epoch=449
06/21/2022 15:42:54 - INFO - __main__ - Step 910 Global step 910 Train loss 4.22 on epoch=454
06/21/2022 15:42:55 - INFO - __main__ - Step 920 Global step 920 Train loss 4.05 on epoch=459
06/21/2022 15:42:56 - INFO - __main__ - Step 930 Global step 930 Train loss 3.96 on epoch=464
06/21/2022 15:42:58 - INFO - __main__ - Step 940 Global step 940 Train loss 3.94 on epoch=469
06/21/2022 15:42:59 - INFO - __main__ - Step 950 Global step 950 Train loss 3.89 on epoch=474
06/21/2022 15:43:00 - INFO - __main__ - Global step 950 Train loss 4.01 ACC 0.0 on epoch=474
06/21/2022 15:43:02 - INFO - __main__ - Step 960 Global step 960 Train loss 3.86 on epoch=479
06/21/2022 15:43:03 - INFO - __main__ - Step 970 Global step 970 Train loss 3.69 on epoch=484
06/21/2022 15:43:05 - INFO - __main__ - Step 980 Global step 980 Train loss 3.74 on epoch=489
06/21/2022 15:43:06 - INFO - __main__ - Step 990 Global step 990 Train loss 3.84 on epoch=494
06/21/2022 15:43:08 - INFO - __main__ - Step 1000 Global step 1000 Train loss 3.74 on epoch=499
06/21/2022 15:43:09 - INFO - __main__ - Global step 1000 Train loss 3.77 ACC 0.0 on epoch=499
06/21/2022 15:43:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 3.71 on epoch=504
06/21/2022 15:43:12 - INFO - __main__ - Step 1020 Global step 1020 Train loss 3.71 on epoch=509
06/21/2022 15:43:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 3.65 on epoch=514
06/21/2022 15:43:15 - INFO - __main__ - Step 1040 Global step 1040 Train loss 3.48 on epoch=519
06/21/2022 15:43:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 3.58 on epoch=524
06/21/2022 15:43:18 - INFO - __main__ - Global step 1050 Train loss 3.63 ACC 0.0 on epoch=524
06/21/2022 15:43:19 - INFO - __main__ - Step 1060 Global step 1060 Train loss 3.49 on epoch=529
06/21/2022 15:43:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 3.42 on epoch=534
06/21/2022 15:43:22 - INFO - __main__ - Step 1080 Global step 1080 Train loss 3.30 on epoch=539
06/21/2022 15:43:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 3.40 on epoch=544
06/21/2022 15:43:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 3.35 on epoch=549
06/21/2022 15:43:28 - INFO - __main__ - Global step 1100 Train loss 3.39 ACC 0.0 on epoch=549
06/21/2022 15:43:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 3.27 on epoch=554
06/21/2022 15:43:30 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.25 on epoch=559
06/21/2022 15:43:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.17 on epoch=564
06/21/2022 15:43:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.17 on epoch=569
06/21/2022 15:43:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.25 on epoch=574
06/21/2022 15:43:38 - INFO - __main__ - Global step 1150 Train loss 3.22 ACC 0.0 on epoch=574
06/21/2022 15:43:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 2.91 on epoch=579
06/21/2022 15:43:40 - INFO - __main__ - Step 1170 Global step 1170 Train loss 2.97 on epoch=584
06/21/2022 15:43:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.05 on epoch=589
06/21/2022 15:43:43 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.05 on epoch=594
06/21/2022 15:43:44 - INFO - __main__ - Step 1200 Global step 1200 Train loss 2.96 on epoch=599
06/21/2022 15:43:47 - INFO - __main__ - Global step 1200 Train loss 2.99 ACC 0.0 on epoch=599
06/21/2022 15:43:49 - INFO - __main__ - Step 1210 Global step 1210 Train loss 2.86 on epoch=604
06/21/2022 15:43:50 - INFO - __main__ - Step 1220 Global step 1220 Train loss 2.75 on epoch=609
06/21/2022 15:43:52 - INFO - __main__ - Step 1230 Global step 1230 Train loss 2.75 on epoch=614
06/21/2022 15:43:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 2.64 on epoch=619
06/21/2022 15:43:54 - INFO - __main__ - Step 1250 Global step 1250 Train loss 2.63 on epoch=624
06/21/2022 15:43:57 - INFO - __main__ - Global step 1250 Train loss 2.73 ACC 0.0 on epoch=624
06/21/2022 15:43:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 2.64 on epoch=629
06/21/2022 15:44:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 2.53 on epoch=634
06/21/2022 15:44:01 - INFO - __main__ - Step 1280 Global step 1280 Train loss 2.44 on epoch=639
06/21/2022 15:44:02 - INFO - __main__ - Step 1290 Global step 1290 Train loss 2.53 on epoch=644
06/21/2022 15:44:04 - INFO - __main__ - Step 1300 Global step 1300 Train loss 2.42 on epoch=649
06/21/2022 15:44:07 - INFO - __main__ - Global step 1300 Train loss 2.51 ACC 0.0 on epoch=649
06/21/2022 15:44:08 - INFO - __main__ - Step 1310 Global step 1310 Train loss 2.45 on epoch=654
06/21/2022 15:44:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 2.45 on epoch=659
06/21/2022 15:44:11 - INFO - __main__ - Step 1330 Global step 1330 Train loss 2.36 on epoch=664
06/21/2022 15:44:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 2.44 on epoch=669
06/21/2022 15:44:14 - INFO - __main__ - Step 1350 Global step 1350 Train loss 2.37 on epoch=674
06/21/2022 15:44:16 - INFO - __main__ - Global step 1350 Train loss 2.41 ACC 0.0 on epoch=674
06/21/2022 15:44:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 2.43 on epoch=679
06/21/2022 15:44:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 2.35 on epoch=684
06/21/2022 15:44:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.29 on epoch=689
06/21/2022 15:44:21 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.19 on epoch=694
06/21/2022 15:44:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.19 on epoch=699
06/21/2022 15:44:25 - INFO - __main__ - Global step 1400 Train loss 2.29 ACC 0.0 on epoch=699
06/21/2022 15:44:27 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.19 on epoch=704
06/21/2022 15:44:28 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.11 on epoch=709
06/21/2022 15:44:30 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.11 on epoch=714
06/21/2022 15:44:31 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.10 on epoch=719
06/21/2022 15:44:32 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.14 on epoch=724
06/21/2022 15:44:34 - INFO - __main__ - Global step 1450 Train loss 2.13 ACC 0.0 on epoch=724
06/21/2022 15:44:36 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.11 on epoch=729
06/21/2022 15:44:37 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.05 on epoch=734
06/21/2022 15:44:39 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.00 on epoch=739
06/21/2022 15:44:40 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.04 on epoch=744
06/21/2022 15:44:42 - INFO - __main__ - Step 1500 Global step 1500 Train loss 1.95 on epoch=749
06/21/2022 15:44:43 - INFO - __main__ - Global step 1500 Train loss 2.03 ACC 0.21875 on epoch=749
06/21/2022 15:44:43 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.21875 on epoch=749, global_step=1500
06/21/2022 15:44:45 - INFO - __main__ - Step 1510 Global step 1510 Train loss 1.98 on epoch=754
06/21/2022 15:44:46 - INFO - __main__ - Step 1520 Global step 1520 Train loss 1.90 on epoch=759
06/21/2022 15:44:48 - INFO - __main__ - Step 1530 Global step 1530 Train loss 1.90 on epoch=764
06/21/2022 15:44:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 1.83 on epoch=769
06/21/2022 15:44:50 - INFO - __main__ - Step 1550 Global step 1550 Train loss 1.86 on epoch=774
06/21/2022 15:44:51 - INFO - __main__ - Global step 1550 Train loss 1.89 ACC 0.5 on epoch=774
06/21/2022 15:44:51 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.5 on epoch=774, global_step=1550
06/21/2022 15:44:53 - INFO - __main__ - Step 1560 Global step 1560 Train loss 1.95 on epoch=779
06/21/2022 15:44:54 - INFO - __main__ - Step 1570 Global step 1570 Train loss 1.88 on epoch=784
06/21/2022 15:44:55 - INFO - __main__ - Step 1580 Global step 1580 Train loss 1.79 on epoch=789
06/21/2022 15:44:57 - INFO - __main__ - Step 1590 Global step 1590 Train loss 1.82 on epoch=794
06/21/2022 15:44:58 - INFO - __main__ - Step 1600 Global step 1600 Train loss 1.84 on epoch=799
06/21/2022 15:44:59 - INFO - __main__ - Global step 1600 Train loss 1.86 ACC 0.5 on epoch=799
06/21/2022 15:45:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.73 on epoch=804
06/21/2022 15:45:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 1.78 on epoch=809
06/21/2022 15:45:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.60 on epoch=814
06/21/2022 15:45:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.66 on epoch=819
06/21/2022 15:45:06 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.68 on epoch=824
06/21/2022 15:45:07 - INFO - __main__ - Global step 1650 Train loss 1.69 ACC 0.5 on epoch=824
06/21/2022 15:45:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.61 on epoch=829
06/21/2022 15:45:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.60 on epoch=834
06/21/2022 15:45:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.52 on epoch=839
06/21/2022 15:45:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.70 on epoch=844
06/21/2022 15:45:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.51 on epoch=849
06/21/2022 15:45:16 - INFO - __main__ - Global step 1700 Train loss 1.59 ACC 0.5 on epoch=849
06/21/2022 15:45:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.68 on epoch=854
06/21/2022 15:45:19 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.55 on epoch=859
06/21/2022 15:45:20 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.53 on epoch=864
06/21/2022 15:45:21 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.42 on epoch=869
06/21/2022 15:45:23 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.43 on epoch=874
06/21/2022 15:45:25 - INFO - __main__ - Global step 1750 Train loss 1.52 ACC 0.5 on epoch=874
06/21/2022 15:45:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.38 on epoch=879
06/21/2022 15:45:28 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.50 on epoch=884
06/21/2022 15:45:29 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.40 on epoch=889
06/21/2022 15:45:31 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.29 on epoch=894
06/21/2022 15:45:32 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.34 on epoch=899
06/21/2022 15:45:34 - INFO - __main__ - Global step 1800 Train loss 1.38 ACC 0.5 on epoch=899
06/21/2022 15:45:35 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.32 on epoch=904
06/21/2022 15:45:37 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.22 on epoch=909
06/21/2022 15:45:38 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.24 on epoch=914
06/21/2022 15:45:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.21 on epoch=919
06/21/2022 15:45:41 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.25 on epoch=924
06/21/2022 15:45:43 - INFO - __main__ - Global step 1850 Train loss 1.25 ACC 0.5 on epoch=924
06/21/2022 15:45:44 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.20 on epoch=929
06/21/2022 15:45:46 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.23 on epoch=934
06/21/2022 15:45:47 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.20 on epoch=939
06/21/2022 15:45:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.13 on epoch=944
06/21/2022 15:45:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.10 on epoch=949
06/21/2022 15:45:51 - INFO - __main__ - Global step 1900 Train loss 1.17 ACC 0.5 on epoch=949
06/21/2022 15:45:53 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.06 on epoch=954
06/21/2022 15:45:54 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.10 on epoch=959
06/21/2022 15:45:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.07 on epoch=964
06/21/2022 15:45:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.03 on epoch=969
06/21/2022 15:45:59 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.94 on epoch=974
06/21/2022 15:46:06 - INFO - __main__ - Global step 1950 Train loss 1.04 ACC 0.46875 on epoch=974
06/21/2022 15:46:07 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.08 on epoch=979
06/21/2022 15:46:09 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.03 on epoch=984
06/21/2022 15:46:10 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.02 on epoch=989
06/21/2022 15:46:12 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.00 on epoch=994
06/21/2022 15:46:13 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.90 on epoch=999
06/21/2022 15:46:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:46:14 - INFO - __main__ - Printing 3 examples
06/21/2022 15:46:14 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/21/2022 15:46:14 - INFO - __main__ - ['equivalent']
06/21/2022 15:46:14 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/21/2022 15:46:14 - INFO - __main__ - ['equivalent']
06/21/2022 15:46:14 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/21/2022 15:46:14 - INFO - __main__ - ['equivalent']
06/21/2022 15:46:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 15:46:14 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:46:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:46:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:46:14 - INFO - __main__ - Printing 3 examples
06/21/2022 15:46:14 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/21/2022 15:46:14 - INFO - __main__ - ['equivalent']
06/21/2022 15:46:14 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/21/2022 15:46:14 - INFO - __main__ - ['equivalent']
06/21/2022 15:46:14 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/21/2022 15:46:14 - INFO - __main__ - ['equivalent']
06/21/2022 15:46:14 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:46:14 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:46:14 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:46:15 - INFO - __main__ - Global step 2000 Train loss 1.01 ACC 0.5625 on epoch=999
06/21/2022 15:46:16 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.5625 on epoch=999, global_step=2000
06/21/2022 15:46:16 - INFO - __main__ - save last model!
06/21/2022 15:46:16 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 15:46:16 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 15:46:16 - INFO - __main__ - Printing 3 examples
06/21/2022 15:46:16 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 15:46:16 - INFO - __main__ - ['equivalent']
06/21/2022 15:46:16 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 15:46:16 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:46:16 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 15:46:16 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:46:16 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:46:16 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:46:16 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 15:46:20 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:46:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:46:21 - INFO - __main__ - Starting training!
06/21/2022 15:47:08 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.3_8_predictions.txt
06/21/2022 15:47:08 - INFO - __main__ - ACC on test data: 0.3554
06/21/2022 15:47:08 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.3, bsz=8, dev_performance=0.5625, test_performance=0.3553921568627451
06/21/2022 15:47:08 - INFO - __main__ - Running ... prefix=glue-mrpc_16_13, lr=0.2, bsz=8 ...
06/21/2022 15:47:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:47:09 - INFO - __main__ - Printing 3 examples
06/21/2022 15:47:09 - INFO - __main__ -  [glue-mrpc] sentence 1: The court 's 1992 decision reaffirmed the basic findings of Roe protecting abortion choice but lessened the standards of protection guaranteed to women by Roe . [SEP] sentence 2: In a 1992 case , the Supreme Court reaffirmed the basic findings of Roe protecting abortion choice , but lessened the standards of protection guaranteed to women by Roe .
06/21/2022 15:47:09 - INFO - __main__ - ['equivalent']
06/21/2022 15:47:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Several cities are competing for the headquarters , including Miami ; Panama City ; Atlanta ; Port-of-Spain , Trinidad ; and Puebla , Mexico . [SEP] sentence 2: But Miami is competing with eight other cities , including Atlanta ; Panama City ; Port-of-Spain , Trinidad ; and Cancn , Mexico .
06/21/2022 15:47:09 - INFO - __main__ - ['equivalent']
06/21/2022 15:47:09 - INFO - __main__ -  [glue-mrpc] sentence 1: Feral 's group was behind a successful tourism boycott about a decade ago that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 . [SEP] sentence 2: Friends of Animals , which touts 200,000 members , was behind a successful tourism boycott that resulted in then-Gov . Walter J. Hickel imposing a moratorium on wolf control in 1992 .
06/21/2022 15:47:09 - INFO - __main__ - ['equivalent']
06/21/2022 15:47:09 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 15:47:09 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:47:09 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:47:09 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:47:09 - INFO - __main__ - Printing 3 examples
06/21/2022 15:47:09 - INFO - __main__ -  [glue-mrpc] sentence 1: The retailer said it came to the decision after hearing the opinions of customers and associates . [SEP] sentence 2: The decision came after " listening to our customers and associates , " Melissa Berryhill , a spokeswoman for Wal-Mart , said .
06/21/2022 15:47:09 - INFO - __main__ - ['equivalent']
06/21/2022 15:47:09 - INFO - __main__ -  [glue-mrpc] sentence 1: The vast majority of trades will be priced at 20 cents per contract or less depending on participation in incentive schemes . " [SEP] sentence 2: Eurex said " the vast majority " of trades on Eurex US would be priced at 20 cents per contract or less depending on " participation in incentive schemes " .
06/21/2022 15:47:09 - INFO - __main__ - ['equivalent']
06/21/2022 15:47:09 - INFO - __main__ -  [glue-mrpc] sentence 1: A grief-stricken old woman , disconsolate with grief , smeared her face with dirt , uttering : " My child , my child . " [SEP] sentence 2: One old woman , disconsolate with grief , smeared her face with dirt , only able to utter : " My child , my child . "
06/21/2022 15:47:09 - INFO - __main__ - ['equivalent']
06/21/2022 15:47:09 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:47:09 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:47:09 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:47:14 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:47:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:47:15 - INFO - __main__ - Starting training!
06/21/2022 15:47:16 - INFO - __main__ - Step 10 Global step 10 Train loss 6.82 on epoch=4
06/21/2022 15:47:18 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/21/2022 15:47:19 - INFO - __main__ - Step 30 Global step 30 Train loss 6.89 on epoch=14
06/21/2022 15:47:20 - INFO - __main__ - Step 40 Global step 40 Train loss 6.83 on epoch=19
06/21/2022 15:47:22 - INFO - __main__ - Step 50 Global step 50 Train loss 6.87 on epoch=24
06/21/2022 15:47:23 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/21/2022 15:47:23 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 15:47:24 - INFO - __main__ - Step 60 Global step 60 Train loss 6.85 on epoch=29
06/21/2022 15:47:25 - INFO - __main__ - Step 70 Global step 70 Train loss 6.71 on epoch=34
06/21/2022 15:47:27 - INFO - __main__ - Step 80 Global step 80 Train loss 6.84 on epoch=39
06/21/2022 15:47:28 - INFO - __main__ - Step 90 Global step 90 Train loss 6.80 on epoch=44
06/21/2022 15:47:30 - INFO - __main__ - Step 100 Global step 100 Train loss 6.79 on epoch=49
06/21/2022 15:47:31 - INFO - __main__ - Global step 100 Train loss 6.80 ACC 0.0 on epoch=49
06/21/2022 15:47:33 - INFO - __main__ - Step 110 Global step 110 Train loss 6.78 on epoch=54
06/21/2022 15:47:34 - INFO - __main__ - Step 120 Global step 120 Train loss 6.81 on epoch=59
06/21/2022 15:47:36 - INFO - __main__ - Step 130 Global step 130 Train loss 6.80 on epoch=64
06/21/2022 15:47:37 - INFO - __main__ - Step 140 Global step 140 Train loss 6.79 on epoch=69
06/21/2022 15:47:39 - INFO - __main__ - Step 150 Global step 150 Train loss 6.77 on epoch=74
06/21/2022 15:47:40 - INFO - __main__ - Global step 150 Train loss 6.79 ACC 0.0 on epoch=74
06/21/2022 15:47:41 - INFO - __main__ - Step 160 Global step 160 Train loss 6.73 on epoch=79
06/21/2022 15:47:42 - INFO - __main__ - Step 170 Global step 170 Train loss 6.76 on epoch=84
06/21/2022 15:47:44 - INFO - __main__ - Step 180 Global step 180 Train loss 6.73 on epoch=89
06/21/2022 15:47:45 - INFO - __main__ - Step 190 Global step 190 Train loss 6.76 on epoch=94
06/21/2022 15:47:47 - INFO - __main__ - Step 200 Global step 200 Train loss 6.65 on epoch=99
06/21/2022 15:47:51 - INFO - __main__ - Global step 200 Train loss 6.73 ACC 0.0 on epoch=99
06/21/2022 15:47:52 - INFO - __main__ - Step 210 Global step 210 Train loss 6.74 on epoch=104
06/21/2022 15:47:54 - INFO - __main__ - Step 220 Global step 220 Train loss 6.66 on epoch=109
06/21/2022 15:47:55 - INFO - __main__ - Step 230 Global step 230 Train loss 6.66 on epoch=114
06/21/2022 15:47:56 - INFO - __main__ - Step 240 Global step 240 Train loss 6.55 on epoch=119
06/21/2022 15:47:58 - INFO - __main__ - Step 250 Global step 250 Train loss 6.64 on epoch=124
06/21/2022 15:48:04 - INFO - __main__ - Global step 250 Train loss 6.65 ACC 0.0 on epoch=124
06/21/2022 15:48:05 - INFO - __main__ - Step 260 Global step 260 Train loss 6.51 on epoch=129
06/21/2022 15:48:07 - INFO - __main__ - Step 270 Global step 270 Train loss 6.54 on epoch=134
06/21/2022 15:48:08 - INFO - __main__ - Step 280 Global step 280 Train loss 6.58 on epoch=139
06/21/2022 15:48:10 - INFO - __main__ - Step 290 Global step 290 Train loss 6.53 on epoch=144
06/21/2022 15:48:11 - INFO - __main__ - Step 300 Global step 300 Train loss 6.49 on epoch=149
06/21/2022 15:48:18 - INFO - __main__ - Global step 300 Train loss 6.53 ACC 0.0 on epoch=149
06/21/2022 15:48:19 - INFO - __main__ - Step 310 Global step 310 Train loss 6.49 on epoch=154
06/21/2022 15:48:21 - INFO - __main__ - Step 320 Global step 320 Train loss 6.47 on epoch=159
06/21/2022 15:48:22 - INFO - __main__ - Step 330 Global step 330 Train loss 6.51 on epoch=164
06/21/2022 15:48:23 - INFO - __main__ - Step 340 Global step 340 Train loss 6.32 on epoch=169
06/21/2022 15:48:25 - INFO - __main__ - Step 350 Global step 350 Train loss 6.35 on epoch=174
06/21/2022 15:48:29 - INFO - __main__ - Global step 350 Train loss 6.43 ACC 0.0 on epoch=174
06/21/2022 15:48:31 - INFO - __main__ - Step 360 Global step 360 Train loss 6.37 on epoch=179
06/21/2022 15:48:32 - INFO - __main__ - Step 370 Global step 370 Train loss 6.30 on epoch=184
06/21/2022 15:48:34 - INFO - __main__ - Step 380 Global step 380 Train loss 6.30 on epoch=189
06/21/2022 15:48:35 - INFO - __main__ - Step 390 Global step 390 Train loss 6.33 on epoch=194
06/21/2022 15:48:36 - INFO - __main__ - Step 400 Global step 400 Train loss 6.19 on epoch=199
06/21/2022 15:48:43 - INFO - __main__ - Global step 400 Train loss 6.30 ACC 0.0 on epoch=199
06/21/2022 15:48:44 - INFO - __main__ - Step 410 Global step 410 Train loss 6.13 on epoch=204
06/21/2022 15:48:46 - INFO - __main__ - Step 420 Global step 420 Train loss 6.19 on epoch=209
06/21/2022 15:48:47 - INFO - __main__ - Step 430 Global step 430 Train loss 6.11 on epoch=214
06/21/2022 15:48:49 - INFO - __main__ - Step 440 Global step 440 Train loss 6.21 on epoch=219
06/21/2022 15:48:50 - INFO - __main__ - Step 450 Global step 450 Train loss 6.21 on epoch=224
06/21/2022 15:49:02 - INFO - __main__ - Global step 450 Train loss 6.17 ACC 0.0 on epoch=224
06/21/2022 15:49:03 - INFO - __main__ - Step 460 Global step 460 Train loss 6.21 on epoch=229
06/21/2022 15:49:05 - INFO - __main__ - Step 470 Global step 470 Train loss 6.14 on epoch=234
06/21/2022 15:49:06 - INFO - __main__ - Step 480 Global step 480 Train loss 6.07 on epoch=239
06/21/2022 15:49:08 - INFO - __main__ - Step 490 Global step 490 Train loss 6.18 on epoch=244
06/21/2022 15:49:09 - INFO - __main__ - Step 500 Global step 500 Train loss 5.99 on epoch=249
06/21/2022 15:49:21 - INFO - __main__ - Global step 500 Train loss 6.12 ACC 0.0 on epoch=249
06/21/2022 15:49:22 - INFO - __main__ - Step 510 Global step 510 Train loss 5.92 on epoch=254
06/21/2022 15:49:24 - INFO - __main__ - Step 520 Global step 520 Train loss 6.03 on epoch=259
06/21/2022 15:49:25 - INFO - __main__ - Step 530 Global step 530 Train loss 5.94 on epoch=264
06/21/2022 15:49:27 - INFO - __main__ - Step 540 Global step 540 Train loss 5.90 on epoch=269
06/21/2022 15:49:28 - INFO - __main__ - Step 550 Global step 550 Train loss 5.91 on epoch=274
06/21/2022 15:49:40 - INFO - __main__ - Global step 550 Train loss 5.94 ACC 0.0 on epoch=274
06/21/2022 15:49:41 - INFO - __main__ - Step 560 Global step 560 Train loss 5.92 on epoch=279
06/21/2022 15:49:42 - INFO - __main__ - Step 570 Global step 570 Train loss 5.84 on epoch=284
06/21/2022 15:49:44 - INFO - __main__ - Step 580 Global step 580 Train loss 5.88 on epoch=289
06/21/2022 15:49:45 - INFO - __main__ - Step 590 Global step 590 Train loss 5.84 on epoch=294
06/21/2022 15:49:47 - INFO - __main__ - Step 600 Global step 600 Train loss 5.79 on epoch=299
06/21/2022 15:49:55 - INFO - __main__ - Global step 600 Train loss 5.86 ACC 0.0 on epoch=299
06/21/2022 15:49:57 - INFO - __main__ - Step 610 Global step 610 Train loss 5.77 on epoch=304
06/21/2022 15:49:58 - INFO - __main__ - Step 620 Global step 620 Train loss 5.83 on epoch=309
06/21/2022 15:50:00 - INFO - __main__ - Step 630 Global step 630 Train loss 5.85 on epoch=314
06/21/2022 15:50:02 - INFO - __main__ - Step 640 Global step 640 Train loss 5.79 on epoch=319
06/21/2022 15:50:03 - INFO - __main__ - Step 650 Global step 650 Train loss 5.75 on epoch=324
06/21/2022 15:50:15 - INFO - __main__ - Global step 650 Train loss 5.80 ACC 0.0 on epoch=324
06/21/2022 15:50:17 - INFO - __main__ - Step 660 Global step 660 Train loss 5.79 on epoch=329
06/21/2022 15:50:19 - INFO - __main__ - Step 670 Global step 670 Train loss 5.69 on epoch=334
06/21/2022 15:50:20 - INFO - __main__ - Step 680 Global step 680 Train loss 5.71 on epoch=339
06/21/2022 15:50:22 - INFO - __main__ - Step 690 Global step 690 Train loss 5.66 on epoch=344
06/21/2022 15:50:24 - INFO - __main__ - Step 700 Global step 700 Train loss 5.65 on epoch=349
06/21/2022 15:50:32 - INFO - __main__ - Global step 700 Train loss 5.70 ACC 0.0 on epoch=349
06/21/2022 15:50:33 - INFO - __main__ - Step 710 Global step 710 Train loss 5.60 on epoch=354
06/21/2022 15:50:35 - INFO - __main__ - Step 720 Global step 720 Train loss 5.68 on epoch=359
06/21/2022 15:50:36 - INFO - __main__ - Step 730 Global step 730 Train loss 5.65 on epoch=364
06/21/2022 15:50:37 - INFO - __main__ - Step 740 Global step 740 Train loss 5.65 on epoch=369
06/21/2022 15:50:39 - INFO - __main__ - Step 750 Global step 750 Train loss 5.63 on epoch=374
06/21/2022 15:50:49 - INFO - __main__ - Global step 750 Train loss 5.64 ACC 0.0 on epoch=374
06/21/2022 15:50:50 - INFO - __main__ - Step 760 Global step 760 Train loss 5.50 on epoch=379
06/21/2022 15:50:52 - INFO - __main__ - Step 770 Global step 770 Train loss 5.52 on epoch=384
06/21/2022 15:50:53 - INFO - __main__ - Step 780 Global step 780 Train loss 5.52 on epoch=389
06/21/2022 15:50:54 - INFO - __main__ - Step 790 Global step 790 Train loss 5.41 on epoch=394
06/21/2022 15:50:56 - INFO - __main__ - Step 800 Global step 800 Train loss 5.47 on epoch=399
06/21/2022 15:51:02 - INFO - __main__ - Global step 800 Train loss 5.48 ACC 0.0 on epoch=399
06/21/2022 15:51:03 - INFO - __main__ - Step 810 Global step 810 Train loss 5.37 on epoch=404
06/21/2022 15:51:05 - INFO - __main__ - Step 820 Global step 820 Train loss 5.41 on epoch=409
06/21/2022 15:51:06 - INFO - __main__ - Step 830 Global step 830 Train loss 5.40 on epoch=414
06/21/2022 15:51:07 - INFO - __main__ - Step 840 Global step 840 Train loss 5.38 on epoch=419
06/21/2022 15:51:09 - INFO - __main__ - Step 850 Global step 850 Train loss 5.21 on epoch=424
06/21/2022 15:51:12 - INFO - __main__ - Global step 850 Train loss 5.35 ACC 0.0 on epoch=424
06/21/2022 15:51:13 - INFO - __main__ - Step 860 Global step 860 Train loss 5.21 on epoch=429
06/21/2022 15:51:14 - INFO - __main__ - Step 870 Global step 870 Train loss 5.34 on epoch=434
06/21/2022 15:51:16 - INFO - __main__ - Step 880 Global step 880 Train loss 5.14 on epoch=439
06/21/2022 15:51:17 - INFO - __main__ - Step 890 Global step 890 Train loss 5.12 on epoch=444
06/21/2022 15:51:18 - INFO - __main__ - Step 900 Global step 900 Train loss 5.20 on epoch=449
06/21/2022 15:51:24 - INFO - __main__ - Global step 900 Train loss 5.20 ACC 0.0 on epoch=449
06/21/2022 15:51:25 - INFO - __main__ - Step 910 Global step 910 Train loss 5.14 on epoch=454
06/21/2022 15:51:26 - INFO - __main__ - Step 920 Global step 920 Train loss 5.14 on epoch=459
06/21/2022 15:51:28 - INFO - __main__ - Step 930 Global step 930 Train loss 5.02 on epoch=464
06/21/2022 15:51:29 - INFO - __main__ - Step 940 Global step 940 Train loss 5.01 on epoch=469
06/21/2022 15:51:31 - INFO - __main__ - Step 950 Global step 950 Train loss 4.96 on epoch=474
06/21/2022 15:51:32 - INFO - __main__ - Global step 950 Train loss 5.05 ACC 0.0 on epoch=474
06/21/2022 15:51:33 - INFO - __main__ - Step 960 Global step 960 Train loss 4.93 on epoch=479
06/21/2022 15:51:35 - INFO - __main__ - Step 970 Global step 970 Train loss 4.87 on epoch=484
06/21/2022 15:51:36 - INFO - __main__ - Step 980 Global step 980 Train loss 4.82 on epoch=489
06/21/2022 15:51:37 - INFO - __main__ - Step 990 Global step 990 Train loss 4.69 on epoch=494
06/21/2022 15:51:39 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.68 on epoch=499
06/21/2022 15:51:42 - INFO - __main__ - Global step 1000 Train loss 4.80 ACC 0.0 on epoch=499
06/21/2022 15:51:43 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.60 on epoch=504
06/21/2022 15:51:44 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.56 on epoch=509
06/21/2022 15:51:46 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.54 on epoch=514
06/21/2022 15:51:47 - INFO - __main__ - Step 1040 Global step 1040 Train loss 4.59 on epoch=519
06/21/2022 15:51:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 4.51 on epoch=524
06/21/2022 15:51:51 - INFO - __main__ - Global step 1050 Train loss 4.56 ACC 0.0 on epoch=524
06/21/2022 15:51:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.44 on epoch=529
06/21/2022 15:51:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.24 on epoch=534
06/21/2022 15:51:55 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.19 on epoch=539
06/21/2022 15:51:57 - INFO - __main__ - Step 1090 Global step 1090 Train loss 4.20 on epoch=544
06/21/2022 15:51:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.23 on epoch=549
06/21/2022 15:52:02 - INFO - __main__ - Global step 1100 Train loss 4.26 ACC 0.0 on epoch=549
06/21/2022 15:52:03 - INFO - __main__ - Step 1110 Global step 1110 Train loss 4.18 on epoch=554
06/21/2022 15:52:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 4.02 on epoch=559
06/21/2022 15:52:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.98 on epoch=564
06/21/2022 15:52:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.81 on epoch=569
06/21/2022 15:52:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 3.89 on epoch=574
06/21/2022 15:52:12 - INFO - __main__ - Global step 1150 Train loss 3.98 ACC 0.0 on epoch=574
06/21/2022 15:52:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 3.71 on epoch=579
06/21/2022 15:52:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.84 on epoch=584
06/21/2022 15:52:16 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.77 on epoch=589
06/21/2022 15:52:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.54 on epoch=594
06/21/2022 15:52:19 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.53 on epoch=599
06/21/2022 15:52:23 - INFO - __main__ - Global step 1200 Train loss 3.68 ACC 0.0 on epoch=599
06/21/2022 15:52:24 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.61 on epoch=604
06/21/2022 15:52:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.60 on epoch=609
06/21/2022 15:52:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.58 on epoch=614
06/21/2022 15:52:28 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.60 on epoch=619
06/21/2022 15:52:29 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.42 on epoch=624
06/21/2022 15:52:36 - INFO - __main__ - Global step 1250 Train loss 3.56 ACC 0.0 on epoch=624
06/21/2022 15:52:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.46 on epoch=629
06/21/2022 15:52:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.41 on epoch=634
06/21/2022 15:52:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.28 on epoch=639
06/21/2022 15:52:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.35 on epoch=644
06/21/2022 15:52:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.27 on epoch=649
06/21/2022 15:52:47 - INFO - __main__ - Global step 1300 Train loss 3.35 ACC 0.0 on epoch=649
06/21/2022 15:52:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.23 on epoch=654
06/21/2022 15:52:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.23 on epoch=659
06/21/2022 15:52:51 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.15 on epoch=664
06/21/2022 15:52:52 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.09 on epoch=669
06/21/2022 15:52:54 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.04 on epoch=674
06/21/2022 15:53:02 - INFO - __main__ - Global step 1350 Train loss 3.15 ACC 0.09375 on epoch=674
06/21/2022 15:53:02 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=674, global_step=1350
06/21/2022 15:53:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.03 on epoch=679
06/21/2022 15:53:04 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.13 on epoch=684
06/21/2022 15:53:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 2.86 on epoch=689
06/21/2022 15:53:07 - INFO - __main__ - Step 1390 Global step 1390 Train loss 2.81 on epoch=694
06/21/2022 15:53:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 2.87 on epoch=699
06/21/2022 15:53:12 - INFO - __main__ - Global step 1400 Train loss 2.94 ACC 0.21875 on epoch=699
06/21/2022 15:53:12 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.21875 on epoch=699, global_step=1400
06/21/2022 15:53:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 2.72 on epoch=704
06/21/2022 15:53:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 2.72 on epoch=709
06/21/2022 15:53:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 2.75 on epoch=714
06/21/2022 15:53:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 2.58 on epoch=719
06/21/2022 15:53:19 - INFO - __main__ - Step 1450 Global step 1450 Train loss 2.71 on epoch=724
06/21/2022 15:53:27 - INFO - __main__ - Global step 1450 Train loss 2.70 ACC 0.375 on epoch=724
06/21/2022 15:53:27 - INFO - __main__ - Saving model with best ACC: 0.21875 -> 0.375 on epoch=724, global_step=1450
06/21/2022 15:53:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 2.59 on epoch=729
06/21/2022 15:53:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 2.58 on epoch=734
06/21/2022 15:53:31 - INFO - __main__ - Step 1480 Global step 1480 Train loss 2.45 on epoch=739
06/21/2022 15:53:32 - INFO - __main__ - Step 1490 Global step 1490 Train loss 2.46 on epoch=744
06/21/2022 15:53:34 - INFO - __main__ - Step 1500 Global step 1500 Train loss 2.35 on epoch=749
06/21/2022 15:53:36 - INFO - __main__ - Global step 1500 Train loss 2.49 ACC 0.46875 on epoch=749
06/21/2022 15:53:36 - INFO - __main__ - Saving model with best ACC: 0.375 -> 0.46875 on epoch=749, global_step=1500
06/21/2022 15:53:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 2.40 on epoch=754
06/21/2022 15:53:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 2.34 on epoch=759
06/21/2022 15:53:40 - INFO - __main__ - Step 1530 Global step 1530 Train loss 2.21 on epoch=764
06/21/2022 15:53:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 2.25 on epoch=769
06/21/2022 15:53:43 - INFO - __main__ - Step 1550 Global step 1550 Train loss 2.11 on epoch=774
06/21/2022 15:53:47 - INFO - __main__ - Global step 1550 Train loss 2.26 ACC 0.5 on epoch=774
06/21/2022 15:53:47 - INFO - __main__ - Saving model with best ACC: 0.46875 -> 0.5 on epoch=774, global_step=1550
06/21/2022 15:53:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 2.07 on epoch=779
06/21/2022 15:53:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 2.02 on epoch=784
06/21/2022 15:53:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 2.13 on epoch=789
06/21/2022 15:53:52 - INFO - __main__ - Step 1590 Global step 1590 Train loss 2.03 on epoch=794
06/21/2022 15:53:54 - INFO - __main__ - Step 1600 Global step 1600 Train loss 2.15 on epoch=799
06/21/2022 15:53:58 - INFO - __main__ - Global step 1600 Train loss 2.08 ACC 0.5 on epoch=799
06/21/2022 15:54:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 1.90 on epoch=804
06/21/2022 15:54:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.05 on epoch=809
06/21/2022 15:54:02 - INFO - __main__ - Step 1630 Global step 1630 Train loss 1.88 on epoch=814
06/21/2022 15:54:04 - INFO - __main__ - Step 1640 Global step 1640 Train loss 1.91 on epoch=819
06/21/2022 15:54:05 - INFO - __main__ - Step 1650 Global step 1650 Train loss 1.99 on epoch=824
06/21/2022 15:54:08 - INFO - __main__ - Global step 1650 Train loss 1.95 ACC 0.46875 on epoch=824
06/21/2022 15:54:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 1.80 on epoch=829
06/21/2022 15:54:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 1.81 on epoch=834
06/21/2022 15:54:12 - INFO - __main__ - Step 1680 Global step 1680 Train loss 1.82 on epoch=839
06/21/2022 15:54:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 1.70 on epoch=844
06/21/2022 15:54:15 - INFO - __main__ - Step 1700 Global step 1700 Train loss 1.77 on epoch=849
06/21/2022 15:54:21 - INFO - __main__ - Global step 1700 Train loss 1.78 ACC 0.5 on epoch=849
06/21/2022 15:54:23 - INFO - __main__ - Step 1710 Global step 1710 Train loss 1.67 on epoch=854
06/21/2022 15:54:24 - INFO - __main__ - Step 1720 Global step 1720 Train loss 1.76 on epoch=859
06/21/2022 15:54:25 - INFO - __main__ - Step 1730 Global step 1730 Train loss 1.70 on epoch=864
06/21/2022 15:54:27 - INFO - __main__ - Step 1740 Global step 1740 Train loss 1.61 on epoch=869
06/21/2022 15:54:28 - INFO - __main__ - Step 1750 Global step 1750 Train loss 1.69 on epoch=874
06/21/2022 15:54:30 - INFO - __main__ - Global step 1750 Train loss 1.69 ACC 0.46875 on epoch=874
06/21/2022 15:54:32 - INFO - __main__ - Step 1760 Global step 1760 Train loss 1.68 on epoch=879
06/21/2022 15:54:33 - INFO - __main__ - Step 1770 Global step 1770 Train loss 1.67 on epoch=884
06/21/2022 15:54:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 1.51 on epoch=889
06/21/2022 15:54:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 1.61 on epoch=894
06/21/2022 15:54:37 - INFO - __main__ - Step 1800 Global step 1800 Train loss 1.55 on epoch=899
06/21/2022 15:54:39 - INFO - __main__ - Global step 1800 Train loss 1.60 ACC 0.53125 on epoch=899
06/21/2022 15:54:39 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=899, global_step=1800
06/21/2022 15:54:41 - INFO - __main__ - Step 1810 Global step 1810 Train loss 1.51 on epoch=904
06/21/2022 15:54:42 - INFO - __main__ - Step 1820 Global step 1820 Train loss 1.53 on epoch=909
06/21/2022 15:54:43 - INFO - __main__ - Step 1830 Global step 1830 Train loss 1.44 on epoch=914
06/21/2022 15:54:45 - INFO - __main__ - Step 1840 Global step 1840 Train loss 1.36 on epoch=919
06/21/2022 15:54:46 - INFO - __main__ - Step 1850 Global step 1850 Train loss 1.29 on epoch=924
06/21/2022 15:54:52 - INFO - __main__ - Global step 1850 Train loss 1.43 ACC 0.5 on epoch=924
06/21/2022 15:54:54 - INFO - __main__ - Step 1860 Global step 1860 Train loss 1.25 on epoch=929
06/21/2022 15:54:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 1.30 on epoch=934
06/21/2022 15:54:56 - INFO - __main__ - Step 1880 Global step 1880 Train loss 1.35 on epoch=939
06/21/2022 15:54:58 - INFO - __main__ - Step 1890 Global step 1890 Train loss 1.34 on epoch=944
06/21/2022 15:54:59 - INFO - __main__ - Step 1900 Global step 1900 Train loss 1.27 on epoch=949
06/21/2022 15:55:02 - INFO - __main__ - Global step 1900 Train loss 1.30 ACC 0.5 on epoch=949
06/21/2022 15:55:03 - INFO - __main__ - Step 1910 Global step 1910 Train loss 1.25 on epoch=954
06/21/2022 15:55:05 - INFO - __main__ - Step 1920 Global step 1920 Train loss 1.27 on epoch=959
06/21/2022 15:55:06 - INFO - __main__ - Step 1930 Global step 1930 Train loss 1.28 on epoch=964
06/21/2022 15:55:07 - INFO - __main__ - Step 1940 Global step 1940 Train loss 1.24 on epoch=969
06/21/2022 15:55:09 - INFO - __main__ - Step 1950 Global step 1950 Train loss 1.19 on epoch=974
06/21/2022 15:55:10 - INFO - __main__ - Global step 1950 Train loss 1.24 ACC 0.5 on epoch=974
06/21/2022 15:55:12 - INFO - __main__ - Step 1960 Global step 1960 Train loss 1.14 on epoch=979
06/21/2022 15:55:13 - INFO - __main__ - Step 1970 Global step 1970 Train loss 1.19 on epoch=984
06/21/2022 15:55:15 - INFO - __main__ - Step 1980 Global step 1980 Train loss 1.03 on epoch=989
06/21/2022 15:55:16 - INFO - __main__ - Step 1990 Global step 1990 Train loss 1.04 on epoch=994
06/21/2022 15:55:17 - INFO - __main__ - Step 2000 Global step 2000 Train loss 1.10 on epoch=999
06/21/2022 15:55:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:55:18 - INFO - __main__ - Printing 3 examples
06/21/2022 15:55:18 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/21/2022 15:55:18 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:18 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher  a three-term congressman from Lexington  had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/21/2022 15:55:18 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/21/2022 15:55:18 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:18 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 15:55:18 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:55:18 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:55:18 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:55:18 - INFO - __main__ - Printing 3 examples
06/21/2022 15:55:18 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/21/2022 15:55:18 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:18 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/21/2022 15:55:18 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:18 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that  $ US51 billion  was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/21/2022 15:55:18 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:18 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:55:18 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:55:19 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:55:20 - INFO - __main__ - Global step 2000 Train loss 1.10 ACC 0.5 on epoch=999
06/21/2022 15:55:20 - INFO - __main__ - save last model!
06/21/2022 15:55:20 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 15:55:20 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 15:55:20 - INFO - __main__ - Printing 3 examples
06/21/2022 15:55:20 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 15:55:20 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:20 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 15:55:20 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:55:20 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 15:55:20 - INFO - __main__ - ['not_equivalent']
06/21/2022 15:55:20 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:55:20 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:55:21 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 15:55:25 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:55:26 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:55:26 - INFO - __main__ - Starting training!
06/21/2022 15:55:46 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_13_0.2_8_predictions.txt
06/21/2022 15:55:46 - INFO - __main__ - ACC on test data: 0.6838
06/21/2022 15:55:46 - INFO - __main__ - prefix=glue-mrpc_16_13, lr=0.2, bsz=8, dev_performance=0.53125, test_performance=0.6838235294117647
06/21/2022 15:55:46 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.5, bsz=8 ...
06/21/2022 15:55:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:55:47 - INFO - __main__ - Printing 3 examples
06/21/2022 15:55:47 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/21/2022 15:55:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:47 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher  a three-term congressman from Lexington  had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/21/2022 15:55:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/21/2022 15:55:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:47 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 15:55:47 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:55:47 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 15:55:47 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 15:55:47 - INFO - __main__ - Printing 3 examples
06/21/2022 15:55:47 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/21/2022 15:55:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:47 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/21/2022 15:55:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:47 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that  $ US51 billion  was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/21/2022 15:55:47 - INFO - __main__ - ['equivalent']
06/21/2022 15:55:47 - INFO - __main__ - Tokenizing Input ...
06/21/2022 15:55:47 - INFO - __main__ - Tokenizing Output ...
06/21/2022 15:55:47 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 15:55:52 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 15:55:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 15:55:52 - INFO - __main__ - Starting training!
06/21/2022 15:55:54 - INFO - __main__ - Step 10 Global step 10 Train loss 6.84 on epoch=4
06/21/2022 15:55:56 - INFO - __main__ - Step 20 Global step 20 Train loss 6.84 on epoch=9
06/21/2022 15:55:57 - INFO - __main__ - Step 30 Global step 30 Train loss 6.79 on epoch=14
06/21/2022 15:55:59 - INFO - __main__ - Step 40 Global step 40 Train loss 6.81 on epoch=19
06/21/2022 15:56:00 - INFO - __main__ - Step 50 Global step 50 Train loss 6.73 on epoch=24
06/21/2022 15:56:02 - INFO - __main__ - Global step 50 Train loss 6.80 ACC 0.0 on epoch=24
06/21/2022 15:56:02 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 15:56:03 - INFO - __main__ - Step 60 Global step 60 Train loss 6.68 on epoch=29
06/21/2022 15:56:04 - INFO - __main__ - Step 70 Global step 70 Train loss 6.74 on epoch=34
06/21/2022 15:56:06 - INFO - __main__ - Step 80 Global step 80 Train loss 6.75 on epoch=39
06/21/2022 15:56:07 - INFO - __main__ - Step 90 Global step 90 Train loss 6.72 on epoch=44
06/21/2022 15:56:08 - INFO - __main__ - Step 100 Global step 100 Train loss 6.58 on epoch=49
06/21/2022 15:56:09 - INFO - __main__ - Global step 100 Train loss 6.69 ACC 0.0 on epoch=49
06/21/2022 15:56:10 - INFO - __main__ - Step 110 Global step 110 Train loss 6.55 on epoch=54
06/21/2022 15:56:12 - INFO - __main__ - Step 120 Global step 120 Train loss 6.44 on epoch=59
06/21/2022 15:56:13 - INFO - __main__ - Step 130 Global step 130 Train loss 6.42 on epoch=64
06/21/2022 15:56:15 - INFO - __main__ - Step 140 Global step 140 Train loss 6.36 on epoch=69
06/21/2022 15:56:16 - INFO - __main__ - Step 150 Global step 150 Train loss 6.32 on epoch=74
06/21/2022 15:56:21 - INFO - __main__ - Global step 150 Train loss 6.42 ACC 0.0 on epoch=74
06/21/2022 15:56:22 - INFO - __main__ - Step 160 Global step 160 Train loss 6.18 on epoch=79
06/21/2022 15:56:24 - INFO - __main__ - Step 170 Global step 170 Train loss 6.12 on epoch=84
06/21/2022 15:56:25 - INFO - __main__ - Step 180 Global step 180 Train loss 6.23 on epoch=89
06/21/2022 15:56:26 - INFO - __main__ - Step 190 Global step 190 Train loss 6.00 on epoch=94
06/21/2022 15:56:28 - INFO - __main__ - Step 200 Global step 200 Train loss 5.96 on epoch=99
06/21/2022 15:56:34 - INFO - __main__ - Global step 200 Train loss 6.10 ACC 0.0 on epoch=99
06/21/2022 15:56:35 - INFO - __main__ - Step 210 Global step 210 Train loss 5.98 on epoch=104
06/21/2022 15:56:37 - INFO - __main__ - Step 220 Global step 220 Train loss 5.95 on epoch=109
06/21/2022 15:56:38 - INFO - __main__ - Step 230 Global step 230 Train loss 5.80 on epoch=114
06/21/2022 15:56:39 - INFO - __main__ - Step 240 Global step 240 Train loss 5.61 on epoch=119
06/21/2022 15:56:41 - INFO - __main__ - Step 250 Global step 250 Train loss 5.59 on epoch=124
06/21/2022 15:56:51 - INFO - __main__ - Global step 250 Train loss 5.79 ACC 0.0 on epoch=124
06/21/2022 15:56:53 - INFO - __main__ - Step 260 Global step 260 Train loss 5.60 on epoch=129
06/21/2022 15:56:54 - INFO - __main__ - Step 270 Global step 270 Train loss 5.48 on epoch=134
06/21/2022 15:56:55 - INFO - __main__ - Step 280 Global step 280 Train loss 5.23 on epoch=139
06/21/2022 15:56:57 - INFO - __main__ - Step 290 Global step 290 Train loss 5.19 on epoch=144
06/21/2022 15:56:58 - INFO - __main__ - Step 300 Global step 300 Train loss 5.18 on epoch=149
06/21/2022 15:56:59 - INFO - __main__ - Global step 300 Train loss 5.34 ACC 0.0 on epoch=149
06/21/2022 15:57:01 - INFO - __main__ - Step 310 Global step 310 Train loss 5.04 on epoch=154
06/21/2022 15:57:02 - INFO - __main__ - Step 320 Global step 320 Train loss 4.81 on epoch=159
06/21/2022 15:57:03 - INFO - __main__ - Step 330 Global step 330 Train loss 4.92 on epoch=164
06/21/2022 15:57:05 - INFO - __main__ - Step 340 Global step 340 Train loss 4.78 on epoch=169
06/21/2022 15:57:06 - INFO - __main__ - Step 350 Global step 350 Train loss 4.77 on epoch=174
06/21/2022 15:57:07 - INFO - __main__ - Global step 350 Train loss 4.86 ACC 0.0 on epoch=174
06/21/2022 15:57:08 - INFO - __main__ - Step 360 Global step 360 Train loss 4.67 on epoch=179
06/21/2022 15:57:10 - INFO - __main__ - Step 370 Global step 370 Train loss 4.55 on epoch=184
06/21/2022 15:57:11 - INFO - __main__ - Step 380 Global step 380 Train loss 4.45 on epoch=189
06/21/2022 15:57:12 - INFO - __main__ - Step 390 Global step 390 Train loss 4.40 on epoch=194
06/21/2022 15:57:14 - INFO - __main__ - Step 400 Global step 400 Train loss 4.43 on epoch=199
06/21/2022 15:57:15 - INFO - __main__ - Global step 400 Train loss 4.50 ACC 0.0 on epoch=199
06/21/2022 15:57:16 - INFO - __main__ - Step 410 Global step 410 Train loss 4.27 on epoch=204
06/21/2022 15:57:18 - INFO - __main__ - Step 420 Global step 420 Train loss 4.16 on epoch=209
06/21/2022 15:57:19 - INFO - __main__ - Step 430 Global step 430 Train loss 4.01 on epoch=214
06/21/2022 15:57:20 - INFO - __main__ - Step 440 Global step 440 Train loss 4.03 on epoch=219
06/21/2022 15:57:22 - INFO - __main__ - Step 450 Global step 450 Train loss 3.96 on epoch=224
06/21/2022 15:57:23 - INFO - __main__ - Global step 450 Train loss 4.09 ACC 0.0 on epoch=224
06/21/2022 15:57:24 - INFO - __main__ - Step 460 Global step 460 Train loss 3.81 on epoch=229
06/21/2022 15:57:25 - INFO - __main__ - Step 470 Global step 470 Train loss 3.78 on epoch=234
06/21/2022 15:57:27 - INFO - __main__ - Step 480 Global step 480 Train loss 3.67 on epoch=239
06/21/2022 15:57:28 - INFO - __main__ - Step 490 Global step 490 Train loss 3.66 on epoch=244
06/21/2022 15:57:29 - INFO - __main__ - Step 500 Global step 500 Train loss 3.59 on epoch=249
06/21/2022 15:57:31 - INFO - __main__ - Global step 500 Train loss 3.70 ACC 0.0 on epoch=249
06/21/2022 15:57:32 - INFO - __main__ - Step 510 Global step 510 Train loss 3.57 on epoch=254
06/21/2022 15:57:33 - INFO - __main__ - Step 520 Global step 520 Train loss 3.60 on epoch=259
06/21/2022 15:57:34 - INFO - __main__ - Step 530 Global step 530 Train loss 3.57 on epoch=264
06/21/2022 15:57:36 - INFO - __main__ - Step 540 Global step 540 Train loss 3.36 on epoch=269
06/21/2022 15:57:37 - INFO - __main__ - Step 550 Global step 550 Train loss 3.37 on epoch=274
06/21/2022 15:57:39 - INFO - __main__ - Global step 550 Train loss 3.49 ACC 0.0 on epoch=274
06/21/2022 15:57:40 - INFO - __main__ - Step 560 Global step 560 Train loss 3.30 on epoch=279
06/21/2022 15:57:41 - INFO - __main__ - Step 570 Global step 570 Train loss 3.29 on epoch=284
06/21/2022 15:57:43 - INFO - __main__ - Step 580 Global step 580 Train loss 3.25 on epoch=289
06/21/2022 15:57:44 - INFO - __main__ - Step 590 Global step 590 Train loss 3.10 on epoch=294
06/21/2022 15:57:45 - INFO - __main__ - Step 600 Global step 600 Train loss 3.24 on epoch=299
06/21/2022 15:57:48 - INFO - __main__ - Global step 600 Train loss 3.24 ACC 0.0 on epoch=299
06/21/2022 15:57:49 - INFO - __main__ - Step 610 Global step 610 Train loss 3.19 on epoch=304
06/21/2022 15:57:51 - INFO - __main__ - Step 620 Global step 620 Train loss 3.16 on epoch=309
06/21/2022 15:57:52 - INFO - __main__ - Step 630 Global step 630 Train loss 3.15 on epoch=314
06/21/2022 15:57:53 - INFO - __main__ - Step 640 Global step 640 Train loss 2.99 on epoch=319
06/21/2022 15:57:54 - INFO - __main__ - Step 650 Global step 650 Train loss 3.08 on epoch=324
06/21/2022 15:58:01 - INFO - __main__ - Global step 650 Train loss 3.12 ACC 0.0 on epoch=324
06/21/2022 15:58:02 - INFO - __main__ - Step 660 Global step 660 Train loss 3.03 on epoch=329
06/21/2022 15:58:04 - INFO - __main__ - Step 670 Global step 670 Train loss 2.87 on epoch=334
06/21/2022 15:58:05 - INFO - __main__ - Step 680 Global step 680 Train loss 2.81 on epoch=339
06/21/2022 15:58:07 - INFO - __main__ - Step 690 Global step 690 Train loss 2.87 on epoch=344
06/21/2022 15:58:08 - INFO - __main__ - Step 700 Global step 700 Train loss 2.74 on epoch=349
06/21/2022 15:58:11 - INFO - __main__ - Global step 700 Train loss 2.86 ACC 0.0 on epoch=349
06/21/2022 15:58:12 - INFO - __main__ - Step 710 Global step 710 Train loss 2.78 on epoch=354
06/21/2022 15:58:14 - INFO - __main__ - Step 720 Global step 720 Train loss 2.58 on epoch=359
06/21/2022 15:58:15 - INFO - __main__ - Step 730 Global step 730 Train loss 2.59 on epoch=364
06/21/2022 15:58:16 - INFO - __main__ - Step 740 Global step 740 Train loss 2.57 on epoch=369
06/21/2022 15:58:18 - INFO - __main__ - Step 750 Global step 750 Train loss 2.47 on epoch=374
06/21/2022 15:58:25 - INFO - __main__ - Global step 750 Train loss 2.60 ACC 0.0 on epoch=374
06/21/2022 15:58:26 - INFO - __main__ - Step 760 Global step 760 Train loss 2.49 on epoch=379
06/21/2022 15:58:28 - INFO - __main__ - Step 770 Global step 770 Train loss 2.39 on epoch=384
06/21/2022 15:58:29 - INFO - __main__ - Step 780 Global step 780 Train loss 2.48 on epoch=389
06/21/2022 15:58:31 - INFO - __main__ - Step 790 Global step 790 Train loss 2.35 on epoch=394
06/21/2022 15:58:32 - INFO - __main__ - Step 800 Global step 800 Train loss 2.40 on epoch=399
06/21/2022 15:58:36 - INFO - __main__ - Global step 800 Train loss 2.42 ACC 0.0 on epoch=399
06/21/2022 15:58:37 - INFO - __main__ - Step 810 Global step 810 Train loss 2.22 on epoch=404
06/21/2022 15:58:39 - INFO - __main__ - Step 820 Global step 820 Train loss 2.37 on epoch=409
06/21/2022 15:58:40 - INFO - __main__ - Step 830 Global step 830 Train loss 2.37 on epoch=414
06/21/2022 15:58:42 - INFO - __main__ - Step 840 Global step 840 Train loss 2.16 on epoch=419
06/21/2022 15:58:43 - INFO - __main__ - Step 850 Global step 850 Train loss 2.12 on epoch=424
06/21/2022 15:58:46 - INFO - __main__ - Global step 850 Train loss 2.25 ACC 0.09375 on epoch=424
06/21/2022 15:58:46 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.09375 on epoch=424, global_step=850
06/21/2022 15:58:48 - INFO - __main__ - Step 860 Global step 860 Train loss 2.09 on epoch=429
06/21/2022 15:58:49 - INFO - __main__ - Step 870 Global step 870 Train loss 2.14 on epoch=434
06/21/2022 15:58:51 - INFO - __main__ - Step 880 Global step 880 Train loss 1.98 on epoch=439
06/21/2022 15:58:52 - INFO - __main__ - Step 890 Global step 890 Train loss 2.03 on epoch=444
06/21/2022 15:58:54 - INFO - __main__ - Step 900 Global step 900 Train loss 1.94 on epoch=449
06/21/2022 15:58:54 - INFO - __main__ - Global step 900 Train loss 2.04 ACC 0.40625 on epoch=449
06/21/2022 15:58:55 - INFO - __main__ - Saving model with best ACC: 0.09375 -> 0.40625 on epoch=449, global_step=900
06/21/2022 15:58:56 - INFO - __main__ - Step 910 Global step 910 Train loss 1.95 on epoch=454
06/21/2022 15:58:57 - INFO - __main__ - Step 920 Global step 920 Train loss 1.84 on epoch=459
06/21/2022 15:58:59 - INFO - __main__ - Step 930 Global step 930 Train loss 1.84 on epoch=464
06/21/2022 15:59:00 - INFO - __main__ - Step 940 Global step 940 Train loss 1.76 on epoch=469
06/21/2022 15:59:02 - INFO - __main__ - Step 950 Global step 950 Train loss 1.76 on epoch=474
06/21/2022 15:59:02 - INFO - __main__ - Global step 950 Train loss 1.83 ACC 0.25 on epoch=474
06/21/2022 15:59:04 - INFO - __main__ - Step 960 Global step 960 Train loss 1.77 on epoch=479
06/21/2022 15:59:05 - INFO - __main__ - Step 970 Global step 970 Train loss 1.81 on epoch=484
06/21/2022 15:59:07 - INFO - __main__ - Step 980 Global step 980 Train loss 1.78 on epoch=489
06/21/2022 15:59:08 - INFO - __main__ - Step 990 Global step 990 Train loss 1.66 on epoch=494
06/21/2022 15:59:10 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.65 on epoch=499
06/21/2022 15:59:10 - INFO - __main__ - Global step 1000 Train loss 1.73 ACC 0.53125 on epoch=499
06/21/2022 15:59:10 - INFO - __main__ - Saving model with best ACC: 0.40625 -> 0.53125 on epoch=499, global_step=1000
06/21/2022 15:59:12 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.63 on epoch=504
06/21/2022 15:59:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.52 on epoch=509
06/21/2022 15:59:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.59 on epoch=514
06/21/2022 15:59:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.47 on epoch=519
06/21/2022 15:59:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.42 on epoch=524
06/21/2022 15:59:19 - INFO - __main__ - Global step 1050 Train loss 1.53 ACC 0.5 on epoch=524
06/21/2022 15:59:20 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.34 on epoch=529
06/21/2022 15:59:21 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.25 on epoch=534
06/21/2022 15:59:23 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.23 on epoch=539
06/21/2022 15:59:24 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.22 on epoch=544
06/21/2022 15:59:26 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.19 on epoch=549
06/21/2022 15:59:28 - INFO - __main__ - Global step 1100 Train loss 1.24 ACC 0.5 on epoch=549
06/21/2022 15:59:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.02 on epoch=554
06/21/2022 15:59:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.11 on epoch=559
06/21/2022 15:59:32 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.19 on epoch=564
06/21/2022 15:59:33 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.03 on epoch=569
06/21/2022 15:59:35 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.00 on epoch=574
06/21/2022 15:59:38 - INFO - __main__ - Global step 1150 Train loss 1.07 ACC 0.5 on epoch=574
06/21/2022 15:59:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.00 on epoch=579
06/21/2022 15:59:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.01 on epoch=584
06/21/2022 15:59:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.89 on epoch=589
06/21/2022 15:59:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.84 on epoch=594
06/21/2022 15:59:45 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.84 on epoch=599
06/21/2022 15:59:46 - INFO - __main__ - Global step 1200 Train loss 0.92 ACC 0.5 on epoch=599
06/21/2022 15:59:48 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.83 on epoch=604
06/21/2022 15:59:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.99 on epoch=609
06/21/2022 15:59:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.82 on epoch=614
06/21/2022 15:59:52 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.80 on epoch=619
06/21/2022 15:59:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.76 on epoch=624
06/21/2022 15:59:58 - INFO - __main__ - Global step 1250 Train loss 0.84 ACC 0.5 on epoch=624
06/21/2022 16:00:00 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.79 on epoch=629
06/21/2022 16:00:01 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.77 on epoch=634
06/21/2022 16:00:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.61 on epoch=639
06/21/2022 16:00:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.72 on epoch=644
06/21/2022 16:00:05 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.74 on epoch=649
06/21/2022 16:00:06 - INFO - __main__ - Global step 1300 Train loss 0.73 ACC 0.5 on epoch=649
06/21/2022 16:00:07 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.78 on epoch=654
06/21/2022 16:00:09 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.76 on epoch=659
06/21/2022 16:00:10 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.65 on epoch=664
06/21/2022 16:00:12 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=669
06/21/2022 16:00:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.68 on epoch=674
06/21/2022 16:00:14 - INFO - __main__ - Global step 1350 Train loss 0.72 ACC 0.5 on epoch=674
06/21/2022 16:00:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.65 on epoch=679
06/21/2022 16:00:17 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.64 on epoch=684
06/21/2022 16:00:18 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.64 on epoch=689
06/21/2022 16:00:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.61 on epoch=694
06/21/2022 16:00:21 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.63 on epoch=699
06/21/2022 16:00:22 - INFO - __main__ - Global step 1400 Train loss 0.63 ACC 0.5 on epoch=699
06/21/2022 16:00:23 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.60 on epoch=704
06/21/2022 16:00:24 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.59 on epoch=709
06/21/2022 16:00:26 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.56 on epoch=714
06/21/2022 16:00:27 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.59 on epoch=719
06/21/2022 16:00:28 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.64 on epoch=724
06/21/2022 16:00:29 - INFO - __main__ - Global step 1450 Train loss 0.60 ACC 0.5 on epoch=724
06/21/2022 16:00:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.57 on epoch=729
06/21/2022 16:00:32 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.53 on epoch=734
06/21/2022 16:00:33 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.56 on epoch=739
06/21/2022 16:00:35 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.64 on epoch=744
06/21/2022 16:00:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.56 on epoch=749
06/21/2022 16:00:37 - INFO - __main__ - Global step 1500 Train loss 0.57 ACC 0.5 on epoch=749
06/21/2022 16:00:38 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.56 on epoch=754
06/21/2022 16:00:40 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.50 on epoch=759
06/21/2022 16:00:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.57 on epoch=764
06/21/2022 16:00:42 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.57 on epoch=769
06/21/2022 16:00:44 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.43 on epoch=774
06/21/2022 16:00:45 - INFO - __main__ - Global step 1550 Train loss 0.53 ACC 0.5 on epoch=774
06/21/2022 16:00:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.59 on epoch=779
06/21/2022 16:00:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.51 on epoch=784
06/21/2022 16:00:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.50 on epoch=789
06/21/2022 16:00:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.48 on epoch=794
06/21/2022 16:00:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.43 on epoch=799
06/21/2022 16:00:53 - INFO - __main__ - Global step 1600 Train loss 0.50 ACC 0.5 on epoch=799
06/21/2022 16:00:54 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.57 on epoch=804
06/21/2022 16:00:56 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.44 on epoch=809
06/21/2022 16:00:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.57 on epoch=814
06/21/2022 16:00:58 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.48 on epoch=819
06/21/2022 16:01:00 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.44 on epoch=824
06/21/2022 16:01:01 - INFO - __main__ - Global step 1650 Train loss 0.50 ACC 0.5 on epoch=824
06/21/2022 16:01:02 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.45 on epoch=829
06/21/2022 16:01:03 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.50 on epoch=834
06/21/2022 16:01:05 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.49 on epoch=839
06/21/2022 16:01:06 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
06/21/2022 16:01:07 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.48 on epoch=849
06/21/2022 16:01:08 - INFO - __main__ - Global step 1700 Train loss 0.48 ACC 0.5 on epoch=849
06/21/2022 16:01:10 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.51 on epoch=854
06/21/2022 16:01:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.53 on epoch=859
06/21/2022 16:01:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.43 on epoch=864
06/21/2022 16:01:14 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.42 on epoch=869
06/21/2022 16:01:16 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
06/21/2022 16:01:16 - INFO - __main__ - Global step 1750 Train loss 0.46 ACC 0.53125 on epoch=874
06/21/2022 16:01:18 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.50 on epoch=879
06/21/2022 16:01:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/21/2022 16:01:21 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.48 on epoch=889
06/21/2022 16:01:22 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.40 on epoch=894
06/21/2022 16:01:24 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.39 on epoch=899
06/21/2022 16:01:24 - INFO - __main__ - Global step 1800 Train loss 0.44 ACC 0.5 on epoch=899
06/21/2022 16:01:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.50 on epoch=904
06/21/2022 16:01:27 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.40 on epoch=909
06/21/2022 16:01:29 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.42 on epoch=914
06/21/2022 16:01:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=919
06/21/2022 16:01:31 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.46 on epoch=924
06/21/2022 16:01:32 - INFO - __main__ - Global step 1850 Train loss 0.45 ACC 0.5 on epoch=924
06/21/2022 16:01:33 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
06/21/2022 16:01:35 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.44 on epoch=934
06/21/2022 16:01:36 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.45 on epoch=939
06/21/2022 16:01:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.43 on epoch=944
06/21/2022 16:01:39 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.41 on epoch=949
06/21/2022 16:01:40 - INFO - __main__ - Global step 1900 Train loss 0.42 ACC 0.5 on epoch=949
06/21/2022 16:01:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.44 on epoch=954
06/21/2022 16:01:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/21/2022 16:01:44 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.36 on epoch=964
06/21/2022 16:01:46 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.43 on epoch=969
06/21/2022 16:01:47 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.41 on epoch=974
06/21/2022 16:01:48 - INFO - __main__ - Global step 1950 Train loss 0.42 ACC 0.5 on epoch=974
06/21/2022 16:01:49 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.41 on epoch=979
06/21/2022 16:01:51 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.44 on epoch=984
06/21/2022 16:01:52 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.38 on epoch=989
06/21/2022 16:01:54 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.40 on epoch=994
06/21/2022 16:01:55 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.46 on epoch=999
06/21/2022 16:01:56 - INFO - __main__ - Global step 2000 Train loss 0.42 ACC 0.5 on epoch=999
06/21/2022 16:01:56 - INFO - __main__ - save last model!
06/21/2022 16:01:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 16:01:56 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 16:01:56 - INFO - __main__ - Printing 3 examples
06/21/2022 16:01:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 16:01:56 - INFO - __main__ - ['equivalent']
06/21/2022 16:01:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 16:01:56 - INFO - __main__ - ['not_equivalent']
06/21/2022 16:01:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 16:01:56 - INFO - __main__ - ['not_equivalent']
06/21/2022 16:01:56 - INFO - __main__ - Tokenizing Input ...
06/21/2022 16:01:56 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:01:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:01:56 - INFO - __main__ - Printing 3 examples
06/21/2022 16:01:56 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/21/2022 16:01:56 - INFO - __main__ - ['equivalent']
06/21/2022 16:01:56 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher  a three-term congressman from Lexington  had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/21/2022 16:01:56 - INFO - __main__ - ['equivalent']
06/21/2022 16:01:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/21/2022 16:01:56 - INFO - __main__ - ['equivalent']
06/21/2022 16:01:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 16:01:56 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:01:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 16:01:56 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:01:56 - INFO - __main__ - Printing 3 examples
06/21/2022 16:01:56 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/21/2022 16:01:56 - INFO - __main__ - ['equivalent']
06/21/2022 16:01:56 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/21/2022 16:01:56 - INFO - __main__ - ['equivalent']
06/21/2022 16:01:56 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that  $ US51 billion  was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/21/2022 16:01:56 - INFO - __main__ - ['equivalent']
06/21/2022 16:01:56 - INFO - __main__ - Tokenizing Input ...
06/21/2022 16:01:56 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:01:56 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 16:01:57 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 16:02:02 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 16:02:03 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 16:02:03 - INFO - __main__ - Starting training!
06/21/2022 16:02:05 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.5_8_predictions.txt
06/21/2022 16:02:05 - INFO - __main__ - ACC on test data: 0.6716
06/21/2022 16:02:06 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.5, bsz=8, dev_performance=0.53125, test_performance=0.6715686274509803
06/21/2022 16:02:06 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.4, bsz=8 ...
06/21/2022 16:02:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:02:07 - INFO - __main__ - Printing 3 examples
06/21/2022 16:02:07 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/21/2022 16:02:07 - INFO - __main__ - ['equivalent']
06/21/2022 16:02:07 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher  a three-term congressman from Lexington  had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/21/2022 16:02:07 - INFO - __main__ - ['equivalent']
06/21/2022 16:02:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/21/2022 16:02:07 - INFO - __main__ - ['equivalent']
06/21/2022 16:02:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 16:02:07 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:02:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 16:02:07 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:02:07 - INFO - __main__ - Printing 3 examples
06/21/2022 16:02:07 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/21/2022 16:02:07 - INFO - __main__ - ['equivalent']
06/21/2022 16:02:07 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/21/2022 16:02:07 - INFO - __main__ - ['equivalent']
06/21/2022 16:02:07 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that  $ US51 billion  was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/21/2022 16:02:07 - INFO - __main__ - ['equivalent']
06/21/2022 16:02:07 - INFO - __main__ - Tokenizing Input ...
06/21/2022 16:02:07 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:02:07 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 16:02:13 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 16:02:14 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 16:02:14 - INFO - __main__ - Starting training!
06/21/2022 16:02:15 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/21/2022 16:02:17 - INFO - __main__ - Step 20 Global step 20 Train loss 6.88 on epoch=9
06/21/2022 16:02:18 - INFO - __main__ - Step 30 Global step 30 Train loss 6.85 on epoch=14
06/21/2022 16:02:19 - INFO - __main__ - Step 40 Global step 40 Train loss 6.77 on epoch=19
06/21/2022 16:02:20 - INFO - __main__ - Step 50 Global step 50 Train loss 6.86 on epoch=24
06/21/2022 16:02:21 - INFO - __main__ - Global step 50 Train loss 6.84 ACC 0.0 on epoch=24
06/21/2022 16:02:21 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 16:02:22 - INFO - __main__ - Step 60 Global step 60 Train loss 6.75 on epoch=29
06/21/2022 16:02:24 - INFO - __main__ - Step 70 Global step 70 Train loss 6.73 on epoch=34
06/21/2022 16:02:25 - INFO - __main__ - Step 80 Global step 80 Train loss 6.69 on epoch=39
06/21/2022 16:02:26 - INFO - __main__ - Step 90 Global step 90 Train loss 6.61 on epoch=44
06/21/2022 16:02:28 - INFO - __main__ - Step 100 Global step 100 Train loss 6.54 on epoch=49
06/21/2022 16:02:30 - INFO - __main__ - Global step 100 Train loss 6.66 ACC 0.0 on epoch=49
06/21/2022 16:02:32 - INFO - __main__ - Step 110 Global step 110 Train loss 6.62 on epoch=54
06/21/2022 16:02:33 - INFO - __main__ - Step 120 Global step 120 Train loss 6.59 on epoch=59
06/21/2022 16:02:34 - INFO - __main__ - Step 130 Global step 130 Train loss 6.44 on epoch=64
06/21/2022 16:02:36 - INFO - __main__ - Step 140 Global step 140 Train loss 6.46 on epoch=69
06/21/2022 16:02:37 - INFO - __main__ - Step 150 Global step 150 Train loss 6.39 on epoch=74
06/21/2022 16:02:40 - INFO - __main__ - Global step 150 Train loss 6.50 ACC 0.0 on epoch=74
06/21/2022 16:02:42 - INFO - __main__ - Step 160 Global step 160 Train loss 6.31 on epoch=79
06/21/2022 16:02:43 - INFO - __main__ - Step 170 Global step 170 Train loss 6.33 on epoch=84
06/21/2022 16:02:44 - INFO - __main__ - Step 180 Global step 180 Train loss 6.25 on epoch=89
06/21/2022 16:02:46 - INFO - __main__ - Step 190 Global step 190 Train loss 6.24 on epoch=94
06/21/2022 16:02:47 - INFO - __main__ - Step 200 Global step 200 Train loss 6.18 on epoch=99
06/21/2022 16:02:51 - INFO - __main__ - Global step 200 Train loss 6.26 ACC 0.0 on epoch=99
06/21/2022 16:02:52 - INFO - __main__ - Step 210 Global step 210 Train loss 6.10 on epoch=104
06/21/2022 16:02:54 - INFO - __main__ - Step 220 Global step 220 Train loss 5.99 on epoch=109
06/21/2022 16:02:55 - INFO - __main__ - Step 230 Global step 230 Train loss 5.96 on epoch=114
06/21/2022 16:02:57 - INFO - __main__ - Step 240 Global step 240 Train loss 5.83 on epoch=119
06/21/2022 16:02:58 - INFO - __main__ - Step 250 Global step 250 Train loss 5.92 on epoch=124
06/21/2022 16:03:06 - INFO - __main__ - Global step 250 Train loss 5.96 ACC 0.0 on epoch=124
06/21/2022 16:03:07 - INFO - __main__ - Step 260 Global step 260 Train loss 5.69 on epoch=129
06/21/2022 16:03:09 - INFO - __main__ - Step 270 Global step 270 Train loss 5.90 on epoch=134
06/21/2022 16:03:10 - INFO - __main__ - Step 280 Global step 280 Train loss 5.74 on epoch=139
06/21/2022 16:03:12 - INFO - __main__ - Step 290 Global step 290 Train loss 5.87 on epoch=144
06/21/2022 16:03:13 - INFO - __main__ - Step 300 Global step 300 Train loss 5.72 on epoch=149
06/21/2022 16:03:16 - INFO - __main__ - Global step 300 Train loss 5.78 ACC 0.0 on epoch=149
06/21/2022 16:03:17 - INFO - __main__ - Step 310 Global step 310 Train loss 5.58 on epoch=154
06/21/2022 16:03:19 - INFO - __main__ - Step 320 Global step 320 Train loss 5.54 on epoch=159
06/21/2022 16:03:20 - INFO - __main__ - Step 330 Global step 330 Train loss 5.52 on epoch=164
06/21/2022 16:03:21 - INFO - __main__ - Step 340 Global step 340 Train loss 5.35 on epoch=169
06/21/2022 16:03:23 - INFO - __main__ - Step 350 Global step 350 Train loss 5.36 on epoch=174
06/21/2022 16:03:26 - INFO - __main__ - Global step 350 Train loss 5.47 ACC 0.0 on epoch=174
06/21/2022 16:03:27 - INFO - __main__ - Step 360 Global step 360 Train loss 5.17 on epoch=179
06/21/2022 16:03:29 - INFO - __main__ - Step 370 Global step 370 Train loss 5.18 on epoch=184
06/21/2022 16:03:30 - INFO - __main__ - Step 380 Global step 380 Train loss 4.99 on epoch=189
06/21/2022 16:03:31 - INFO - __main__ - Step 390 Global step 390 Train loss 4.91 on epoch=194
06/21/2022 16:03:33 - INFO - __main__ - Step 400 Global step 400 Train loss 4.82 on epoch=199
06/21/2022 16:03:36 - INFO - __main__ - Global step 400 Train loss 5.01 ACC 0.0 on epoch=199
06/21/2022 16:03:37 - INFO - __main__ - Step 410 Global step 410 Train loss 4.81 on epoch=204
06/21/2022 16:03:39 - INFO - __main__ - Step 420 Global step 420 Train loss 4.71 on epoch=209
06/21/2022 16:03:40 - INFO - __main__ - Step 430 Global step 430 Train loss 4.68 on epoch=214
06/21/2022 16:03:42 - INFO - __main__ - Step 440 Global step 440 Train loss 4.47 on epoch=219
06/21/2022 16:03:43 - INFO - __main__ - Step 450 Global step 450 Train loss 4.42 on epoch=224
06/21/2022 16:03:45 - INFO - __main__ - Global step 450 Train loss 4.62 ACC 0.0 on epoch=224
06/21/2022 16:03:46 - INFO - __main__ - Step 460 Global step 460 Train loss 4.27 on epoch=229
06/21/2022 16:03:48 - INFO - __main__ - Step 470 Global step 470 Train loss 4.25 on epoch=234
06/21/2022 16:03:49 - INFO - __main__ - Step 480 Global step 480 Train loss 4.13 on epoch=239
06/21/2022 16:03:50 - INFO - __main__ - Step 490 Global step 490 Train loss 4.10 on epoch=244
06/21/2022 16:03:52 - INFO - __main__ - Step 500 Global step 500 Train loss 4.04 on epoch=249
06/21/2022 16:03:59 - INFO - __main__ - Global step 500 Train loss 4.16 ACC 0.0 on epoch=249
06/21/2022 16:04:00 - INFO - __main__ - Step 510 Global step 510 Train loss 3.93 on epoch=254
06/21/2022 16:04:01 - INFO - __main__ - Step 520 Global step 520 Train loss 3.81 on epoch=259
06/21/2022 16:04:03 - INFO - __main__ - Step 530 Global step 530 Train loss 3.73 on epoch=264
06/21/2022 16:04:04 - INFO - __main__ - Step 540 Global step 540 Train loss 3.64 on epoch=269
06/21/2022 16:04:06 - INFO - __main__ - Step 550 Global step 550 Train loss 3.60 on epoch=274
06/21/2022 16:04:14 - INFO - __main__ - Global step 550 Train loss 3.74 ACC 0.0 on epoch=274
06/21/2022 16:04:15 - INFO - __main__ - Step 560 Global step 560 Train loss 3.50 on epoch=279
06/21/2022 16:04:16 - INFO - __main__ - Step 570 Global step 570 Train loss 3.50 on epoch=284
06/21/2022 16:04:18 - INFO - __main__ - Step 580 Global step 580 Train loss 3.44 on epoch=289
06/21/2022 16:04:19 - INFO - __main__ - Step 590 Global step 590 Train loss 3.51 on epoch=294
06/21/2022 16:04:21 - INFO - __main__ - Step 600 Global step 600 Train loss 3.28 on epoch=299
06/21/2022 16:04:25 - INFO - __main__ - Global step 600 Train loss 3.44 ACC 0.0 on epoch=299
06/21/2022 16:04:27 - INFO - __main__ - Step 610 Global step 610 Train loss 3.27 on epoch=304
06/21/2022 16:04:28 - INFO - __main__ - Step 620 Global step 620 Train loss 3.19 on epoch=309
06/21/2022 16:04:29 - INFO - __main__ - Step 630 Global step 630 Train loss 3.24 on epoch=314
06/21/2022 16:04:31 - INFO - __main__ - Step 640 Global step 640 Train loss 3.06 on epoch=319
06/21/2022 16:04:32 - INFO - __main__ - Step 650 Global step 650 Train loss 2.95 on epoch=324
06/21/2022 16:04:42 - INFO - __main__ - Global step 650 Train loss 3.14 ACC 0.0 on epoch=324
06/21/2022 16:04:43 - INFO - __main__ - Step 660 Global step 660 Train loss 3.00 on epoch=329
06/21/2022 16:04:44 - INFO - __main__ - Step 670 Global step 670 Train loss 2.84 on epoch=334
06/21/2022 16:04:45 - INFO - __main__ - Step 680 Global step 680 Train loss 2.78 on epoch=339
06/21/2022 16:04:47 - INFO - __main__ - Step 690 Global step 690 Train loss 2.77 on epoch=344
06/21/2022 16:04:48 - INFO - __main__ - Step 700 Global step 700 Train loss 2.69 on epoch=349
06/21/2022 16:05:00 - INFO - __main__ - Global step 700 Train loss 2.81 ACC 0.0 on epoch=349
06/21/2022 16:05:01 - INFO - __main__ - Step 710 Global step 710 Train loss 2.69 on epoch=354
06/21/2022 16:05:03 - INFO - __main__ - Step 720 Global step 720 Train loss 2.54 on epoch=359
06/21/2022 16:05:04 - INFO - __main__ - Step 730 Global step 730 Train loss 2.54 on epoch=364
06/21/2022 16:05:06 - INFO - __main__ - Step 740 Global step 740 Train loss 2.41 on epoch=369
06/21/2022 16:05:07 - INFO - __main__ - Step 750 Global step 750 Train loss 2.39 on epoch=374
06/21/2022 16:05:10 - INFO - __main__ - Global step 750 Train loss 2.51 ACC 0.03125 on epoch=374
06/21/2022 16:05:10 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.03125 on epoch=374, global_step=750
06/21/2022 16:05:12 - INFO - __main__ - Step 760 Global step 760 Train loss 2.39 on epoch=379
06/21/2022 16:05:13 - INFO - __main__ - Step 770 Global step 770 Train loss 2.37 on epoch=384
06/21/2022 16:05:14 - INFO - __main__ - Step 780 Global step 780 Train loss 2.28 on epoch=389
06/21/2022 16:05:16 - INFO - __main__ - Step 790 Global step 790 Train loss 2.24 on epoch=394
06/21/2022 16:05:17 - INFO - __main__ - Step 800 Global step 800 Train loss 2.03 on epoch=399
06/21/2022 16:05:19 - INFO - __main__ - Global step 800 Train loss 2.26 ACC 0.4375 on epoch=399
06/21/2022 16:05:19 - INFO - __main__ - Saving model with best ACC: 0.03125 -> 0.4375 on epoch=399, global_step=800
06/21/2022 16:05:21 - INFO - __main__ - Step 810 Global step 810 Train loss 2.09 on epoch=404
06/21/2022 16:05:22 - INFO - __main__ - Step 820 Global step 820 Train loss 1.95 on epoch=409
06/21/2022 16:05:23 - INFO - __main__ - Step 830 Global step 830 Train loss 1.97 on epoch=414
06/21/2022 16:05:25 - INFO - __main__ - Step 840 Global step 840 Train loss 1.88 on epoch=419
06/21/2022 16:05:26 - INFO - __main__ - Step 850 Global step 850 Train loss 1.87 on epoch=424
06/21/2022 16:05:27 - INFO - __main__ - Global step 850 Train loss 1.95 ACC 0.5 on epoch=424
06/21/2022 16:05:27 - INFO - __main__ - Saving model with best ACC: 0.4375 -> 0.5 on epoch=424, global_step=850
06/21/2022 16:05:29 - INFO - __main__ - Step 860 Global step 860 Train loss 1.80 on epoch=429
06/21/2022 16:05:30 - INFO - __main__ - Step 870 Global step 870 Train loss 1.84 on epoch=434
06/21/2022 16:05:31 - INFO - __main__ - Step 880 Global step 880 Train loss 1.68 on epoch=439
06/21/2022 16:05:33 - INFO - __main__ - Step 890 Global step 890 Train loss 1.75 on epoch=444
06/21/2022 16:05:34 - INFO - __main__ - Step 900 Global step 900 Train loss 1.62 on epoch=449
06/21/2022 16:05:36 - INFO - __main__ - Global step 900 Train loss 1.74 ACC 0.53125 on epoch=449
06/21/2022 16:05:36 - INFO - __main__ - Saving model with best ACC: 0.5 -> 0.53125 on epoch=449, global_step=900
06/21/2022 16:05:37 - INFO - __main__ - Step 910 Global step 910 Train loss 1.64 on epoch=454
06/21/2022 16:05:39 - INFO - __main__ - Step 920 Global step 920 Train loss 1.58 on epoch=459
06/21/2022 16:05:40 - INFO - __main__ - Step 930 Global step 930 Train loss 1.56 on epoch=464
06/21/2022 16:05:42 - INFO - __main__ - Step 940 Global step 940 Train loss 1.49 on epoch=469
06/21/2022 16:05:43 - INFO - __main__ - Step 950 Global step 950 Train loss 1.57 on epoch=474
06/21/2022 16:05:44 - INFO - __main__ - Global step 950 Train loss 1.57 ACC 0.40625 on epoch=474
06/21/2022 16:05:46 - INFO - __main__ - Step 960 Global step 960 Train loss 1.60 on epoch=479
06/21/2022 16:05:47 - INFO - __main__ - Step 970 Global step 970 Train loss 1.45 on epoch=484
06/21/2022 16:05:49 - INFO - __main__ - Step 980 Global step 980 Train loss 1.46 on epoch=489
06/21/2022 16:05:50 - INFO - __main__ - Step 990 Global step 990 Train loss 1.41 on epoch=494
06/21/2022 16:05:52 - INFO - __main__ - Step 1000 Global step 1000 Train loss 1.28 on epoch=499
06/21/2022 16:05:53 - INFO - __main__ - Global step 1000 Train loss 1.44 ACC 0.40625 on epoch=499
06/21/2022 16:05:54 - INFO - __main__ - Step 1010 Global step 1010 Train loss 1.43 on epoch=504
06/21/2022 16:05:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 1.31 on epoch=509
06/21/2022 16:05:57 - INFO - __main__ - Step 1030 Global step 1030 Train loss 1.38 on epoch=514
06/21/2022 16:05:58 - INFO - __main__ - Step 1040 Global step 1040 Train loss 1.22 on epoch=519
06/21/2022 16:06:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 1.26 on epoch=524
06/21/2022 16:06:01 - INFO - __main__ - Global step 1050 Train loss 1.32 ACC 0.53125 on epoch=524
06/21/2022 16:06:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 1.18 on epoch=529
06/21/2022 16:06:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 1.28 on epoch=534
06/21/2022 16:06:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 1.16 on epoch=539
06/21/2022 16:06:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 1.11 on epoch=544
06/21/2022 16:06:08 - INFO - __main__ - Step 1100 Global step 1100 Train loss 1.04 on epoch=549
06/21/2022 16:06:10 - INFO - __main__ - Global step 1100 Train loss 1.16 ACC 0.5 on epoch=549
06/21/2022 16:06:11 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.16 on epoch=554
06/21/2022 16:06:13 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.03 on epoch=559
06/21/2022 16:06:14 - INFO - __main__ - Step 1130 Global step 1130 Train loss 1.07 on epoch=564
06/21/2022 16:06:16 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.99 on epoch=569
06/21/2022 16:06:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.07 on epoch=574
06/21/2022 16:06:19 - INFO - __main__ - Global step 1150 Train loss 1.06 ACC 0.5 on epoch=574
06/21/2022 16:06:20 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.02 on epoch=579
06/21/2022 16:06:22 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.00 on epoch=584
06/21/2022 16:06:23 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.03 on epoch=589
06/21/2022 16:06:25 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.99 on epoch=594
06/21/2022 16:06:26 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.90 on epoch=599
06/21/2022 16:06:28 - INFO - __main__ - Global step 1200 Train loss 0.99 ACC 0.5 on epoch=599
06/21/2022 16:06:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.86 on epoch=604
06/21/2022 16:06:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.94 on epoch=609
06/21/2022 16:06:32 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.86 on epoch=614
06/21/2022 16:06:34 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.88 on epoch=619
06/21/2022 16:06:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.84 on epoch=624
06/21/2022 16:06:37 - INFO - __main__ - Global step 1250 Train loss 0.88 ACC 0.5 on epoch=624
06/21/2022 16:06:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.78 on epoch=629
06/21/2022 16:06:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.83 on epoch=634
06/21/2022 16:06:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.81 on epoch=639
06/21/2022 16:06:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.72 on epoch=644
06/21/2022 16:06:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.76 on epoch=649
06/21/2022 16:06:46 - INFO - __main__ - Global step 1300 Train loss 0.78 ACC 0.5 on epoch=649
06/21/2022 16:06:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.76 on epoch=654
06/21/2022 16:06:48 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.77 on epoch=659
06/21/2022 16:06:50 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.74 on epoch=664
06/21/2022 16:06:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.71 on epoch=669
06/21/2022 16:06:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.66 on epoch=674
06/21/2022 16:06:56 - INFO - __main__ - Global step 1350 Train loss 0.73 ACC 0.5 on epoch=674
06/21/2022 16:06:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.67 on epoch=679
06/21/2022 16:06:59 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.71 on epoch=684
06/21/2022 16:07:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.62 on epoch=689
06/21/2022 16:07:02 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.73 on epoch=694
06/21/2022 16:07:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.65 on epoch=699
06/21/2022 16:07:05 - INFO - __main__ - Global step 1400 Train loss 0.68 ACC 0.5 on epoch=699
06/21/2022 16:07:06 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.67 on epoch=704
06/21/2022 16:07:08 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.66 on epoch=709
06/21/2022 16:07:09 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.60 on epoch=714
06/21/2022 16:07:11 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.57 on epoch=719
06/21/2022 16:07:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.67 on epoch=724
06/21/2022 16:07:14 - INFO - __main__ - Global step 1450 Train loss 0.64 ACC 0.5 on epoch=724
06/21/2022 16:07:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.54 on epoch=729
06/21/2022 16:07:17 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.57 on epoch=734
06/21/2022 16:07:19 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.62 on epoch=739
06/21/2022 16:07:20 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.59 on epoch=744
06/21/2022 16:07:22 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.60 on epoch=749
06/21/2022 16:07:25 - INFO - __main__ - Global step 1500 Train loss 0.58 ACC 0.5 on epoch=749
06/21/2022 16:07:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.55 on epoch=754
06/21/2022 16:07:28 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.58 on epoch=759
06/21/2022 16:07:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
06/21/2022 16:07:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.56 on epoch=769
06/21/2022 16:07:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.56 on epoch=774
06/21/2022 16:07:36 - INFO - __main__ - Global step 1550 Train loss 0.55 ACC 0.5 on epoch=774
06/21/2022 16:07:37 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.56 on epoch=779
06/21/2022 16:07:39 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.54 on epoch=784
06/21/2022 16:07:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.53 on epoch=789
06/21/2022 16:07:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.55 on epoch=794
06/21/2022 16:07:43 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.51 on epoch=799
06/21/2022 16:07:46 - INFO - __main__ - Global step 1600 Train loss 0.54 ACC 0.5 on epoch=799
06/21/2022 16:07:48 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
06/21/2022 16:07:49 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.56 on epoch=809
06/21/2022 16:07:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.49 on epoch=814
06/21/2022 16:07:52 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.46 on epoch=819
06/21/2022 16:07:53 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.55 on epoch=824
06/21/2022 16:07:57 - INFO - __main__ - Global step 1650 Train loss 0.51 ACC 0.5 on epoch=824
06/21/2022 16:07:58 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.49 on epoch=829
06/21/2022 16:08:00 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.46 on epoch=834
06/21/2022 16:08:01 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.50 on epoch=839
06/21/2022 16:08:02 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.50 on epoch=844
06/21/2022 16:08:04 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.37 on epoch=849
06/21/2022 16:08:07 - INFO - __main__ - Global step 1700 Train loss 0.46 ACC 0.5 on epoch=849
06/21/2022 16:08:08 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.49 on epoch=854
06/21/2022 16:08:10 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.48 on epoch=859
06/21/2022 16:08:11 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.51 on epoch=864
06/21/2022 16:08:13 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.51 on epoch=869
06/21/2022 16:08:14 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.51 on epoch=874
06/21/2022 16:08:16 - INFO - __main__ - Global step 1750 Train loss 0.50 ACC 0.5 on epoch=874
06/21/2022 16:08:17 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.46 on epoch=879
06/21/2022 16:08:19 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.43 on epoch=884
06/21/2022 16:08:20 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.46 on epoch=889
06/21/2022 16:08:21 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.47 on epoch=894
06/21/2022 16:08:23 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.45 on epoch=899
06/21/2022 16:08:25 - INFO - __main__ - Global step 1800 Train loss 0.45 ACC 0.5 on epoch=899
06/21/2022 16:08:27 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.50 on epoch=904
06/21/2022 16:08:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.44 on epoch=909
06/21/2022 16:08:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.44 on epoch=914
06/21/2022 16:08:31 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.43 on epoch=919
06/21/2022 16:08:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.45 on epoch=924
06/21/2022 16:08:33 - INFO - __main__ - Global step 1850 Train loss 0.45 ACC 0.5 on epoch=924
06/21/2022 16:08:34 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.45 on epoch=929
06/21/2022 16:08:36 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.46 on epoch=934
06/21/2022 16:08:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.43 on epoch=939
06/21/2022 16:08:38 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.50 on epoch=944
06/21/2022 16:08:40 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.47 on epoch=949
06/21/2022 16:08:41 - INFO - __main__ - Global step 1900 Train loss 0.46 ACC 0.5 on epoch=949
06/21/2022 16:08:42 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.40 on epoch=954
06/21/2022 16:08:44 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.43 on epoch=959
06/21/2022 16:08:45 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.39 on epoch=964
06/21/2022 16:08:47 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.48 on epoch=969
06/21/2022 16:08:49 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.43 on epoch=974
06/21/2022 16:08:49 - INFO - __main__ - Global step 1950 Train loss 0.43 ACC 0.5 on epoch=974
06/21/2022 16:08:51 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.47 on epoch=979
06/21/2022 16:08:52 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.46 on epoch=984
06/21/2022 16:08:54 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.43 on epoch=989
06/21/2022 16:08:55 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.42 on epoch=994
06/21/2022 16:08:56 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.43 on epoch=999
06/21/2022 16:08:57 - INFO - __main__ - Global step 2000 Train loss 0.44 ACC 0.5 on epoch=999
06/21/2022 16:08:57 - INFO - __main__ - save last model!
06/21/2022 16:08:57 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 16:08:57 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 16:08:57 - INFO - __main__ - Printing 3 examples
06/21/2022 16:08:57 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 16:08:57 - INFO - __main__ - ['equivalent']
06/21/2022 16:08:57 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 16:08:57 - INFO - __main__ - ['not_equivalent']
06/21/2022 16:08:57 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 16:08:57 - INFO - __main__ - ['not_equivalent']
06/21/2022 16:08:57 - INFO - __main__ - Tokenizing Input ...
06/21/2022 16:08:57 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:08:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:08:58 - INFO - __main__ - Printing 3 examples
06/21/2022 16:08:58 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/21/2022 16:08:58 - INFO - __main__ - ['equivalent']
06/21/2022 16:08:58 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher  a three-term congressman from Lexington  had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/21/2022 16:08:58 - INFO - __main__ - ['equivalent']
06/21/2022 16:08:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/21/2022 16:08:58 - INFO - __main__ - ['equivalent']
06/21/2022 16:08:58 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 16:08:58 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:08:58 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 16:08:58 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:08:58 - INFO - __main__ - Printing 3 examples
06/21/2022 16:08:58 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/21/2022 16:08:58 - INFO - __main__ - ['equivalent']
06/21/2022 16:08:58 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/21/2022 16:08:58 - INFO - __main__ - ['equivalent']
06/21/2022 16:08:58 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that  $ US51 billion  was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/21/2022 16:08:58 - INFO - __main__ - ['equivalent']
06/21/2022 16:08:58 - INFO - __main__ - Tokenizing Input ...
06/21/2022 16:08:58 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:08:58 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 16:08:58 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 16:09:04 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 16:09:05 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 16:09:05 - INFO - __main__ - Starting training!
06/21/2022 16:09:07 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.4_8_predictions.txt
06/21/2022 16:09:07 - INFO - __main__ - ACC on test data: 0.6814
06/21/2022 16:09:07 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.4, bsz=8, dev_performance=0.53125, test_performance=0.6813725490196079
06/21/2022 16:09:07 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.3, bsz=8 ...
06/21/2022 16:09:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:09:08 - INFO - __main__ - Printing 3 examples
06/21/2022 16:09:08 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/21/2022 16:09:08 - INFO - __main__ - ['equivalent']
06/21/2022 16:09:08 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher  a three-term congressman from Lexington  had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/21/2022 16:09:08 - INFO - __main__ - ['equivalent']
06/21/2022 16:09:08 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/21/2022 16:09:08 - INFO - __main__ - ['equivalent']
06/21/2022 16:09:08 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 16:09:08 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:09:08 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 16:09:08 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:09:08 - INFO - __main__ - Printing 3 examples
06/21/2022 16:09:08 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/21/2022 16:09:08 - INFO - __main__ - ['equivalent']
06/21/2022 16:09:08 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/21/2022 16:09:08 - INFO - __main__ - ['equivalent']
06/21/2022 16:09:08 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that  $ US51 billion  was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/21/2022 16:09:08 - INFO - __main__ - ['equivalent']
06/21/2022 16:09:08 - INFO - __main__ - Tokenizing Input ...
06/21/2022 16:09:08 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:09:08 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 16:09:15 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 16:09:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 16:09:15 - INFO - __main__ - Starting training!
06/21/2022 16:09:17 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/21/2022 16:09:19 - INFO - __main__ - Step 20 Global step 20 Train loss 6.95 on epoch=9
06/21/2022 16:09:20 - INFO - __main__ - Step 30 Global step 30 Train loss 6.87 on epoch=14
06/21/2022 16:09:22 - INFO - __main__ - Step 40 Global step 40 Train loss 6.77 on epoch=19
06/21/2022 16:09:23 - INFO - __main__ - Step 50 Global step 50 Train loss 6.81 on epoch=24
06/21/2022 16:09:31 - INFO - __main__ - Global step 50 Train loss 6.85 ACC 0.0 on epoch=24
06/21/2022 16:09:31 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 16:09:33 - INFO - __main__ - Step 60 Global step 60 Train loss 6.74 on epoch=29
06/21/2022 16:09:34 - INFO - __main__ - Step 70 Global step 70 Train loss 6.73 on epoch=34
06/21/2022 16:09:36 - INFO - __main__ - Step 80 Global step 80 Train loss 6.68 on epoch=39
06/21/2022 16:09:37 - INFO - __main__ - Step 90 Global step 90 Train loss 6.69 on epoch=44
06/21/2022 16:09:39 - INFO - __main__ - Step 100 Global step 100 Train loss 6.68 on epoch=49
06/21/2022 16:09:42 - INFO - __main__ - Global step 100 Train loss 6.70 ACC 0.0 on epoch=49
06/21/2022 16:09:43 - INFO - __main__ - Step 110 Global step 110 Train loss 6.61 on epoch=54
06/21/2022 16:09:45 - INFO - __main__ - Step 120 Global step 120 Train loss 6.71 on epoch=59
06/21/2022 16:09:46 - INFO - __main__ - Step 130 Global step 130 Train loss 6.57 on epoch=64
06/21/2022 16:09:48 - INFO - __main__ - Step 140 Global step 140 Train loss 6.65 on epoch=69
06/21/2022 16:09:49 - INFO - __main__ - Step 150 Global step 150 Train loss 6.66 on epoch=74
06/21/2022 16:09:56 - INFO - __main__ - Global step 150 Train loss 6.64 ACC 0.0 on epoch=74
06/21/2022 16:09:58 - INFO - __main__ - Step 160 Global step 160 Train loss 6.61 on epoch=79
06/21/2022 16:09:59 - INFO - __main__ - Step 170 Global step 170 Train loss 6.54 on epoch=84
06/21/2022 16:10:01 - INFO - __main__ - Step 180 Global step 180 Train loss 6.44 on epoch=89
06/21/2022 16:10:02 - INFO - __main__ - Step 190 Global step 190 Train loss 6.48 on epoch=94
06/21/2022 16:10:03 - INFO - __main__ - Step 200 Global step 200 Train loss 6.38 on epoch=99
06/21/2022 16:10:11 - INFO - __main__ - Global step 200 Train loss 6.49 ACC 0.0 on epoch=99
06/21/2022 16:10:12 - INFO - __main__ - Step 210 Global step 210 Train loss 6.42 on epoch=104
06/21/2022 16:10:14 - INFO - __main__ - Step 220 Global step 220 Train loss 6.38 on epoch=109
06/21/2022 16:10:15 - INFO - __main__ - Step 230 Global step 230 Train loss 6.35 on epoch=114
06/21/2022 16:10:16 - INFO - __main__ - Step 240 Global step 240 Train loss 6.40 on epoch=119
06/21/2022 16:10:18 - INFO - __main__ - Step 250 Global step 250 Train loss 6.27 on epoch=124
06/21/2022 16:10:29 - INFO - __main__ - Global step 250 Train loss 6.36 ACC 0.0 on epoch=124
06/21/2022 16:10:30 - INFO - __main__ - Step 260 Global step 260 Train loss 6.15 on epoch=129
06/21/2022 16:10:31 - INFO - __main__ - Step 270 Global step 270 Train loss 6.13 on epoch=134
06/21/2022 16:10:33 - INFO - __main__ - Step 280 Global step 280 Train loss 6.05 on epoch=139
06/21/2022 16:10:34 - INFO - __main__ - Step 290 Global step 290 Train loss 5.95 on epoch=144
06/21/2022 16:10:36 - INFO - __main__ - Step 300 Global step 300 Train loss 5.94 on epoch=149
06/21/2022 16:10:40 - INFO - __main__ - Global step 300 Train loss 6.04 ACC 0.0 on epoch=149
06/21/2022 16:10:41 - INFO - __main__ - Step 310 Global step 310 Train loss 5.73 on epoch=154
06/21/2022 16:10:42 - INFO - __main__ - Step 320 Global step 320 Train loss 5.71 on epoch=159
06/21/2022 16:10:44 - INFO - __main__ - Step 330 Global step 330 Train loss 5.63 on epoch=164
06/21/2022 16:10:45 - INFO - __main__ - Step 340 Global step 340 Train loss 5.60 on epoch=169
06/21/2022 16:10:47 - INFO - __main__ - Step 350 Global step 350 Train loss 5.55 on epoch=174
06/21/2022 16:10:48 - INFO - __main__ - Global step 350 Train loss 5.64 ACC 0.0 on epoch=174
06/21/2022 16:10:49 - INFO - __main__ - Step 360 Global step 360 Train loss 5.32 on epoch=179
06/21/2022 16:10:51 - INFO - __main__ - Step 370 Global step 370 Train loss 5.32 on epoch=184
06/21/2022 16:10:52 - INFO - __main__ - Step 380 Global step 380 Train loss 5.22 on epoch=189
06/21/2022 16:10:53 - INFO - __main__ - Step 390 Global step 390 Train loss 5.21 on epoch=194
06/21/2022 16:10:55 - INFO - __main__ - Step 400 Global step 400 Train loss 4.98 on epoch=199
06/21/2022 16:10:57 - INFO - __main__ - Global step 400 Train loss 5.21 ACC 0.0 on epoch=199
06/21/2022 16:10:58 - INFO - __main__ - Step 410 Global step 410 Train loss 5.01 on epoch=204
06/21/2022 16:10:59 - INFO - __main__ - Step 420 Global step 420 Train loss 4.94 on epoch=209
06/21/2022 16:11:01 - INFO - __main__ - Step 430 Global step 430 Train loss 4.94 on epoch=214
06/21/2022 16:11:02 - INFO - __main__ - Step 440 Global step 440 Train loss 4.88 on epoch=219
06/21/2022 16:11:03 - INFO - __main__ - Step 450 Global step 450 Train loss 4.67 on epoch=224
06/21/2022 16:11:05 - INFO - __main__ - Global step 450 Train loss 4.89 ACC 0.0 on epoch=224
06/21/2022 16:11:06 - INFO - __main__ - Step 460 Global step 460 Train loss 4.68 on epoch=229
06/21/2022 16:11:07 - INFO - __main__ - Step 470 Global step 470 Train loss 4.51 on epoch=234
06/21/2022 16:11:09 - INFO - __main__ - Step 480 Global step 480 Train loss 4.54 on epoch=239
06/21/2022 16:11:10 - INFO - __main__ - Step 490 Global step 490 Train loss 4.51 on epoch=244
06/21/2022 16:11:12 - INFO - __main__ - Step 500 Global step 500 Train loss 4.44 on epoch=249
06/21/2022 16:11:13 - INFO - __main__ - Global step 500 Train loss 4.54 ACC 0.0 on epoch=249
06/21/2022 16:11:15 - INFO - __main__ - Step 510 Global step 510 Train loss 4.58 on epoch=254
06/21/2022 16:11:16 - INFO - __main__ - Step 520 Global step 520 Train loss 4.48 on epoch=259
06/21/2022 16:11:18 - INFO - __main__ - Step 530 Global step 530 Train loss 4.37 on epoch=264
06/21/2022 16:11:19 - INFO - __main__ - Step 540 Global step 540 Train loss 4.37 on epoch=269
06/21/2022 16:11:21 - INFO - __main__ - Step 550 Global step 550 Train loss 4.25 on epoch=274
06/21/2022 16:11:28 - INFO - __main__ - Global step 550 Train loss 4.41 ACC 0.0 on epoch=274
06/21/2022 16:11:29 - INFO - __main__ - Step 560 Global step 560 Train loss 4.31 on epoch=279
06/21/2022 16:11:31 - INFO - __main__ - Step 570 Global step 570 Train loss 4.16 on epoch=284
06/21/2022 16:11:32 - INFO - __main__ - Step 580 Global step 580 Train loss 4.27 on epoch=289
06/21/2022 16:11:33 - INFO - __main__ - Step 590 Global step 590 Train loss 4.08 on epoch=294
06/21/2022 16:11:35 - INFO - __main__ - Step 600 Global step 600 Train loss 4.10 on epoch=299
06/21/2022 16:11:38 - INFO - __main__ - Global step 600 Train loss 4.18 ACC 0.0 on epoch=299
06/21/2022 16:11:40 - INFO - __main__ - Step 610 Global step 610 Train loss 4.18 on epoch=304
06/21/2022 16:11:41 - INFO - __main__ - Step 620 Global step 620 Train loss 3.96 on epoch=309
06/21/2022 16:11:43 - INFO - __main__ - Step 630 Global step 630 Train loss 3.93 on epoch=314
06/21/2022 16:11:45 - INFO - __main__ - Step 640 Global step 640 Train loss 3.90 on epoch=319
06/21/2022 16:11:46 - INFO - __main__ - Step 650 Global step 650 Train loss 3.88 on epoch=324
06/21/2022 16:11:54 - INFO - __main__ - Global step 650 Train loss 3.97 ACC 0.0 on epoch=324
06/21/2022 16:11:55 - INFO - __main__ - Step 660 Global step 660 Train loss 3.76 on epoch=329
06/21/2022 16:11:57 - INFO - __main__ - Step 670 Global step 670 Train loss 3.76 on epoch=334
06/21/2022 16:11:59 - INFO - __main__ - Step 680 Global step 680 Train loss 3.81 on epoch=339
06/21/2022 16:12:00 - INFO - __main__ - Step 690 Global step 690 Train loss 3.59 on epoch=344
06/21/2022 16:12:02 - INFO - __main__ - Step 700 Global step 700 Train loss 3.65 on epoch=349
06/21/2022 16:12:13 - INFO - __main__ - Global step 700 Train loss 3.71 ACC 0.0 on epoch=349
06/21/2022 16:12:15 - INFO - __main__ - Step 710 Global step 710 Train loss 3.58 on epoch=354
06/21/2022 16:12:16 - INFO - __main__ - Step 720 Global step 720 Train loss 3.52 on epoch=359
06/21/2022 16:12:17 - INFO - __main__ - Step 730 Global step 730 Train loss 3.41 on epoch=364
06/21/2022 16:12:19 - INFO - __main__ - Step 740 Global step 740 Train loss 3.42 on epoch=369
06/21/2022 16:12:20 - INFO - __main__ - Step 750 Global step 750 Train loss 3.38 on epoch=374
06/21/2022 16:12:32 - INFO - __main__ - Global step 750 Train loss 3.46 ACC 0.0 on epoch=374
06/21/2022 16:12:33 - INFO - __main__ - Step 760 Global step 760 Train loss 3.17 on epoch=379
06/21/2022 16:12:35 - INFO - __main__ - Step 770 Global step 770 Train loss 3.24 on epoch=384
06/21/2022 16:12:36 - INFO - __main__ - Step 780 Global step 780 Train loss 3.22 on epoch=389
06/21/2022 16:12:38 - INFO - __main__ - Step 790 Global step 790 Train loss 3.35 on epoch=394
06/21/2022 16:12:39 - INFO - __main__ - Step 800 Global step 800 Train loss 3.12 on epoch=399
06/21/2022 16:12:51 - INFO - __main__ - Global step 800 Train loss 3.22 ACC 0.0 on epoch=399
06/21/2022 16:12:52 - INFO - __main__ - Step 810 Global step 810 Train loss 3.05 on epoch=404
06/21/2022 16:12:53 - INFO - __main__ - Step 820 Global step 820 Train loss 2.99 on epoch=409
06/21/2022 16:12:55 - INFO - __main__ - Step 830 Global step 830 Train loss 3.18 on epoch=414
06/21/2022 16:12:56 - INFO - __main__ - Step 840 Global step 840 Train loss 3.03 on epoch=419
06/21/2022 16:12:58 - INFO - __main__ - Step 850 Global step 850 Train loss 2.96 on epoch=424
06/21/2022 16:13:09 - INFO - __main__ - Global step 850 Train loss 3.04 ACC 0.0 on epoch=424
06/21/2022 16:13:11 - INFO - __main__ - Step 860 Global step 860 Train loss 2.95 on epoch=429
06/21/2022 16:13:12 - INFO - __main__ - Step 870 Global step 870 Train loss 2.88 on epoch=434
06/21/2022 16:13:13 - INFO - __main__ - Step 880 Global step 880 Train loss 2.82 on epoch=439
06/21/2022 16:13:15 - INFO - __main__ - Step 890 Global step 890 Train loss 2.66 on epoch=444
06/21/2022 16:13:16 - INFO - __main__ - Step 900 Global step 900 Train loss 2.70 on epoch=449
06/21/2022 16:13:28 - INFO - __main__ - Global step 900 Train loss 2.80 ACC 0.0 on epoch=449
06/21/2022 16:13:30 - INFO - __main__ - Step 910 Global step 910 Train loss 2.67 on epoch=454
06/21/2022 16:13:32 - INFO - __main__ - Step 920 Global step 920 Train loss 2.75 on epoch=459
06/21/2022 16:13:33 - INFO - __main__ - Step 930 Global step 930 Train loss 2.67 on epoch=464
06/21/2022 16:13:35 - INFO - __main__ - Step 940 Global step 940 Train loss 2.55 on epoch=469
06/21/2022 16:13:36 - INFO - __main__ - Step 950 Global step 950 Train loss 2.44 on epoch=474
06/21/2022 16:13:44 - INFO - __main__ - Global step 950 Train loss 2.62 ACC 0.0 on epoch=474
06/21/2022 16:13:46 - INFO - __main__ - Step 960 Global step 960 Train loss 2.56 on epoch=479
06/21/2022 16:13:47 - INFO - __main__ - Step 970 Global step 970 Train loss 2.44 on epoch=484
06/21/2022 16:13:48 - INFO - __main__ - Step 980 Global step 980 Train loss 2.36 on epoch=489
06/21/2022 16:13:50 - INFO - __main__ - Step 990 Global step 990 Train loss 2.32 on epoch=494
06/21/2022 16:13:51 - INFO - __main__ - Step 1000 Global step 1000 Train loss 2.25 on epoch=499
06/21/2022 16:13:53 - INFO - __main__ - Global step 1000 Train loss 2.39 ACC 0.15625 on epoch=499
06/21/2022 16:13:54 - INFO - __main__ - Saving model with best ACC: 0.0 -> 0.15625 on epoch=499, global_step=1000
06/21/2022 16:13:55 - INFO - __main__ - Step 1010 Global step 1010 Train loss 2.22 on epoch=504
06/21/2022 16:13:56 - INFO - __main__ - Step 1020 Global step 1020 Train loss 2.24 on epoch=509
06/21/2022 16:13:58 - INFO - __main__ - Step 1030 Global step 1030 Train loss 2.14 on epoch=514
06/21/2022 16:13:59 - INFO - __main__ - Step 1040 Global step 1040 Train loss 2.17 on epoch=519
06/21/2022 16:14:01 - INFO - __main__ - Step 1050 Global step 1050 Train loss 2.10 on epoch=524
06/21/2022 16:14:02 - INFO - __main__ - Global step 1050 Train loss 2.17 ACC 0.53125 on epoch=524
06/21/2022 16:14:02 - INFO - __main__ - Saving model with best ACC: 0.15625 -> 0.53125 on epoch=524, global_step=1050
06/21/2022 16:14:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 2.21 on epoch=529
06/21/2022 16:14:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 2.09 on epoch=534
06/21/2022 16:14:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 2.12 on epoch=539
06/21/2022 16:14:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 2.13 on epoch=544
06/21/2022 16:14:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 2.04 on epoch=549
06/21/2022 16:14:11 - INFO - __main__ - Global step 1100 Train loss 2.11 ACC 0.5 on epoch=549
06/21/2022 16:14:13 - INFO - __main__ - Step 1110 Global step 1110 Train loss 1.93 on epoch=554
06/21/2022 16:14:14 - INFO - __main__ - Step 1120 Global step 1120 Train loss 1.95 on epoch=559
06/21/2022 16:14:16 - INFO - __main__ - Step 1130 Global step 1130 Train loss 2.02 on epoch=564
06/21/2022 16:14:17 - INFO - __main__ - Step 1140 Global step 1140 Train loss 1.86 on epoch=569
06/21/2022 16:14:19 - INFO - __main__ - Step 1150 Global step 1150 Train loss 1.93 on epoch=574
06/21/2022 16:14:23 - INFO - __main__ - Global step 1150 Train loss 1.94 ACC 0.5625 on epoch=574
06/21/2022 16:14:23 - INFO - __main__ - Saving model with best ACC: 0.53125 -> 0.5625 on epoch=574, global_step=1150
06/21/2022 16:14:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 1.86 on epoch=579
06/21/2022 16:14:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 1.77 on epoch=584
06/21/2022 16:14:27 - INFO - __main__ - Step 1180 Global step 1180 Train loss 1.75 on epoch=589
06/21/2022 16:14:29 - INFO - __main__ - Step 1190 Global step 1190 Train loss 1.68 on epoch=594
06/21/2022 16:14:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 1.59 on epoch=599
06/21/2022 16:14:37 - INFO - __main__ - Global step 1200 Train loss 1.73 ACC 0.46875 on epoch=599
06/21/2022 16:14:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 1.61 on epoch=604
06/21/2022 16:14:40 - INFO - __main__ - Step 1220 Global step 1220 Train loss 1.56 on epoch=609
06/21/2022 16:14:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 1.52 on epoch=614
06/21/2022 16:14:43 - INFO - __main__ - Step 1240 Global step 1240 Train loss 1.46 on epoch=619
06/21/2022 16:14:44 - INFO - __main__ - Step 1250 Global step 1250 Train loss 1.54 on epoch=624
06/21/2022 16:14:46 - INFO - __main__ - Global step 1250 Train loss 1.54 ACC 0.53125 on epoch=624
06/21/2022 16:14:47 - INFO - __main__ - Step 1260 Global step 1260 Train loss 1.40 on epoch=629
06/21/2022 16:14:49 - INFO - __main__ - Step 1270 Global step 1270 Train loss 1.32 on epoch=634
06/21/2022 16:14:50 - INFO - __main__ - Step 1280 Global step 1280 Train loss 1.28 on epoch=639
06/21/2022 16:14:52 - INFO - __main__ - Step 1290 Global step 1290 Train loss 1.42 on epoch=644
06/21/2022 16:14:53 - INFO - __main__ - Step 1300 Global step 1300 Train loss 1.29 on epoch=649
06/21/2022 16:14:56 - INFO - __main__ - Global step 1300 Train loss 1.34 ACC 0.46875 on epoch=649
06/21/2022 16:14:57 - INFO - __main__ - Step 1310 Global step 1310 Train loss 1.23 on epoch=654
06/21/2022 16:14:59 - INFO - __main__ - Step 1320 Global step 1320 Train loss 1.25 on epoch=659
06/21/2022 16:15:00 - INFO - __main__ - Step 1330 Global step 1330 Train loss 1.24 on epoch=664
06/21/2022 16:15:02 - INFO - __main__ - Step 1340 Global step 1340 Train loss 1.28 on epoch=669
06/21/2022 16:15:03 - INFO - __main__ - Step 1350 Global step 1350 Train loss 1.28 on epoch=674
06/21/2022 16:15:05 - INFO - __main__ - Global step 1350 Train loss 1.26 ACC 0.4375 on epoch=674
06/21/2022 16:15:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 1.25 on epoch=679
06/21/2022 16:15:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 1.12 on epoch=684
06/21/2022 16:15:09 - INFO - __main__ - Step 1380 Global step 1380 Train loss 1.13 on epoch=689
06/21/2022 16:15:11 - INFO - __main__ - Step 1390 Global step 1390 Train loss 1.14 on epoch=694
06/21/2022 16:15:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 1.05 on epoch=699
06/21/2022 16:15:20 - INFO - __main__ - Global step 1400 Train loss 1.14 ACC 0.5625 on epoch=699
06/21/2022 16:15:21 - INFO - __main__ - Step 1410 Global step 1410 Train loss 1.03 on epoch=704
06/21/2022 16:15:23 - INFO - __main__ - Step 1420 Global step 1420 Train loss 1.17 on epoch=709
06/21/2022 16:15:24 - INFO - __main__ - Step 1430 Global step 1430 Train loss 1.04 on epoch=714
06/21/2022 16:15:26 - INFO - __main__ - Step 1440 Global step 1440 Train loss 1.00 on epoch=719
06/21/2022 16:15:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 1.00 on epoch=724
06/21/2022 16:15:30 - INFO - __main__ - Global step 1450 Train loss 1.05 ACC 0.5 on epoch=724
06/21/2022 16:15:32 - INFO - __main__ - Step 1460 Global step 1460 Train loss 1.01 on epoch=729
06/21/2022 16:15:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 1.03 on epoch=734
06/21/2022 16:15:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.93 on epoch=739
06/21/2022 16:15:36 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.87 on epoch=744
06/21/2022 16:15:37 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.91 on epoch=749
06/21/2022 16:15:39 - INFO - __main__ - Global step 1500 Train loss 0.95 ACC 0.53125 on epoch=749
06/21/2022 16:15:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.92 on epoch=754
06/21/2022 16:15:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.89 on epoch=759
06/21/2022 16:15:43 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.84 on epoch=764
06/21/2022 16:15:45 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.85 on epoch=769
06/21/2022 16:15:46 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.84 on epoch=774
06/21/2022 16:15:48 - INFO - __main__ - Global step 1550 Train loss 0.87 ACC 0.5 on epoch=774
06/21/2022 16:15:49 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.85 on epoch=779
06/21/2022 16:15:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.79 on epoch=784
06/21/2022 16:15:52 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.71 on epoch=789
06/21/2022 16:15:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.67 on epoch=794
06/21/2022 16:15:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.75 on epoch=799
06/21/2022 16:15:56 - INFO - __main__ - Global step 1600 Train loss 0.76 ACC 0.5 on epoch=799
06/21/2022 16:15:58 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.71 on epoch=804
06/21/2022 16:15:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.75 on epoch=809
06/21/2022 16:16:01 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.70 on epoch=814
06/21/2022 16:16:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.66 on epoch=819
06/21/2022 16:16:04 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.58 on epoch=824
06/21/2022 16:16:05 - INFO - __main__ - Global step 1650 Train loss 0.68 ACC 0.5 on epoch=824
06/21/2022 16:16:07 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.57 on epoch=829
06/21/2022 16:16:08 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.58 on epoch=834
06/21/2022 16:16:10 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.58 on epoch=839
06/21/2022 16:16:11 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.56 on epoch=844
06/21/2022 16:16:12 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.57 on epoch=849
06/21/2022 16:16:14 - INFO - __main__ - Global step 1700 Train loss 0.57 ACC 0.5 on epoch=849
06/21/2022 16:16:16 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.62 on epoch=854
06/21/2022 16:16:17 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.60 on epoch=859
06/21/2022 16:16:19 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.50 on epoch=864
06/21/2022 16:16:20 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.61 on epoch=869
06/21/2022 16:16:22 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.50 on epoch=874
06/21/2022 16:16:24 - INFO - __main__ - Global step 1750 Train loss 0.57 ACC 0.5 on epoch=874
06/21/2022 16:16:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.59 on epoch=879
06/21/2022 16:16:27 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.60 on epoch=884
06/21/2022 16:16:28 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.64 on epoch=889
06/21/2022 16:16:30 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.51 on epoch=894
06/21/2022 16:16:31 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.56 on epoch=899
06/21/2022 16:16:32 - INFO - __main__ - Global step 1800 Train loss 0.58 ACC 0.5 on epoch=899
06/21/2022 16:16:33 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.59 on epoch=904
06/21/2022 16:16:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.51 on epoch=909
06/21/2022 16:16:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.50 on epoch=914
06/21/2022 16:16:38 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.49 on epoch=919
06/21/2022 16:16:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.47 on epoch=924
06/21/2022 16:16:40 - INFO - __main__ - Global step 1850 Train loss 0.51 ACC 0.5 on epoch=924
06/21/2022 16:16:42 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.59 on epoch=929
06/21/2022 16:16:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.54 on epoch=934
06/21/2022 16:16:44 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.40 on epoch=939
06/21/2022 16:16:46 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.55 on epoch=944
06/21/2022 16:16:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.49 on epoch=949
06/21/2022 16:16:48 - INFO - __main__ - Global step 1900 Train loss 0.51 ACC 0.53125 on epoch=949
06/21/2022 16:16:49 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.50 on epoch=954
06/21/2022 16:16:51 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.45 on epoch=959
06/21/2022 16:16:52 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.51 on epoch=964
06/21/2022 16:16:54 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.53 on epoch=969
06/21/2022 16:16:55 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.46 on epoch=974
06/21/2022 16:16:56 - INFO - __main__ - Global step 1950 Train loss 0.49 ACC 0.59375 on epoch=974
06/21/2022 16:16:56 - INFO - __main__ - Saving model with best ACC: 0.5625 -> 0.59375 on epoch=974, global_step=1950
06/21/2022 16:16:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.45 on epoch=979
06/21/2022 16:16:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.43 on epoch=984
06/21/2022 16:17:00 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.45 on epoch=989
06/21/2022 16:17:02 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.45 on epoch=994
06/21/2022 16:17:03 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.44 on epoch=999
06/21/2022 16:17:04 - INFO - __main__ - Global step 2000 Train loss 0.44 ACC 0.46875 on epoch=999
06/21/2022 16:17:04 - INFO - __main__ - save last model!
06/21/2022 16:17:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
06/21/2022 16:17:04 - INFO - __main__ - Start tokenizing ... 408 instances
06/21/2022 16:17:04 - INFO - __main__ - Printing 3 examples
06/21/2022 16:17:04 - INFO - __main__ -  [glue-mrpc] sentence 1: He said the foodservice pie business doesn 't fit the company 's long-term growth strategy . [SEP] sentence 2: " The foodservice pie business does not fit our long-term growth strategy .
06/21/2022 16:17:04 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:04 - INFO - __main__ -  [glue-mrpc] sentence 1: Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war . [SEP] sentence 2: His wife said he was " 100 percent behind George Bush " and looked forward to using his years of training in the war .
06/21/2022 16:17:04 - INFO - __main__ - ['not_equivalent']
06/21/2022 16:17:04 - INFO - __main__ -  [glue-mrpc] sentence 1: The dollar was at 116.92 yen against the yen , flat on the session , and at 1.2891 against the Swiss franc , also flat . [SEP] sentence 2: The dollar was at 116.78 yen JPY = , virtually flat on the session , and at 1.2871 against the Swiss franc CHF = , down 0.1 percent .
06/21/2022 16:17:04 - INFO - __main__ - ['not_equivalent']
06/21/2022 16:17:04 - INFO - __main__ - Tokenizing Input ...
06/21/2022 16:17:04 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:17:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:17:05 - INFO - __main__ - Printing 3 examples
06/21/2022 16:17:05 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/21/2022 16:17:05 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:05 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher  a three-term congressman from Lexington  had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/21/2022 16:17:05 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:05 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/21/2022 16:17:05 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
06/21/2022 16:17:05 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:17:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 16:17:05 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:17:05 - INFO - __main__ - Printing 3 examples
06/21/2022 16:17:05 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/21/2022 16:17:05 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:05 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/21/2022 16:17:05 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:05 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that  $ US51 billion  was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/21/2022 16:17:05 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:05 - INFO - __main__ - Tokenizing Input ...
06/21/2022 16:17:05 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:17:05 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 16:17:05 - INFO - __main__ - Loaded 408 examples from test data
06/21/2022 16:17:11 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 16:17:11 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 16:17:11 - INFO - __main__ - Starting training!
06/21/2022 16:17:13 - INFO - __main__ - Saved prediction in models/T5-base-reptile-nopara2para-3e-5-2-5000-5e-1-10/singletask-glue-mrpc/glue-mrpc_16_21_0.3_8_predictions.txt
06/21/2022 16:17:13 - INFO - __main__ - ACC on test data: 0.6569
06/21/2022 16:17:13 - INFO - __main__ - prefix=glue-mrpc_16_21, lr=0.3, bsz=8, dev_performance=0.59375, test_performance=0.6568627450980392
06/21/2022 16:17:13 - INFO - __main__ - Running ... prefix=glue-mrpc_16_21, lr=0.2, bsz=8 ...
06/21/2022 16:17:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:17:14 - INFO - __main__ - Printing 3 examples
06/21/2022 16:17:14 - INFO - __main__ -  [glue-mrpc] sentence 1: The national denomination of the Episcopal Church , with 2.3 million members , is the U.S. branch of the 77 million-member Anglican Communion . [SEP] sentence 2: The Episcopal Church , with 2.3 million members , is the American branch of the worldwide Anglican Communion , which has 77 million adherents .
06/21/2022 16:17:14 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:14 - INFO - __main__ -  [glue-mrpc] sentence 1: With all precincts reporting , Fletcher  a three-term congressman from Lexington  had an overwhelming 57 percent of the vote . [SEP] sentence 2: With all precincts reporting , Fletcher had 88,747 votes , or 57 percent of the total .
06/21/2022 16:17:14 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:14 - INFO - __main__ -  [glue-mrpc] sentence 1: Handset market share for the second quarter , it said , is higher than the first quarter . [SEP] sentence 2: Nokia 's market share for the second quarter is estimated to be higher than the first quarter , 2003 . "
06/21/2022 16:17:14 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:14 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
06/21/2022 16:17:14 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:17:14 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
06/21/2022 16:17:14 - INFO - __main__ - Start tokenizing ... 32 instances
06/21/2022 16:17:14 - INFO - __main__ - Printing 3 examples
06/21/2022 16:17:14 - INFO - __main__ -  [glue-mrpc] sentence 1: He planned Monday to formally announce the effort alongside several union presidents . [SEP] sentence 2: He plans to announce the effort formally tomorrow in Cincinnati alongside several union presidents .
06/21/2022 16:17:14 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:14 - INFO - __main__ -  [glue-mrpc] sentence 1: They named the man charged as Noureddinne Mouleff , a 36-year-old of North African origin who was arrested in the southern coastal town of Eastbourne last week . [SEP] sentence 2: Last week , Nur al-Din Muliff , a 36-year-old of North African origin , was arrested in the southern coastal town of Eastbourne .
06/21/2022 16:17:14 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:14 - INFO - __main__ -  [glue-mrpc] sentence 1: Most of that - $ 51 billion - was for American troops in Iraq , while another $ 10 billion was for U.S. forces in Afghanistan . [SEP] sentence 2: Most of that  $ US51 billion  was for American troops in Iraq , while another $ US10 billion was for US forces in Afghanistan .
06/21/2022 16:17:14 - INFO - __main__ - ['equivalent']
06/21/2022 16:17:14 - INFO - __main__ - Tokenizing Input ...
06/21/2022 16:17:14 - INFO - __main__ - Tokenizing Output ...
06/21/2022 16:17:14 - INFO - __main__ - Loaded 32 examples from dev data
06/21/2022 16:17:21 - INFO - __main__ - load prompt embedding from ckpt
06/21/2022 16:17:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 236.18M parameters
06/21/2022 16:17:21 - INFO - __main__ - Starting training!
06/21/2022 16:17:23 - INFO - __main__ - Step 10 Global step 10 Train loss 6.87 on epoch=4
06/21/2022 16:17:24 - INFO - __main__ - Step 20 Global step 20 Train loss 6.78 on epoch=9
06/21/2022 16:17:26 - INFO - __main__ - Step 30 Global step 30 Train loss 6.83 on epoch=14
06/21/2022 16:17:27 - INFO - __main__ - Step 40 Global step 40 Train loss 6.87 on epoch=19
06/21/2022 16:17:29 - INFO - __main__ - Step 50 Global step 50 Train loss 6.82 on epoch=24
06/21/2022 16:17:32 - INFO - __main__ - Global step 50 Train loss 6.83 ACC 0.0 on epoch=24
06/21/2022 16:17:32 - INFO - __main__ - Saving model with best ACC: -1.0 -> 0.0 on epoch=24, global_step=50
06/21/2022 16:17:33 - INFO - __main__ - Step 60 Global step 60 Train loss 6.76 on epoch=29
06/21/2022 16:17:34 - INFO - __main__ - Step 70 Global step 70 Train loss 6.83 on epoch=34
06/21/2022 16:17:36 - INFO - __main__ - Step 80 Global step 80 Train loss 6.91 on epoch=39
06/21/2022 16:17:37 - INFO - __main__ - Step 90 Global step 90 Train loss 6.86 on epoch=44
06/21/2022 16:17:39 - INFO - __main__ - Step 100 Global step 100 Train loss 6.88 on epoch=49
06/21/2022 16:17:41 - INFO - __main__ - Global step 100 Train loss 6.85 ACC 0.0 on epoch=49
06/21/2022 16:17:42 - INFO - __main__ - Step 110 Global step 110 Train loss 6.83 on epoch=54
06/21/2022 16:17:44 - INFO - __main__ - Step 120 Global step 120 Train loss 6.81 on epoch=59
06/21/2022 16:17:45 - INFO - __main__ - Step 130 Global step 130 Train loss 6.80 on epoch=64
06/21/2022 16:17:46 - INFO - __main__ - Step 140 Global step 140 Train loss 6.78 on epoch=69
06/21/2022 16:17:48 - INFO - __main__ - Step 150 Global step 150 Train loss 6.75 on epoch=74
06/21/2022 16:17:49 - INFO - __main__ - Global step 150 Train loss 6.79 ACC 0.0 on epoch=74
06/21/2022 16:17:50 - INFO - __main__ - Step 160 Global step 160 Train loss 6.71 on epoch=79
06/21/2022 16:17:52 - INFO - __main__ - Step 170 Global step 170 Train loss 6.69 on epoch=84
06/21/2022 16:17:53 - INFO - __main__ - Step 180 Global step 180 Train loss 6.67 on epoch=89
06/21/2022 16:17:55 - INFO - __main__ - Step 190 Global step 190 Train loss 6.77 on epoch=94
06/21/2022 16:17:56 - INFO - __main__ - Step 200 Global step 200 Train loss 6.66 on epoch=99
06/21/2022 16:17:59 - INFO - __main__ - Global step 200 Train loss 6.70 ACC 0.0 on epoch=99
06/21/2022 16:18:00 - INFO - __main__ - Step 210 Global step 210 Train loss 6.67 on epoch=104
06/21/2022 16:18:01 - INFO - __main__ - Step 220 Global step 220 Train loss 6.64 on epoch=109
06/21/2022 16:18:03 - INFO - __main__ - Step 230 Global step 230 Train loss 6.68 on epoch=114
06/21/2022 16:18:04 - INFO - __main__ - Step 240 Global step 240 Train loss 6.57 on epoch=119
06/21/2022 16:18:06 - INFO - __main__ - Step 250 Global step 250 Train loss 6.51 on epoch=124
06/21/2022 16:18:13 - INFO - __main__ - Global step 250 Train loss 6.61 ACC 0.0 on epoch=124
06/21/2022 16:18:14 - INFO - __main__ - Step 260 Global step 260 Train loss 6.55 on epoch=129
06/21/2022 16:18:15 - INFO - __main__ - Step 270 Global step 270 Train loss 6.54 on epoch=134
06/21/2022 16:18:17 - INFO - __main__ - Step 280 Global step 280 Train loss 6.42 on epoch=139
06/21/2022 16:18:18 - INFO - __main__ - Step 290 Global step 290 Train loss 6.47 on epoch=144
06/21/2022 16:18:20 - INFO - __main__ - Step 300 Global step 300 Train loss 6.38 on epoch=149
06/21/2022 16:18:27 - INFO - __main__ - Global step 300 Train loss 6.47 ACC 0.0 on epoch=149
06/21/2022 16:18:28 - INFO - __main__ - Step 310 Global step 310 Train loss 6.37 on epoch=154
06/21/2022 16:18:30 - INFO - __main__ - Step 320 Global step 320 Train loss 6.40 on epoch=159
06/21/2022 16:18:31 - INFO - __main__ - Step 330 Global step 330 Train loss 6.38 on epoch=164
06/21/2022 16:18:32 - INFO - __main__ - Step 340 Global step 340 Train loss 6.27 on epoch=169
06/21/2022 16:18:34 - INFO - __main__ - Step 350 Global step 350 Train loss 6.22 on epoch=174
06/21/2022 16:18:38 - INFO - __main__ - Global step 350 Train loss 6.33 ACC 0.0 on epoch=174
06/21/2022 16:18:39 - INFO - __main__ - Step 360 Global step 360 Train loss 6.18 on epoch=179
06/21/2022 16:18:41 - INFO - __main__ - Step 370 Global step 370 Train loss 6.24 on epoch=184
06/21/2022 16:18:42 - INFO - __main__ - Step 380 Global step 380 Train loss 6.11 on epoch=189
06/21/2022 16:18:43 - INFO - __main__ - Step 390 Global step 390 Train loss 6.10 on epoch=194
06/21/2022 16:18:45 - INFO - __main__ - Step 400 Global step 400 Train loss 6.16 on epoch=199
06/21/2022 16:18:47 - INFO - __main__ - Global step 400 Train loss 6.16 ACC 0.0 on epoch=199
06/21/2022 16:18:49 - INFO - __main__ - Step 410 Global step 410 Train loss 6.07 on epoch=204
06/21/2022 16:18:50 - INFO - __main__ - Step 420 Global step 420 Train loss 6.07 on epoch=209
06/21/2022 16:18:52 - INFO - __main__ - Step 430 Global step 430 Train loss 5.95 on epoch=214
06/21/2022 16:18:53 - INFO - __main__ - Step 440 Global step 440 Train loss 5.89 on epoch=219
06/21/2022 16:18:54 - INFO - __main__ - Step 450 Global step 450 Train loss 5.96 on epoch=224
06/21/2022 16:18:57 - INFO - __main__ - Global step 450 Train loss 5.99 ACC 0.0 on epoch=224
06/21/2022 16:18:58 - INFO - __main__ - Step 460 Global step 460 Train loss 5.75 on epoch=229
06/21/2022 16:18:59 - INFO - __main__ - Step 470 Global step 470 Train loss 5.71 on epoch=234
06/21/2022 16:19:00 - INFO - __main__ - Step 480 Global step 480 Train loss 5.71 on epoch=239
06/21/2022 16:19:02 - INFO - __main__ - Step 490 Global step 490 Train loss 5.78 on epoch=244
06/21/2022 16:19:03 - INFO - __main__ - Step 500 Global step 500 Train loss 5.56 on epoch=249
06/21/2022 16:19:05 - INFO - __main__ - Global step 500 Train loss 5.70 ACC 0.0 on epoch=249
06/21/2022 16:19:07 - INFO - __main__ - Step 510 Global step 510 Train loss 5.48 on epoch=254
06/21/2022 16:19:08 - INFO - __main__ - Step 520 Global step 520 Train loss 5.48 on epoch=259
06/21/2022 16:19:09 - INFO - __main__ - Step 530 Global step 530 Train loss 5.35 on epoch=264
06/21/2022 16:19:11 - INFO - __main__ - Step 540 Global step 540 Train loss 5.21 on epoch=269
06/21/2022 16:19:12 - INFO - __main__ - Step 550 Global step 550 Train loss 5.33 on epoch=274
06/21/2022 16:19:14 - INFO - __main__ - Global step 550 Train loss 5.37 ACC 0.0 on epoch=274
06/21/2022 16:19:15 - INFO - __main__ - Step 560 Global step 560 Train loss 5.33 on epoch=279
06/21/2022 16:19:16 - INFO - __main__ - Step 570 Global step 570 Train loss 4.98 on epoch=284
06/21/2022 16:19:18 - INFO - __main__ - Step 580 Global step 580 Train loss 5.21 on epoch=289
06/21/2022 16:19:19 - INFO - __main__ - Step 590 Global step 590 Train loss 5.41 on epoch=294
06/21/2022 16:19:21 - INFO - __main__ - Step 600 Global step 600 Train loss 5.43 on epoch=299
06/21/2022 16:19:27 - INFO - __main__ - Global step 600 Train loss 5.27 ACC 0.0 on epoch=299
06/21/2022 16:19:28 - INFO - __main__ - Step 610 Global step 610 Train loss 5.31 on epoch=304
06/21/2022 16:19:30 - INFO - __main__ - Step 620 Global step 620 Train loss 5.32 on epoch=309
06/21/2022 16:19:31 - INFO - __main__ - Step 630 Global step 630 Train loss 5.13 on epoch=314
06/21/2022 16:19:33 - INFO - __main__ - Step 640 Global step 640 Train loss 4.95 on epoch=319
06/21/2022 16:19:34 - INFO - __main__ - Step 650 Global step 650 Train loss 4.93 on epoch=324
06/21/2022 16:19:41 - INFO - __main__ - Global step 650 Train loss 5.13 ACC 0.0 on epoch=324
06/21/2022 16:19:42 - INFO - __main__ - Step 660 Global step 660 Train loss 4.94 on epoch=329
06/21/2022 16:19:43 - INFO - __main__ - Step 670 Global step 670 Train loss 4.91 on epoch=334
06/21/2022 16:19:45 - INFO - __main__ - Step 680 Global step 680 Train loss 4.84 on epoch=339
06/21/2022 16:19:46 - INFO - __main__ - Step 690 Global step 690 Train loss 4.91 on epoch=344
06/21/2022 16:19:47 - INFO - __main__ - Step 700 Global step 700 Train loss 4.83 on epoch=349
06/21/2022 16:19:50 - INFO - __main__ - Global step 700 Train loss 4.89 ACC 0.0 on epoch=349
06/21/2022 16:19:52 - INFO - __main__ - Step 710 Global step 710 Train loss 4.77 on epoch=354
06/21/2022 16:19:53 - INFO - __main__ - Step 720 Global step 720 Train loss 4.85 on epoch=359
06/21/2022 16:19:54 - INFO - __main__ - Step 730 Global step 730 Train loss 4.70 on epoch=364
06/21/2022 16:19:56 - INFO - __main__ - Step 740 Global step 740 Train loss 4.63 on epoch=369
06/21/2022 16:19:57 - INFO - __main__ - Step 750 Global step 750 Train loss 4.66 on epoch=374
06/21/2022 16:19:58 - INFO - __main__ - Global step 750 Train loss 4.72 ACC 0.0 on epoch=374
06/21/2022 16:20:00 - INFO - __main__ - Step 760 Global step 760 Train loss 4.63 on epoch=379
06/21/2022 16:20:01 - INFO - __main__ - Step 770 Global step 770 Train loss 4.67 on epoch=384
06/21/2022 16:20:02 - INFO - __main__ - Step 780 Global step 780 Train loss 4.79 on epoch=389
06/21/2022 16:20:04 - INFO - __main__ - Step 790 Global step 790 Train loss 4.63 on epoch=394
06/21/2022 16:20:05 - INFO - __main__ - Step 800 Global step 800 Train loss 4.68 on epoch=399
06/21/2022 16:20:12 - INFO - __main__ - Global step 800 Train loss 4.68 ACC 0.0 on epoch=399
06/21/2022 16:20:14 - INFO - __main__ - Step 810 Global step 810 Train loss 4.46 on epoch=404
06/21/2022 16:20:15 - INFO - __main__ - Step 820 Global step 820 Train loss 4.53 on epoch=409
06/21/2022 16:20:16 - INFO - __main__ - Step 830 Global step 830 Train loss 4.43 on epoch=414
06/21/2022 16:20:18 - INFO - __main__ - Step 840 Global step 840 Train loss 4.56 on epoch=419
06/21/2022 16:20:19 - INFO - __main__ - Step 850 Global step 850 Train loss 4.51 on epoch=424
06/21/2022 16:20:20 - INFO - __main__ - Global step 850 Train loss 4.50 ACC 0.0 on epoch=424
06/21/2022 16:20:22 - INFO - __main__ - Step 860 Global step 860 Train loss 4.43 on epoch=429
06/21/2022 16:20:23 - INFO - __main__ - Step 870 Global step 870 Train loss 4.43 on epoch=434
06/21/2022 16:20:25 - INFO - __main__ - Step 880 Global step 880 Train loss 4.53 on epoch=439
06/21/2022 16:20:26 - INFO - __main__ - Step 890 Global step 890 Train loss 4.47 on epoch=444
06/21/2022 16:20:27 - INFO - __main__ - Step 900 Global step 900 Train loss 4.33 on epoch=449
06/21/2022 16:20:28 - INFO - __main__ - Global step 900 Train loss 4.44 ACC 0.0 on epoch=449
06/21/2022 16:20:30 - INFO - __main__ - Step 910 Global step 910 Train loss 4.44 on epoch=454
06/21/2022 16:20:31 - INFO - __main__ - Step 920 Global step 920 Train loss 4.35 on epoch=459
06/21/2022 16:20:32 - INFO - __main__ - Step 930 Global step 930 Train loss 4.32 on epoch=464
06/21/2022 16:20:34 - INFO - __main__ - Step 940 Global step 940 Train loss 4.41 on epoch=469
06/21/2022 16:20:35 - INFO - __main__ - Step 950 Global step 950 Train loss 4.34 on epoch=474
06/21/2022 16:20:36 - INFO - __main__ - Global step 950 Train loss 4.37 ACC 0.0 on epoch=474
06/21/2022 16:20:38 - INFO - __main__ - Step 960 Global step 960 Train loss 4.48 on epoch=479
06/21/2022 16:20:39 - INFO - __main__ - Step 970 Global step 970 Train loss 4.20 on epoch=484
06/21/2022 16:20:40 - INFO - __main__ - Step 980 Global step 980 Train loss 4.35 on epoch=489
06/21/2022 16:20:42 - INFO - __main__ - Step 990 Global step 990 Train loss 4.26 on epoch=494
06/21/2022 16:20:43 - INFO - __main__ - Step 1000 Global step 1000 Train loss 4.27 on epoch=499
06/21/2022 16:20:45 - INFO - __main__ - Global step 1000 Train loss 4.31 ACC 0.0 on epoch=499
06/21/2022 16:20:46 - INFO - __main__ - Step 1010 Global step 1010 Train loss 4.31 on epoch=504
06/21/2022 16:20:47 - INFO - __main__ - Step 1020 Global step 1020 Train loss 4.15 on epoch=509
06/21/2022 16:20:49 - INFO - __main__ - Step 1030 Global step 1030 Train loss 4.13 on epoch=514
06/21/2022 16:20:50 - INFO - __main__ - Step 1040 Global step 1040 Train loss 4.18 on epoch=519
06/21/2022 16:20:52 - INFO - __main__ - Step 1050 Global step 1050 Train loss 4.18 on epoch=524
06/21/2022 16:20:54 - INFO - __main__ - Global step 1050 Train loss 4.19 ACC 0.0 on epoch=524
06/21/2022 16:20:55 - INFO - __main__ - Step 1060 Global step 1060 Train loss 4.12 on epoch=529
06/21/2022 16:20:56 - INFO - __main__ - Step 1070 Global step 1070 Train loss 4.04 on epoch=534
06/21/2022 16:20:58 - INFO - __main__ - Step 1080 Global step 1080 Train loss 4.09 on epoch=539
06/21/2022 16:20:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 4.06 on epoch=544
06/21/2022 16:21:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 4.00 on epoch=549
06/21/2022 16:21:02 - INFO - __main__ - Global step 1100 Train loss 4.06 ACC 0.0 on epoch=549
06/21/2022 16:21:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 4.03 on epoch=554
06/21/2022 16:21:05 - INFO - __main__ - Step 1120 Global step 1120 Train loss 3.95 on epoch=559
06/21/2022 16:21:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 3.94 on epoch=564
06/21/2022 16:21:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 3.92 on epoch=569
06/21/2022 16:21:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 4.08 on epoch=574
06/21/2022 16:21:11 - INFO - __main__ - Global step 1150 Train loss 3.98 ACC 0.0 on epoch=574
06/21/2022 16:21:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 4.05 on epoch=579
06/21/2022 16:21:14 - INFO - __main__ - Step 1170 Global step 1170 Train loss 3.96 on epoch=584
06/21/2022 16:21:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 3.98 on epoch=589
06/21/2022 16:21:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 3.88 on epoch=594
06/21/2022 16:21:18 - INFO - __main__ - Step 1200 Global step 1200 Train loss 3.93 on epoch=599
06/21/2022 16:21:19 - INFO - __main__ - Global step 1200 Train loss 3.96 ACC 0.0 on epoch=599
06/21/2022 16:21:21 - INFO - __main__ - Step 1210 Global step 1210 Train loss 3.90 on epoch=604
06/21/2022 16:21:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 3.65 on epoch=609
06/21/2022 16:21:23 - INFO - __main__ - Step 1230 Global step 1230 Train loss 3.88 on epoch=614
06/21/2022 16:21:25 - INFO - __main__ - Step 1240 Global step 1240 Train loss 3.81 on epoch=619
06/21/2022 16:21:26 - INFO - __main__ - Step 1250 Global step 1250 Train loss 3.80 on epoch=624
06/21/2022 16:21:27 - INFO - __main__ - Global step 1250 Train loss 3.81 ACC 0.0 on epoch=624
06/21/2022 16:21:28 - INFO - __main__ - Step 1260 Global step 1260 Train loss 3.80 on epoch=629
06/21/2022 16:21:30 - INFO - __main__ - Step 1270 Global step 1270 Train loss 3.88 on epoch=634
06/21/2022 16:21:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 3.75 on epoch=639
06/21/2022 16:21:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 3.66 on epoch=644
06/21/2022 16:21:34 - INFO - __main__ - Step 1300 Global step 1300 Train loss 3.66 on epoch=649
06/21/2022 16:21:41 - INFO - __main__ - Global step 1300 Train loss 3.75 ACC 0.0 on epoch=649
06/21/2022 16:21:43 - INFO - __main__ - Step 1310 Global step 1310 Train loss 3.48 on epoch=654
06/21/2022 16:21:44 - INFO - __main__ - Step 1320 Global step 1320 Train loss 3.65 on epoch=659
06/21/2022 16:21:45 - INFO - __main__ - Step 1330 Global step 1330 Train loss 3.63 on epoch=664
06/21/2022 16:21:47 - INFO - __main__ - Step 1340 Global step 1340 Train loss 3.65 on epoch=669
06/21/2022 16:21:48 - INFO - __main__ - Step 1350 Global step 1350 Train loss 3.52 on epoch=674
06/21/2022 16:21:55 - INFO - __main__ - Global step 1350 Train loss 3.59 ACC 0.0 on epoch=674
06/21/2022 16:21:57 - INFO - __main__ - Step 1360 Global step 1360 Train loss 3.60 on epoch=679
06/21/2022 16:21:58 - INFO - __main__ - Step 1370 Global step 1370 Train loss 3.46 on epoch=684
06/21/2022 16:22:00 - INFO - __main__ - Step 1380 Global step 1380 Train loss 3.61 on epoch=689
06/21/2022 16:22:01 - INFO - __main__ - Step 1390 Global step 1390 Train loss 3.48 on epoch=694
06/21/2022 16:22:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 3.40 on epoch=699
06/21/2022 16:22:06 - INFO - __main__ - Global step 1400 Train loss 3.51 ACC 0.0 on epoch=699
06/21/2022 16:22:07 - INFO - __main__ - Step 1410 Global step 1410 Train loss 3.37 on epoch=704
06/21/2022 16:22:09 - INFO - __main__ - Step 1420 Global step 1420 Train loss 3.36 on epoch=709
06/21/2022 16:22:10 - INFO - __main__ - Step 1430 Global step 1430 Train loss 3.32 on epoch=714
06/21/2022 16:22:12 - INFO - __main__ - Step 1440 Global step 1440 Train loss 3.31 on epoch=719
06/21/2022 16:22:13 - INFO - __main__ - Step 1450 Global step 1450 Train loss 3.23 on epoch=724
06/21/2022 16:22:16 - INFO - __main__ - Global step 1450 Train loss 3.31 ACC 0.0 on epoch=724
06/21/2022 16:22:18 - INFO - __main__ - Step 1460 Global step 1460 Train loss 3.37 on epoch=729
06/21/2022 16:22:19 - INFO - __main__ - Step 1470 Global step 1470 Train loss 3.40 on epoch=734
06/21/2022 16:22:21 - INFO - __main__ - Step 1480 Global step 1480 Train loss 3.30 on epoch=739
06/21/2022 16:22:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 3.15 on epoch=744
06/21/2022 16:22:23 - INFO - __main__ - Step 1500 Global step 1500 Train loss 3.29 on epoch=749
06/21/2022 16:22:26 - INFO - __main__ - Global step 1500 Train loss 3.30 ACC 0.0 on epoch=749
06/21/2022 16:22:27 - INFO - __main__ - Step 1510 Global step 1510 Train loss 3.32 on epoch=754
06/21/2022 16:22:29 - INFO - __main__ - Step 1520 Global step 1520 Train loss 3.17 on epoch=759
06/21/2022 16:22:30 - INFO - __main__ - Step 1530 Global step 1530 Train loss 3.25 on epoch=764
06/21/2022 16:22:32 - INFO - __main__ - Step 1540 Global step 1540 Train loss 3.11 on epoch=769
06/21/2022 16:22:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 3.20 on epoch=774
06/21/2022 16:22:44 - INFO - __main__ - Global step 1550 Train loss 3.21 ACC 0.0 on epoch=774
06/21/2022 16:22:45 - INFO - __main__ - Step 1560 Global step 1560 Train loss 3.19 on epoch=779
06/21/2022 16:22:47 - INFO - __main__ - Step 1570 Global step 1570 Train loss 3.07 on epoch=784
06/21/2022 16:22:49 - INFO - __main__ - Step 1580 Global step 1580 Train loss 3.19 on epoch=789
06/21/2022 16:22:50 - INFO - __main__ - Step 1590 Global step 1590 Train loss 3.05 on epoch=794
06/21/2022 16:22:52 - INFO - __main__ - Step 1600 Global step 1600 Train loss 3.03 on epoch=799
06/21/2022 16:22:56 - INFO - __main__ - Global step 1600 Train loss 3.11 ACC 0.0 on epoch=799
06/21/2022 16:22:57 - INFO - __main__ - Step 1610 Global step 1610 Train loss 2.89 on epoch=804
06/21/2022 16:22:59 - INFO - __main__ - Step 1620 Global step 1620 Train loss 2.95 on epoch=809
06/21/2022 16:23:00 - INFO - __main__ - Step 1630 Global step 1630 Train loss 2.90 on epoch=814
06/21/2022 16:23:02 - INFO - __main__ - Step 1640 Global step 1640 Train loss 2.95 on epoch=819
06/21/2022 16:23:03 - INFO - __main__ - Step 1650 Global step 1650 Train loss 2.93 on epoch=824
06/21/2022 16:23:07 - INFO - __main__ - Global step 1650 Train loss 2.92 ACC 0.0 on epoch=824
06/21/2022 16:23:08 - INFO - __main__ - Step 1660 Global step 1660 Train loss 2.78 on epoch=829
06/21/2022 16:23:10 - INFO - __main__ - Step 1670 Global step 1670 Train loss 2.89 on epoch=834
06/21/2022 16:23:11 - INFO - __main__ - Step 1680 Global step 1680 Train loss 2.82 on epoch=839
06/21/2022 16:23:13 - INFO - __main__ - Step 1690 Global step 1690 Train loss 2.90 on epoch=844
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          06/21/2022 16:24:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 2.27 on epoch=949
06/21/2022 16:24:05 - INFO - __main__ - Global step 1900 Train loss 2.30 ACC 0.0 on epoch=949
06/21/2022 16:24:06 - INFO - __main__ - Step 1910 Global step 1910 Train loss 2.25 on epoch=954
