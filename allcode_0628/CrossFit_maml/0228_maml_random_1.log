nohup: ignoring input
Task: wiki_split, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29548
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_xpb7p_tn/none_36f3hzer
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_xpb7p_tn/none_36f3hzer/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_xpb7p_tn/none_36f3hzer/attempt_0/1/error.json
02/28/2022 16:09:01 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 16:09:01 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
Output directory () already exists and is not empty.
02/28/2022 16:09:01 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 16:09:01 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
02/28/2022 16:09:01 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
02/28/2022 16:09:01 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
02/28/2022 16:09:01 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
02/28/2022 16:09:01 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
02/28/2022 16:09:01 - INFO - __main__ - args.device: cuda:0
02/28/2022 16:09:01 - INFO - __main__ - Using 2 gpus
02/28/2022 16:09:01 - INFO - __main__ - args.device: cuda:1
02/28/2022 16:09:01 - INFO - __main__ - Using 2 gpus
02/28/2022 16:09:01 - INFO - __main__ - Fine-tuning the following samples: ['wiki_split_32_100', 'wiki_split_32_13', 'wiki_split_32_21', 'wiki_split_32_42', 'wiki_split_32_87']
02/28/2022 16:09:01 - INFO - __main__ - Fine-tuning the following samples: ['wiki_split_32_100', 'wiki_split_32_13', 'wiki_split_32_21', 'wiki_split_32_42', 'wiki_split_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
02/28/2022 16:09:06 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.5, bsz=8 ...
02/28/2022 16:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 16:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 16:09:07 - INFO - __main__ - Printing 3 examples
02/28/2022 16:09:07 - INFO - __main__ - Printing 3 examples
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/28/2022 16:09:07 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/28/2022 16:09:07 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/28/2022 16:09:07 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/28/2022 16:09:07 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/28/2022 16:09:07 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/28/2022 16:09:07 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/28/2022 16:09:07 - INFO - __main__ - Tokenizing Input ...
02/28/2022 16:09:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/28/2022 16:09:07 - INFO - __main__ - Tokenizing Output ...
02/28/2022 16:09:07 - INFO - __main__ - Tokenizing Output ...
02/28/2022 16:09:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 16:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 16:09:07 - INFO - __main__ - Printing 3 examples
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/28/2022 16:09:07 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/28/2022 16:09:07 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/28/2022 16:09:07 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/28/2022 16:09:07 - INFO - __main__ - Tokenizing Input ...
02/28/2022 16:09:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 16:09:07 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 16:09:07 - INFO - __main__ - Printing 3 examples
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/28/2022 16:09:07 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/28/2022 16:09:07 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/28/2022 16:09:07 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/28/2022 16:09:07 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/28/2022 16:09:07 - INFO - __main__ - Tokenizing Input ...
02/28/2022 16:09:07 - INFO - __main__ - Tokenizing Output ...
02/28/2022 16:09:07 - INFO - __main__ - Tokenizing Output ...
02/28/2022 16:09:07 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 16:09:07 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 16:10:34 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 16:10:34 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 16:10:35 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 16:10:35 - INFO - __main__ - Starting training!
02/28/2022 16:10:40 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 16:10:40 - INFO - __main__ - Starting training!
02/28/2022 16:10:43 - INFO - __main__ - Step 10 Global step 10 Train loss 0.81 on epoch=4
02/28/2022 16:10:45 - INFO - __main__ - Step 20 Global step 20 Train loss 0.71 on epoch=9
02/28/2022 16:10:48 - INFO - __main__ - Step 30 Global step 30 Train loss 0.67 on epoch=14
02/28/2022 16:10:50 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=19
02/28/2022 16:10:52 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
02/28/2022 16:11:04 - INFO - __main__ - Global step 50 Train loss 0.68 Rouge-L 0.7781286102544258 on epoch=24
02/28/2022 16:11:04 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.7781286102544258 on epoch=24, global_step=50
02/28/2022 16:11:06 - INFO - __main__ - Step 60 Global step 60 Train loss 0.57 on epoch=29
02/28/2022 16:11:09 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=34
02/28/2022 16:11:11 - INFO - __main__ - Step 80 Global step 80 Train loss 0.54 on epoch=39
02/28/2022 16:11:13 - INFO - __main__ - Step 90 Global step 90 Train loss 0.54 on epoch=44
02/28/2022 16:11:16 - INFO - __main__ - Step 100 Global step 100 Train loss 0.52 on epoch=49
02/28/2022 16:11:26 - INFO - __main__ - Global step 100 Train loss 0.54 Rouge-L 0.8538303344628733 on epoch=49
02/28/2022 16:11:26 - INFO - __main__ - Saving model with best Rouge-L: 0.7781286102544258 -> 0.8538303344628733 on epoch=49, global_step=100
02/28/2022 16:11:29 - INFO - __main__ - Step 110 Global step 110 Train loss 0.50 on epoch=54
02/28/2022 16:11:31 - INFO - __main__ - Step 120 Global step 120 Train loss 0.49 on epoch=59
02/28/2022 16:11:34 - INFO - __main__ - Step 130 Global step 130 Train loss 0.48 on epoch=64
02/28/2022 16:11:36 - INFO - __main__ - Step 140 Global step 140 Train loss 0.47 on epoch=69
02/28/2022 16:11:38 - INFO - __main__ - Step 150 Global step 150 Train loss 0.45 on epoch=74
02/28/2022 16:11:50 - INFO - __main__ - Global step 150 Train loss 0.48 Rouge-L 0.8642392035516904 on epoch=74
02/28/2022 16:11:50 - INFO - __main__ - Saving model with best Rouge-L: 0.8538303344628733 -> 0.8642392035516904 on epoch=74, global_step=150
02/28/2022 16:11:52 - INFO - __main__ - Step 160 Global step 160 Train loss 0.46 on epoch=79
02/28/2022 16:11:55 - INFO - __main__ - Step 170 Global step 170 Train loss 0.44 on epoch=84
02/28/2022 16:11:57 - INFO - __main__ - Step 180 Global step 180 Train loss 0.45 on epoch=89
02/28/2022 16:11:59 - INFO - __main__ - Step 190 Global step 190 Train loss 0.43 on epoch=94
02/28/2022 16:12:01 - INFO - __main__ - Step 200 Global step 200 Train loss 0.42 on epoch=99
02/28/2022 16:12:11 - INFO - __main__ - Global step 200 Train loss 0.44 Rouge-L 0.8741078704048544 on epoch=99
02/28/2022 16:12:11 - INFO - __main__ - Saving model with best Rouge-L: 0.8642392035516904 -> 0.8741078704048544 on epoch=99, global_step=200
02/28/2022 16:12:14 - INFO - __main__ - Step 210 Global step 210 Train loss 0.41 on epoch=104
02/28/2022 16:12:16 - INFO - __main__ - Step 220 Global step 220 Train loss 0.41 on epoch=109
02/28/2022 16:12:18 - INFO - __main__ - Step 230 Global step 230 Train loss 0.40 on epoch=114
02/28/2022 16:12:20 - INFO - __main__ - Step 240 Global step 240 Train loss 0.40 on epoch=119
02/28/2022 16:12:22 - INFO - __main__ - Step 250 Global step 250 Train loss 0.37 on epoch=124
02/28/2022 16:12:32 - INFO - __main__ - Global step 250 Train loss 0.40 Rouge-L 0.8657543208994521 on epoch=124
02/28/2022 16:12:35 - INFO - __main__ - Step 260 Global step 260 Train loss 0.37 on epoch=129
02/28/2022 16:12:37 - INFO - __main__ - Step 270 Global step 270 Train loss 0.37 on epoch=134
02/28/2022 16:12:39 - INFO - __main__ - Step 280 Global step 280 Train loss 0.38 on epoch=139
02/28/2022 16:12:41 - INFO - __main__ - Step 290 Global step 290 Train loss 0.36 on epoch=144
02/28/2022 16:12:43 - INFO - __main__ - Step 300 Global step 300 Train loss 0.36 on epoch=149
02/28/2022 16:12:54 - INFO - __main__ - Global step 300 Train loss 0.37 Rouge-L 0.8726547057813338 on epoch=149
02/28/2022 16:12:56 - INFO - __main__ - Step 310 Global step 310 Train loss 0.35 on epoch=154
02/28/2022 16:12:58 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=159
02/28/2022 16:13:00 - INFO - __main__ - Step 330 Global step 330 Train loss 0.34 on epoch=164
02/28/2022 16:13:02 - INFO - __main__ - Step 340 Global step 340 Train loss 0.33 on epoch=169
02/28/2022 16:13:05 - INFO - __main__ - Step 350 Global step 350 Train loss 0.32 on epoch=174
02/28/2022 16:13:16 - INFO - __main__ - Global step 350 Train loss 0.34 Rouge-L 0.8632374286056811 on epoch=174
02/28/2022 16:13:19 - INFO - __main__ - Step 360 Global step 360 Train loss 0.32 on epoch=179
02/28/2022 16:13:21 - INFO - __main__ - Step 370 Global step 370 Train loss 0.33 on epoch=184
02/28/2022 16:13:23 - INFO - __main__ - Step 380 Global step 380 Train loss 0.31 on epoch=189
02/28/2022 16:13:25 - INFO - __main__ - Step 390 Global step 390 Train loss 0.32 on epoch=194
02/28/2022 16:13:28 - INFO - __main__ - Step 400 Global step 400 Train loss 0.30 on epoch=199
02/28/2022 16:13:39 - INFO - __main__ - Global step 400 Train loss 0.32 Rouge-L 0.8777851518906259 on epoch=199
02/28/2022 16:13:39 - INFO - __main__ - Saving model with best Rouge-L: 0.8741078704048544 -> 0.8777851518906259 on epoch=199, global_step=400
02/28/2022 16:13:41 - INFO - __main__ - Step 410 Global step 410 Train loss 0.31 on epoch=204
02/28/2022 16:13:44 - INFO - __main__ - Step 420 Global step 420 Train loss 0.29 on epoch=209
02/28/2022 16:13:46 - INFO - __main__ - Step 430 Global step 430 Train loss 0.28 on epoch=214
02/28/2022 16:13:48 - INFO - __main__ - Step 440 Global step 440 Train loss 0.28 on epoch=219
02/28/2022 16:13:50 - INFO - __main__ - Step 450 Global step 450 Train loss 0.28 on epoch=224
02/28/2022 16:14:01 - INFO - __main__ - Global step 450 Train loss 0.29 Rouge-L 0.8662379627530266 on epoch=224
02/28/2022 16:14:03 - INFO - __main__ - Step 460 Global step 460 Train loss 0.28 on epoch=229
02/28/2022 16:14:06 - INFO - __main__ - Step 470 Global step 470 Train loss 0.28 on epoch=234
02/28/2022 16:14:08 - INFO - __main__ - Step 480 Global step 480 Train loss 0.26 on epoch=239
02/28/2022 16:14:10 - INFO - __main__ - Step 490 Global step 490 Train loss 0.27 on epoch=244
02/28/2022 16:14:13 - INFO - __main__ - Step 500 Global step 500 Train loss 0.26 on epoch=249
02/28/2022 16:14:25 - INFO - __main__ - Global step 500 Train loss 0.27 Rouge-L 0.8635126243339849 on epoch=249
02/28/2022 16:14:27 - INFO - __main__ - Step 510 Global step 510 Train loss 0.26 on epoch=254
02/28/2022 16:14:29 - INFO - __main__ - Step 520 Global step 520 Train loss 0.26 on epoch=259
02/28/2022 16:14:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.26 on epoch=264
02/28/2022 16:14:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.25 on epoch=269
02/28/2022 16:14:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.26 on epoch=274
02/28/2022 16:14:47 - INFO - __main__ - Global step 550 Train loss 0.26 Rouge-L 0.8720432620087791 on epoch=274
02/28/2022 16:14:49 - INFO - __main__ - Step 560 Global step 560 Train loss 0.26 on epoch=279
02/28/2022 16:14:51 - INFO - __main__ - Step 570 Global step 570 Train loss 0.24 on epoch=284
02/28/2022 16:14:54 - INFO - __main__ - Step 580 Global step 580 Train loss 0.23 on epoch=289
02/28/2022 16:14:56 - INFO - __main__ - Step 590 Global step 590 Train loss 0.23 on epoch=294
02/28/2022 16:14:58 - INFO - __main__ - Step 600 Global step 600 Train loss 0.23 on epoch=299
02/28/2022 16:15:09 - INFO - __main__ - Global step 600 Train loss 0.24 Rouge-L 0.8476942703705852 on epoch=299
02/28/2022 16:15:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.23 on epoch=304
02/28/2022 16:15:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.23 on epoch=309
02/28/2022 16:15:16 - INFO - __main__ - Step 630 Global step 630 Train loss 0.23 on epoch=314
02/28/2022 16:15:18 - INFO - __main__ - Step 640 Global step 640 Train loss 0.22 on epoch=319
02/28/2022 16:15:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.22 on epoch=324
02/28/2022 16:15:31 - INFO - __main__ - Global step 650 Train loss 0.23 Rouge-L 0.8680862915380182 on epoch=324
02/28/2022 16:15:33 - INFO - __main__ - Step 660 Global step 660 Train loss 0.21 on epoch=329
02/28/2022 16:15:36 - INFO - __main__ - Step 670 Global step 670 Train loss 0.21 on epoch=334
02/28/2022 16:15:38 - INFO - __main__ - Step 680 Global step 680 Train loss 0.21 on epoch=339
02/28/2022 16:15:40 - INFO - __main__ - Step 690 Global step 690 Train loss 0.20 on epoch=344
02/28/2022 16:15:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.21 on epoch=349
02/28/2022 16:15:53 - INFO - __main__ - Global step 700 Train loss 0.21 Rouge-L 0.8543279804409483 on epoch=349
02/28/2022 16:15:55 - INFO - __main__ - Step 710 Global step 710 Train loss 0.21 on epoch=354
02/28/2022 16:15:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.20 on epoch=359
02/28/2022 16:16:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.20 on epoch=364
02/28/2022 16:16:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.20 on epoch=369
02/28/2022 16:16:05 - INFO - __main__ - Step 750 Global step 750 Train loss 0.21 on epoch=374
02/28/2022 16:16:16 - INFO - __main__ - Global step 750 Train loss 0.21 Rouge-L 0.8706292499207532 on epoch=374
02/28/2022 16:16:19 - INFO - __main__ - Step 760 Global step 760 Train loss 0.21 on epoch=379
02/28/2022 16:16:21 - INFO - __main__ - Step 770 Global step 770 Train loss 0.19 on epoch=384
02/28/2022 16:16:23 - INFO - __main__ - Step 780 Global step 780 Train loss 0.20 on epoch=389
02/28/2022 16:16:25 - INFO - __main__ - Step 790 Global step 790 Train loss 0.19 on epoch=394
02/28/2022 16:16:28 - INFO - __main__ - Step 800 Global step 800 Train loss 0.20 on epoch=399
02/28/2022 16:16:39 - INFO - __main__ - Global step 800 Train loss 0.20 Rouge-L 0.8585620742591544 on epoch=399
02/28/2022 16:16:41 - INFO - __main__ - Step 810 Global step 810 Train loss 0.17 on epoch=404
02/28/2022 16:16:44 - INFO - __main__ - Step 820 Global step 820 Train loss 0.18 on epoch=409
02/28/2022 16:16:46 - INFO - __main__ - Step 830 Global step 830 Train loss 0.18 on epoch=414
02/28/2022 16:16:48 - INFO - __main__ - Step 840 Global step 840 Train loss 0.17 on epoch=419
02/28/2022 16:16:50 - INFO - __main__ - Step 850 Global step 850 Train loss 0.17 on epoch=424
02/28/2022 16:17:01 - INFO - __main__ - Global step 850 Train loss 0.18 Rouge-L 0.8702837935832088 on epoch=424
02/28/2022 16:17:04 - INFO - __main__ - Step 860 Global step 860 Train loss 0.16 on epoch=429
02/28/2022 16:17:06 - INFO - __main__ - Step 870 Global step 870 Train loss 0.18 on epoch=434
02/28/2022 16:17:08 - INFO - __main__ - Step 880 Global step 880 Train loss 0.18 on epoch=439
02/28/2022 16:17:10 - INFO - __main__ - Step 890 Global step 890 Train loss 0.18 on epoch=444
02/28/2022 16:17:13 - INFO - __main__ - Step 900 Global step 900 Train loss 0.16 on epoch=449
02/28/2022 16:17:24 - INFO - __main__ - Global step 900 Train loss 0.17 Rouge-L 0.8726947553576883 on epoch=449
02/28/2022 16:17:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.17 on epoch=454
02/28/2022 16:17:28 - INFO - __main__ - Step 920 Global step 920 Train loss 0.17 on epoch=459
02/28/2022 16:17:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.18 on epoch=464
02/28/2022 16:17:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.16 on epoch=469
02/28/2022 16:17:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.17 on epoch=474
02/28/2022 16:17:47 - INFO - __main__ - Global step 950 Train loss 0.17 Rouge-L 0.8793965007064838 on epoch=474
02/28/2022 16:17:47 - INFO - __main__ - Saving model with best Rouge-L: 0.8777851518906259 -> 0.8793965007064838 on epoch=474, global_step=950
02/28/2022 16:17:49 - INFO - __main__ - Step 960 Global step 960 Train loss 0.17 on epoch=479
02/28/2022 16:17:51 - INFO - __main__ - Step 970 Global step 970 Train loss 0.17 on epoch=484
02/28/2022 16:17:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.16 on epoch=489
02/28/2022 16:17:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.16 on epoch=494
02/28/2022 16:17:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.16 on epoch=499
02/28/2022 16:18:09 - INFO - __main__ - Global step 1000 Train loss 0.16 Rouge-L 0.8765402925804338 on epoch=499
02/28/2022 16:18:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.15 on epoch=504
02/28/2022 16:18:14 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.15 on epoch=509
02/28/2022 16:18:16 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.14 on epoch=514
02/28/2022 16:18:18 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.15 on epoch=519
02/28/2022 16:18:21 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.15 on epoch=524
02/28/2022 16:18:33 - INFO - __main__ - Global step 1050 Train loss 0.15 Rouge-L 0.8796073939007538 on epoch=524
02/28/2022 16:18:33 - INFO - __main__ - Saving model with best Rouge-L: 0.8793965007064838 -> 0.8796073939007538 on epoch=524, global_step=1050
02/28/2022 16:18:35 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.15 on epoch=529
02/28/2022 16:18:37 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.14 on epoch=534
02/28/2022 16:18:40 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.15 on epoch=539
02/28/2022 16:18:42 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.14 on epoch=544
02/28/2022 16:18:44 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=549
02/28/2022 16:18:56 - INFO - __main__ - Global step 1100 Train loss 0.15 Rouge-L 0.870766816533314 on epoch=549
02/28/2022 16:18:58 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.14 on epoch=554
02/28/2022 16:19:00 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.14 on epoch=559
02/28/2022 16:19:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.14 on epoch=564
02/28/2022 16:19:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.13 on epoch=569
02/28/2022 16:19:07 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.13 on epoch=574
02/28/2022 16:19:18 - INFO - __main__ - Global step 1150 Train loss 0.14 Rouge-L 0.8742813926478841 on epoch=574
02/28/2022 16:19:21 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.13 on epoch=579
02/28/2022 16:19:23 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.13 on epoch=584
02/28/2022 16:19:25 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.13 on epoch=589
02/28/2022 16:19:27 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.14 on epoch=594
02/28/2022 16:19:30 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.13 on epoch=599
02/28/2022 16:19:41 - INFO - __main__ - Global step 1200 Train loss 0.13 Rouge-L 0.8703243321315595 on epoch=599
02/28/2022 16:19:44 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.13 on epoch=604
02/28/2022 16:19:46 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.13 on epoch=609
02/28/2022 16:19:48 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.12 on epoch=614
02/28/2022 16:19:50 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.12 on epoch=619
02/28/2022 16:19:53 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.13 on epoch=624
02/28/2022 16:20:04 - INFO - __main__ - Global step 1250 Train loss 0.13 Rouge-L 0.8810300663949966 on epoch=624
02/28/2022 16:20:04 - INFO - __main__ - Saving model with best Rouge-L: 0.8796073939007538 -> 0.8810300663949966 on epoch=624, global_step=1250
02/28/2022 16:20:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.13 on epoch=629
02/28/2022 16:20:08 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.13 on epoch=634
02/28/2022 16:20:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.11 on epoch=639
02/28/2022 16:20:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.12 on epoch=644
02/28/2022 16:20:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.11 on epoch=649
02/28/2022 16:20:27 - INFO - __main__ - Global step 1300 Train loss 0.12 Rouge-L 0.8777935641404695 on epoch=649
02/28/2022 16:20:29 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.12 on epoch=654
02/28/2022 16:20:32 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.12 on epoch=659
02/28/2022 16:20:34 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.11 on epoch=664
02/28/2022 16:20:36 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.10 on epoch=669
02/28/2022 16:20:39 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.12 on epoch=674
02/28/2022 16:20:50 - INFO - __main__ - Global step 1350 Train loss 0.11 Rouge-L 0.8629073472006705 on epoch=674
02/28/2022 16:20:52 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.11 on epoch=679
02/28/2022 16:20:54 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.11 on epoch=684
02/28/2022 16:20:57 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.10 on epoch=689
02/28/2022 16:20:59 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.10 on epoch=694
02/28/2022 16:21:01 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.12 on epoch=699
02/28/2022 16:21:13 - INFO - __main__ - Global step 1400 Train loss 0.11 Rouge-L 0.8709948117432789 on epoch=699
02/28/2022 16:21:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.11 on epoch=704
02/28/2022 16:21:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.10 on epoch=709
02/28/2022 16:21:20 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=714
02/28/2022 16:21:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.10 on epoch=719
02/28/2022 16:21:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.10 on epoch=724
02/28/2022 16:21:35 - INFO - __main__ - Global step 1450 Train loss 0.10 Rouge-L 0.8779954322247983 on epoch=724
02/28/2022 16:21:38 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.10 on epoch=729
02/28/2022 16:21:40 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.10 on epoch=734
02/28/2022 16:21:42 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.09 on epoch=739
02/28/2022 16:21:44 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.09 on epoch=744
02/28/2022 16:21:47 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.10 on epoch=749
02/28/2022 16:21:58 - INFO - __main__ - Global step 1500 Train loss 0.09 Rouge-L 0.8554228422578504 on epoch=749
02/28/2022 16:22:00 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.09 on epoch=754
02/28/2022 16:22:02 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.09 on epoch=759
02/28/2022 16:22:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.09 on epoch=764
02/28/2022 16:22:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.08 on epoch=769
02/28/2022 16:22:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.08 on epoch=774
02/28/2022 16:22:21 - INFO - __main__ - Global step 1550 Train loss 0.09 Rouge-L 0.8747773271856951 on epoch=774
02/28/2022 16:22:23 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.10 on epoch=779
02/28/2022 16:22:26 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.10 on epoch=784
02/28/2022 16:22:28 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.09 on epoch=789
02/28/2022 16:22:30 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.09 on epoch=794
02/28/2022 16:22:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.09 on epoch=799
02/28/2022 16:22:43 - INFO - __main__ - Global step 1600 Train loss 0.09 Rouge-L 0.882081281674053 on epoch=799
02/28/2022 16:22:43 - INFO - __main__ - Saving model with best Rouge-L: 0.8810300663949966 -> 0.882081281674053 on epoch=799, global_step=1600
02/28/2022 16:22:46 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.10 on epoch=804
02/28/2022 16:22:48 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.08 on epoch=809
02/28/2022 16:22:50 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=814
02/28/2022 16:22:53 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.08 on epoch=819
02/28/2022 16:22:55 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.08 on epoch=824
02/28/2022 16:23:07 - INFO - __main__ - Global step 1650 Train loss 0.09 Rouge-L 0.8907172199243032 on epoch=824
02/28/2022 16:23:07 - INFO - __main__ - Saving model with best Rouge-L: 0.882081281674053 -> 0.8907172199243032 on epoch=824, global_step=1650
02/28/2022 16:23:09 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=829
02/28/2022 16:23:11 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.09 on epoch=834
02/28/2022 16:23:14 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=839
02/28/2022 16:23:16 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=844
02/28/2022 16:23:18 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.08 on epoch=849
02/28/2022 16:23:30 - INFO - __main__ - Global step 1700 Train loss 0.08 Rouge-L 0.890183197145211 on epoch=849
02/28/2022 16:23:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.08 on epoch=854
02/28/2022 16:23:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.08 on epoch=859
02/28/2022 16:23:37 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.07 on epoch=864
02/28/2022 16:23:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.07 on epoch=869
02/28/2022 16:23:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=874
02/28/2022 16:23:52 - INFO - __main__ - Global step 1750 Train loss 0.08 Rouge-L 0.8845109089968147 on epoch=874
02/28/2022 16:23:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=879
02/28/2022 16:23:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=884
02/28/2022 16:23:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=889
02/28/2022 16:24:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=894
02/28/2022 16:24:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=899
02/28/2022 16:24:15 - INFO - __main__ - Global step 1800 Train loss 0.07 Rouge-L 0.8797404508794329 on epoch=899
02/28/2022 16:24:18 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.07 on epoch=904
02/28/2022 16:24:20 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.07 on epoch=909
02/28/2022 16:24:22 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.08 on epoch=914
02/28/2022 16:24:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.07 on epoch=919
02/28/2022 16:24:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.07 on epoch=924
02/28/2022 16:24:38 - INFO - __main__ - Global step 1850 Train loss 0.07 Rouge-L 0.8678642422289469 on epoch=924
02/28/2022 16:24:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.08 on epoch=929
02/28/2022 16:24:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=934
02/28/2022 16:24:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.07 on epoch=939
02/28/2022 16:24:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
02/28/2022 16:24:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=949
02/28/2022 16:25:02 - INFO - __main__ - Global step 1900 Train loss 0.07 Rouge-L 0.8739520008317885 on epoch=949
02/28/2022 16:25:04 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
02/28/2022 16:25:06 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.07 on epoch=959
02/28/2022 16:25:08 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=964
02/28/2022 16:25:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.07 on epoch=969
02/28/2022 16:25:13 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.07 on epoch=974
02/28/2022 16:25:25 - INFO - __main__ - Global step 1950 Train loss 0.07 Rouge-L 0.8862140875257328 on epoch=974
02/28/2022 16:25:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.07 on epoch=979
02/28/2022 16:25:29 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.07 on epoch=984
02/28/2022 16:25:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=989
02/28/2022 16:25:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=994
02/28/2022 16:25:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=999
02/28/2022 16:25:38 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 16:25:38 - INFO - __main__ - Printing 3 examples
02/28/2022 16:25:38 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/28/2022 16:25:38 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/28/2022 16:25:38 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/28/2022 16:25:38 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/28/2022 16:25:38 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/28/2022 16:25:38 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/28/2022 16:25:38 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 16:25:38 - INFO - __main__ - Tokenizing Output ...
02/28/2022 16:25:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 16:25:38 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 16:25:38 - INFO - __main__ - Printing 3 examples
02/28/2022 16:25:38 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/28/2022 16:25:38 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/28/2022 16:25:38 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/28/2022 16:25:38 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/28/2022 16:25:38 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/28/2022 16:25:38 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/28/2022 16:25:38 - INFO - __main__ - Tokenizing Input ...
02/28/2022 16:25:38 - INFO - __main__ - Tokenizing Output ...
02/28/2022 16:25:38 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 16:25:48 - INFO - __main__ - Global step 2000 Train loss 0.07 Rouge-L 0.8806234568388156 on epoch=999
02/28/2022 16:25:48 - INFO - __main__ - save last model!
02/28/2022 16:25:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/28/2022 16:25:48 - INFO - __main__ - Start tokenizing ... 5000 instances
02/28/2022 16:25:48 - INFO - __main__ - Printing 3 examples
02/28/2022 16:25:48 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/28/2022 16:25:48 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/28/2022 16:25:48 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/28/2022 16:25:48 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/28/2022 16:25:48 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/28/2022 16:25:48 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/28/2022 16:25:48 - INFO - __main__ - Tokenizing Input ...
02/28/2022 16:25:50 - INFO - __main__ - Tokenizing Output ...
02/28/2022 16:25:51 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 16:25:51 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 16:25:51 - INFO - __main__ - Starting training!
02/28/2022 16:25:56 - INFO - __main__ - Loaded 5000 examples from test data
02/28/2022 16:54:12 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split/wiki_split_32_100_0.5_8_predictions.txt
02/28/2022 16:54:18 - INFO - __main__ - Rouge-L on test data: 0.8703
02/28/2022 16:54:18 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.5, bsz=8, dev_performance=0.8907172199243032, test_performance=0.8702633735700505
02/28/2022 16:54:18 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.4, bsz=8 ...
02/28/2022 16:54:19 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 16:54:19 - INFO - __main__ - Printing 3 examples
02/28/2022 16:54:19 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/28/2022 16:54:19 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/28/2022 16:54:19 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/28/2022 16:54:19 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/28/2022 16:54:19 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/28/2022 16:54:19 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/28/2022 16:54:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 16:54:19 - INFO - __main__ - Tokenizing Output ...
02/28/2022 16:54:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 16:54:19 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 16:54:19 - INFO - __main__ - Printing 3 examples
02/28/2022 16:54:19 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/28/2022 16:54:19 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/28/2022 16:54:19 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/28/2022 16:54:19 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/28/2022 16:54:19 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/28/2022 16:54:19 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/28/2022 16:54:19 - INFO - __main__ - Tokenizing Input ...
02/28/2022 16:54:19 - INFO - __main__ - Tokenizing Output ...
02/28/2022 16:54:19 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 16:54:33 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 16:54:34 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 16:54:34 - INFO - __main__ - Starting training!
02/28/2022 16:54:37 - INFO - __main__ - Step 10 Global step 10 Train loss 0.82 on epoch=4
02/28/2022 16:54:39 - INFO - __main__ - Step 20 Global step 20 Train loss 0.70 on epoch=9
02/28/2022 16:54:42 - INFO - __main__ - Step 30 Global step 30 Train loss 0.64 on epoch=14
02/28/2022 16:54:44 - INFO - __main__ - Step 40 Global step 40 Train loss 0.61 on epoch=19
02/28/2022 16:54:46 - INFO - __main__ - Step 50 Global step 50 Train loss 0.59 on epoch=24
02/28/2022 16:54:58 - INFO - __main__ - Global step 50 Train loss 0.67 Rouge-L 0.7875387293812833 on epoch=24
02/28/2022 16:54:58 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.7875387293812833 on epoch=24, global_step=50
02/28/2022 16:55:00 - INFO - __main__ - Step 60 Global step 60 Train loss 0.58 on epoch=29
02/28/2022 16:55:03 - INFO - __main__ - Step 70 Global step 70 Train loss 0.56 on epoch=34
02/28/2022 16:55:05 - INFO - __main__ - Step 80 Global step 80 Train loss 0.55 on epoch=39
02/28/2022 16:55:07 - INFO - __main__ - Step 90 Global step 90 Train loss 0.53 on epoch=44
02/28/2022 16:55:09 - INFO - __main__ - Step 100 Global step 100 Train loss 0.53 on epoch=49
02/28/2022 16:55:21 - INFO - __main__ - Global step 100 Train loss 0.55 Rouge-L 0.8586240157470528 on epoch=49
02/28/2022 16:55:21 - INFO - __main__ - Saving model with best Rouge-L: 0.7875387293812833 -> 0.8586240157470528 on epoch=49, global_step=100
02/28/2022 16:55:23 - INFO - __main__ - Step 110 Global step 110 Train loss 0.53 on epoch=54
02/28/2022 16:55:26 - INFO - __main__ - Step 120 Global step 120 Train loss 0.50 on epoch=59
02/28/2022 16:55:28 - INFO - __main__ - Step 130 Global step 130 Train loss 0.50 on epoch=64
02/28/2022 16:55:30 - INFO - __main__ - Step 140 Global step 140 Train loss 0.49 on epoch=69
02/28/2022 16:55:32 - INFO - __main__ - Step 150 Global step 150 Train loss 0.48 on epoch=74
02/28/2022 16:55:44 - INFO - __main__ - Global step 150 Train loss 0.50 Rouge-L 0.8559061549559362 on epoch=74
02/28/2022 16:55:46 - INFO - __main__ - Step 160 Global step 160 Train loss 0.47 on epoch=79
02/28/2022 16:55:48 - INFO - __main__ - Step 170 Global step 170 Train loss 0.47 on epoch=84
02/28/2022 16:55:51 - INFO - __main__ - Step 180 Global step 180 Train loss 0.46 on epoch=89
02/28/2022 16:55:53 - INFO - __main__ - Step 190 Global step 190 Train loss 0.44 on epoch=94
02/28/2022 16:55:55 - INFO - __main__ - Step 200 Global step 200 Train loss 0.44 on epoch=99
02/28/2022 16:56:05 - INFO - __main__ - Global step 200 Train loss 0.46 Rouge-L 0.8560781747128157 on epoch=99
02/28/2022 16:56:07 - INFO - __main__ - Step 210 Global step 210 Train loss 0.44 on epoch=104
02/28/2022 16:56:10 - INFO - __main__ - Step 220 Global step 220 Train loss 0.43 on epoch=109
02/28/2022 16:56:12 - INFO - __main__ - Step 230 Global step 230 Train loss 0.42 on epoch=114
02/28/2022 16:56:14 - INFO - __main__ - Step 240 Global step 240 Train loss 0.43 on epoch=119
02/28/2022 16:56:17 - INFO - __main__ - Step 250 Global step 250 Train loss 0.42 on epoch=124
02/28/2022 16:56:28 - INFO - __main__ - Global step 250 Train loss 0.43 Rouge-L 0.8729491387977737 on epoch=124
02/28/2022 16:56:28 - INFO - __main__ - Saving model with best Rouge-L: 0.8586240157470528 -> 0.8729491387977737 on epoch=124, global_step=250
02/28/2022 16:56:30 - INFO - __main__ - Step 260 Global step 260 Train loss 0.40 on epoch=129
02/28/2022 16:56:32 - INFO - __main__ - Step 270 Global step 270 Train loss 0.41 on epoch=134
02/28/2022 16:56:35 - INFO - __main__ - Step 280 Global step 280 Train loss 0.39 on epoch=139
02/28/2022 16:56:37 - INFO - __main__ - Step 290 Global step 290 Train loss 0.40 on epoch=144
02/28/2022 16:56:39 - INFO - __main__ - Step 300 Global step 300 Train loss 0.38 on epoch=149
02/28/2022 16:56:49 - INFO - __main__ - Global step 300 Train loss 0.39 Rouge-L 0.8714349175782756 on epoch=149
02/28/2022 16:56:51 - INFO - __main__ - Step 310 Global step 310 Train loss 0.38 on epoch=154
02/28/2022 16:56:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.38 on epoch=159
02/28/2022 16:56:56 - INFO - __main__ - Step 330 Global step 330 Train loss 0.36 on epoch=164
02/28/2022 16:56:58 - INFO - __main__ - Step 340 Global step 340 Train loss 0.35 on epoch=169
02/28/2022 16:57:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.36 on epoch=174
02/28/2022 16:57:10 - INFO - __main__ - Global step 350 Train loss 0.37 Rouge-L 0.8574464774223701 on epoch=174
02/28/2022 16:57:13 - INFO - __main__ - Step 360 Global step 360 Train loss 0.35 on epoch=179
02/28/2022 16:57:15 - INFO - __main__ - Step 370 Global step 370 Train loss 0.36 on epoch=184
02/28/2022 16:57:17 - INFO - __main__ - Step 380 Global step 380 Train loss 0.35 on epoch=189
02/28/2022 16:57:20 - INFO - __main__ - Step 390 Global step 390 Train loss 0.34 on epoch=194
02/28/2022 16:57:22 - INFO - __main__ - Step 400 Global step 400 Train loss 0.34 on epoch=199
02/28/2022 16:57:32 - INFO - __main__ - Global step 400 Train loss 0.35 Rouge-L 0.8689568306234412 on epoch=199
02/28/2022 16:57:34 - INFO - __main__ - Step 410 Global step 410 Train loss 0.33 on epoch=204
02/28/2022 16:57:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.34 on epoch=209
02/28/2022 16:57:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.32 on epoch=214
02/28/2022 16:57:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.33 on epoch=219
02/28/2022 16:57:43 - INFO - __main__ - Step 450 Global step 450 Train loss 0.33 on epoch=224
02/28/2022 16:57:54 - INFO - __main__ - Global step 450 Train loss 0.33 Rouge-L 0.8748200531835366 on epoch=224
02/28/2022 16:57:54 - INFO - __main__ - Saving model with best Rouge-L: 0.8729491387977737 -> 0.8748200531835366 on epoch=224, global_step=450
02/28/2022 16:57:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.31 on epoch=229
02/28/2022 16:57:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.31 on epoch=234
02/28/2022 16:58:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.31 on epoch=239
02/28/2022 16:58:03 - INFO - __main__ - Step 490 Global step 490 Train loss 0.30 on epoch=244
02/28/2022 16:58:05 - INFO - __main__ - Step 500 Global step 500 Train loss 0.31 on epoch=249
02/28/2022 16:58:16 - INFO - __main__ - Global step 500 Train loss 0.31 Rouge-L 0.8669077704130745 on epoch=249
02/28/2022 16:58:19 - INFO - __main__ - Step 510 Global step 510 Train loss 0.31 on epoch=254
02/28/2022 16:58:21 - INFO - __main__ - Step 520 Global step 520 Train loss 0.31 on epoch=259
02/28/2022 16:58:23 - INFO - __main__ - Step 530 Global step 530 Train loss 0.29 on epoch=264
02/28/2022 16:58:26 - INFO - __main__ - Step 540 Global step 540 Train loss 0.30 on epoch=269
02/28/2022 16:58:28 - INFO - __main__ - Step 550 Global step 550 Train loss 0.29 on epoch=274
02/28/2022 16:58:39 - INFO - __main__ - Global step 550 Train loss 0.30 Rouge-L 0.8778378009294665 on epoch=274
02/28/2022 16:58:39 - INFO - __main__ - Saving model with best Rouge-L: 0.8748200531835366 -> 0.8778378009294665 on epoch=274, global_step=550
02/28/2022 16:58:42 - INFO - __main__ - Step 560 Global step 560 Train loss 0.30 on epoch=279
02/28/2022 16:58:44 - INFO - __main__ - Step 570 Global step 570 Train loss 0.27 on epoch=284
02/28/2022 16:58:46 - INFO - __main__ - Step 580 Global step 580 Train loss 0.28 on epoch=289
02/28/2022 16:58:49 - INFO - __main__ - Step 590 Global step 590 Train loss 0.28 on epoch=294
02/28/2022 16:58:51 - INFO - __main__ - Step 600 Global step 600 Train loss 0.27 on epoch=299
02/28/2022 16:59:02 - INFO - __main__ - Global step 600 Train loss 0.28 Rouge-L 0.8640347506038931 on epoch=299
02/28/2022 16:59:05 - INFO - __main__ - Step 610 Global step 610 Train loss 0.28 on epoch=304
02/28/2022 16:59:07 - INFO - __main__ - Step 620 Global step 620 Train loss 0.27 on epoch=309
02/28/2022 16:59:09 - INFO - __main__ - Step 630 Global step 630 Train loss 0.28 on epoch=314
02/28/2022 16:59:12 - INFO - __main__ - Step 640 Global step 640 Train loss 0.26 on epoch=319
02/28/2022 16:59:14 - INFO - __main__ - Step 650 Global step 650 Train loss 0.26 on epoch=324
02/28/2022 16:59:25 - INFO - __main__ - Global step 650 Train loss 0.27 Rouge-L 0.8717537916268548 on epoch=324
02/28/2022 16:59:28 - INFO - __main__ - Step 660 Global step 660 Train loss 0.27 on epoch=329
02/28/2022 16:59:30 - INFO - __main__ - Step 670 Global step 670 Train loss 0.26 on epoch=334
02/28/2022 16:59:32 - INFO - __main__ - Step 680 Global step 680 Train loss 0.27 on epoch=339
02/28/2022 16:59:35 - INFO - __main__ - Step 690 Global step 690 Train loss 0.25 on epoch=344
02/28/2022 16:59:37 - INFO - __main__ - Step 700 Global step 700 Train loss 0.25 on epoch=349
02/28/2022 16:59:47 - INFO - __main__ - Global step 700 Train loss 0.26 Rouge-L 0.8610735579430746 on epoch=349
02/28/2022 16:59:50 - INFO - __main__ - Step 710 Global step 710 Train loss 0.25 on epoch=354
02/28/2022 16:59:52 - INFO - __main__ - Step 720 Global step 720 Train loss 0.24 on epoch=359
02/28/2022 16:59:54 - INFO - __main__ - Step 730 Global step 730 Train loss 0.25 on epoch=364
02/28/2022 16:59:57 - INFO - __main__ - Step 740 Global step 740 Train loss 0.25 on epoch=369
02/28/2022 16:59:59 - INFO - __main__ - Step 750 Global step 750 Train loss 0.24 on epoch=374
02/28/2022 17:00:10 - INFO - __main__ - Global step 750 Train loss 0.24 Rouge-L 0.8607345119211891 on epoch=374
02/28/2022 17:00:12 - INFO - __main__ - Step 760 Global step 760 Train loss 0.22 on epoch=379
02/28/2022 17:00:15 - INFO - __main__ - Step 770 Global step 770 Train loss 0.25 on epoch=384
02/28/2022 17:00:17 - INFO - __main__ - Step 780 Global step 780 Train loss 0.23 on epoch=389
02/28/2022 17:00:19 - INFO - __main__ - Step 790 Global step 790 Train loss 0.23 on epoch=394
02/28/2022 17:00:22 - INFO - __main__ - Step 800 Global step 800 Train loss 0.23 on epoch=399
02/28/2022 17:00:33 - INFO - __main__ - Global step 800 Train loss 0.23 Rouge-L 0.865809674147316 on epoch=399
02/28/2022 17:00:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.24 on epoch=404
02/28/2022 17:00:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.24 on epoch=409
02/28/2022 17:00:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.23 on epoch=414
02/28/2022 17:00:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.21 on epoch=419
02/28/2022 17:00:45 - INFO - __main__ - Step 850 Global step 850 Train loss 0.22 on epoch=424
02/28/2022 17:00:56 - INFO - __main__ - Global step 850 Train loss 0.22 Rouge-L 0.8786743388961471 on epoch=424
02/28/2022 17:00:56 - INFO - __main__ - Saving model with best Rouge-L: 0.8778378009294665 -> 0.8786743388961471 on epoch=424, global_step=850
02/28/2022 17:00:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.22 on epoch=429
02/28/2022 17:01:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.21 on epoch=434
02/28/2022 17:01:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.22 on epoch=439
02/28/2022 17:01:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.21 on epoch=444
02/28/2022 17:01:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.21 on epoch=449
02/28/2022 17:01:19 - INFO - __main__ - Global step 900 Train loss 0.22 Rouge-L 0.882064520344348 on epoch=449
02/28/2022 17:01:19 - INFO - __main__ - Saving model with best Rouge-L: 0.8786743388961471 -> 0.882064520344348 on epoch=449, global_step=900
02/28/2022 17:01:21 - INFO - __main__ - Step 910 Global step 910 Train loss 0.22 on epoch=454
02/28/2022 17:01:23 - INFO - __main__ - Step 920 Global step 920 Train loss 0.21 on epoch=459
02/28/2022 17:01:26 - INFO - __main__ - Step 930 Global step 930 Train loss 0.22 on epoch=464
02/28/2022 17:01:28 - INFO - __main__ - Step 940 Global step 940 Train loss 0.20 on epoch=469
02/28/2022 17:01:30 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
02/28/2022 17:01:42 - INFO - __main__ - Global step 950 Train loss 0.21 Rouge-L 0.8774693342228399 on epoch=474
02/28/2022 17:01:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.21 on epoch=479
02/28/2022 17:01:46 - INFO - __main__ - Step 970 Global step 970 Train loss 0.21 on epoch=484
02/28/2022 17:01:49 - INFO - __main__ - Step 980 Global step 980 Train loss 0.20 on epoch=489
02/28/2022 17:01:51 - INFO - __main__ - Step 990 Global step 990 Train loss 0.20 on epoch=494
02/28/2022 17:01:53 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.18 on epoch=499
02/28/2022 17:02:04 - INFO - __main__ - Global step 1000 Train loss 0.20 Rouge-L 0.8649560045995071 on epoch=499
02/28/2022 17:02:07 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.19 on epoch=504
02/28/2022 17:02:09 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.19 on epoch=509
02/28/2022 17:02:11 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.20 on epoch=514
02/28/2022 17:02:14 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.19 on epoch=519
02/28/2022 17:02:16 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.18 on epoch=524
02/28/2022 17:02:27 - INFO - __main__ - Global step 1050 Train loss 0.19 Rouge-L 0.8581223010979814 on epoch=524
02/28/2022 17:02:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.20 on epoch=529
02/28/2022 17:02:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.18 on epoch=534
02/28/2022 17:02:34 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.18 on epoch=539
02/28/2022 17:02:36 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.19 on epoch=544
02/28/2022 17:02:38 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=549
02/28/2022 17:02:49 - INFO - __main__ - Global step 1100 Train loss 0.19 Rouge-L 0.8711108462961412 on epoch=549
02/28/2022 17:02:52 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.19 on epoch=554
02/28/2022 17:02:54 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.17 on epoch=559
02/28/2022 17:02:56 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.18 on epoch=564
02/28/2022 17:02:59 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.17 on epoch=569
02/28/2022 17:03:01 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.16 on epoch=574
02/28/2022 17:03:12 - INFO - __main__ - Global step 1150 Train loss 0.17 Rouge-L 0.8671876990094731 on epoch=574
02/28/2022 17:03:15 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.17 on epoch=579
02/28/2022 17:03:17 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=584
02/28/2022 17:03:19 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.18 on epoch=589
02/28/2022 17:03:22 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.17 on epoch=594
02/28/2022 17:03:24 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.17 on epoch=599
02/28/2022 17:03:34 - INFO - __main__ - Global step 1200 Train loss 0.17 Rouge-L 0.846029000625618 on epoch=599
02/28/2022 17:03:37 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.17 on epoch=604
02/28/2022 17:03:39 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.17 on epoch=609
02/28/2022 17:03:41 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=614
02/28/2022 17:03:44 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.14 on epoch=619
02/28/2022 17:03:46 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.17 on epoch=624
02/28/2022 17:03:57 - INFO - __main__ - Global step 1250 Train loss 0.17 Rouge-L 0.87615258133478 on epoch=624
02/28/2022 17:03:59 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.15 on epoch=629
02/28/2022 17:04:02 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.16 on epoch=634
02/28/2022 17:04:04 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.15 on epoch=639
02/28/2022 17:04:06 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=644
02/28/2022 17:04:09 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.15 on epoch=649
02/28/2022 17:04:22 - INFO - __main__ - Global step 1300 Train loss 0.15 Rouge-L 0.8703298089753528 on epoch=649
02/28/2022 17:04:24 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.16 on epoch=654
02/28/2022 17:04:26 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=659
02/28/2022 17:04:28 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.16 on epoch=664
02/28/2022 17:04:31 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.15 on epoch=669
02/28/2022 17:04:33 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=674
02/28/2022 17:04:47 - INFO - __main__ - Global step 1350 Train loss 0.15 Rouge-L 0.8667129024117586 on epoch=674
02/28/2022 17:04:49 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.16 on epoch=679
02/28/2022 17:04:51 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=684
02/28/2022 17:04:54 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=689
02/28/2022 17:04:56 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.15 on epoch=694
02/28/2022 17:04:58 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.14 on epoch=699
02/28/2022 17:05:10 - INFO - __main__ - Global step 1400 Train loss 0.15 Rouge-L 0.8650120761800646 on epoch=699
02/28/2022 17:05:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.15 on epoch=704
02/28/2022 17:05:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
02/28/2022 17:05:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=714
02/28/2022 17:05:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.14 on epoch=719
02/28/2022 17:05:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=724
02/28/2022 17:05:33 - INFO - __main__ - Global step 1450 Train loss 0.15 Rouge-L 0.8656909145062417 on epoch=724
02/28/2022 17:05:35 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=729
02/28/2022 17:05:38 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.13 on epoch=734
02/28/2022 17:05:40 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=739
02/28/2022 17:05:42 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.15 on epoch=744
02/28/2022 17:05:45 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=749
02/28/2022 17:05:55 - INFO - __main__ - Global step 1500 Train loss 0.14 Rouge-L 0.8546507647358703 on epoch=749
02/28/2022 17:05:58 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.14 on epoch=754
02/28/2022 17:06:00 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=759
02/28/2022 17:06:02 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.13 on epoch=764
02/28/2022 17:06:05 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=769
02/28/2022 17:06:07 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=774
02/28/2022 17:06:20 - INFO - __main__ - Global step 1550 Train loss 0.13 Rouge-L 0.8541546330261862 on epoch=774
02/28/2022 17:06:22 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
02/28/2022 17:06:25 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.13 on epoch=784
02/28/2022 17:06:27 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=789
02/28/2022 17:06:29 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=794
02/28/2022 17:06:32 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.13 on epoch=799
02/28/2022 17:06:46 - INFO - __main__ - Global step 1600 Train loss 0.13 Rouge-L 0.8538531921165384 on epoch=799
02/28/2022 17:06:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.13 on epoch=804
02/28/2022 17:06:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=809
02/28/2022 17:06:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=814
02/28/2022 17:06:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.12 on epoch=819
02/28/2022 17:06:58 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=824
02/28/2022 17:07:09 - INFO - __main__ - Global step 1650 Train loss 0.13 Rouge-L 0.8686114427580847 on epoch=824
02/28/2022 17:07:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.12 on epoch=829
02/28/2022 17:07:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=834
02/28/2022 17:07:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.12 on epoch=839
02/28/2022 17:07:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.12 on epoch=844
02/28/2022 17:07:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.11 on epoch=849
02/28/2022 17:07:31 - INFO - __main__ - Global step 1700 Train loss 0.12 Rouge-L 0.8603269657974021 on epoch=849
02/28/2022 17:07:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.11 on epoch=854
02/28/2022 17:07:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=859
02/28/2022 17:07:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
02/28/2022 17:07:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=869
02/28/2022 17:07:43 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.11 on epoch=874
02/28/2022 17:07:55 - INFO - __main__ - Global step 1750 Train loss 0.11 Rouge-L 0.8666548312063961 on epoch=874
02/28/2022 17:07:57 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
02/28/2022 17:08:00 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.12 on epoch=884
02/28/2022 17:08:02 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.10 on epoch=889
02/28/2022 17:08:04 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=894
02/28/2022 17:08:07 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.11 on epoch=899
02/28/2022 17:08:18 - INFO - __main__ - Global step 1800 Train loss 0.11 Rouge-L 0.8770753620538285 on epoch=899
02/28/2022 17:08:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.09 on epoch=904
02/28/2022 17:08:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=909
02/28/2022 17:08:25 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.11 on epoch=914
02/28/2022 17:08:27 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.10 on epoch=919
02/28/2022 17:08:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
02/28/2022 17:08:44 - INFO - __main__ - Global step 1850 Train loss 0.10 Rouge-L 0.8662054743528288 on epoch=924
02/28/2022 17:08:46 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=929
02/28/2022 17:08:48 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=934
02/28/2022 17:08:51 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.09 on epoch=939
02/28/2022 17:08:53 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=944
02/28/2022 17:08:55 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.10 on epoch=949
02/28/2022 17:09:06 - INFO - __main__ - Global step 1900 Train loss 0.10 Rouge-L 0.8639375268877503 on epoch=949
02/28/2022 17:09:08 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=954
02/28/2022 17:09:11 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=959
02/28/2022 17:09:13 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=964
02/28/2022 17:09:15 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=969
02/28/2022 17:09:18 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.08 on epoch=974
02/28/2022 17:09:29 - INFO - __main__ - Global step 1950 Train loss 0.09 Rouge-L 0.8695753637474208 on epoch=974
02/28/2022 17:09:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.10 on epoch=979
02/28/2022 17:09:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.09 on epoch=984
02/28/2022 17:09:36 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=989
02/28/2022 17:09:38 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
02/28/2022 17:09:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=999
02/28/2022 17:09:42 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 17:09:42 - INFO - __main__ - Printing 3 examples
02/28/2022 17:09:42 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/28/2022 17:09:42 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/28/2022 17:09:42 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/28/2022 17:09:42 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/28/2022 17:09:42 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/28/2022 17:09:42 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/28/2022 17:09:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 17:09:42 - INFO - __main__ - Tokenizing Output ...
02/28/2022 17:09:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 17:09:42 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 17:09:42 - INFO - __main__ - Printing 3 examples
02/28/2022 17:09:42 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/28/2022 17:09:42 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/28/2022 17:09:42 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/28/2022 17:09:42 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/28/2022 17:09:42 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/28/2022 17:09:42 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/28/2022 17:09:42 - INFO - __main__ - Tokenizing Input ...
02/28/2022 17:09:42 - INFO - __main__ - Tokenizing Output ...
02/28/2022 17:09:42 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 17:09:52 - INFO - __main__ - Global step 2000 Train loss 0.09 Rouge-L 0.8693945158742733 on epoch=999
02/28/2022 17:09:52 - INFO - __main__ - save last model!
02/28/2022 17:09:52 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/28/2022 17:09:52 - INFO - __main__ - Start tokenizing ... 5000 instances
02/28/2022 17:09:52 - INFO - __main__ - Printing 3 examples
02/28/2022 17:09:52 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/28/2022 17:09:52 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/28/2022 17:09:52 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/28/2022 17:09:52 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/28/2022 17:09:52 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/28/2022 17:09:52 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/28/2022 17:09:52 - INFO - __main__ - Tokenizing Input ...
02/28/2022 17:09:54 - INFO - __main__ - Tokenizing Output ...
02/28/2022 17:09:56 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 17:09:57 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 17:09:57 - INFO - __main__ - Starting training!
02/28/2022 17:10:00 - INFO - __main__ - Loaded 5000 examples from test data
02/28/2022 17:38:24 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split/wiki_split_32_100_0.4_8_predictions.txt
02/28/2022 17:38:29 - INFO - __main__ - Rouge-L on test data: 0.8707
02/28/2022 17:38:30 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.4, bsz=8, dev_performance=0.882064520344348, test_performance=0.8706791290964444
02/28/2022 17:38:30 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.3, bsz=8 ...
02/28/2022 17:38:31 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 17:38:31 - INFO - __main__ - Printing 3 examples
02/28/2022 17:38:31 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/28/2022 17:38:31 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/28/2022 17:38:31 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/28/2022 17:38:31 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/28/2022 17:38:31 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/28/2022 17:38:31 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/28/2022 17:38:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/28/2022 17:38:31 - INFO - __main__ - Tokenizing Output ...
02/28/2022 17:38:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 17:38:31 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 17:38:31 - INFO - __main__ - Printing 3 examples
02/28/2022 17:38:31 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/28/2022 17:38:31 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/28/2022 17:38:31 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/28/2022 17:38:31 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/28/2022 17:38:31 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/28/2022 17:38:31 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/28/2022 17:38:31 - INFO - __main__ - Tokenizing Input ...
02/28/2022 17:38:31 - INFO - __main__ - Tokenizing Output ...
02/28/2022 17:38:31 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 17:38:44 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 17:38:45 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 17:38:45 - INFO - __main__ - Starting training!
02/28/2022 17:38:48 - INFO - __main__ - Step 10 Global step 10 Train loss 0.85 on epoch=4
02/28/2022 17:38:51 - INFO - __main__ - Step 20 Global step 20 Train loss 0.74 on epoch=9
02/28/2022 17:38:53 - INFO - __main__ - Step 30 Global step 30 Train loss 0.69 on epoch=14
02/28/2022 17:38:55 - INFO - __main__ - Step 40 Global step 40 Train loss 0.65 on epoch=19
02/28/2022 17:38:57 - INFO - __main__ - Step 50 Global step 50 Train loss 0.62 on epoch=24
02/28/2022 17:39:08 - INFO - __main__ - Global step 50 Train loss 0.71 Rouge-L 0.6703325722708713 on epoch=24
02/28/2022 17:39:08 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.6703325722708713 on epoch=24, global_step=50
02/28/2022 17:39:10 - INFO - __main__ - Step 60 Global step 60 Train loss 0.60 on epoch=29
02/28/2022 17:39:13 - INFO - __main__ - Step 70 Global step 70 Train loss 0.58 on epoch=34
02/28/2022 17:39:15 - INFO - __main__ - Step 80 Global step 80 Train loss 0.57 on epoch=39
02/28/2022 17:39:17 - INFO - __main__ - Step 90 Global step 90 Train loss 0.56 on epoch=44
02/28/2022 17:39:19 - INFO - __main__ - Step 100 Global step 100 Train loss 0.56 on epoch=49
02/28/2022 17:39:31 - INFO - __main__ - Global step 100 Train loss 0.57 Rouge-L 0.8379783155956653 on epoch=49
02/28/2022 17:39:31 - INFO - __main__ - Saving model with best Rouge-L: 0.6703325722708713 -> 0.8379783155956653 on epoch=49, global_step=100
02/28/2022 17:39:33 - INFO - __main__ - Step 110 Global step 110 Train loss 0.55 on epoch=54
02/28/2022 17:39:35 - INFO - __main__ - Step 120 Global step 120 Train loss 0.54 on epoch=59
02/28/2022 17:39:37 - INFO - __main__ - Step 130 Global step 130 Train loss 0.52 on epoch=64
02/28/2022 17:39:40 - INFO - __main__ - Step 140 Global step 140 Train loss 0.53 on epoch=69
02/28/2022 17:39:42 - INFO - __main__ - Step 150 Global step 150 Train loss 0.52 on epoch=74
02/28/2022 17:39:53 - INFO - __main__ - Global step 150 Train loss 0.53 Rouge-L 0.8575625991161984 on epoch=74
02/28/2022 17:39:53 - INFO - __main__ - Saving model with best Rouge-L: 0.8379783155956653 -> 0.8575625991161984 on epoch=74, global_step=150
02/28/2022 17:39:56 - INFO - __main__ - Step 160 Global step 160 Train loss 0.51 on epoch=79
02/28/2022 17:39:58 - INFO - __main__ - Step 170 Global step 170 Train loss 0.50 on epoch=84
02/28/2022 17:40:00 - INFO - __main__ - Step 180 Global step 180 Train loss 0.50 on epoch=89
02/28/2022 17:40:02 - INFO - __main__ - Step 190 Global step 190 Train loss 0.49 on epoch=94
02/28/2022 17:40:05 - INFO - __main__ - Step 200 Global step 200 Train loss 0.49 on epoch=99
02/28/2022 17:40:17 - INFO - __main__ - Global step 200 Train loss 0.50 Rouge-L 0.8577389373418454 on epoch=99
02/28/2022 17:40:17 - INFO - __main__ - Saving model with best Rouge-L: 0.8575625991161984 -> 0.8577389373418454 on epoch=99, global_step=200
02/28/2022 17:40:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.48 on epoch=104
02/28/2022 17:40:22 - INFO - __main__ - Step 220 Global step 220 Train loss 0.48 on epoch=109
02/28/2022 17:40:24 - INFO - __main__ - Step 230 Global step 230 Train loss 0.46 on epoch=114
02/28/2022 17:40:26 - INFO - __main__ - Step 240 Global step 240 Train loss 0.46 on epoch=119
02/28/2022 17:40:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.46 on epoch=124
02/28/2022 17:40:40 - INFO - __main__ - Global step 250 Train loss 0.47 Rouge-L 0.8562694228958173 on epoch=124
02/28/2022 17:40:42 - INFO - __main__ - Step 260 Global step 260 Train loss 0.46 on epoch=129
02/28/2022 17:40:44 - INFO - __main__ - Step 270 Global step 270 Train loss 0.46 on epoch=134
02/28/2022 17:40:46 - INFO - __main__ - Step 280 Global step 280 Train loss 0.45 on epoch=139
02/28/2022 17:40:49 - INFO - __main__ - Step 290 Global step 290 Train loss 0.44 on epoch=144
02/28/2022 17:40:51 - INFO - __main__ - Step 300 Global step 300 Train loss 0.43 on epoch=149
02/28/2022 17:41:04 - INFO - __main__ - Global step 300 Train loss 0.45 Rouge-L 0.8706854936599691 on epoch=149
02/28/2022 17:41:04 - INFO - __main__ - Saving model with best Rouge-L: 0.8577389373418454 -> 0.8706854936599691 on epoch=149, global_step=300
02/28/2022 17:41:06 - INFO - __main__ - Step 310 Global step 310 Train loss 0.42 on epoch=154
02/28/2022 17:41:08 - INFO - __main__ - Step 320 Global step 320 Train loss 0.41 on epoch=159
02/28/2022 17:41:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.41 on epoch=164
02/28/2022 17:41:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.41 on epoch=169
02/28/2022 17:41:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.41 on epoch=174
02/28/2022 17:41:26 - INFO - __main__ - Global step 350 Train loss 0.42 Rouge-L 0.8736456346265205 on epoch=174
02/28/2022 17:41:26 - INFO - __main__ - Saving model with best Rouge-L: 0.8706854936599691 -> 0.8736456346265205 on epoch=174, global_step=350
02/28/2022 17:41:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.42 on epoch=179
02/28/2022 17:41:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.40 on epoch=184
02/28/2022 17:41:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.40 on epoch=189
02/28/2022 17:41:36 - INFO - __main__ - Step 390 Global step 390 Train loss 0.40 on epoch=194
02/28/2022 17:41:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.39 on epoch=199
02/28/2022 17:41:50 - INFO - __main__ - Global step 400 Train loss 0.40 Rouge-L 0.8608427202567708 on epoch=199
02/28/2022 17:41:52 - INFO - __main__ - Step 410 Global step 410 Train loss 0.39 on epoch=204
02/28/2022 17:41:54 - INFO - __main__ - Step 420 Global step 420 Train loss 0.39 on epoch=209
02/28/2022 17:41:57 - INFO - __main__ - Step 430 Global step 430 Train loss 0.38 on epoch=214
02/28/2022 17:41:59 - INFO - __main__ - Step 440 Global step 440 Train loss 0.37 on epoch=219
02/28/2022 17:42:01 - INFO - __main__ - Step 450 Global step 450 Train loss 0.36 on epoch=224
02/28/2022 17:42:14 - INFO - __main__ - Global step 450 Train loss 0.38 Rouge-L 0.8602764349476679 on epoch=224
02/28/2022 17:42:16 - INFO - __main__ - Step 460 Global step 460 Train loss 0.38 on epoch=229
02/28/2022 17:42:18 - INFO - __main__ - Step 470 Global step 470 Train loss 0.36 on epoch=234
02/28/2022 17:42:20 - INFO - __main__ - Step 480 Global step 480 Train loss 0.38 on epoch=239
02/28/2022 17:42:23 - INFO - __main__ - Step 490 Global step 490 Train loss 0.36 on epoch=244
02/28/2022 17:42:25 - INFO - __main__ - Step 500 Global step 500 Train loss 0.35 on epoch=249
02/28/2022 17:42:36 - INFO - __main__ - Global step 500 Train loss 0.37 Rouge-L 0.8609840802008331 on epoch=249
02/28/2022 17:42:38 - INFO - __main__ - Step 510 Global step 510 Train loss 0.36 on epoch=254
02/28/2022 17:42:41 - INFO - __main__ - Step 520 Global step 520 Train loss 0.36 on epoch=259
02/28/2022 17:42:43 - INFO - __main__ - Step 530 Global step 530 Train loss 0.33 on epoch=264
02/28/2022 17:42:45 - INFO - __main__ - Step 540 Global step 540 Train loss 0.35 on epoch=269
02/28/2022 17:42:47 - INFO - __main__ - Step 550 Global step 550 Train loss 0.34 on epoch=274
02/28/2022 17:42:58 - INFO - __main__ - Global step 550 Train loss 0.35 Rouge-L 0.8592322201298236 on epoch=274
02/28/2022 17:43:01 - INFO - __main__ - Step 560 Global step 560 Train loss 0.33 on epoch=279
02/28/2022 17:43:03 - INFO - __main__ - Step 570 Global step 570 Train loss 0.32 on epoch=284
02/28/2022 17:43:05 - INFO - __main__ - Step 580 Global step 580 Train loss 0.34 on epoch=289
02/28/2022 17:43:08 - INFO - __main__ - Step 590 Global step 590 Train loss 0.33 on epoch=294
02/28/2022 17:43:10 - INFO - __main__ - Step 600 Global step 600 Train loss 0.33 on epoch=299
02/28/2022 17:43:21 - INFO - __main__ - Global step 600 Train loss 0.33 Rouge-L 0.8564343710891305 on epoch=299
02/28/2022 17:43:23 - INFO - __main__ - Step 610 Global step 610 Train loss 0.32 on epoch=304
02/28/2022 17:43:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.32 on epoch=309
02/28/2022 17:43:28 - INFO - __main__ - Step 630 Global step 630 Train loss 0.30 on epoch=314
02/28/2022 17:43:30 - INFO - __main__ - Step 640 Global step 640 Train loss 0.32 on epoch=319
02/28/2022 17:43:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.31 on epoch=324
02/28/2022 17:43:44 - INFO - __main__ - Global step 650 Train loss 0.31 Rouge-L 0.8615583861579461 on epoch=324
02/28/2022 17:43:47 - INFO - __main__ - Step 660 Global step 660 Train loss 0.30 on epoch=329
02/28/2022 17:43:49 - INFO - __main__ - Step 670 Global step 670 Train loss 0.31 on epoch=334
02/28/2022 17:43:51 - INFO - __main__ - Step 680 Global step 680 Train loss 0.31 on epoch=339
02/28/2022 17:43:54 - INFO - __main__ - Step 690 Global step 690 Train loss 0.29 on epoch=344
02/28/2022 17:43:56 - INFO - __main__ - Step 700 Global step 700 Train loss 0.30 on epoch=349
02/28/2022 17:44:07 - INFO - __main__ - Global step 700 Train loss 0.30 Rouge-L 0.8657239116197821 on epoch=349
02/28/2022 17:44:09 - INFO - __main__ - Step 710 Global step 710 Train loss 0.29 on epoch=354
02/28/2022 17:44:12 - INFO - __main__ - Step 720 Global step 720 Train loss 0.28 on epoch=359
02/28/2022 17:44:14 - INFO - __main__ - Step 730 Global step 730 Train loss 0.30 on epoch=364
02/28/2022 17:44:16 - INFO - __main__ - Step 740 Global step 740 Train loss 0.31 on epoch=369
02/28/2022 17:44:18 - INFO - __main__ - Step 750 Global step 750 Train loss 0.29 on epoch=374
02/28/2022 17:44:30 - INFO - __main__ - Global step 750 Train loss 0.29 Rouge-L 0.874322890126205 on epoch=374
02/28/2022 17:44:30 - INFO - __main__ - Saving model with best Rouge-L: 0.8736456346265205 -> 0.874322890126205 on epoch=374, global_step=750
02/28/2022 17:44:32 - INFO - __main__ - Step 760 Global step 760 Train loss 0.29 on epoch=379
02/28/2022 17:44:34 - INFO - __main__ - Step 770 Global step 770 Train loss 0.28 on epoch=384
02/28/2022 17:44:37 - INFO - __main__ - Step 780 Global step 780 Train loss 0.28 on epoch=389
02/28/2022 17:44:39 - INFO - __main__ - Step 790 Global step 790 Train loss 0.29 on epoch=394
02/28/2022 17:44:41 - INFO - __main__ - Step 800 Global step 800 Train loss 0.28 on epoch=399
02/28/2022 17:44:53 - INFO - __main__ - Global step 800 Train loss 0.28 Rouge-L 0.8714501663335025 on epoch=399
02/28/2022 17:44:55 - INFO - __main__ - Step 810 Global step 810 Train loss 0.28 on epoch=404
02/28/2022 17:44:58 - INFO - __main__ - Step 820 Global step 820 Train loss 0.27 on epoch=409
02/28/2022 17:45:00 - INFO - __main__ - Step 830 Global step 830 Train loss 0.27 on epoch=414
02/28/2022 17:45:02 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=419
02/28/2022 17:45:05 - INFO - __main__ - Step 850 Global step 850 Train loss 0.26 on epoch=424
02/28/2022 17:45:16 - INFO - __main__ - Global step 850 Train loss 0.27 Rouge-L 0.8691017735280061 on epoch=424
02/28/2022 17:45:18 - INFO - __main__ - Step 860 Global step 860 Train loss 0.26 on epoch=429
02/28/2022 17:45:21 - INFO - __main__ - Step 870 Global step 870 Train loss 0.26 on epoch=434
02/28/2022 17:45:23 - INFO - __main__ - Step 880 Global step 880 Train loss 0.27 on epoch=439
02/28/2022 17:45:25 - INFO - __main__ - Step 890 Global step 890 Train loss 0.26 on epoch=444
02/28/2022 17:45:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.26 on epoch=449
02/28/2022 17:45:39 - INFO - __main__ - Global step 900 Train loss 0.26 Rouge-L 0.8589428101351071 on epoch=449
02/28/2022 17:45:41 - INFO - __main__ - Step 910 Global step 910 Train loss 0.26 on epoch=454
02/28/2022 17:45:43 - INFO - __main__ - Step 920 Global step 920 Train loss 0.26 on epoch=459
02/28/2022 17:45:46 - INFO - __main__ - Step 930 Global step 930 Train loss 0.26 on epoch=464
02/28/2022 17:45:48 - INFO - __main__ - Step 940 Global step 940 Train loss 0.25 on epoch=469
02/28/2022 17:45:50 - INFO - __main__ - Step 950 Global step 950 Train loss 0.26 on epoch=474
02/28/2022 17:46:02 - INFO - __main__ - Global step 950 Train loss 0.26 Rouge-L 0.8561298829975514 on epoch=474
02/28/2022 17:46:05 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=479
02/28/2022 17:46:07 - INFO - __main__ - Step 970 Global step 970 Train loss 0.24 on epoch=484
02/28/2022 17:46:09 - INFO - __main__ - Step 980 Global step 980 Train loss 0.25 on epoch=489
02/28/2022 17:46:12 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=494
02/28/2022 17:46:14 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.24 on epoch=499
02/28/2022 17:46:26 - INFO - __main__ - Global step 1000 Train loss 0.25 Rouge-L 0.8902086325135342 on epoch=499
02/28/2022 17:46:26 - INFO - __main__ - Saving model with best Rouge-L: 0.874322890126205 -> 0.8902086325135342 on epoch=499, global_step=1000
02/28/2022 17:46:28 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.23 on epoch=504
02/28/2022 17:46:30 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
02/28/2022 17:46:33 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.25 on epoch=514
02/28/2022 17:46:35 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.23 on epoch=519
02/28/2022 17:46:37 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.23 on epoch=524
02/28/2022 17:46:49 - INFO - __main__ - Global step 1050 Train loss 0.23 Rouge-L 0.8801406716523484 on epoch=524
02/28/2022 17:46:51 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.22 on epoch=529
02/28/2022 17:46:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=534
02/28/2022 17:46:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.22 on epoch=539
02/28/2022 17:46:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
02/28/2022 17:47:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.24 on epoch=549
02/28/2022 17:47:13 - INFO - __main__ - Global step 1100 Train loss 0.23 Rouge-L 0.882680091620462 on epoch=549
02/28/2022 17:47:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=554
02/28/2022 17:47:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.23 on epoch=559
02/28/2022 17:47:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=564
02/28/2022 17:47:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.23 on epoch=569
02/28/2022 17:47:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.22 on epoch=574
02/28/2022 17:47:36 - INFO - __main__ - Global step 1150 Train loss 0.22 Rouge-L 0.8723348514661294 on epoch=574
02/28/2022 17:47:38 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.22 on epoch=579
02/28/2022 17:47:41 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.22 on epoch=584
02/28/2022 17:47:43 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.22 on epoch=589
02/28/2022 17:47:45 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=594
02/28/2022 17:47:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=599
02/28/2022 17:48:00 - INFO - __main__ - Global step 1200 Train loss 0.21 Rouge-L 0.8794352755116706 on epoch=599
02/28/2022 17:48:02 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
02/28/2022 17:48:04 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.21 on epoch=609
02/28/2022 17:48:07 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.20 on epoch=614
02/28/2022 17:48:09 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=619
02/28/2022 17:48:11 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=624
02/28/2022 17:48:23 - INFO - __main__ - Global step 1250 Train loss 0.21 Rouge-L 0.8782213899179967 on epoch=624
02/28/2022 17:48:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.20 on epoch=629
02/28/2022 17:48:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=634
02/28/2022 17:48:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.19 on epoch=639
02/28/2022 17:48:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.21 on epoch=644
02/28/2022 17:48:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.19 on epoch=649
02/28/2022 17:48:47 - INFO - __main__ - Global step 1300 Train loss 0.20 Rouge-L 0.8611688927690472 on epoch=649
02/28/2022 17:48:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=654
02/28/2022 17:48:52 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.20 on epoch=659
02/28/2022 17:48:54 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.20 on epoch=664
02/28/2022 17:48:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
02/28/2022 17:48:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.19 on epoch=674
02/28/2022 17:49:10 - INFO - __main__ - Global step 1350 Train loss 0.20 Rouge-L 0.8679885128685927 on epoch=674
02/28/2022 17:49:13 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.19 on epoch=679
02/28/2022 17:49:15 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.18 on epoch=684
02/28/2022 17:49:17 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=689
02/28/2022 17:49:20 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.19 on epoch=694
02/28/2022 17:49:22 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.19 on epoch=699
02/28/2022 17:49:34 - INFO - __main__ - Global step 1400 Train loss 0.19 Rouge-L 0.8702048853540544 on epoch=699
02/28/2022 17:49:36 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.19 on epoch=704
02/28/2022 17:49:38 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=709
02/28/2022 17:49:41 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=714
02/28/2022 17:49:43 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.18 on epoch=719
02/28/2022 17:49:45 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.19 on epoch=724
02/28/2022 17:49:57 - INFO - __main__ - Global step 1450 Train loss 0.18 Rouge-L 0.8495710405673885 on epoch=724
02/28/2022 17:50:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
02/28/2022 17:50:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
02/28/2022 17:50:04 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
02/28/2022 17:50:06 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.18 on epoch=744
02/28/2022 17:50:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.18 on epoch=749
02/28/2022 17:50:21 - INFO - __main__ - Global step 1500 Train loss 0.18 Rouge-L 0.8587021298705381 on epoch=749
02/28/2022 17:50:23 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
02/28/2022 17:50:25 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.16 on epoch=759
02/28/2022 17:50:27 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.17 on epoch=764
02/28/2022 17:50:30 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.17 on epoch=769
02/28/2022 17:50:32 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=774
02/28/2022 17:50:44 - INFO - __main__ - Global step 1550 Train loss 0.17 Rouge-L 0.8531892199315451 on epoch=774
02/28/2022 17:50:46 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.16 on epoch=779
02/28/2022 17:50:48 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.16 on epoch=784
02/28/2022 17:50:51 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.16 on epoch=789
02/28/2022 17:50:53 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.16 on epoch=794
02/28/2022 17:50:55 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=799
02/28/2022 17:51:07 - INFO - __main__ - Global step 1600 Train loss 0.16 Rouge-L 0.8707788575602984 on epoch=799
02/28/2022 17:51:10 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=804
02/28/2022 17:51:12 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.16 on epoch=809
02/28/2022 17:51:14 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.15 on epoch=814
02/28/2022 17:51:16 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=819
02/28/2022 17:51:19 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=824
02/28/2022 17:51:31 - INFO - __main__ - Global step 1650 Train loss 0.16 Rouge-L 0.8611901903332222 on epoch=824
02/28/2022 17:51:33 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=829
02/28/2022 17:51:35 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.16 on epoch=834
02/28/2022 17:51:37 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=839
02/28/2022 17:51:40 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=844
02/28/2022 17:51:42 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.16 on epoch=849
02/28/2022 17:51:54 - INFO - __main__ - Global step 1700 Train loss 0.15 Rouge-L 0.868878162315733 on epoch=849
02/28/2022 17:51:56 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=854
02/28/2022 17:51:58 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.14 on epoch=859
02/28/2022 17:52:01 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.15 on epoch=864
02/28/2022 17:52:03 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.15 on epoch=869
02/28/2022 17:52:05 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=874
02/28/2022 17:52:17 - INFO - __main__ - Global step 1750 Train loss 0.15 Rouge-L 0.8604285251484705 on epoch=874
02/28/2022 17:52:19 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=879
02/28/2022 17:52:22 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=884
02/28/2022 17:52:24 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.14 on epoch=889
02/28/2022 17:52:26 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=894
02/28/2022 17:52:28 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=899
02/28/2022 17:52:40 - INFO - __main__ - Global step 1800 Train loss 0.14 Rouge-L 0.8558655711346017 on epoch=899
02/28/2022 17:52:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=904
02/28/2022 17:52:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.14 on epoch=909
02/28/2022 17:52:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=914
02/28/2022 17:52:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=919
02/28/2022 17:52:52 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.14 on epoch=924
02/28/2022 17:53:03 - INFO - __main__ - Global step 1850 Train loss 0.14 Rouge-L 0.8781082314035675 on epoch=924
02/28/2022 17:53:06 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=929
02/28/2022 17:53:08 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
02/28/2022 17:53:10 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=939
02/28/2022 17:53:13 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
02/28/2022 17:53:15 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=949
02/28/2022 17:53:27 - INFO - __main__ - Global step 1900 Train loss 0.13 Rouge-L 0.853573338602069 on epoch=949
02/28/2022 17:53:29 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.13 on epoch=954
02/28/2022 17:53:31 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.13 on epoch=959
02/28/2022 17:53:34 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.14 on epoch=964
02/28/2022 17:53:36 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=969
02/28/2022 17:53:38 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=974
02/28/2022 17:53:50 - INFO - __main__ - Global step 1950 Train loss 0.13 Rouge-L 0.8734796721818749 on epoch=974
02/28/2022 17:53:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=979
02/28/2022 17:53:55 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.12 on epoch=984
02/28/2022 17:53:57 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=989
02/28/2022 17:53:59 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.13 on epoch=994
02/28/2022 17:54:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=999
02/28/2022 17:54:03 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 17:54:03 - INFO - __main__ - Printing 3 examples
02/28/2022 17:54:03 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/28/2022 17:54:03 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/28/2022 17:54:03 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/28/2022 17:54:03 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/28/2022 17:54:03 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/28/2022 17:54:03 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/28/2022 17:54:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 17:54:03 - INFO - __main__ - Tokenizing Output ...
02/28/2022 17:54:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 17:54:03 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 17:54:03 - INFO - __main__ - Printing 3 examples
02/28/2022 17:54:03 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/28/2022 17:54:03 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/28/2022 17:54:03 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/28/2022 17:54:03 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/28/2022 17:54:03 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/28/2022 17:54:03 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/28/2022 17:54:03 - INFO - __main__ - Tokenizing Input ...
02/28/2022 17:54:03 - INFO - __main__ - Tokenizing Output ...
02/28/2022 17:54:03 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 17:54:14 - INFO - __main__ - Global step 2000 Train loss 0.12 Rouge-L 0.8610201734986361 on epoch=999
02/28/2022 17:54:14 - INFO - __main__ - save last model!
02/28/2022 17:54:14 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/28/2022 17:54:14 - INFO - __main__ - Start tokenizing ... 5000 instances
02/28/2022 17:54:14 - INFO - __main__ - Printing 3 examples
02/28/2022 17:54:14 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/28/2022 17:54:14 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/28/2022 17:54:14 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/28/2022 17:54:14 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/28/2022 17:54:14 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/28/2022 17:54:14 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/28/2022 17:54:14 - INFO - __main__ - Tokenizing Input ...
02/28/2022 17:54:16 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 17:54:16 - INFO - __main__ - Tokenizing Output ...
02/28/2022 17:54:16 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 17:54:16 - INFO - __main__ - Starting training!
02/28/2022 17:54:22 - INFO - __main__ - Loaded 5000 examples from test data
02/28/2022 18:23:59 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split/wiki_split_32_100_0.3_8_predictions.txt
02/28/2022 18:24:04 - INFO - __main__ - Rouge-L on test data: 0.8720
02/28/2022 18:24:04 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.3, bsz=8, dev_performance=0.8902086325135342, test_performance=0.8719526484565373
02/28/2022 18:24:04 - INFO - __main__ - Running ... prefix=wiki_split_32_100, lr=0.2, bsz=8 ...
02/28/2022 18:24:05 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 18:24:05 - INFO - __main__ - Printing 3 examples
02/28/2022 18:24:05 - INFO - __main__ -  [wiki_split] sentence 1: KHOTOUM is a Costume Design ( installation & photography ) exhibition , Opened on November 20th , 2014 in Cairo , Egypt .  [SEP] sentence 2:  Khotoum exhibition is inspired by Egyptian Painter , Abd El - Hady el Gazzar .
02/28/2022 18:24:05 - INFO - __main__ - ['KHOTOUM is a Costume Design ( installation & photography ) exhibition that is inspired by the great Egyptian Painter , Abd El - Hady el Gazzar .']
02/28/2022 18:24:05 - INFO - __main__ -  [wiki_split] sentence 1: The instrument is not dodecaphonically tempered , instead using the Istrian scale .  [SEP] sentence 2:  Its sound is distinct and unusual .
02/28/2022 18:24:05 - INFO - __main__ - ['The instrument is not dodecaphonically tempered , it uses the Istrian scale and its sound is distinct and unusual .']
02/28/2022 18:24:05 - INFO - __main__ -  [wiki_split] sentence 1: Male Tapanuli orangutans produce loud , long distance calls that can last for up to two minutes .  [SEP] sentence 2:  Their calls have a higher frequency and a higher number of pulses than those of other orangutan species .
02/28/2022 18:24:05 - INFO - __main__ - ['Male Tapanuli orangutans produce loud , long distance calls with a high frequency and a high number of pulses distinct from those of other orangutan species .']
02/28/2022 18:24:05 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/28/2022 18:24:05 - INFO - __main__ - Tokenizing Output ...
02/28/2022 18:24:05 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 18:24:05 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 18:24:05 - INFO - __main__ - Printing 3 examples
02/28/2022 18:24:05 - INFO - __main__ -  [wiki_split] sentence 1: Under the name '' J.J. Solari '' , he became a writer of short stories .  [SEP] sentence 2:  They were collected in a 2007 anthology called '' When Bikers Meet Humans '' .
02/28/2022 18:24:05 - INFO - __main__ - ["Under the name '' J.J. Solari '' , he became a writer of short stories for biker magazines such as , which were collected in a 2007 anthology called '' When Bikers Meet Humans '' ."]
02/28/2022 18:24:05 - INFO - __main__ -  [wiki_split] sentence 1: It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants .  [SEP] sentence 2:  The most notable being Brett Favre of the New York Jets after coming out of retirement .
02/28/2022 18:24:05 - INFO - __main__ - ['It featured others , notables like David Tyree who was a free agent due to the lack of depth on the New York Giants , with the most notable being Favre of the Jets after coming out of retirement .']
02/28/2022 18:24:05 - INFO - __main__ -  [wiki_split] sentence 1: LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs .  [SEP] sentence 2:  Most LED lamps replace incandescent bulbs rated from 5 to 60 watts .
02/28/2022 18:24:05 - INFO - __main__ - ['LED lamps are made that replace screw - in incandescent or compact fluorescent light bulbs , mostly replacing incandescent bulbs rated from 5 to 60 watts .']
02/28/2022 18:24:05 - INFO - __main__ - Tokenizing Input ...
02/28/2022 18:24:05 - INFO - __main__ - Tokenizing Output ...
02/28/2022 18:24:05 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 18:24:18 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 18:24:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 18:24:18 - INFO - __main__ - Starting training!
02/28/2022 18:24:25 - INFO - __main__ - Step 10 Global step 10 Train loss 0.85 on epoch=4
02/28/2022 18:24:27 - INFO - __main__ - Step 20 Global step 20 Train loss 0.76 on epoch=9
02/28/2022 18:24:29 - INFO - __main__ - Step 30 Global step 30 Train loss 0.71 on epoch=14
02/28/2022 18:24:32 - INFO - __main__ - Step 40 Global step 40 Train loss 0.67 on epoch=19
02/28/2022 18:24:34 - INFO - __main__ - Step 50 Global step 50 Train loss 0.65 on epoch=24
02/28/2022 18:24:42 - INFO - __main__ - Global step 50 Train loss 0.73 Rouge-L 0.6393399104402411 on epoch=24
02/28/2022 18:24:42 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.6393399104402411 on epoch=24, global_step=50
02/28/2022 18:24:44 - INFO - __main__ - Step 60 Global step 60 Train loss 0.64 on epoch=29
02/28/2022 18:24:47 - INFO - __main__ - Step 70 Global step 70 Train loss 0.62 on epoch=34
02/28/2022 18:24:49 - INFO - __main__ - Step 80 Global step 80 Train loss 0.59 on epoch=39
02/28/2022 18:24:51 - INFO - __main__ - Step 90 Global step 90 Train loss 0.59 on epoch=44
02/28/2022 18:24:54 - INFO - __main__ - Step 100 Global step 100 Train loss 0.57 on epoch=49
02/28/2022 18:25:05 - INFO - __main__ - Global step 100 Train loss 0.60 Rouge-L 0.8223550208826764 on epoch=49
02/28/2022 18:25:05 - INFO - __main__ - Saving model with best Rouge-L: 0.6393399104402411 -> 0.8223550208826764 on epoch=49, global_step=100
02/28/2022 18:25:07 - INFO - __main__ - Step 110 Global step 110 Train loss 0.57 on epoch=54
02/28/2022 18:25:09 - INFO - __main__ - Step 120 Global step 120 Train loss 0.55 on epoch=59
02/28/2022 18:25:12 - INFO - __main__ - Step 130 Global step 130 Train loss 0.54 on epoch=64
02/28/2022 18:25:14 - INFO - __main__ - Step 140 Global step 140 Train loss 0.54 on epoch=69
02/28/2022 18:25:16 - INFO - __main__ - Step 150 Global step 150 Train loss 0.56 on epoch=74
02/28/2022 18:25:28 - INFO - __main__ - Global step 150 Train loss 0.55 Rouge-L 0.8412343878140167 on epoch=74
02/28/2022 18:25:28 - INFO - __main__ - Saving model with best Rouge-L: 0.8223550208826764 -> 0.8412343878140167 on epoch=74, global_step=150
02/28/2022 18:25:30 - INFO - __main__ - Step 160 Global step 160 Train loss 0.53 on epoch=79
02/28/2022 18:25:33 - INFO - __main__ - Step 170 Global step 170 Train loss 0.53 on epoch=84
02/28/2022 18:25:35 - INFO - __main__ - Step 180 Global step 180 Train loss 0.52 on epoch=89
02/28/2022 18:25:37 - INFO - __main__ - Step 190 Global step 190 Train loss 0.52 on epoch=94
02/28/2022 18:25:40 - INFO - __main__ - Step 200 Global step 200 Train loss 0.51 on epoch=99
02/28/2022 18:25:52 - INFO - __main__ - Global step 200 Train loss 0.52 Rouge-L 0.8636847386532296 on epoch=99
02/28/2022 18:25:52 - INFO - __main__ - Saving model with best Rouge-L: 0.8412343878140167 -> 0.8636847386532296 on epoch=99, global_step=200
02/28/2022 18:25:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.52 on epoch=104
02/28/2022 18:25:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.51 on epoch=109
02/28/2022 18:25:59 - INFO - __main__ - Step 230 Global step 230 Train loss 0.50 on epoch=114
02/28/2022 18:26:01 - INFO - __main__ - Step 240 Global step 240 Train loss 0.50 on epoch=119
02/28/2022 18:26:03 - INFO - __main__ - Step 250 Global step 250 Train loss 0.49 on epoch=124
02/28/2022 18:26:14 - INFO - __main__ - Global step 250 Train loss 0.50 Rouge-L 0.8584411886215023 on epoch=124
02/28/2022 18:26:17 - INFO - __main__ - Step 260 Global step 260 Train loss 0.48 on epoch=129
02/28/2022 18:26:19 - INFO - __main__ - Step 270 Global step 270 Train loss 0.48 on epoch=134
02/28/2022 18:26:21 - INFO - __main__ - Step 280 Global step 280 Train loss 0.47 on epoch=139
02/28/2022 18:26:24 - INFO - __main__ - Step 290 Global step 290 Train loss 0.48 on epoch=144
02/28/2022 18:26:26 - INFO - __main__ - Step 300 Global step 300 Train loss 0.46 on epoch=149
02/28/2022 18:26:37 - INFO - __main__ - Global step 300 Train loss 0.48 Rouge-L 0.8593177004737184 on epoch=149
02/28/2022 18:26:40 - INFO - __main__ - Step 310 Global step 310 Train loss 0.45 on epoch=154
02/28/2022 18:26:42 - INFO - __main__ - Step 320 Global step 320 Train loss 0.46 on epoch=159
02/28/2022 18:26:44 - INFO - __main__ - Step 330 Global step 330 Train loss 0.46 on epoch=164
02/28/2022 18:26:47 - INFO - __main__ - Step 340 Global step 340 Train loss 0.45 on epoch=169
02/28/2022 18:26:49 - INFO - __main__ - Step 350 Global step 350 Train loss 0.46 on epoch=174
02/28/2022 18:27:00 - INFO - __main__ - Global step 350 Train loss 0.46 Rouge-L 0.868066789941753 on epoch=174
02/28/2022 18:27:00 - INFO - __main__ - Saving model with best Rouge-L: 0.8636847386532296 -> 0.868066789941753 on epoch=174, global_step=350
02/28/2022 18:27:02 - INFO - __main__ - Step 360 Global step 360 Train loss 0.47 on epoch=179
02/28/2022 18:27:05 - INFO - __main__ - Step 370 Global step 370 Train loss 0.45 on epoch=184
02/28/2022 18:27:07 - INFO - __main__ - Step 380 Global step 380 Train loss 0.45 on epoch=189
02/28/2022 18:27:09 - INFO - __main__ - Step 390 Global step 390 Train loss 0.44 on epoch=194
02/28/2022 18:27:12 - INFO - __main__ - Step 400 Global step 400 Train loss 0.44 on epoch=199
02/28/2022 18:27:22 - INFO - __main__ - Global step 400 Train loss 0.45 Rouge-L 0.8553051815483416 on epoch=199
02/28/2022 18:27:24 - INFO - __main__ - Step 410 Global step 410 Train loss 0.42 on epoch=204
02/28/2022 18:27:26 - INFO - __main__ - Step 420 Global step 420 Train loss 0.43 on epoch=209
02/28/2022 18:27:28 - INFO - __main__ - Step 430 Global step 430 Train loss 0.43 on epoch=214
02/28/2022 18:27:31 - INFO - __main__ - Step 440 Global step 440 Train loss 0.42 on epoch=219
02/28/2022 18:27:33 - INFO - __main__ - Step 450 Global step 450 Train loss 0.43 on epoch=224
02/28/2022 18:27:43 - INFO - __main__ - Global step 450 Train loss 0.43 Rouge-L 0.8701628099600061 on epoch=224
02/28/2022 18:27:43 - INFO - __main__ - Saving model with best Rouge-L: 0.868066789941753 -> 0.8701628099600061 on epoch=224, global_step=450
02/28/2022 18:27:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.41 on epoch=229
02/28/2022 18:27:47 - INFO - __main__ - Step 470 Global step 470 Train loss 0.43 on epoch=234
02/28/2022 18:27:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.41 on epoch=239
02/28/2022 18:27:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.41 on epoch=244
02/28/2022 18:27:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.40 on epoch=249
02/28/2022 18:28:04 - INFO - __main__ - Global step 500 Train loss 0.41 Rouge-L 0.8703770672612197 on epoch=249
02/28/2022 18:28:04 - INFO - __main__ - Saving model with best Rouge-L: 0.8701628099600061 -> 0.8703770672612197 on epoch=249, global_step=500
02/28/2022 18:28:07 - INFO - __main__ - Step 510 Global step 510 Train loss 0.40 on epoch=254
02/28/2022 18:28:09 - INFO - __main__ - Step 520 Global step 520 Train loss 0.40 on epoch=259
02/28/2022 18:28:11 - INFO - __main__ - Step 530 Global step 530 Train loss 0.40 on epoch=264
02/28/2022 18:28:13 - INFO - __main__ - Step 540 Global step 540 Train loss 0.40 on epoch=269
02/28/2022 18:28:15 - INFO - __main__ - Step 550 Global step 550 Train loss 0.38 on epoch=274
02/28/2022 18:28:26 - INFO - __main__ - Global step 550 Train loss 0.40 Rouge-L 0.8695557638256068 on epoch=274
02/28/2022 18:28:28 - INFO - __main__ - Step 560 Global step 560 Train loss 0.40 on epoch=279
02/28/2022 18:28:30 - INFO - __main__ - Step 570 Global step 570 Train loss 0.39 on epoch=284
02/28/2022 18:28:33 - INFO - __main__ - Step 580 Global step 580 Train loss 0.38 on epoch=289
02/28/2022 18:28:35 - INFO - __main__ - Step 590 Global step 590 Train loss 0.38 on epoch=294
02/28/2022 18:28:37 - INFO - __main__ - Step 600 Global step 600 Train loss 0.37 on epoch=299
02/28/2022 18:28:47 - INFO - __main__ - Global step 600 Train loss 0.39 Rouge-L 0.8720955995126379 on epoch=299
02/28/2022 18:28:47 - INFO - __main__ - Saving model with best Rouge-L: 0.8703770672612197 -> 0.8720955995126379 on epoch=299, global_step=600
02/28/2022 18:28:50 - INFO - __main__ - Step 610 Global step 610 Train loss 0.39 on epoch=304
02/28/2022 18:28:52 - INFO - __main__ - Step 620 Global step 620 Train loss 0.37 on epoch=309
02/28/2022 18:28:54 - INFO - __main__ - Step 630 Global step 630 Train loss 0.36 on epoch=314
02/28/2022 18:28:56 - INFO - __main__ - Step 640 Global step 640 Train loss 0.37 on epoch=319
02/28/2022 18:28:58 - INFO - __main__ - Step 650 Global step 650 Train loss 0.36 on epoch=324
02/28/2022 18:29:09 - INFO - __main__ - Global step 650 Train loss 0.37 Rouge-L 0.871827423471669 on epoch=324
02/28/2022 18:29:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.37 on epoch=329
02/28/2022 18:29:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.36 on epoch=334
02/28/2022 18:29:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.36 on epoch=339
02/28/2022 18:29:17 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=344
02/28/2022 18:29:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.35 on epoch=349
02/28/2022 18:29:30 - INFO - __main__ - Global step 700 Train loss 0.36 Rouge-L 0.870109175613839 on epoch=349
02/28/2022 18:29:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.34 on epoch=354
02/28/2022 18:29:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.36 on epoch=359
02/28/2022 18:29:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.35 on epoch=364
02/28/2022 18:29:39 - INFO - __main__ - Step 740 Global step 740 Train loss 0.36 on epoch=369
02/28/2022 18:29:41 - INFO - __main__ - Step 750 Global step 750 Train loss 0.34 on epoch=374
02/28/2022 18:29:51 - INFO - __main__ - Global step 750 Train loss 0.35 Rouge-L 0.8721939125667949 on epoch=374
02/28/2022 18:29:51 - INFO - __main__ - Saving model with best Rouge-L: 0.8720955995126379 -> 0.8721939125667949 on epoch=374, global_step=750
02/28/2022 18:29:53 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=379
02/28/2022 18:29:55 - INFO - __main__ - Step 770 Global step 770 Train loss 0.32 on epoch=384
02/28/2022 18:29:58 - INFO - __main__ - Step 780 Global step 780 Train loss 0.34 on epoch=389
02/28/2022 18:30:00 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=394
02/28/2022 18:30:02 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=399
02/28/2022 18:30:12 - INFO - __main__ - Global step 800 Train loss 0.33 Rouge-L 0.8615386535836759 on epoch=399
02/28/2022 18:30:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=404
02/28/2022 18:30:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.32 on epoch=409
02/28/2022 18:30:19 - INFO - __main__ - Step 830 Global step 830 Train loss 0.32 on epoch=414
02/28/2022 18:30:21 - INFO - __main__ - Step 840 Global step 840 Train loss 0.31 on epoch=419
02/28/2022 18:30:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=424
02/28/2022 18:30:33 - INFO - __main__ - Global step 850 Train loss 0.32 Rouge-L 0.859919978006345 on epoch=424
02/28/2022 18:30:35 - INFO - __main__ - Step 860 Global step 860 Train loss 0.31 on epoch=429
02/28/2022 18:30:38 - INFO - __main__ - Step 870 Global step 870 Train loss 0.31 on epoch=434
02/28/2022 18:30:40 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=439
02/28/2022 18:30:42 - INFO - __main__ - Step 890 Global step 890 Train loss 0.31 on epoch=444
02/28/2022 18:30:44 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=449
02/28/2022 18:30:54 - INFO - __main__ - Global step 900 Train loss 0.31 Rouge-L 0.8590451221147513 on epoch=449
02/28/2022 18:30:56 - INFO - __main__ - Step 910 Global step 910 Train loss 0.30 on epoch=454
02/28/2022 18:30:59 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=459
02/28/2022 18:31:01 - INFO - __main__ - Step 930 Global step 930 Train loss 0.30 on epoch=464
02/28/2022 18:31:03 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=469
02/28/2022 18:31:05 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=474
02/28/2022 18:31:15 - INFO - __main__ - Global step 950 Train loss 0.30 Rouge-L 0.856634687664731 on epoch=474
02/28/2022 18:31:18 - INFO - __main__ - Step 960 Global step 960 Train loss 0.29 on epoch=479
02/28/2022 18:31:20 - INFO - __main__ - Step 970 Global step 970 Train loss 0.29 on epoch=484
02/28/2022 18:31:22 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=489
02/28/2022 18:31:24 - INFO - __main__ - Step 990 Global step 990 Train loss 0.30 on epoch=494
02/28/2022 18:31:26 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.29 on epoch=499
02/28/2022 18:31:36 - INFO - __main__ - Global step 1000 Train loss 0.29 Rouge-L 0.8558298577801192 on epoch=499
02/28/2022 18:31:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.29 on epoch=504
02/28/2022 18:31:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.28 on epoch=509
02/28/2022 18:31:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.27 on epoch=514
02/28/2022 18:31:45 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=519
02/28/2022 18:31:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=524
02/28/2022 18:31:58 - INFO - __main__ - Global step 1050 Train loss 0.28 Rouge-L 0.8544624849458257 on epoch=524
02/28/2022 18:32:00 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.28 on epoch=529
02/28/2022 18:32:03 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.26 on epoch=534
02/28/2022 18:32:05 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.27 on epoch=539
02/28/2022 18:32:07 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.27 on epoch=544
02/28/2022 18:32:09 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.27 on epoch=549
02/28/2022 18:32:20 - INFO - __main__ - Global step 1100 Train loss 0.27 Rouge-L 0.8530507514045123 on epoch=549
02/28/2022 18:32:22 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=554
02/28/2022 18:32:24 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.26 on epoch=559
02/28/2022 18:32:26 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.26 on epoch=564
02/28/2022 18:32:29 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.27 on epoch=569
02/28/2022 18:32:31 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.27 on epoch=574
02/28/2022 18:32:41 - INFO - __main__ - Global step 1150 Train loss 0.27 Rouge-L 0.8512818696729929 on epoch=574
02/28/2022 18:32:43 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=579
02/28/2022 18:32:45 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=584
02/28/2022 18:32:47 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.26 on epoch=589
02/28/2022 18:32:50 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.25 on epoch=594
02/28/2022 18:32:52 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.26 on epoch=599
02/28/2022 18:33:03 - INFO - __main__ - Global step 1200 Train loss 0.26 Rouge-L 0.8548074775998629 on epoch=599
02/28/2022 18:33:05 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.26 on epoch=604
02/28/2022 18:33:07 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.25 on epoch=609
02/28/2022 18:33:10 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=614
02/28/2022 18:33:12 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.25 on epoch=619
02/28/2022 18:33:14 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=624
02/28/2022 18:33:24 - INFO - __main__ - Global step 1250 Train loss 0.25 Rouge-L 0.8556730336986016 on epoch=624
02/28/2022 18:33:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=629
02/28/2022 18:33:29 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.24 on epoch=634
02/28/2022 18:33:31 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=639
02/28/2022 18:33:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=644
02/28/2022 18:33:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=649
02/28/2022 18:33:46 - INFO - __main__ - Global step 1300 Train loss 0.24 Rouge-L 0.8610916762679379 on epoch=649
02/28/2022 18:33:48 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.24 on epoch=654
02/28/2022 18:33:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.24 on epoch=659
02/28/2022 18:33:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.24 on epoch=664
02/28/2022 18:33:55 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.24 on epoch=669
02/28/2022 18:33:57 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
02/28/2022 18:34:08 - INFO - __main__ - Global step 1350 Train loss 0.24 Rouge-L 0.8595965796555449 on epoch=674
02/28/2022 18:34:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=679
02/28/2022 18:34:13 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.24 on epoch=684
02/28/2022 18:34:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.23 on epoch=689
02/28/2022 18:34:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.23 on epoch=694
02/28/2022 18:34:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=699
02/28/2022 18:34:30 - INFO - __main__ - Global step 1400 Train loss 0.23 Rouge-L 0.8650036953963482 on epoch=699
02/28/2022 18:34:32 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=704
02/28/2022 18:34:34 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.24 on epoch=709
02/28/2022 18:34:36 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.23 on epoch=714
02/28/2022 18:34:39 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=719
02/28/2022 18:34:41 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.23 on epoch=724
02/28/2022 18:34:52 - INFO - __main__ - Global step 1450 Train loss 0.23 Rouge-L 0.8531217436517281 on epoch=724
02/28/2022 18:34:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=729
02/28/2022 18:34:57 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=734
02/28/2022 18:34:59 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=739
02/28/2022 18:35:01 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.23 on epoch=744
02/28/2022 18:35:04 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=749
02/28/2022 18:35:15 - INFO - __main__ - Global step 1500 Train loss 0.22 Rouge-L 0.8489676062237576 on epoch=749
02/28/2022 18:35:17 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.23 on epoch=754
02/28/2022 18:35:20 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=759
02/28/2022 18:35:22 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.22 on epoch=764
02/28/2022 18:35:24 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.22 on epoch=769
02/28/2022 18:35:27 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=774
02/28/2022 18:35:38 - INFO - __main__ - Global step 1550 Train loss 0.22 Rouge-L 0.8608459154839629 on epoch=774
02/28/2022 18:35:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=779
02/28/2022 18:35:42 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=784
02/28/2022 18:35:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.21 on epoch=789
02/28/2022 18:35:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=794
02/28/2022 18:35:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.21 on epoch=799
02/28/2022 18:36:01 - INFO - __main__ - Global step 1600 Train loss 0.21 Rouge-L 0.8573842347029749 on epoch=799
02/28/2022 18:36:03 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=804
02/28/2022 18:36:05 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=809
02/28/2022 18:36:08 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=814
02/28/2022 18:36:10 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.20 on epoch=819
02/28/2022 18:36:12 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.20 on epoch=824
02/28/2022 18:36:23 - INFO - __main__ - Global step 1650 Train loss 0.21 Rouge-L 0.8603758005203995 on epoch=824
02/28/2022 18:36:25 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=829
02/28/2022 18:36:28 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.20 on epoch=834
02/28/2022 18:36:30 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.21 on epoch=839
02/28/2022 18:36:32 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=844
02/28/2022 18:36:35 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=849
02/28/2022 18:36:46 - INFO - __main__ - Global step 1700 Train loss 0.21 Rouge-L 0.8641385237462085 on epoch=849
02/28/2022 18:36:48 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.21 on epoch=854
02/28/2022 18:36:50 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.19 on epoch=859
02/28/2022 18:36:52 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.21 on epoch=864
02/28/2022 18:36:55 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=869
02/28/2022 18:36:57 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=874
02/28/2022 18:37:08 - INFO - __main__ - Global step 1750 Train loss 0.20 Rouge-L 0.8544976664023172 on epoch=874
02/28/2022 18:37:10 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=879
02/28/2022 18:37:12 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.20 on epoch=884
02/28/2022 18:37:14 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=889
02/28/2022 18:37:17 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=894
02/28/2022 18:37:19 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=899
02/28/2022 18:37:30 - INFO - __main__ - Global step 1800 Train loss 0.19 Rouge-L 0.8608941768095064 on epoch=899
02/28/2022 18:37:32 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.19 on epoch=904
02/28/2022 18:37:35 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.18 on epoch=909
02/28/2022 18:37:37 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.18 on epoch=914
02/28/2022 18:37:39 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.19 on epoch=919
02/28/2022 18:37:42 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.18 on epoch=924
02/28/2022 18:37:53 - INFO - __main__ - Global step 1850 Train loss 0.18 Rouge-L 0.8585158533006867 on epoch=924
02/28/2022 18:37:56 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.19 on epoch=929
02/28/2022 18:37:58 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.19 on epoch=934
02/28/2022 18:38:00 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=939
02/28/2022 18:38:03 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.18 on epoch=944
02/28/2022 18:38:05 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.18 on epoch=949
02/28/2022 18:38:15 - INFO - __main__ - Global step 1900 Train loss 0.18 Rouge-L 0.8606859352121912 on epoch=949
02/28/2022 18:38:18 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=954
02/28/2022 18:38:20 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=959
02/28/2022 18:38:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.19 on epoch=964
02/28/2022 18:38:25 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.18 on epoch=969
02/28/2022 18:38:27 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=974
02/28/2022 18:38:38 - INFO - __main__ - Global step 1950 Train loss 0.18 Rouge-L 0.8623625037367153 on epoch=974
02/28/2022 18:38:41 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=979
02/28/2022 18:38:43 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=984
02/28/2022 18:38:45 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.18 on epoch=989
02/28/2022 18:38:48 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.17 on epoch=994
02/28/2022 18:38:50 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=999
02/28/2022 18:38:52 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 18:38:52 - INFO - __main__ - Printing 3 examples
02/28/2022 18:38:52 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
02/28/2022 18:38:52 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
02/28/2022 18:38:52 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
02/28/2022 18:38:52 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
02/28/2022 18:38:52 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
02/28/2022 18:38:52 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
02/28/2022 18:38:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 18:38:52 - INFO - __main__ - Tokenizing Output ...
02/28/2022 18:38:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 18:38:52 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 18:38:52 - INFO - __main__ - Printing 3 examples
02/28/2022 18:38:52 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
02/28/2022 18:38:52 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
02/28/2022 18:38:52 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
02/28/2022 18:38:52 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
02/28/2022 18:38:52 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
02/28/2022 18:38:52 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
02/28/2022 18:38:52 - INFO - __main__ - Tokenizing Input ...
02/28/2022 18:38:52 - INFO - __main__ - Tokenizing Output ...
02/28/2022 18:38:52 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 18:39:02 - INFO - __main__ - Global step 2000 Train loss 0.18 Rouge-L 0.8577783318937383 on epoch=999
02/28/2022 18:39:02 - INFO - __main__ - save last model!
02/28/2022 18:39:02 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/28/2022 18:39:02 - INFO - __main__ - Start tokenizing ... 5000 instances
02/28/2022 18:39:02 - INFO - __main__ - Printing 3 examples
02/28/2022 18:39:02 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/28/2022 18:39:02 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/28/2022 18:39:02 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/28/2022 18:39:02 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/28/2022 18:39:02 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/28/2022 18:39:02 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/28/2022 18:39:02 - INFO - __main__ - Tokenizing Input ...
02/28/2022 18:39:04 - INFO - __main__ - Tokenizing Output ...
02/28/2022 18:39:05 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 18:39:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 18:39:06 - INFO - __main__ - Starting training!
02/28/2022 18:39:10 - INFO - __main__ - Loaded 5000 examples from test data
02/28/2022 19:07:44 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split/wiki_split_32_100_0.2_8_predictions.txt
02/28/2022 19:07:49 - INFO - __main__ - Rouge-L on test data: 0.8681
02/28/2022 19:07:51 - INFO - __main__ - prefix=wiki_split_32_100, lr=0.2, bsz=8, dev_performance=0.8721939125667949, test_performance=0.8680701494132711
02/28/2022 19:07:51 - INFO - __main__ - Running ... prefix=wiki_split_32_13, lr=0.5, bsz=8 ...
02/28/2022 19:07:52 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 19:07:52 - INFO - __main__ - Printing 3 examples
02/28/2022 19:07:52 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
02/28/2022 19:07:52 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
02/28/2022 19:07:52 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
02/28/2022 19:07:52 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
02/28/2022 19:07:52 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
02/28/2022 19:07:52 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
02/28/2022 19:07:52 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/28/2022 19:07:52 - INFO - __main__ - Tokenizing Output ...
02/28/2022 19:07:52 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 19:07:52 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 19:07:52 - INFO - __main__ - Printing 3 examples
02/28/2022 19:07:52 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
02/28/2022 19:07:52 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
02/28/2022 19:07:52 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
02/28/2022 19:07:52 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
02/28/2022 19:07:52 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
02/28/2022 19:07:52 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
02/28/2022 19:07:52 - INFO - __main__ - Tokenizing Input ...
02/28/2022 19:07:52 - INFO - __main__ - Tokenizing Output ...
02/28/2022 19:07:52 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 19:08:06 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 19:08:07 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 19:08:07 - INFO - __main__ - Starting training!
02/28/2022 19:08:10 - INFO - __main__ - Step 10 Global step 10 Train loss 0.73 on epoch=4
02/28/2022 19:08:12 - INFO - __main__ - Step 20 Global step 20 Train loss 0.62 on epoch=9
02/28/2022 19:08:14 - INFO - __main__ - Step 30 Global step 30 Train loss 0.56 on epoch=14
02/28/2022 19:08:16 - INFO - __main__ - Step 40 Global step 40 Train loss 0.52 on epoch=19
02/28/2022 19:08:19 - INFO - __main__ - Step 50 Global step 50 Train loss 0.51 on epoch=24
02/28/2022 19:08:29 - INFO - __main__ - Global step 50 Train loss 0.59 Rouge-L 0.8469553800994757 on epoch=24
02/28/2022 19:08:29 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.8469553800994757 on epoch=24, global_step=50
02/28/2022 19:08:31 - INFO - __main__ - Step 60 Global step 60 Train loss 0.48 on epoch=29
02/28/2022 19:08:33 - INFO - __main__ - Step 70 Global step 70 Train loss 0.47 on epoch=34
02/28/2022 19:08:36 - INFO - __main__ - Step 80 Global step 80 Train loss 0.46 on epoch=39
02/28/2022 19:08:38 - INFO - __main__ - Step 90 Global step 90 Train loss 0.45 on epoch=44
02/28/2022 19:08:40 - INFO - __main__ - Step 100 Global step 100 Train loss 0.43 on epoch=49
02/28/2022 19:08:50 - INFO - __main__ - Global step 100 Train loss 0.46 Rouge-L 0.8487788063971275 on epoch=49
02/28/2022 19:08:50 - INFO - __main__ - Saving model with best Rouge-L: 0.8469553800994757 -> 0.8487788063971275 on epoch=49, global_step=100
02/28/2022 19:08:52 - INFO - __main__ - Step 110 Global step 110 Train loss 0.44 on epoch=54
02/28/2022 19:08:54 - INFO - __main__ - Step 120 Global step 120 Train loss 0.41 on epoch=59
02/28/2022 19:08:57 - INFO - __main__ - Step 130 Global step 130 Train loss 0.42 on epoch=64
02/28/2022 19:08:59 - INFO - __main__ - Step 140 Global step 140 Train loss 0.40 on epoch=69
02/28/2022 19:09:01 - INFO - __main__ - Step 150 Global step 150 Train loss 0.38 on epoch=74
02/28/2022 19:09:12 - INFO - __main__ - Global step 150 Train loss 0.41 Rouge-L 0.8483581180911076 on epoch=74
02/28/2022 19:09:14 - INFO - __main__ - Step 160 Global step 160 Train loss 0.37 on epoch=79
02/28/2022 19:09:16 - INFO - __main__ - Step 170 Global step 170 Train loss 0.37 on epoch=84
02/28/2022 19:09:18 - INFO - __main__ - Step 180 Global step 180 Train loss 0.35 on epoch=89
02/28/2022 19:09:21 - INFO - __main__ - Step 190 Global step 190 Train loss 0.35 on epoch=94
02/28/2022 19:09:23 - INFO - __main__ - Step 200 Global step 200 Train loss 0.35 on epoch=99
02/28/2022 19:09:34 - INFO - __main__ - Global step 200 Train loss 0.36 Rouge-L 0.8565549150086744 on epoch=99
02/28/2022 19:09:34 - INFO - __main__ - Saving model with best Rouge-L: 0.8487788063971275 -> 0.8565549150086744 on epoch=99, global_step=200
02/28/2022 19:09:36 - INFO - __main__ - Step 210 Global step 210 Train loss 0.33 on epoch=104
02/28/2022 19:09:38 - INFO - __main__ - Step 220 Global step 220 Train loss 0.32 on epoch=109
02/28/2022 19:09:40 - INFO - __main__ - Step 230 Global step 230 Train loss 0.31 on epoch=114
02/28/2022 19:09:43 - INFO - __main__ - Step 240 Global step 240 Train loss 0.30 on epoch=119
02/28/2022 19:09:45 - INFO - __main__ - Step 250 Global step 250 Train loss 0.31 on epoch=124
02/28/2022 19:09:56 - INFO - __main__ - Global step 250 Train loss 0.32 Rouge-L 0.8746627096990172 on epoch=124
02/28/2022 19:09:56 - INFO - __main__ - Saving model with best Rouge-L: 0.8565549150086744 -> 0.8746627096990172 on epoch=124, global_step=250
02/28/2022 19:09:58 - INFO - __main__ - Step 260 Global step 260 Train loss 0.28 on epoch=129
02/28/2022 19:10:00 - INFO - __main__ - Step 270 Global step 270 Train loss 0.29 on epoch=134
02/28/2022 19:10:03 - INFO - __main__ - Step 280 Global step 280 Train loss 0.26 on epoch=139
02/28/2022 19:10:05 - INFO - __main__ - Step 290 Global step 290 Train loss 0.25 on epoch=144
02/28/2022 19:10:07 - INFO - __main__ - Step 300 Global step 300 Train loss 0.26 on epoch=149
02/28/2022 19:10:19 - INFO - __main__ - Global step 300 Train loss 0.27 Rouge-L 0.8631375610003581 on epoch=149
02/28/2022 19:10:21 - INFO - __main__ - Step 310 Global step 310 Train loss 0.26 on epoch=154
02/28/2022 19:10:23 - INFO - __main__ - Step 320 Global step 320 Train loss 0.25 on epoch=159
02/28/2022 19:10:25 - INFO - __main__ - Step 330 Global step 330 Train loss 0.24 on epoch=164
02/28/2022 19:10:27 - INFO - __main__ - Step 340 Global step 340 Train loss 0.23 on epoch=169
02/28/2022 19:10:29 - INFO - __main__ - Step 350 Global step 350 Train loss 0.22 on epoch=174
02/28/2022 19:10:41 - INFO - __main__ - Global step 350 Train loss 0.24 Rouge-L 0.8663581223899564 on epoch=174
02/28/2022 19:10:43 - INFO - __main__ - Step 360 Global step 360 Train loss 0.21 on epoch=179
02/28/2022 19:10:45 - INFO - __main__ - Step 370 Global step 370 Train loss 0.21 on epoch=184
02/28/2022 19:10:47 - INFO - __main__ - Step 380 Global step 380 Train loss 0.21 on epoch=189
02/28/2022 19:10:49 - INFO - __main__ - Step 390 Global step 390 Train loss 0.20 on epoch=194
02/28/2022 19:10:51 - INFO - __main__ - Step 400 Global step 400 Train loss 0.20 on epoch=199
02/28/2022 19:11:03 - INFO - __main__ - Global step 400 Train loss 0.21 Rouge-L 0.8608990873575485 on epoch=199
02/28/2022 19:11:05 - INFO - __main__ - Step 410 Global step 410 Train loss 0.19 on epoch=204
02/28/2022 19:11:07 - INFO - __main__ - Step 420 Global step 420 Train loss 0.18 on epoch=209
02/28/2022 19:11:09 - INFO - __main__ - Step 430 Global step 430 Train loss 0.19 on epoch=214
02/28/2022 19:11:12 - INFO - __main__ - Step 440 Global step 440 Train loss 0.19 on epoch=219
02/28/2022 19:11:14 - INFO - __main__ - Step 450 Global step 450 Train loss 0.19 on epoch=224
02/28/2022 19:11:25 - INFO - __main__ - Global step 450 Train loss 0.19 Rouge-L 0.8499094452934937 on epoch=224
02/28/2022 19:11:27 - INFO - __main__ - Step 460 Global step 460 Train loss 0.17 on epoch=229
02/28/2022 19:11:29 - INFO - __main__ - Step 470 Global step 470 Train loss 0.17 on epoch=234
02/28/2022 19:11:31 - INFO - __main__ - Step 480 Global step 480 Train loss 0.17 on epoch=239
02/28/2022 19:11:34 - INFO - __main__ - Step 490 Global step 490 Train loss 0.16 on epoch=244
02/28/2022 19:11:36 - INFO - __main__ - Step 500 Global step 500 Train loss 0.16 on epoch=249
02/28/2022 19:11:47 - INFO - __main__ - Global step 500 Train loss 0.17 Rouge-L 0.8586514537695256 on epoch=249
02/28/2022 19:11:49 - INFO - __main__ - Step 510 Global step 510 Train loss 0.15 on epoch=254
02/28/2022 19:11:51 - INFO - __main__ - Step 520 Global step 520 Train loss 0.14 on epoch=259
02/28/2022 19:11:54 - INFO - __main__ - Step 530 Global step 530 Train loss 0.15 on epoch=264
02/28/2022 19:11:56 - INFO - __main__ - Step 540 Global step 540 Train loss 0.15 on epoch=269
02/28/2022 19:11:58 - INFO - __main__ - Step 550 Global step 550 Train loss 0.15 on epoch=274
02/28/2022 19:12:09 - INFO - __main__ - Global step 550 Train loss 0.15 Rouge-L 0.855160609366467 on epoch=274
02/28/2022 19:12:11 - INFO - __main__ - Step 560 Global step 560 Train loss 0.14 on epoch=279
02/28/2022 19:12:13 - INFO - __main__ - Step 570 Global step 570 Train loss 0.14 on epoch=284
02/28/2022 19:12:15 - INFO - __main__ - Step 580 Global step 580 Train loss 0.15 on epoch=289
02/28/2022 19:12:17 - INFO - __main__ - Step 590 Global step 590 Train loss 0.15 on epoch=294
02/28/2022 19:12:20 - INFO - __main__ - Step 600 Global step 600 Train loss 0.14 on epoch=299
02/28/2022 19:12:31 - INFO - __main__ - Global step 600 Train loss 0.14 Rouge-L 0.8723136163859937 on epoch=299
02/28/2022 19:12:33 - INFO - __main__ - Step 610 Global step 610 Train loss 0.14 on epoch=304
02/28/2022 19:12:35 - INFO - __main__ - Step 620 Global step 620 Train loss 0.12 on epoch=309
02/28/2022 19:12:37 - INFO - __main__ - Step 630 Global step 630 Train loss 0.13 on epoch=314
02/28/2022 19:12:39 - INFO - __main__ - Step 640 Global step 640 Train loss 0.12 on epoch=319
02/28/2022 19:12:42 - INFO - __main__ - Step 650 Global step 650 Train loss 0.12 on epoch=324
02/28/2022 19:12:53 - INFO - __main__ - Global step 650 Train loss 0.13 Rouge-L 0.8669944207233078 on epoch=324
02/28/2022 19:12:55 - INFO - __main__ - Step 660 Global step 660 Train loss 0.13 on epoch=329
02/28/2022 19:12:57 - INFO - __main__ - Step 670 Global step 670 Train loss 0.12 on epoch=334
02/28/2022 19:12:59 - INFO - __main__ - Step 680 Global step 680 Train loss 0.13 on epoch=339
02/28/2022 19:13:01 - INFO - __main__ - Step 690 Global step 690 Train loss 0.10 on epoch=344
02/28/2022 19:13:04 - INFO - __main__ - Step 700 Global step 700 Train loss 0.12 on epoch=349
02/28/2022 19:13:15 - INFO - __main__ - Global step 700 Train loss 0.12 Rouge-L 0.8581095535062648 on epoch=349
02/28/2022 19:13:17 - INFO - __main__ - Step 710 Global step 710 Train loss 0.10 on epoch=354
02/28/2022 19:13:19 - INFO - __main__ - Step 720 Global step 720 Train loss 0.10 on epoch=359
02/28/2022 19:13:21 - INFO - __main__ - Step 730 Global step 730 Train loss 0.10 on epoch=364
02/28/2022 19:13:24 - INFO - __main__ - Step 740 Global step 740 Train loss 0.10 on epoch=369
02/28/2022 19:13:26 - INFO - __main__ - Step 750 Global step 750 Train loss 0.11 on epoch=374
02/28/2022 19:13:37 - INFO - __main__ - Global step 750 Train loss 0.10 Rouge-L 0.8605126620979242 on epoch=374
02/28/2022 19:13:39 - INFO - __main__ - Step 760 Global step 760 Train loss 0.10 on epoch=379
02/28/2022 19:13:42 - INFO - __main__ - Step 770 Global step 770 Train loss 0.10 on epoch=384
02/28/2022 19:13:44 - INFO - __main__ - Step 780 Global step 780 Train loss 0.10 on epoch=389
02/28/2022 19:13:46 - INFO - __main__ - Step 790 Global step 790 Train loss 0.10 on epoch=394
02/28/2022 19:13:48 - INFO - __main__ - Step 800 Global step 800 Train loss 0.10 on epoch=399
02/28/2022 19:13:59 - INFO - __main__ - Global step 800 Train loss 0.10 Rouge-L 0.8541254493372906 on epoch=399
02/28/2022 19:14:01 - INFO - __main__ - Step 810 Global step 810 Train loss 0.11 on epoch=404
02/28/2022 19:14:04 - INFO - __main__ - Step 820 Global step 820 Train loss 0.11 on epoch=409
02/28/2022 19:14:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.10 on epoch=414
02/28/2022 19:14:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.10 on epoch=419
02/28/2022 19:14:10 - INFO - __main__ - Step 850 Global step 850 Train loss 0.09 on epoch=424
02/28/2022 19:14:22 - INFO - __main__ - Global step 850 Train loss 0.10 Rouge-L 0.8565112613618402 on epoch=424
02/28/2022 19:14:24 - INFO - __main__ - Step 860 Global step 860 Train loss 0.09 on epoch=429
02/28/2022 19:14:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.09 on epoch=434
02/28/2022 19:14:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.09 on epoch=439
02/28/2022 19:14:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.08 on epoch=444
02/28/2022 19:14:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.10 on epoch=449
02/28/2022 19:14:44 - INFO - __main__ - Global step 900 Train loss 0.09 Rouge-L 0.8701506912007463 on epoch=449
02/28/2022 19:14:46 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=454
02/28/2022 19:14:48 - INFO - __main__ - Step 920 Global step 920 Train loss 0.12 on epoch=459
02/28/2022 19:14:50 - INFO - __main__ - Step 930 Global step 930 Train loss 0.10 on epoch=464
02/28/2022 19:14:53 - INFO - __main__ - Step 940 Global step 940 Train loss 0.10 on epoch=469
02/28/2022 19:14:55 - INFO - __main__ - Step 950 Global step 950 Train loss 0.09 on epoch=474
02/28/2022 19:15:06 - INFO - __main__ - Global step 950 Train loss 0.10 Rouge-L 0.8520600948942088 on epoch=474
02/28/2022 19:15:08 - INFO - __main__ - Step 960 Global step 960 Train loss 0.08 on epoch=479
02/28/2022 19:15:10 - INFO - __main__ - Step 970 Global step 970 Train loss 0.09 on epoch=484
02/28/2022 19:15:12 - INFO - __main__ - Step 980 Global step 980 Train loss 0.09 on epoch=489
02/28/2022 19:15:15 - INFO - __main__ - Step 990 Global step 990 Train loss 0.08 on epoch=494
02/28/2022 19:15:17 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.07 on epoch=499
02/28/2022 19:15:28 - INFO - __main__ - Global step 1000 Train loss 0.08 Rouge-L 0.8333182761430193 on epoch=499
02/28/2022 19:15:30 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.08 on epoch=504
02/28/2022 19:15:32 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.08 on epoch=509
02/28/2022 19:15:35 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.08 on epoch=514
02/28/2022 19:15:37 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.08 on epoch=519
02/28/2022 19:15:39 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.08 on epoch=524
02/28/2022 19:15:50 - INFO - __main__ - Global step 1050 Train loss 0.08 Rouge-L 0.8630173432831745 on epoch=524
02/28/2022 19:15:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.08 on epoch=529
02/28/2022 19:15:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.08 on epoch=534
02/28/2022 19:15:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.08 on epoch=539
02/28/2022 19:15:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.08 on epoch=544
02/28/2022 19:16:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.07 on epoch=549
02/28/2022 19:16:13 - INFO - __main__ - Global step 1100 Train loss 0.08 Rouge-L 0.860142033258366 on epoch=549
02/28/2022 19:16:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.07 on epoch=554
02/28/2022 19:16:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.07 on epoch=559
02/28/2022 19:16:19 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.06 on epoch=564
02/28/2022 19:16:21 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.07 on epoch=569
02/28/2022 19:16:23 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.06 on epoch=574
02/28/2022 19:16:35 - INFO - __main__ - Global step 1150 Train loss 0.07 Rouge-L 0.8559285519510859 on epoch=574
02/28/2022 19:16:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.07 on epoch=579
02/28/2022 19:16:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.07 on epoch=584
02/28/2022 19:16:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.07 on epoch=589
02/28/2022 19:16:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.07 on epoch=594
02/28/2022 19:16:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.06 on epoch=599
02/28/2022 19:16:57 - INFO - __main__ - Global step 1200 Train loss 0.07 Rouge-L 0.8613238896618562 on epoch=599
02/28/2022 19:17:00 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.06 on epoch=604
02/28/2022 19:17:02 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.07 on epoch=609
02/28/2022 19:17:04 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.07 on epoch=614
02/28/2022 19:17:06 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.06 on epoch=619
02/28/2022 19:17:08 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.07 on epoch=624
02/28/2022 19:17:20 - INFO - __main__ - Global step 1250 Train loss 0.07 Rouge-L 0.8664646596822145 on epoch=624
02/28/2022 19:17:22 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=629
02/28/2022 19:17:24 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.05 on epoch=634
02/28/2022 19:17:26 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.06 on epoch=639
02/28/2022 19:17:28 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.06 on epoch=644
02/28/2022 19:17:30 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.05 on epoch=649
02/28/2022 19:17:42 - INFO - __main__ - Global step 1300 Train loss 0.06 Rouge-L 0.8627395861152345 on epoch=649
02/28/2022 19:17:44 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.05 on epoch=654
02/28/2022 19:17:46 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.06 on epoch=659
02/28/2022 19:17:48 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.06 on epoch=664
02/28/2022 19:17:51 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.06 on epoch=669
02/28/2022 19:17:53 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.05 on epoch=674
02/28/2022 19:18:04 - INFO - __main__ - Global step 1350 Train loss 0.06 Rouge-L 0.8638126603197451 on epoch=674
02/28/2022 19:18:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.06 on epoch=679
02/28/2022 19:18:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.06 on epoch=684
02/28/2022 19:18:11 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.06 on epoch=689
02/28/2022 19:18:13 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.06 on epoch=694
02/28/2022 19:18:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.05 on epoch=699
02/28/2022 19:18:26 - INFO - __main__ - Global step 1400 Train loss 0.06 Rouge-L 0.8559935303265951 on epoch=699
02/28/2022 19:18:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.05 on epoch=704
02/28/2022 19:18:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.06 on epoch=709
02/28/2022 19:18:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.05 on epoch=714
02/28/2022 19:18:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.06 on epoch=719
02/28/2022 19:18:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=724
02/28/2022 19:18:48 - INFO - __main__ - Global step 1450 Train loss 0.06 Rouge-L 0.851728436437093 on epoch=724
02/28/2022 19:18:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.04 on epoch=729
02/28/2022 19:18:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.05 on epoch=734
02/28/2022 19:18:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.04 on epoch=739
02/28/2022 19:18:57 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.06 on epoch=744
02/28/2022 19:18:59 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.05 on epoch=749
02/28/2022 19:19:11 - INFO - __main__ - Global step 1500 Train loss 0.05 Rouge-L 0.8441742815104072 on epoch=749
02/28/2022 19:19:13 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.05 on epoch=754
02/28/2022 19:19:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.05 on epoch=759
02/28/2022 19:19:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.05 on epoch=764
02/28/2022 19:19:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.05 on epoch=769
02/28/2022 19:19:22 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.05 on epoch=774
02/28/2022 19:19:33 - INFO - __main__ - Global step 1550 Train loss 0.05 Rouge-L 0.8443926899839347 on epoch=774
02/28/2022 19:19:35 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.04 on epoch=779
02/28/2022 19:19:38 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.05 on epoch=784
02/28/2022 19:19:40 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.05 on epoch=789
02/28/2022 19:19:42 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.04 on epoch=794
02/28/2022 19:19:44 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.04 on epoch=799
02/28/2022 19:19:56 - INFO - __main__ - Global step 1600 Train loss 0.05 Rouge-L 0.8547234897238972 on epoch=799
02/28/2022 19:19:59 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.05 on epoch=804
02/28/2022 19:20:01 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.05 on epoch=809
02/28/2022 19:20:03 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.05 on epoch=814
02/28/2022 19:20:05 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.04 on epoch=819
02/28/2022 19:20:07 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.05 on epoch=824
02/28/2022 19:20:19 - INFO - __main__ - Global step 1650 Train loss 0.05 Rouge-L 0.8613921715377725 on epoch=824
02/28/2022 19:20:22 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.04 on epoch=829
02/28/2022 19:20:24 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.05 on epoch=834
02/28/2022 19:20:26 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.05 on epoch=839
02/28/2022 19:20:28 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.04 on epoch=844
02/28/2022 19:20:31 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.04 on epoch=849
02/28/2022 19:20:42 - INFO - __main__ - Global step 1700 Train loss 0.04 Rouge-L 0.8643284913320297 on epoch=849
02/28/2022 19:20:45 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.05 on epoch=854
02/28/2022 19:20:47 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.04 on epoch=859
02/28/2022 19:20:49 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.04 on epoch=864
02/28/2022 19:20:51 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.03 on epoch=869
02/28/2022 19:20:53 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.04 on epoch=874
02/28/2022 19:21:05 - INFO - __main__ - Global step 1750 Train loss 0.04 Rouge-L 0.8584651111246736 on epoch=874
02/28/2022 19:21:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.05 on epoch=879
02/28/2022 19:21:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.04 on epoch=884
02/28/2022 19:21:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.05 on epoch=889
02/28/2022 19:21:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.04 on epoch=894
02/28/2022 19:21:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.04 on epoch=899
02/28/2022 19:21:28 - INFO - __main__ - Global step 1800 Train loss 0.04 Rouge-L 0.8545563709672606 on epoch=899
02/28/2022 19:21:30 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.04 on epoch=904
02/28/2022 19:21:32 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=909
02/28/2022 19:21:35 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.05 on epoch=914
02/28/2022 19:21:37 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.04 on epoch=919
02/28/2022 19:21:39 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.04 on epoch=924
02/28/2022 19:21:51 - INFO - __main__ - Global step 1850 Train loss 0.04 Rouge-L 0.8638618591660923 on epoch=924
02/28/2022 19:21:53 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.04 on epoch=929
02/28/2022 19:21:55 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.04 on epoch=934
02/28/2022 19:21:57 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.04 on epoch=939
02/28/2022 19:21:59 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.03 on epoch=944
02/28/2022 19:22:02 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.04 on epoch=949
02/28/2022 19:22:14 - INFO - __main__ - Global step 1900 Train loss 0.04 Rouge-L 0.8470480907223514 on epoch=949
02/28/2022 19:22:16 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.03 on epoch=954
02/28/2022 19:22:18 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.03 on epoch=959
02/28/2022 19:22:20 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.04 on epoch=964
02/28/2022 19:22:22 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.04 on epoch=969
02/28/2022 19:22:25 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.04 on epoch=974
02/28/2022 19:22:37 - INFO - __main__ - Global step 1950 Train loss 0.03 Rouge-L 0.8554069455495148 on epoch=974
02/28/2022 19:22:39 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.03 on epoch=979
02/28/2022 19:22:41 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.03 on epoch=984
02/28/2022 19:22:43 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.04 on epoch=989
02/28/2022 19:22:45 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.03 on epoch=994
02/28/2022 19:22:48 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.03 on epoch=999
02/28/2022 19:22:49 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 19:22:49 - INFO - __main__ - Printing 3 examples
02/28/2022 19:22:49 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
02/28/2022 19:22:49 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
02/28/2022 19:22:49 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
02/28/2022 19:22:49 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
02/28/2022 19:22:49 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
02/28/2022 19:22:49 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
02/28/2022 19:22:49 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 19:22:49 - INFO - __main__ - Tokenizing Output ...
02/28/2022 19:22:49 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 19:22:49 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 19:22:49 - INFO - __main__ - Printing 3 examples
02/28/2022 19:22:49 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
02/28/2022 19:22:49 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
02/28/2022 19:22:49 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
02/28/2022 19:22:49 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
02/28/2022 19:22:49 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
02/28/2022 19:22:49 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
02/28/2022 19:22:49 - INFO - __main__ - Tokenizing Input ...
02/28/2022 19:22:49 - INFO - __main__ - Tokenizing Output ...
02/28/2022 19:22:49 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 19:22:59 - INFO - __main__ - Global step 2000 Train loss 0.03 Rouge-L 0.863770815652483 on epoch=999
02/28/2022 19:23:00 - INFO - __main__ - save last model!
02/28/2022 19:23:00 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/28/2022 19:23:00 - INFO - __main__ - Start tokenizing ... 5000 instances
02/28/2022 19:23:00 - INFO - __main__ - Printing 3 examples
02/28/2022 19:23:00 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/28/2022 19:23:00 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/28/2022 19:23:00 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/28/2022 19:23:00 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/28/2022 19:23:00 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/28/2022 19:23:00 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/28/2022 19:23:00 - INFO - __main__ - Tokenizing Input ...
02/28/2022 19:23:01 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 19:23:02 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 19:23:02 - INFO - __main__ - Starting training!
02/28/2022 19:23:02 - INFO - __main__ - Tokenizing Output ...
02/28/2022 19:23:08 - INFO - __main__ - Loaded 5000 examples from test data
02/28/2022 19:52:34 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split/wiki_split_32_13_0.5_8_predictions.txt
02/28/2022 19:52:39 - INFO - __main__ - Rouge-L on test data: 0.8807
02/28/2022 19:52:40 - INFO - __main__ - prefix=wiki_split_32_13, lr=0.5, bsz=8, dev_performance=0.8746627096990172, test_performance=0.8806977680957782
02/28/2022 19:52:40 - INFO - __main__ - Running ... prefix=wiki_split_32_13, lr=0.4, bsz=8 ...
02/28/2022 19:52:41 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 19:52:41 - INFO - __main__ - Printing 3 examples
02/28/2022 19:52:41 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
02/28/2022 19:52:41 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
02/28/2022 19:52:41 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
02/28/2022 19:52:41 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
02/28/2022 19:52:41 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
02/28/2022 19:52:41 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
02/28/2022 19:52:41 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/28/2022 19:52:41 - INFO - __main__ - Tokenizing Output ...
02/28/2022 19:52:41 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 19:52:41 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 19:52:41 - INFO - __main__ - Printing 3 examples
02/28/2022 19:52:41 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
02/28/2022 19:52:41 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
02/28/2022 19:52:41 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
02/28/2022 19:52:41 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
02/28/2022 19:52:41 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
02/28/2022 19:52:41 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
02/28/2022 19:52:41 - INFO - __main__ - Tokenizing Input ...
02/28/2022 19:52:41 - INFO - __main__ - Tokenizing Output ...
02/28/2022 19:52:41 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 19:52:55 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 19:52:56 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 19:52:56 - INFO - __main__ - Starting training!
02/28/2022 19:52:59 - INFO - __main__ - Step 10 Global step 10 Train loss 0.75 on epoch=4
02/28/2022 19:53:01 - INFO - __main__ - Step 20 Global step 20 Train loss 0.62 on epoch=9
02/28/2022 19:53:03 - INFO - __main__ - Step 30 Global step 30 Train loss 0.58 on epoch=14
02/28/2022 19:53:05 - INFO - __main__ - Step 40 Global step 40 Train loss 0.57 on epoch=19
02/28/2022 19:53:07 - INFO - __main__ - Step 50 Global step 50 Train loss 0.54 on epoch=24
02/28/2022 19:53:17 - INFO - __main__ - Global step 50 Train loss 0.61 Rouge-L 0.7852017509558122 on epoch=24
02/28/2022 19:53:17 - INFO - __main__ - Saving model with best Rouge-L: -1.0 -> 0.7852017509558122 on epoch=24, global_step=50
02/28/2022 19:53:19 - INFO - __main__ - Step 60 Global step 60 Train loss 0.51 on epoch=29
02/28/2022 19:53:21 - INFO - __main__ - Step 70 Global step 70 Train loss 0.52 on epoch=34
02/28/2022 19:53:24 - INFO - __main__ - Step 80 Global step 80 Train loss 0.48 on epoch=39
02/28/2022 19:53:26 - INFO - __main__ - Step 90 Global step 90 Train loss 0.47 on epoch=44
02/28/2022 19:53:28 - INFO - __main__ - Step 100 Global step 100 Train loss 0.47 on epoch=49
02/28/2022 19:53:39 - INFO - __main__ - Global step 100 Train loss 0.49 Rouge-L 0.8446381482112727 on epoch=49
02/28/2022 19:53:39 - INFO - __main__ - Saving model with best Rouge-L: 0.7852017509558122 -> 0.8446381482112727 on epoch=49, global_step=100
02/28/2022 19:53:41 - INFO - __main__ - Step 110 Global step 110 Train loss 0.46 on epoch=54
02/28/2022 19:53:43 - INFO - __main__ - Step 120 Global step 120 Train loss 0.45 on epoch=59
02/28/2022 19:53:45 - INFO - __main__ - Step 130 Global step 130 Train loss 0.43 on epoch=64
02/28/2022 19:53:47 - INFO - __main__ - Step 140 Global step 140 Train loss 0.44 on epoch=69
02/28/2022 19:53:49 - INFO - __main__ - Step 150 Global step 150 Train loss 0.43 on epoch=74
02/28/2022 19:54:00 - INFO - __main__ - Global step 150 Train loss 0.44 Rouge-L 0.8641762198139671 on epoch=74
02/28/2022 19:54:00 - INFO - __main__ - Saving model with best Rouge-L: 0.8446381482112727 -> 0.8641762198139671 on epoch=74, global_step=150
02/28/2022 19:54:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.42 on epoch=79
02/28/2022 19:54:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.40 on epoch=84
02/28/2022 19:54:06 - INFO - __main__ - Step 180 Global step 180 Train loss 0.41 on epoch=89
02/28/2022 19:54:08 - INFO - __main__ - Step 190 Global step 190 Train loss 0.41 on epoch=94
02/28/2022 19:54:10 - INFO - __main__ - Step 200 Global step 200 Train loss 0.40 on epoch=99
02/28/2022 19:54:20 - INFO - __main__ - Global step 200 Train loss 0.41 Rouge-L 0.8491368971430648 on epoch=99
02/28/2022 19:54:22 - INFO - __main__ - Step 210 Global step 210 Train loss 0.39 on epoch=104
02/28/2022 19:54:25 - INFO - __main__ - Step 220 Global step 220 Train loss 0.37 on epoch=109
02/28/2022 19:54:27 - INFO - __main__ - Step 230 Global step 230 Train loss 0.39 on epoch=114
02/28/2022 19:54:29 - INFO - __main__ - Step 240 Global step 240 Train loss 0.37 on epoch=119
02/28/2022 19:54:31 - INFO - __main__ - Step 250 Global step 250 Train loss 0.36 on epoch=124
02/28/2022 19:54:42 - INFO - __main__ - Global step 250 Train loss 0.38 Rouge-L 0.8623723581570346 on epoch=124
02/28/2022 19:54:45 - INFO - __main__ - Step 260 Global step 260 Train loss 0.36 on epoch=129
02/28/2022 19:54:47 - INFO - __main__ - Step 270 Global step 270 Train loss 0.35 on epoch=134
02/28/2022 19:54:49 - INFO - __main__ - Step 280 Global step 280 Train loss 0.34 on epoch=139
02/28/2022 19:54:51 - INFO - __main__ - Step 290 Global step 290 Train loss 0.35 on epoch=144
02/28/2022 19:54:53 - INFO - __main__ - Step 300 Global step 300 Train loss 0.33 on epoch=149
02/28/2022 19:55:05 - INFO - __main__ - Global step 300 Train loss 0.34 Rouge-L 0.8698299182608886 on epoch=149
02/28/2022 19:55:05 - INFO - __main__ - Saving model with best Rouge-L: 0.8641762198139671 -> 0.8698299182608886 on epoch=149, global_step=300
02/28/2022 19:55:07 - INFO - __main__ - Step 310 Global step 310 Train loss 0.33 on epoch=154
02/28/2022 19:55:09 - INFO - __main__ - Step 320 Global step 320 Train loss 0.33 on epoch=159
02/28/2022 19:55:11 - INFO - __main__ - Step 330 Global step 330 Train loss 0.31 on epoch=164
02/28/2022 19:55:13 - INFO - __main__ - Step 340 Global step 340 Train loss 0.29 on epoch=169
02/28/2022 19:55:15 - INFO - __main__ - Step 350 Global step 350 Train loss 0.30 on epoch=174
02/28/2022 19:55:27 - INFO - __main__ - Global step 350 Train loss 0.31 Rouge-L 0.8717227186384602 on epoch=174
02/28/2022 19:55:27 - INFO - __main__ - Saving model with best Rouge-L: 0.8698299182608886 -> 0.8717227186384602 on epoch=174, global_step=350
02/28/2022 19:55:29 - INFO - __main__ - Step 360 Global step 360 Train loss 0.27 on epoch=179
02/28/2022 19:55:31 - INFO - __main__ - Step 370 Global step 370 Train loss 0.29 on epoch=184
02/28/2022 19:55:33 - INFO - __main__ - Step 380 Global step 380 Train loss 0.26 on epoch=189
02/28/2022 19:55:35 - INFO - __main__ - Step 390 Global step 390 Train loss 0.27 on epoch=194
02/28/2022 19:55:38 - INFO - __main__ - Step 400 Global step 400 Train loss 0.26 on epoch=199
02/28/2022 19:55:49 - INFO - __main__ - Global step 400 Train loss 0.27 Rouge-L 0.8730637339541822 on epoch=199
02/28/2022 19:55:49 - INFO - __main__ - Saving model with best Rouge-L: 0.8717227186384602 -> 0.8730637339541822 on epoch=199, global_step=400
02/28/2022 19:55:51 - INFO - __main__ - Step 410 Global step 410 Train loss 0.26 on epoch=204
02/28/2022 19:55:53 - INFO - __main__ - Step 420 Global step 420 Train loss 0.24 on epoch=209
02/28/2022 19:55:56 - INFO - __main__ - Step 430 Global step 430 Train loss 0.24 on epoch=214
02/28/2022 19:55:58 - INFO - __main__ - Step 440 Global step 440 Train loss 0.24 on epoch=219
02/28/2022 19:56:00 - INFO - __main__ - Step 450 Global step 450 Train loss 0.22 on epoch=224
02/28/2022 19:56:11 - INFO - __main__ - Global step 450 Train loss 0.24 Rouge-L 0.8555196234076416 on epoch=224
02/28/2022 19:56:13 - INFO - __main__ - Step 460 Global step 460 Train loss 0.24 on epoch=229
02/28/2022 19:56:16 - INFO - __main__ - Step 470 Global step 470 Train loss 0.23 on epoch=234
02/28/2022 19:56:18 - INFO - __main__ - Step 480 Global step 480 Train loss 0.22 on epoch=239
02/28/2022 19:56:20 - INFO - __main__ - Step 490 Global step 490 Train loss 0.23 on epoch=244
02/28/2022 19:56:22 - INFO - __main__ - Step 500 Global step 500 Train loss 0.22 on epoch=249
02/28/2022 19:56:33 - INFO - __main__ - Global step 500 Train loss 0.23 Rouge-L 0.861154542314654 on epoch=249
02/28/2022 19:56:35 - INFO - __main__ - Step 510 Global step 510 Train loss 0.22 on epoch=254
02/28/2022 19:56:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.21 on epoch=259
02/28/2022 19:56:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.20 on epoch=264
02/28/2022 19:56:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.22 on epoch=269
02/28/2022 19:56:44 - INFO - __main__ - Step 550 Global step 550 Train loss 0.21 on epoch=274
02/28/2022 19:56:56 - INFO - __main__ - Global step 550 Train loss 0.21 Rouge-L 0.8751515399863982 on epoch=274
02/28/2022 19:56:56 - INFO - __main__ - Saving model with best Rouge-L: 0.8730637339541822 -> 0.8751515399863982 on epoch=274, global_step=550
02/28/2022 19:56:58 - INFO - __main__ - Step 560 Global step 560 Train loss 0.20 on epoch=279
02/28/2022 19:57:00 - INFO - __main__ - Step 570 Global step 570 Train loss 0.20 on epoch=284
02/28/2022 19:57:02 - INFO - __main__ - Step 580 Global step 580 Train loss 0.19 on epoch=289
02/28/2022 19:57:04 - INFO - __main__ - Step 590 Global step 590 Train loss 0.19 on epoch=294
02/28/2022 19:57:06 - INFO - __main__ - Step 600 Global step 600 Train loss 0.18 on epoch=299
02/28/2022 19:57:18 - INFO - __main__ - Global step 600 Train loss 0.19 Rouge-L 0.8640855246762518 on epoch=299
02/28/2022 19:57:20 - INFO - __main__ - Step 610 Global step 610 Train loss 0.18 on epoch=304
02/28/2022 19:57:22 - INFO - __main__ - Step 620 Global step 620 Train loss 0.18 on epoch=309
02/28/2022 19:57:24 - INFO - __main__ - Step 630 Global step 630 Train loss 0.18 on epoch=314
02/28/2022 19:57:26 - INFO - __main__ - Step 640 Global step 640 Train loss 0.17 on epoch=319
02/28/2022 19:57:28 - INFO - __main__ - Step 650 Global step 650 Train loss 0.17 on epoch=324
02/28/2022 19:57:40 - INFO - __main__ - Global step 650 Train loss 0.18 Rouge-L 0.879912452211592 on epoch=324
02/28/2022 19:57:40 - INFO - __main__ - Saving model with best Rouge-L: 0.8751515399863982 -> 0.879912452211592 on epoch=324, global_step=650
02/28/2022 19:57:42 - INFO - __main__ - Step 660 Global step 660 Train loss 0.17 on epoch=329
02/28/2022 19:57:44 - INFO - __main__ - Step 670 Global step 670 Train loss 0.16 on epoch=334
02/28/2022 19:57:46 - INFO - __main__ - Step 680 Global step 680 Train loss 0.17 on epoch=339
02/28/2022 19:57:49 - INFO - __main__ - Step 690 Global step 690 Train loss 0.16 on epoch=344
02/28/2022 19:57:51 - INFO - __main__ - Step 700 Global step 700 Train loss 0.16 on epoch=349
02/28/2022 19:58:02 - INFO - __main__ - Global step 700 Train loss 0.16 Rouge-L 0.8606405966383368 on epoch=349
02/28/2022 19:58:04 - INFO - __main__ - Step 710 Global step 710 Train loss 0.16 on epoch=354
02/28/2022 19:58:06 - INFO - __main__ - Step 720 Global step 720 Train loss 0.16 on epoch=359
02/28/2022 19:58:08 - INFO - __main__ - Step 730 Global step 730 Train loss 0.15 on epoch=364
02/28/2022 19:58:11 - INFO - __main__ - Step 740 Global step 740 Train loss 0.16 on epoch=369
02/28/2022 19:58:13 - INFO - __main__ - Step 750 Global step 750 Train loss 0.14 on epoch=374
02/28/2022 19:58:24 - INFO - __main__ - Global step 750 Train loss 0.15 Rouge-L 0.8757283448878447 on epoch=374
02/28/2022 19:58:26 - INFO - __main__ - Step 760 Global step 760 Train loss 0.13 on epoch=379
02/28/2022 19:58:28 - INFO - __main__ - Step 770 Global step 770 Train loss 0.15 on epoch=384
02/28/2022 19:58:30 - INFO - __main__ - Step 780 Global step 780 Train loss 0.15 on epoch=389
02/28/2022 19:58:32 - INFO - __main__ - Step 790 Global step 790 Train loss 0.14 on epoch=394
02/28/2022 19:58:34 - INFO - __main__ - Step 800 Global step 800 Train loss 0.13 on epoch=399
02/28/2022 19:58:46 - INFO - __main__ - Global step 800 Train loss 0.14 Rouge-L 0.8701450367178265 on epoch=399
02/28/2022 19:58:48 - INFO - __main__ - Step 810 Global step 810 Train loss 0.15 on epoch=404
02/28/2022 19:58:50 - INFO - __main__ - Step 820 Global step 820 Train loss 0.14 on epoch=409
02/28/2022 19:58:52 - INFO - __main__ - Step 830 Global step 830 Train loss 0.13 on epoch=414
02/28/2022 19:58:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.13 on epoch=419
02/28/2022 19:58:57 - INFO - __main__ - Step 850 Global step 850 Train loss 0.13 on epoch=424
02/28/2022 19:59:08 - INFO - __main__ - Global step 850 Train loss 0.13 Rouge-L 0.8685464171630277 on epoch=424
02/28/2022 19:59:10 - INFO - __main__ - Step 860 Global step 860 Train loss 0.12 on epoch=429
02/28/2022 19:59:12 - INFO - __main__ - Step 870 Global step 870 Train loss 0.13 on epoch=434
02/28/2022 19:59:15 - INFO - __main__ - Step 880 Global step 880 Train loss 0.12 on epoch=439
02/28/2022 19:59:17 - INFO - __main__ - Step 890 Global step 890 Train loss 0.12 on epoch=444
02/28/2022 19:59:19 - INFO - __main__ - Step 900 Global step 900 Train loss 0.11 on epoch=449
02/28/2022 19:59:30 - INFO - __main__ - Global step 900 Train loss 0.12 Rouge-L 0.8697992657400362 on epoch=449
02/28/2022 19:59:33 - INFO - __main__ - Step 910 Global step 910 Train loss 0.11 on epoch=454
02/28/2022 19:59:35 - INFO - __main__ - Step 920 Global step 920 Train loss 0.11 on epoch=459
02/28/2022 19:59:37 - INFO - __main__ - Step 930 Global step 930 Train loss 0.13 on epoch=464
02/28/2022 19:59:39 - INFO - __main__ - Step 940 Global step 940 Train loss 0.12 on epoch=469
02/28/2022 19:59:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.12 on epoch=474
02/28/2022 19:59:53 - INFO - __main__ - Global step 950 Train loss 0.12 Rouge-L 0.8735565547025053 on epoch=474
02/28/2022 19:59:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.11 on epoch=479
02/28/2022 19:59:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.11 on epoch=484
02/28/2022 19:59:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.12 on epoch=489
02/28/2022 20:00:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.11 on epoch=494
02/28/2022 20:00:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.11 on epoch=499
02/28/2022 20:00:15 - INFO - __main__ - Global step 1000 Train loss 0.11 Rouge-L 0.8547571010721386 on epoch=499
02/28/2022 20:00:17 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.11 on epoch=504
02/28/2022 20:00:19 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.11 on epoch=509
02/28/2022 20:00:22 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.10 on epoch=514
02/28/2022 20:00:24 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.10 on epoch=519
02/28/2022 20:00:26 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.11 on epoch=524
02/28/2022 20:00:37 - INFO - __main__ - Global step 1050 Train loss 0.11 Rouge-L 0.8719248675679402 on epoch=524
02/28/2022 20:00:39 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.09 on epoch=529
02/28/2022 20:00:42 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.11 on epoch=534
02/28/2022 20:00:44 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.10 on epoch=539
02/28/2022 20:00:46 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.10 on epoch=544
02/28/2022 20:00:48 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.09 on epoch=549
02/28/2022 20:01:00 - INFO - __main__ - Global step 1100 Train loss 0.10 Rouge-L 0.868890296878006 on epoch=549
02/28/2022 20:01:02 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.10 on epoch=554
02/28/2022 20:01:04 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.10 on epoch=559
02/28/2022 20:01:06 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.09 on epoch=564
02/28/2022 20:01:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.10 on epoch=569
02/28/2022 20:01:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.09 on epoch=574
02/28/2022 20:01:22 - INFO - __main__ - Global step 1150 Train loss 0.09 Rouge-L 0.875590401887056 on epoch=574
02/28/2022 20:01:24 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.08 on epoch=579
02/28/2022 20:01:26 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.09 on epoch=584
02/28/2022 20:01:28 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.10 on epoch=589
02/28/2022 20:01:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.08 on epoch=594
02/28/2022 20:01:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.09 on epoch=599
02/28/2022 20:01:44 - INFO - __main__ - Global step 1200 Train loss 0.09 Rouge-L 0.8719066317269619 on epoch=599
02/28/2022 20:01:46 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.09 on epoch=604
02/28/2022 20:01:49 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.09 on epoch=609
02/28/2022 20:01:51 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.08 on epoch=614
02/28/2022 20:01:53 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.09 on epoch=619
02/28/2022 20:01:55 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.09 on epoch=624
02/28/2022 20:02:07 - INFO - __main__ - Global step 1250 Train loss 0.09 Rouge-L 0.8696415844248433 on epoch=624
02/28/2022 20:02:09 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.07 on epoch=629
02/28/2022 20:02:11 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.09 on epoch=634
02/28/2022 20:02:13 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.08 on epoch=639
02/28/2022 20:02:15 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.09 on epoch=644
02/28/2022 20:02:18 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.09 on epoch=649
02/28/2022 20:02:29 - INFO - __main__ - Global step 1300 Train loss 0.08 Rouge-L 0.8631413707754592 on epoch=649
02/28/2022 20:02:31 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.09 on epoch=654
02/28/2022 20:02:34 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.08 on epoch=659
02/28/2022 20:02:36 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.08 on epoch=664
02/28/2022 20:02:38 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.08 on epoch=669
02/28/2022 20:02:40 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.07 on epoch=674
02/28/2022 20:02:52 - INFO - __main__ - Global step 1350 Train loss 0.08 Rouge-L 0.8550655404964389 on epoch=674
02/28/2022 20:02:54 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.08 on epoch=679
02/28/2022 20:02:56 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.07 on epoch=684
02/28/2022 20:02:58 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.08 on epoch=689
02/28/2022 20:03:00 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.08 on epoch=694
02/28/2022 20:03:03 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.06 on epoch=699
02/28/2022 20:03:14 - INFO - __main__ - Global step 1400 Train loss 0.07 Rouge-L 0.8652028290798722 on epoch=699
02/28/2022 20:03:16 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.07 on epoch=704
02/28/2022 20:03:18 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.08 on epoch=709
02/28/2022 20:03:21 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.08 on epoch=714
02/28/2022 20:03:23 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.07 on epoch=719
02/28/2022 20:03:25 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.07 on epoch=724
02/28/2022 20:03:40 - INFO - __main__ - Global step 1450 Train loss 0.07 Rouge-L 0.8607730527392561 on epoch=724
02/28/2022 20:03:42 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.08 on epoch=729
02/28/2022 20:03:44 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.08 on epoch=734
02/28/2022 20:03:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.06 on epoch=739
02/28/2022 20:03:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.07 on epoch=744
02/28/2022 20:03:51 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.08 on epoch=749
02/28/2022 20:04:02 - INFO - __main__ - Global step 1500 Train loss 0.07 Rouge-L 0.861456890838852 on epoch=749
02/28/2022 20:04:04 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.07 on epoch=754
02/28/2022 20:04:06 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.07 on epoch=759
02/28/2022 20:04:09 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.06 on epoch=764
02/28/2022 20:04:11 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.07 on epoch=769
02/28/2022 20:04:13 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.07 on epoch=774
02/28/2022 20:04:24 - INFO - __main__ - Global step 1550 Train loss 0.07 Rouge-L 0.8624446574396673 on epoch=774
02/28/2022 20:04:27 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.07 on epoch=779
02/28/2022 20:04:29 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.06 on epoch=784
02/28/2022 20:04:31 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.07 on epoch=789
02/28/2022 20:04:33 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.07 on epoch=794
02/28/2022 20:04:35 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.07 on epoch=799
02/28/2022 20:04:47 - INFO - __main__ - Global step 1600 Train loss 0.07 Rouge-L 0.8694953922448276 on epoch=799
02/28/2022 20:04:49 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=804
02/28/2022 20:04:51 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.07 on epoch=809
02/28/2022 20:04:53 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.07 on epoch=814
02/28/2022 20:04:55 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.06 on epoch=819
02/28/2022 20:04:57 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.07 on epoch=824
02/28/2022 20:05:09 - INFO - __main__ - Global step 1650 Train loss 0.07 Rouge-L 0.8495296307266951 on epoch=824
02/28/2022 20:05:11 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.06 on epoch=829
02/28/2022 20:05:13 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.06 on epoch=834
02/28/2022 20:05:15 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.06 on epoch=839
02/28/2022 20:05:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.07 on epoch=844
02/28/2022 20:05:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.07 on epoch=849
02/28/2022 20:05:31 - INFO - __main__ - Global step 1700 Train loss 0.07 Rouge-L 0.8649684230202999 on epoch=849
02/28/2022 20:05:34 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.07 on epoch=854
02/28/2022 20:05:36 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.06 on epoch=859
02/28/2022 20:05:38 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.06 on epoch=864
02/28/2022 20:05:40 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.06 on epoch=869
02/28/2022 20:05:42 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.07 on epoch=874
02/28/2022 20:05:54 - INFO - __main__ - Global step 1750 Train loss 0.06 Rouge-L 0.8497176790229396 on epoch=874
02/28/2022 20:05:56 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=879
02/28/2022 20:05:58 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.07 on epoch=884
02/28/2022 20:06:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.07 on epoch=889
02/28/2022 20:06:03 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.06 on epoch=894
02/28/2022 20:06:05 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.06 on epoch=899
02/28/2022 20:06:17 - INFO - __main__ - Global step 1800 Train loss 0.07 Rouge-L 0.8471046463955161 on epoch=899
02/28/2022 20:06:19 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.06 on epoch=904
02/28/2022 20:06:21 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.05 on epoch=909
02/28/2022 20:06:23 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=914
02/28/2022 20:06:25 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.06 on epoch=919
02/28/2022 20:06:27 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.06 on epoch=924
02/28/2022 20:06:39 - INFO - __main__ - Global step 1850 Train loss 0.06 Rouge-L 0.8473480436427766 on epoch=924
02/28/2022 20:06:41 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
02/28/2022 20:06:43 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.07 on epoch=934
02/28/2022 20:06:45 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.06 on epoch=939
02/28/2022 20:06:48 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.06 on epoch=944
02/28/2022 20:06:50 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.06 on epoch=949
02/28/2022 20:07:03 - INFO - __main__ - Global step 1900 Train loss 0.06 Rouge-L 0.8676394218772729 on epoch=949
02/28/2022 20:07:05 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.06 on epoch=954
02/28/2022 20:07:07 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.06 on epoch=959
02/28/2022 20:07:09 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=964
02/28/2022 20:07:11 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.06 on epoch=969
02/28/2022 20:07:14 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.05 on epoch=974
02/28/2022 20:07:25 - INFO - __main__ - Global step 1950 Train loss 0.06 Rouge-L 0.8478616188845381 on epoch=974
02/28/2022 20:07:27 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.06 on epoch=979
02/28/2022 20:07:30 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.05 on epoch=984
02/28/2022 20:07:32 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.05 on epoch=989
02/28/2022 20:07:34 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=994
02/28/2022 20:07:36 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.05 on epoch=999
02/28/2022 20:07:37 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 20:07:37 - INFO - __main__ - Printing 3 examples
02/28/2022 20:07:37 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
02/28/2022 20:07:37 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
02/28/2022 20:07:37 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
02/28/2022 20:07:37 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
02/28/2022 20:07:37 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
02/28/2022 20:07:37 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
02/28/2022 20:07:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 20:07:37 - INFO - __main__ - Tokenizing Output ...
02/28/2022 20:07:38 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 20:07:38 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 20:07:38 - INFO - __main__ - Printing 3 examples
02/28/2022 20:07:38 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
02/28/2022 20:07:38 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
02/28/2022 20:07:38 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
02/28/2022 20:07:38 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
02/28/2022 20:07:38 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
02/28/2022 20:07:38 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
02/28/2022 20:07:38 - INFO - __main__ - Tokenizing Input ...
02/28/2022 20:07:38 - INFO - __main__ - Tokenizing Output ...
02/28/2022 20:07:38 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 20:07:48 - INFO - __main__ - Global step 2000 Train loss 0.05 Rouge-L 0.8659857651935414 on epoch=999
02/28/2022 20:07:48 - INFO - __main__ - save last model!
02/28/2022 20:07:48 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/28/2022 20:07:48 - INFO - __main__ - Start tokenizing ... 5000 instances
02/28/2022 20:07:48 - INFO - __main__ - Printing 3 examples
02/28/2022 20:07:48 - INFO - __main__ -  [wiki_split] sentence 1: ' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , is the daughter of Jim Crace who is a contemporary English writer .  [SEP] sentence 2:  Crace currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' .
02/28/2022 20:07:48 - INFO - __main__ - ["' Lauren Rose Crace ' , born 25th of May 1986 in Birmingham , England , currently plays the part of '' Ronnie Mitchell 's '' long lost daughter Danielle Jones ( Amy ) in the soap opera '' EastEnders '' ."]
02/28/2022 20:07:48 - INFO - __main__ -  [wiki_split] sentence 1: '' A Living Library '' was Sherk 's work that consisted of transforming buried urban streams and asphalted public spaces into thriving art gardens .  [SEP] sentence 2:  She transformed these spaces for to build educations centers for children in communities in San Francisco and New York City .
02/28/2022 20:07:48 - INFO - __main__ - ["'' A Living Library '' was Bonnie Sherk 's work of transforming buried urban streams and asphalted public spaces into thriving educational art gardens for communities in San Francisco and New York City ."]
02/28/2022 20:07:48 - INFO - __main__ -  [wiki_split] sentence 1: '' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season .  [SEP] sentence 2:  It originally aired on the Fox network in the United States on January 4 , 1998 .
02/28/2022 20:07:48 - INFO - __main__ - ["'' All Singing , All Dancing '' is the eleventh episode of '' The Simpsons '' ' ninth season and originally aired on the Fox network on January 4 , 1998 ."]
02/28/2022 20:07:48 - INFO - __main__ - Tokenizing Input ...
02/28/2022 20:07:50 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 20:07:50 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 20:07:51 - INFO - __main__ - Starting training!
02/28/2022 20:07:51 - INFO - __main__ - Tokenizing Output ...
02/28/2022 20:07:56 - INFO - __main__ - Loaded 5000 examples from test data
[E ProcessGroupNCCL.cpp:566] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1801128 milliseconds before timing out.
02/28/2022 20:37:56 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split/wiki_split_32_13_0.4_8_predictions.txt
02/28/2022 20:38:02 - INFO - __main__ - Rouge-L on test data: 0.8840
02/28/2022 20:38:02 - INFO - __main__ - prefix=wiki_split_32_13, lr=0.4, bsz=8, dev_performance=0.879912452211592, test_performance=0.8839925795308518
02/28/2022 20:38:02 - INFO - __main__ - Running ... prefix=wiki_split_32_13, lr=0.3, bsz=8 ...
02/28/2022 20:38:03 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 20:38:03 - INFO - __main__ - Printing 3 examples
02/28/2022 20:38:03 - INFO - __main__ -  [wiki_split] sentence 1: It was the last game of the year .  [SEP] sentence 2:  Lenhovda was taking on ? on their home ground in front of 1,100 attendants .
02/28/2022 20:38:03 - INFO - __main__ - ['It was the last game of the year , when Lenhovda played a game on their home ground in front of 1,100 attendants .']
02/28/2022 20:38:03 - INFO - __main__ -  [wiki_split] sentence 1: His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur .  [SEP] sentence 2:  Naveen completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 .
02/28/2022 20:38:03 - INFO - __main__ - ["His father Dr. Sachinand Tiwari was a Professor at Indian Institute of Technology , Kanpur India and completed his Bachelor 's degree in Mechanical Engineering from Indian Institute of Technology , Kanpur ( IIT ) in 2000 ."]
02/28/2022 20:38:03 - INFO - __main__ -  [wiki_split] sentence 1: The Desolation of Smaug '' is a 2013 epic fantasy adventure film directed by Peter Jackson .  [SEP] sentence 2:  It was produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films .
02/28/2022 20:38:03 - INFO - __main__ - ["The Desolation of Smaug '' is a 2013 epic fantasy adventure film produced by New Line Cinema and Metro - Goldwyn - Mayer in collaboration with Wingnut Films ."]
02/28/2022 20:38:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/28/2022 20:38:03 - INFO - __main__ - Tokenizing Output ...
02/28/2022 20:38:03 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 20:38:03 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 20:38:03 - INFO - __main__ - Printing 3 examples
02/28/2022 20:38:03 - INFO - __main__ -  [wiki_split] sentence 1: She becomes angry and files a divorce petition in court .  [SEP] sentence 2:  However , the next court hearing takes place in six months , and meanwhile Aastha is ordered to stay with Shlok .
02/28/2022 20:38:03 - INFO - __main__ - ['She becomes angry and files a divorce petition in the court and the next hearing in court is delayed to six months , and Aastha is ordered to stay with Shlok .']
02/28/2022 20:38:03 - INFO - __main__ -  [wiki_split] sentence 1: He narrowly avoids being seen by a crowd of pale - skinned humanoids .  [SEP] sentence 2:  These creatures possess heightened senses of smell and strength and wield primitive spears .
02/28/2022 20:38:03 - INFO - __main__ - ['He narrowly avoids being seen by a crowd of pale - skinned humanoids with heightened senses of smell and strength and wield primitive spears .']
02/28/2022 20:38:03 - INFO - __main__ -  [wiki_split] sentence 1: In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz .  [SEP] sentence 2:  In a letter to Robert Erskine , physician and advisor to Russian Tsar Peter the Great , Leibniz later wrote that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention .
02/28/2022 20:38:03 - INFO - __main__ - ["In Draschwitz , Bessler received a visit from the eminent mathematician Gottfried Wilhelm Leibniz , who later wrote to Russian Tsar Peter the Great 's physician that Bessler was '' one of my friends '' and that he believed Bessler 's wheel to be a valuable invention ."]
02/28/2022 20:38:03 - INFO - __main__ - Tokenizing Input ...
02/28/2022 20:38:03 - INFO - __main__ - Tokenizing Output ...
02/28/2022 20:38:03 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 20:38:17 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 20:38:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 20:38:18 - INFO - __main__ - Starting training!
[E ProcessGroupNCCL.cpp:325] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1801128 milliseconds before timing out.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 2166) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_xpb7p_tn/none_36f3hzer/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_xpb7p_tn/none_36f3hzer/attempt_1/1/error.json
Output directory () already exists and is not empty.
02/28/2022 20:42:54 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 20:42:54 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
02/28/2022 20:42:54 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 20:42:54 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
02/28/2022 20:42:54 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
02/28/2022 20:42:54 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
02/28/2022 20:43:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:43:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:44:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:45:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:46:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:47:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:48:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:49:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:50:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:51:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:52:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:53:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:54:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:55:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:56:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:57:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:58:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 20:59:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:00:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:01:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:02:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:03:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:04:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:05:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:06:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:07:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:08:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:09:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:10:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:11:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 21:12:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2543) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_xpb7p_tn/none_36f3hzer/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_xpb7p_tn/none_36f3hzer/attempt_2/1/error.json
Output directory () already exists and is not empty.
02/28/2022 21:13:01 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 21:13:01 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
02/28/2022 21:13:01 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 21:13:01 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
02/28/2022 21:13:01 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
02/28/2022 21:13:01 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
02/28/2022 21:13:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:13:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:13:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:13:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:13:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:13:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:13:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:13:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:13:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:13:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:14:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:15:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:16:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:17:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:18:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:19:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:20:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:21:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:22:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:23:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:24:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:25:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:26:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:27:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:28:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:29:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:30:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:31:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:32:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:33:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:34:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:35:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:36:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:37:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:38:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:39:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:40:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:41:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
02/28/2022 21:42:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2591) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_xpb7p_tn/none_36f3hzer/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_xpb7p_tn/none_36f3hzer/attempt_3/1/error.json
Output directory () already exists and is not empty.
02/28/2022 21:43:08 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 21:43:08 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
02/28/2022 21:43:08 - INFO - __main__ - Namespace(task_dir='data/wiki_split/', task_name='wiki_split', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 21:43:08 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-wiki_split
02/28/2022 21:43:08 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
02/28/2022 21:43:08 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
02/28/2022 21:43:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:43:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:43:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:43:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:43:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:43:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:43:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:43:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:43:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:43:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:44:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:45:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:46:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:47:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:48:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:49:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:50:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:51:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:52:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:53:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:54:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:55:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:56:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:57:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:58:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 21:59:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:00:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:01:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:02:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:03:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:04:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:05:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:06:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:07:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:08:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:09:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:10:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:11:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:09 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:19 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:29 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:39 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:49 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
02/28/2022 22:12:59 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2639) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00042057037353515625 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "2639", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 21853, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "2640", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 21853, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 21853, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2639 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     singletask_from_meta.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-02-28_22:13:13
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2639)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-02-28_22:13:13
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2640)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (2747): No such process
Task: break-QDMR, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29548
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_sc17shdk/none_toayta73
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_sc17shdk/none_toayta73/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_sc17shdk/none_toayta73/attempt_0/1/error.json
Output directory () already exists and is not empty.
02/28/2022 22:13:16 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 22:13:16 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
02/28/2022 22:13:16 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 22:13:16 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
02/28/2022 22:13:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
02/28/2022 22:13:16 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
02/28/2022 22:13:16 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
02/28/2022 22:13:16 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
02/28/2022 22:13:16 - INFO - __main__ - args.device: cuda:0
02/28/2022 22:13:16 - INFO - __main__ - Using 2 gpus
02/28/2022 22:13:16 - INFO - __main__ - args.device: cuda:1
02/28/2022 22:13:16 - INFO - __main__ - Using 2 gpus
02/28/2022 22:13:16 - INFO - __main__ - Fine-tuning the following samples: ['break-QDMR_32_100', 'break-QDMR_32_13', 'break-QDMR_32_21', 'break-QDMR_32_42', 'break-QDMR_32_87']
02/28/2022 22:13:16 - INFO - __main__ - Fine-tuning the following samples: ['break-QDMR_32_100', 'break-QDMR_32_13', 'break-QDMR_32_21', 'break-QDMR_32_42', 'break-QDMR_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
02/28/2022 22:13:25 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.5, bsz=8 ...
02/28/2022 22:13:26 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 22:13:26 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 22:13:26 - INFO - __main__ - Printing 3 examples
02/28/2022 22:13:26 - INFO - __main__ - Printing 3 examples
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
02/28/2022 22:13:26 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
02/28/2022 22:13:26 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
02/28/2022 22:13:26 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
02/28/2022 22:13:26 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
02/28/2022 22:13:26 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
02/28/2022 22:13:26 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
02/28/2022 22:13:26 - INFO - __main__ - Tokenizing Input ...
02/28/2022 22:13:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/28/2022 22:13:26 - INFO - __main__ - Tokenizing Output ...
02/28/2022 22:13:26 - INFO - __main__ - Tokenizing Output ...
02/28/2022 22:13:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 22:13:26 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 22:13:26 - INFO - __main__ - Printing 3 examples
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
02/28/2022 22:13:26 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
02/28/2022 22:13:26 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
02/28/2022 22:13:26 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
02/28/2022 22:13:26 - INFO - __main__ - Tokenizing Input ...
02/28/2022 22:13:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 22:13:26 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 22:13:26 - INFO - __main__ - Printing 3 examples
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
02/28/2022 22:13:26 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
02/28/2022 22:13:26 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
02/28/2022 22:13:26 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
02/28/2022 22:13:26 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
02/28/2022 22:13:26 - INFO - __main__ - Tokenizing Input ...
02/28/2022 22:13:26 - INFO - __main__ - Tokenizing Output ...
02/28/2022 22:13:26 - INFO - __main__ - Tokenizing Output ...
02/28/2022 22:13:26 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 22:13:26 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 22:13:41 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 22:13:41 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 22:13:42 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 22:13:42 - INFO - __main__ - Starting training!
02/28/2022 22:13:46 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 22:13:46 - INFO - __main__ - Starting training!
02/28/2022 22:13:49 - INFO - __main__ - Step 10 Global step 10 Train loss 2.94 on epoch=4
02/28/2022 22:13:51 - INFO - __main__ - Step 20 Global step 20 Train loss 1.97 on epoch=9
02/28/2022 22:13:53 - INFO - __main__ - Step 30 Global step 30 Train loss 1.58 on epoch=14
02/28/2022 22:13:55 - INFO - __main__ - Step 40 Global step 40 Train loss 1.38 on epoch=19
02/28/2022 22:13:58 - INFO - __main__ - Step 50 Global step 50 Train loss 1.20 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
02/28/2022 22:14:03 - INFO - __main__ - Global step 50 Train loss 1.81 EM 0.03125 on epoch=24
02/28/2022 22:14:04 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
02/28/2022 22:14:06 - INFO - __main__ - Step 60 Global step 60 Train loss 1.12 on epoch=29
02/28/2022 22:14:08 - INFO - __main__ - Step 70 Global step 70 Train loss 1.06 on epoch=34
02/28/2022 22:14:10 - INFO - __main__ - Step 80 Global step 80 Train loss 1.01 on epoch=39
02/28/2022 22:14:12 - INFO - __main__ - Step 90 Global step 90 Train loss 0.96 on epoch=44
02/28/2022 22:14:14 - INFO - __main__ - Step 100 Global step 100 Train loss 0.93 on epoch=49
02/28/2022 22:14:20 - INFO - __main__ - Global step 100 Train loss 1.02 EM 0.0 on epoch=49
02/28/2022 22:14:22 - INFO - __main__ - Step 110 Global step 110 Train loss 0.90 on epoch=54
02/28/2022 22:14:24 - INFO - __main__ - Step 120 Global step 120 Train loss 0.87 on epoch=59
02/28/2022 22:14:26 - INFO - __main__ - Step 130 Global step 130 Train loss 0.86 on epoch=64
02/28/2022 22:14:28 - INFO - __main__ - Step 140 Global step 140 Train loss 0.80 on epoch=69
02/28/2022 22:14:30 - INFO - __main__ - Step 150 Global step 150 Train loss 0.78 on epoch=74
02/28/2022 22:14:36 - INFO - __main__ - Global step 150 Train loss 0.84 EM 0.03125 on epoch=74
02/28/2022 22:14:38 - INFO - __main__ - Step 160 Global step 160 Train loss 0.75 on epoch=79
02/28/2022 22:14:40 - INFO - __main__ - Step 170 Global step 170 Train loss 0.75 on epoch=84
02/28/2022 22:14:42 - INFO - __main__ - Step 180 Global step 180 Train loss 0.72 on epoch=89
02/28/2022 22:14:44 - INFO - __main__ - Step 190 Global step 190 Train loss 0.70 on epoch=94
02/28/2022 22:14:46 - INFO - __main__ - Step 200 Global step 200 Train loss 0.70 on epoch=99
02/28/2022 22:14:52 - INFO - __main__ - Global step 200 Train loss 0.73 EM 0.09375 on epoch=99
02/28/2022 22:14:52 - INFO - __main__ - Saving model with best EM: 0.03125 -> 0.09375 on epoch=99, global_step=200
02/28/2022 22:14:54 - INFO - __main__ - Step 210 Global step 210 Train loss 0.67 on epoch=104
02/28/2022 22:14:56 - INFO - __main__ - Step 220 Global step 220 Train loss 0.65 on epoch=109
02/28/2022 22:14:58 - INFO - __main__ - Step 230 Global step 230 Train loss 0.68 on epoch=114
02/28/2022 22:15:00 - INFO - __main__ - Step 240 Global step 240 Train loss 0.65 on epoch=119
02/28/2022 22:15:02 - INFO - __main__ - Step 250 Global step 250 Train loss 0.63 on epoch=124
02/28/2022 22:15:08 - INFO - __main__ - Global step 250 Train loss 0.66 EM 0.125 on epoch=124
02/28/2022 22:15:08 - INFO - __main__ - Saving model with best EM: 0.09375 -> 0.125 on epoch=124, global_step=250
02/28/2022 22:15:10 - INFO - __main__ - Step 260 Global step 260 Train loss 0.64 on epoch=129
02/28/2022 22:15:12 - INFO - __main__ - Step 270 Global step 270 Train loss 0.59 on epoch=134
02/28/2022 22:15:15 - INFO - __main__ - Step 280 Global step 280 Train loss 0.59 on epoch=139
02/28/2022 22:15:17 - INFO - __main__ - Step 290 Global step 290 Train loss 0.61 on epoch=144
02/28/2022 22:15:19 - INFO - __main__ - Step 300 Global step 300 Train loss 0.57 on epoch=149
02/28/2022 22:15:24 - INFO - __main__ - Global step 300 Train loss 0.60 EM 0.0625 on epoch=149
02/28/2022 22:15:26 - INFO - __main__ - Step 310 Global step 310 Train loss 0.58 on epoch=154
02/28/2022 22:15:29 - INFO - __main__ - Step 320 Global step 320 Train loss 0.57 on epoch=159
02/28/2022 22:15:31 - INFO - __main__ - Step 330 Global step 330 Train loss 0.54 on epoch=164
02/28/2022 22:15:33 - INFO - __main__ - Step 340 Global step 340 Train loss 0.55 on epoch=169
02/28/2022 22:15:35 - INFO - __main__ - Step 350 Global step 350 Train loss 0.54 on epoch=174
02/28/2022 22:15:39 - INFO - __main__ - Global step 350 Train loss 0.56 EM 0.0625 on epoch=174
02/28/2022 22:15:42 - INFO - __main__ - Step 360 Global step 360 Train loss 0.52 on epoch=179
02/28/2022 22:15:44 - INFO - __main__ - Step 370 Global step 370 Train loss 0.51 on epoch=184
02/28/2022 22:15:46 - INFO - __main__ - Step 380 Global step 380 Train loss 0.52 on epoch=189
02/28/2022 22:15:48 - INFO - __main__ - Step 390 Global step 390 Train loss 0.51 on epoch=194
02/28/2022 22:15:50 - INFO - __main__ - Step 400 Global step 400 Train loss 0.51 on epoch=199
02/28/2022 22:15:55 - INFO - __main__ - Global step 400 Train loss 0.52 EM 0.09375 on epoch=199
02/28/2022 22:15:57 - INFO - __main__ - Step 410 Global step 410 Train loss 0.49 on epoch=204
02/28/2022 22:15:59 - INFO - __main__ - Step 420 Global step 420 Train loss 0.48 on epoch=209
02/28/2022 22:16:01 - INFO - __main__ - Step 430 Global step 430 Train loss 0.49 on epoch=214
02/28/2022 22:16:03 - INFO - __main__ - Step 440 Global step 440 Train loss 0.48 on epoch=219
02/28/2022 22:16:05 - INFO - __main__ - Step 450 Global step 450 Train loss 0.51 on epoch=224
02/28/2022 22:16:10 - INFO - __main__ - Global step 450 Train loss 0.49 EM 0.0625 on epoch=224
02/28/2022 22:16:12 - INFO - __main__ - Step 460 Global step 460 Train loss 0.49 on epoch=229
02/28/2022 22:16:15 - INFO - __main__ - Step 470 Global step 470 Train loss 0.48 on epoch=234
02/28/2022 22:16:17 - INFO - __main__ - Step 480 Global step 480 Train loss 0.46 on epoch=239
02/28/2022 22:16:19 - INFO - __main__ - Step 490 Global step 490 Train loss 0.44 on epoch=244
02/28/2022 22:16:21 - INFO - __main__ - Step 500 Global step 500 Train loss 0.47 on epoch=249
02/28/2022 22:16:25 - INFO - __main__ - Global step 500 Train loss 0.47 EM 0.0625 on epoch=249
02/28/2022 22:16:28 - INFO - __main__ - Step 510 Global step 510 Train loss 0.46 on epoch=254
02/28/2022 22:16:30 - INFO - __main__ - Step 520 Global step 520 Train loss 0.45 on epoch=259
02/28/2022 22:16:32 - INFO - __main__ - Step 530 Global step 530 Train loss 0.43 on epoch=264
02/28/2022 22:16:34 - INFO - __main__ - Step 540 Global step 540 Train loss 0.45 on epoch=269
02/28/2022 22:16:36 - INFO - __main__ - Step 550 Global step 550 Train loss 0.42 on epoch=274
02/28/2022 22:16:41 - INFO - __main__ - Global step 550 Train loss 0.44 EM 0.0625 on epoch=274
02/28/2022 22:16:43 - INFO - __main__ - Step 560 Global step 560 Train loss 0.43 on epoch=279
02/28/2022 22:16:45 - INFO - __main__ - Step 570 Global step 570 Train loss 0.43 on epoch=284
02/28/2022 22:16:47 - INFO - __main__ - Step 580 Global step 580 Train loss 0.40 on epoch=289
02/28/2022 22:16:50 - INFO - __main__ - Step 590 Global step 590 Train loss 0.41 on epoch=294
02/28/2022 22:16:52 - INFO - __main__ - Step 600 Global step 600 Train loss 0.42 on epoch=299
02/28/2022 22:16:57 - INFO - __main__ - Global step 600 Train loss 0.42 EM 0.03125 on epoch=299
02/28/2022 22:17:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.40 on epoch=304
02/28/2022 22:17:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.40 on epoch=309
02/28/2022 22:17:04 - INFO - __main__ - Step 630 Global step 630 Train loss 0.41 on epoch=314
02/28/2022 22:17:06 - INFO - __main__ - Step 640 Global step 640 Train loss 0.41 on epoch=319
02/28/2022 22:17:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.39 on epoch=324
02/28/2022 22:17:14 - INFO - __main__ - Global step 650 Train loss 0.40 EM 0.0625 on epoch=324
02/28/2022 22:17:16 - INFO - __main__ - Step 660 Global step 660 Train loss 0.40 on epoch=329
02/28/2022 22:17:18 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=334
02/28/2022 22:17:20 - INFO - __main__ - Step 680 Global step 680 Train loss 0.39 on epoch=339
02/28/2022 22:17:22 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=344
02/28/2022 22:17:24 - INFO - __main__ - Step 700 Global step 700 Train loss 0.37 on epoch=349
02/28/2022 22:17:29 - INFO - __main__ - Global step 700 Train loss 0.39 EM 0.03125 on epoch=349
02/28/2022 22:17:32 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=354
02/28/2022 22:17:34 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=359
02/28/2022 22:17:36 - INFO - __main__ - Step 730 Global step 730 Train loss 0.36 on epoch=364
02/28/2022 22:17:38 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=369
02/28/2022 22:17:40 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=374
02/28/2022 22:17:46 - INFO - __main__ - Global step 750 Train loss 0.37 EM 0.0625 on epoch=374
02/28/2022 22:17:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.35 on epoch=379
02/28/2022 22:17:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.36 on epoch=384
02/28/2022 22:17:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.35 on epoch=389
02/28/2022 22:17:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=394
02/28/2022 22:17:56 - INFO - __main__ - Step 800 Global step 800 Train loss 0.35 on epoch=399
02/28/2022 22:18:03 - INFO - __main__ - Global step 800 Train loss 0.35 EM 0.03125 on epoch=399
02/28/2022 22:18:05 - INFO - __main__ - Step 810 Global step 810 Train loss 0.37 on epoch=404
02/28/2022 22:18:07 - INFO - __main__ - Step 820 Global step 820 Train loss 0.36 on epoch=409
02/28/2022 22:18:09 - INFO - __main__ - Step 830 Global step 830 Train loss 0.34 on epoch=414
02/28/2022 22:18:11 - INFO - __main__ - Step 840 Global step 840 Train loss 0.34 on epoch=419
02/28/2022 22:18:14 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=424
02/28/2022 22:18:19 - INFO - __main__ - Global step 850 Train loss 0.35 EM 0.03125 on epoch=424
02/28/2022 22:18:21 - INFO - __main__ - Step 860 Global step 860 Train loss 0.34 on epoch=429
02/28/2022 22:18:23 - INFO - __main__ - Step 870 Global step 870 Train loss 0.33 on epoch=434
02/28/2022 22:18:25 - INFO - __main__ - Step 880 Global step 880 Train loss 0.34 on epoch=439
02/28/2022 22:18:27 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
02/28/2022 22:18:30 - INFO - __main__ - Step 900 Global step 900 Train loss 0.33 on epoch=449
02/28/2022 22:18:36 - INFO - __main__ - Global step 900 Train loss 0.33 EM 0.03125 on epoch=449
02/28/2022 22:18:38 - INFO - __main__ - Step 910 Global step 910 Train loss 0.32 on epoch=454
02/28/2022 22:18:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=459
02/28/2022 22:18:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.33 on epoch=464
02/28/2022 22:18:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.31 on epoch=469
02/28/2022 22:18:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=474
02/28/2022 22:18:53 - INFO - __main__ - Global step 950 Train loss 0.32 EM 0.03125 on epoch=474
02/28/2022 22:18:55 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=479
02/28/2022 22:18:57 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=484
02/28/2022 22:18:59 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=489
02/28/2022 22:19:01 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=494
02/28/2022 22:19:03 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.32 on epoch=499
02/28/2022 22:19:09 - INFO - __main__ - Global step 1000 Train loss 0.32 EM 0.03125 on epoch=499
02/28/2022 22:19:11 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.30 on epoch=504
02/28/2022 22:19:13 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.30 on epoch=509
02/28/2022 22:19:15 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=514
02/28/2022 22:19:17 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.29 on epoch=519
02/28/2022 22:19:20 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.31 on epoch=524
02/28/2022 22:19:26 - INFO - __main__ - Global step 1050 Train loss 0.30 EM 0.0625 on epoch=524
02/28/2022 22:19:29 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.30 on epoch=529
02/28/2022 22:19:31 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.30 on epoch=534
02/28/2022 22:19:33 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=539
02/28/2022 22:19:35 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=544
02/28/2022 22:19:37 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.30 on epoch=549
02/28/2022 22:19:43 - INFO - __main__ - Global step 1100 Train loss 0.30 EM 0.09375 on epoch=549
02/28/2022 22:19:45 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.30 on epoch=554
02/28/2022 22:19:47 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=559
02/28/2022 22:19:49 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.30 on epoch=564
02/28/2022 22:19:52 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.29 on epoch=569
02/28/2022 22:19:54 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=574
02/28/2022 22:20:00 - INFO - __main__ - Global step 1150 Train loss 0.29 EM 0.03125 on epoch=574
02/28/2022 22:20:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.26 on epoch=579
02/28/2022 22:20:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=584
02/28/2022 22:20:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.28 on epoch=589
02/28/2022 22:20:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.26 on epoch=594
02/28/2022 22:20:11 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.27 on epoch=599
02/28/2022 22:20:17 - INFO - __main__ - Global step 1200 Train loss 0.27 EM 0.03125 on epoch=599
02/28/2022 22:20:19 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.28 on epoch=604
02/28/2022 22:20:22 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.28 on epoch=609
02/28/2022 22:20:24 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.27 on epoch=614
02/28/2022 22:20:26 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.28 on epoch=619
02/28/2022 22:20:28 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.27 on epoch=624
02/28/2022 22:20:34 - INFO - __main__ - Global step 1250 Train loss 0.28 EM 0.03125 on epoch=624
02/28/2022 22:20:36 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=629
02/28/2022 22:20:38 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=634
02/28/2022 22:20:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.25 on epoch=639
02/28/2022 22:20:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.26 on epoch=644
02/28/2022 22:20:45 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
02/28/2022 22:20:51 - INFO - __main__ - Global step 1300 Train loss 0.26 EM 0.03125 on epoch=649
02/28/2022 22:20:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=654
02/28/2022 22:20:55 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.28 on epoch=659
02/28/2022 22:20:57 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.26 on epoch=664
02/28/2022 22:20:59 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
02/28/2022 22:21:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.26 on epoch=674
02/28/2022 22:21:08 - INFO - __main__ - Global step 1350 Train loss 0.26 EM 0.03125 on epoch=674
02/28/2022 22:21:10 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.26 on epoch=679
02/28/2022 22:21:12 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.25 on epoch=684
02/28/2022 22:21:15 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.25 on epoch=689
02/28/2022 22:21:17 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=694
02/28/2022 22:21:19 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.25 on epoch=699
02/28/2022 22:21:26 - INFO - __main__ - Global step 1400 Train loss 0.26 EM 0.0625 on epoch=699
02/28/2022 22:21:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.26 on epoch=704
02/28/2022 22:21:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.25 on epoch=709
02/28/2022 22:21:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.25 on epoch=714
02/28/2022 22:21:34 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=719
02/28/2022 22:21:36 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.26 on epoch=724
02/28/2022 22:21:42 - INFO - __main__ - Global step 1450 Train loss 0.25 EM 0.0625 on epoch=724
02/28/2022 22:21:45 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=729
02/28/2022 22:21:47 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.27 on epoch=734
02/28/2022 22:21:49 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.23 on epoch=739
02/28/2022 22:21:51 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.24 on epoch=744
02/28/2022 22:21:53 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=749
02/28/2022 22:21:58 - INFO - __main__ - Global step 1500 Train loss 0.24 EM 0.0625 on epoch=749
02/28/2022 22:22:01 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=754
02/28/2022 22:22:03 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.23 on epoch=759
02/28/2022 22:22:05 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.23 on epoch=764
02/28/2022 22:22:07 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=769
02/28/2022 22:22:09 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.22 on epoch=774
02/28/2022 22:22:15 - INFO - __main__ - Global step 1550 Train loss 0.23 EM 0.03125 on epoch=774
02/28/2022 22:22:17 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.25 on epoch=779
02/28/2022 22:22:19 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=784
02/28/2022 22:22:22 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.24 on epoch=789
02/28/2022 22:22:24 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=794
02/28/2022 22:22:26 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=799
02/28/2022 22:22:32 - INFO - __main__ - Global step 1600 Train loss 0.23 EM 0.0625 on epoch=799
02/28/2022 22:22:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.21 on epoch=804
02/28/2022 22:22:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=809
02/28/2022 22:22:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.21 on epoch=814
02/28/2022 22:22:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.22 on epoch=819
02/28/2022 22:22:42 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=824
02/28/2022 22:22:49 - INFO - __main__ - Global step 1650 Train loss 0.22 EM 0.03125 on epoch=824
02/28/2022 22:22:51 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.21 on epoch=829
02/28/2022 22:22:54 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.22 on epoch=834
02/28/2022 22:22:56 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.22 on epoch=839
02/28/2022 22:22:58 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.21 on epoch=844
02/28/2022 22:23:00 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.22 on epoch=849
02/28/2022 22:23:07 - INFO - __main__ - Global step 1700 Train loss 0.22 EM 0.0625 on epoch=849
02/28/2022 22:23:09 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=854
02/28/2022 22:23:11 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.21 on epoch=859
02/28/2022 22:23:13 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.24 on epoch=864
02/28/2022 22:23:16 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.20 on epoch=869
02/28/2022 22:23:18 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=874
02/28/2022 22:23:24 - INFO - __main__ - Global step 1750 Train loss 0.21 EM 0.03125 on epoch=874
02/28/2022 22:23:26 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.20 on epoch=879
02/28/2022 22:23:29 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=884
02/28/2022 22:23:31 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.20 on epoch=889
02/28/2022 22:23:33 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=894
02/28/2022 22:23:35 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=899
02/28/2022 22:23:40 - INFO - __main__ - Global step 1800 Train loss 0.20 EM 0.0625 on epoch=899
02/28/2022 22:23:43 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.21 on epoch=904
02/28/2022 22:23:45 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.20 on epoch=909
02/28/2022 22:23:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.21 on epoch=914
02/28/2022 22:23:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=919
02/28/2022 22:23:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.20 on epoch=924
02/28/2022 22:23:58 - INFO - __main__ - Global step 1850 Train loss 0.21 EM 0.0625 on epoch=924
02/28/2022 22:24:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.21 on epoch=929
02/28/2022 22:24:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.20 on epoch=934
02/28/2022 22:24:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.20 on epoch=939
02/28/2022 22:24:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.20 on epoch=944
02/28/2022 22:24:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=949
02/28/2022 22:24:15 - INFO - __main__ - Global step 1900 Train loss 0.20 EM 0.0625 on epoch=949
02/28/2022 22:24:17 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.19 on epoch=954
02/28/2022 22:24:19 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.19 on epoch=959
02/28/2022 22:24:22 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=964
02/28/2022 22:24:24 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.20 on epoch=969
02/28/2022 22:24:26 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=974
02/28/2022 22:24:32 - INFO - __main__ - Global step 1950 Train loss 0.19 EM 0.0625 on epoch=974
02/28/2022 22:24:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.18 on epoch=979
02/28/2022 22:24:37 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
02/28/2022 22:24:39 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.20 on epoch=989
02/28/2022 22:24:41 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=994
02/28/2022 22:24:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=999
02/28/2022 22:24:44 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 22:24:44 - INFO - __main__ - Printing 3 examples
02/28/2022 22:24:44 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
02/28/2022 22:24:44 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
02/28/2022 22:24:44 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
02/28/2022 22:24:44 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
02/28/2022 22:24:44 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
02/28/2022 22:24:44 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
02/28/2022 22:24:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 22:24:44 - INFO - __main__ - Tokenizing Output ...
02/28/2022 22:24:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 22:24:44 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 22:24:44 - INFO - __main__ - Printing 3 examples
02/28/2022 22:24:44 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
02/28/2022 22:24:44 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
02/28/2022 22:24:44 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
02/28/2022 22:24:44 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
02/28/2022 22:24:44 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
02/28/2022 22:24:44 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
02/28/2022 22:24:44 - INFO - __main__ - Tokenizing Input ...
02/28/2022 22:24:44 - INFO - __main__ - Tokenizing Output ...
02/28/2022 22:24:44 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 22:24:49 - INFO - __main__ - Global step 2000 Train loss 0.19 EM 0.03125 on epoch=999
02/28/2022 22:24:49 - INFO - __main__ - save last model!
02/28/2022 22:24:49 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/28/2022 22:24:49 - INFO - __main__ - Start tokenizing ... 7760 instances
02/28/2022 22:24:49 - INFO - __main__ - Printing 3 examples
02/28/2022 22:24:49 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
02/28/2022 22:24:49 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
02/28/2022 22:24:49 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
02/28/2022 22:24:49 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
02/28/2022 22:24:49 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
02/28/2022 22:24:49 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
02/28/2022 22:24:49 - INFO - __main__ - Tokenizing Input ...
02/28/2022 22:24:52 - INFO - __main__ - Tokenizing Output ...
02/28/2022 22:24:58 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 22:24:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 22:24:59 - INFO - __main__ - Starting training!
02/28/2022 22:25:00 - INFO - __main__ - Loaded 7760 examples from test data
02/28/2022 22:49:50 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR/break-QDMR_32_100_0.5_8_predictions.txt
02/28/2022 22:49:51 - INFO - __main__ - EM on test data: 0.0329
02/28/2022 22:49:52 - INFO - __main__ - prefix=break-QDMR_32_100, lr=0.5, bsz=8, dev_performance=0.125, test_performance=0.03286082474226804
02/28/2022 22:49:52 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.4, bsz=8 ...
02/28/2022 22:49:53 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 22:49:53 - INFO - __main__ - Printing 3 examples
02/28/2022 22:49:53 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
02/28/2022 22:49:53 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
02/28/2022 22:49:53 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
02/28/2022 22:49:53 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
02/28/2022 22:49:53 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
02/28/2022 22:49:53 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
02/28/2022 22:49:53 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 22:49:53 - INFO - __main__ - Tokenizing Output ...
02/28/2022 22:49:53 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 22:49:53 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 22:49:53 - INFO - __main__ - Printing 3 examples
02/28/2022 22:49:53 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
02/28/2022 22:49:53 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
02/28/2022 22:49:53 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
02/28/2022 22:49:53 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
02/28/2022 22:49:53 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
02/28/2022 22:49:53 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
02/28/2022 22:49:53 - INFO - __main__ - Tokenizing Input ...
02/28/2022 22:49:53 - INFO - __main__ - Tokenizing Output ...
02/28/2022 22:49:53 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 22:50:05 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 22:50:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 22:50:06 - INFO - __main__ - Starting training!
02/28/2022 22:50:09 - INFO - __main__ - Step 10 Global step 10 Train loss 3.07 on epoch=4
02/28/2022 22:50:11 - INFO - __main__ - Step 20 Global step 20 Train loss 2.09 on epoch=9
02/28/2022 22:50:13 - INFO - __main__ - Step 30 Global step 30 Train loss 1.65 on epoch=14
02/28/2022 22:50:15 - INFO - __main__ - Step 40 Global step 40 Train loss 1.43 on epoch=19
02/28/2022 22:50:18 - INFO - __main__ - Step 50 Global step 50 Train loss 1.33 on epoch=24
02/28/2022 22:50:25 - INFO - __main__ - Global step 50 Train loss 1.91 EM 0.0 on epoch=24
02/28/2022 22:50:25 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
02/28/2022 22:50:27 - INFO - __main__ - Step 60 Global step 60 Train loss 1.22 on epoch=29
02/28/2022 22:50:29 - INFO - __main__ - Step 70 Global step 70 Train loss 1.15 on epoch=34
02/28/2022 22:50:31 - INFO - __main__ - Step 80 Global step 80 Train loss 1.12 on epoch=39
02/28/2022 22:50:33 - INFO - __main__ - Step 90 Global step 90 Train loss 1.06 on epoch=44
02/28/2022 22:50:36 - INFO - __main__ - Step 100 Global step 100 Train loss 1.00 on epoch=49
02/28/2022 22:50:43 - INFO - __main__ - Global step 100 Train loss 1.11 EM 0.0 on epoch=49
02/28/2022 22:50:45 - INFO - __main__ - Step 110 Global step 110 Train loss 1.00 on epoch=54
02/28/2022 22:50:47 - INFO - __main__ - Step 120 Global step 120 Train loss 0.95 on epoch=59
02/28/2022 22:50:50 - INFO - __main__ - Step 130 Global step 130 Train loss 0.93 on epoch=64
02/28/2022 22:50:52 - INFO - __main__ - Step 140 Global step 140 Train loss 0.89 on epoch=69
02/28/2022 22:50:54 - INFO - __main__ - Step 150 Global step 150 Train loss 0.86 on epoch=74
02/28/2022 22:51:00 - INFO - __main__ - Global step 150 Train loss 0.93 EM 0.0 on epoch=74
02/28/2022 22:51:02 - INFO - __main__ - Step 160 Global step 160 Train loss 0.84 on epoch=79
02/28/2022 22:51:04 - INFO - __main__ - Step 170 Global step 170 Train loss 0.81 on epoch=84
02/28/2022 22:51:07 - INFO - __main__ - Step 180 Global step 180 Train loss 0.83 on epoch=89
02/28/2022 22:51:09 - INFO - __main__ - Step 190 Global step 190 Train loss 0.78 on epoch=94
02/28/2022 22:51:11 - INFO - __main__ - Step 200 Global step 200 Train loss 0.75 on epoch=99
02/28/2022 22:51:17 - INFO - __main__ - Global step 200 Train loss 0.80 EM 0.0 on epoch=99
02/28/2022 22:51:19 - INFO - __main__ - Step 210 Global step 210 Train loss 0.76 on epoch=104
02/28/2022 22:51:21 - INFO - __main__ - Step 220 Global step 220 Train loss 0.73 on epoch=109
02/28/2022 22:51:23 - INFO - __main__ - Step 230 Global step 230 Train loss 0.71 on epoch=114
02/28/2022 22:51:25 - INFO - __main__ - Step 240 Global step 240 Train loss 0.71 on epoch=119
02/28/2022 22:51:28 - INFO - __main__ - Step 250 Global step 250 Train loss 0.70 on epoch=124
02/28/2022 22:51:33 - INFO - __main__ - Global step 250 Train loss 0.72 EM 0.0 on epoch=124
02/28/2022 22:51:36 - INFO - __main__ - Step 260 Global step 260 Train loss 0.67 on epoch=129
02/28/2022 22:51:38 - INFO - __main__ - Step 270 Global step 270 Train loss 0.67 on epoch=134
02/28/2022 22:51:40 - INFO - __main__ - Step 280 Global step 280 Train loss 0.67 on epoch=139
02/28/2022 22:51:42 - INFO - __main__ - Step 290 Global step 290 Train loss 0.65 on epoch=144
02/28/2022 22:51:45 - INFO - __main__ - Step 300 Global step 300 Train loss 0.63 on epoch=149
02/28/2022 22:51:50 - INFO - __main__ - Global step 300 Train loss 0.66 EM 0.09375 on epoch=149
02/28/2022 22:51:50 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.09375 on epoch=149, global_step=300
02/28/2022 22:51:52 - INFO - __main__ - Step 310 Global step 310 Train loss 0.62 on epoch=154
02/28/2022 22:51:54 - INFO - __main__ - Step 320 Global step 320 Train loss 0.64 on epoch=159
02/28/2022 22:51:57 - INFO - __main__ - Step 330 Global step 330 Train loss 0.62 on epoch=164
02/28/2022 22:51:59 - INFO - __main__ - Step 340 Global step 340 Train loss 0.62 on epoch=169
02/28/2022 22:52:01 - INFO - __main__ - Step 350 Global step 350 Train loss 0.61 on epoch=174
02/28/2022 22:52:06 - INFO - __main__ - Global step 350 Train loss 0.62 EM 0.09375 on epoch=174
02/28/2022 22:52:09 - INFO - __main__ - Step 360 Global step 360 Train loss 0.60 on epoch=179
02/28/2022 22:52:11 - INFO - __main__ - Step 370 Global step 370 Train loss 0.58 on epoch=184
02/28/2022 22:52:13 - INFO - __main__ - Step 380 Global step 380 Train loss 0.60 on epoch=189
02/28/2022 22:52:15 - INFO - __main__ - Step 390 Global step 390 Train loss 0.59 on epoch=194
02/28/2022 22:52:17 - INFO - __main__ - Step 400 Global step 400 Train loss 0.56 on epoch=199
02/28/2022 22:52:23 - INFO - __main__ - Global step 400 Train loss 0.58 EM 0.09375 on epoch=199
02/28/2022 22:52:25 - INFO - __main__ - Step 410 Global step 410 Train loss 0.56 on epoch=204
02/28/2022 22:52:27 - INFO - __main__ - Step 420 Global step 420 Train loss 0.54 on epoch=209
02/28/2022 22:52:30 - INFO - __main__ - Step 430 Global step 430 Train loss 0.54 on epoch=214
02/28/2022 22:52:32 - INFO - __main__ - Step 440 Global step 440 Train loss 0.54 on epoch=219
02/28/2022 22:52:34 - INFO - __main__ - Step 450 Global step 450 Train loss 0.53 on epoch=224
02/28/2022 22:52:39 - INFO - __main__ - Global step 450 Train loss 0.54 EM 0.09375 on epoch=224
02/28/2022 22:52:42 - INFO - __main__ - Step 460 Global step 460 Train loss 0.52 on epoch=229
02/28/2022 22:52:44 - INFO - __main__ - Step 470 Global step 470 Train loss 0.54 on epoch=234
02/28/2022 22:52:46 - INFO - __main__ - Step 480 Global step 480 Train loss 0.53 on epoch=239
02/28/2022 22:52:48 - INFO - __main__ - Step 490 Global step 490 Train loss 0.49 on epoch=244
02/28/2022 22:52:51 - INFO - __main__ - Step 500 Global step 500 Train loss 0.52 on epoch=249
02/28/2022 22:52:56 - INFO - __main__ - Global step 500 Train loss 0.52 EM 0.09375 on epoch=249
02/28/2022 22:52:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.50 on epoch=254
02/28/2022 22:53:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.48 on epoch=259
02/28/2022 22:53:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.48 on epoch=264
02/28/2022 22:53:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.48 on epoch=269
02/28/2022 22:53:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.47 on epoch=274
02/28/2022 22:53:12 - INFO - __main__ - Global step 550 Train loss 0.48 EM 0.09375 on epoch=274
02/28/2022 22:53:14 - INFO - __main__ - Step 560 Global step 560 Train loss 0.47 on epoch=279
02/28/2022 22:53:16 - INFO - __main__ - Step 570 Global step 570 Train loss 0.49 on epoch=284
02/28/2022 22:53:18 - INFO - __main__ - Step 580 Global step 580 Train loss 0.46 on epoch=289
02/28/2022 22:53:20 - INFO - __main__ - Step 590 Global step 590 Train loss 0.46 on epoch=294
02/28/2022 22:53:22 - INFO - __main__ - Step 600 Global step 600 Train loss 0.46 on epoch=299
02/28/2022 22:53:28 - INFO - __main__ - Global step 600 Train loss 0.47 EM 0.09375 on epoch=299
02/28/2022 22:53:30 - INFO - __main__ - Step 610 Global step 610 Train loss 0.46 on epoch=304
02/28/2022 22:53:32 - INFO - __main__ - Step 620 Global step 620 Train loss 0.43 on epoch=309
02/28/2022 22:53:34 - INFO - __main__ - Step 630 Global step 630 Train loss 0.44 on epoch=314
02/28/2022 22:53:36 - INFO - __main__ - Step 640 Global step 640 Train loss 0.45 on epoch=319
02/28/2022 22:53:38 - INFO - __main__ - Step 650 Global step 650 Train loss 0.44 on epoch=324
02/28/2022 22:53:43 - INFO - __main__ - Global step 650 Train loss 0.45 EM 0.0625 on epoch=324
02/28/2022 22:53:45 - INFO - __main__ - Step 660 Global step 660 Train loss 0.43 on epoch=329
02/28/2022 22:53:47 - INFO - __main__ - Step 670 Global step 670 Train loss 0.43 on epoch=334
02/28/2022 22:53:49 - INFO - __main__ - Step 680 Global step 680 Train loss 0.44 on epoch=339
02/28/2022 22:53:51 - INFO - __main__ - Step 690 Global step 690 Train loss 0.43 on epoch=344
02/28/2022 22:53:54 - INFO - __main__ - Step 700 Global step 700 Train loss 0.42 on epoch=349
02/28/2022 22:53:59 - INFO - __main__ - Global step 700 Train loss 0.43 EM 0.0625 on epoch=349
02/28/2022 22:54:01 - INFO - __main__ - Step 710 Global step 710 Train loss 0.42 on epoch=354
02/28/2022 22:54:03 - INFO - __main__ - Step 720 Global step 720 Train loss 0.42 on epoch=359
02/28/2022 22:54:05 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
02/28/2022 22:54:07 - INFO - __main__ - Step 740 Global step 740 Train loss 0.41 on epoch=369
02/28/2022 22:54:09 - INFO - __main__ - Step 750 Global step 750 Train loss 0.38 on epoch=374
02/28/2022 22:54:14 - INFO - __main__ - Global step 750 Train loss 0.41 EM 0.0625 on epoch=374
02/28/2022 22:54:16 - INFO - __main__ - Step 760 Global step 760 Train loss 0.41 on epoch=379
02/28/2022 22:54:18 - INFO - __main__ - Step 770 Global step 770 Train loss 0.39 on epoch=384
02/28/2022 22:54:20 - INFO - __main__ - Step 780 Global step 780 Train loss 0.40 on epoch=389
02/28/2022 22:54:23 - INFO - __main__ - Step 790 Global step 790 Train loss 0.39 on epoch=394
02/28/2022 22:54:25 - INFO - __main__ - Step 800 Global step 800 Train loss 0.38 on epoch=399
02/28/2022 22:54:30 - INFO - __main__ - Global step 800 Train loss 0.39 EM 0.09375 on epoch=399
02/28/2022 22:54:32 - INFO - __main__ - Step 810 Global step 810 Train loss 0.38 on epoch=404
02/28/2022 22:54:34 - INFO - __main__ - Step 820 Global step 820 Train loss 0.39 on epoch=409
02/28/2022 22:54:36 - INFO - __main__ - Step 830 Global step 830 Train loss 0.38 on epoch=414
02/28/2022 22:54:38 - INFO - __main__ - Step 840 Global step 840 Train loss 0.38 on epoch=419
02/28/2022 22:54:40 - INFO - __main__ - Step 850 Global step 850 Train loss 0.41 on epoch=424
02/28/2022 22:54:46 - INFO - __main__ - Global step 850 Train loss 0.39 EM 0.0625 on epoch=424
02/28/2022 22:54:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.38 on epoch=429
02/28/2022 22:54:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.38 on epoch=434
02/28/2022 22:54:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=439
02/28/2022 22:54:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.38 on epoch=444
02/28/2022 22:54:56 - INFO - __main__ - Step 900 Global step 900 Train loss 0.37 on epoch=449
02/28/2022 22:55:02 - INFO - __main__ - Global step 900 Train loss 0.38 EM 0.0625 on epoch=449
02/28/2022 22:55:04 - INFO - __main__ - Step 910 Global step 910 Train loss 0.38 on epoch=454
02/28/2022 22:55:06 - INFO - __main__ - Step 920 Global step 920 Train loss 0.34 on epoch=459
02/28/2022 22:55:08 - INFO - __main__ - Step 930 Global step 930 Train loss 0.35 on epoch=464
02/28/2022 22:55:10 - INFO - __main__ - Step 940 Global step 940 Train loss 0.35 on epoch=469
02/28/2022 22:55:12 - INFO - __main__ - Step 950 Global step 950 Train loss 0.36 on epoch=474
02/28/2022 22:55:18 - INFO - __main__ - Global step 950 Train loss 0.35 EM 0.0625 on epoch=474
02/28/2022 22:55:20 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=479
02/28/2022 22:55:22 - INFO - __main__ - Step 970 Global step 970 Train loss 0.36 on epoch=484
02/28/2022 22:55:24 - INFO - __main__ - Step 980 Global step 980 Train loss 0.34 on epoch=489
02/28/2022 22:55:26 - INFO - __main__ - Step 990 Global step 990 Train loss 0.35 on epoch=494
02/28/2022 22:55:28 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.37 on epoch=499
02/28/2022 22:55:34 - INFO - __main__ - Global step 1000 Train loss 0.36 EM 0.0625 on epoch=499
02/28/2022 22:55:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.34 on epoch=504
02/28/2022 22:55:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.35 on epoch=509
02/28/2022 22:55:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.34 on epoch=514
02/28/2022 22:55:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.33 on epoch=519
02/28/2022 22:55:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=524
02/28/2022 22:55:51 - INFO - __main__ - Global step 1050 Train loss 0.34 EM 0.0625 on epoch=524
02/28/2022 22:55:53 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=529
02/28/2022 22:55:55 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=534
02/28/2022 22:55:57 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.33 on epoch=539
02/28/2022 22:55:59 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.35 on epoch=544
02/28/2022 22:56:01 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.34 on epoch=549
02/28/2022 22:56:06 - INFO - __main__ - Global step 1100 Train loss 0.34 EM 0.0625 on epoch=549
02/28/2022 22:56:09 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=554
02/28/2022 22:56:11 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.34 on epoch=559
02/28/2022 22:56:13 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.32 on epoch=564
02/28/2022 22:56:15 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=569
02/28/2022 22:56:17 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.31 on epoch=574
02/28/2022 22:56:23 - INFO - __main__ - Global step 1150 Train loss 0.33 EM 0.0625 on epoch=574
02/28/2022 22:56:25 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.31 on epoch=579
02/28/2022 22:56:27 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.31 on epoch=584
02/28/2022 22:56:29 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.32 on epoch=589
02/28/2022 22:56:31 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.33 on epoch=594
02/28/2022 22:56:33 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.31 on epoch=599
02/28/2022 22:56:39 - INFO - __main__ - Global step 1200 Train loss 0.32 EM 0.0625 on epoch=599
02/28/2022 22:56:41 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
02/28/2022 22:56:43 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.32 on epoch=609
02/28/2022 22:56:45 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.32 on epoch=614
02/28/2022 22:56:47 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.31 on epoch=619
02/28/2022 22:56:50 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.30 on epoch=624
02/28/2022 22:56:56 - INFO - __main__ - Global step 1250 Train loss 0.31 EM 0.0625 on epoch=624
02/28/2022 22:56:58 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.32 on epoch=629
02/28/2022 22:57:00 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.29 on epoch=634
02/28/2022 22:57:02 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.31 on epoch=639
02/28/2022 22:57:04 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.29 on epoch=644
02/28/2022 22:57:07 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.28 on epoch=649
02/28/2022 22:57:13 - INFO - __main__ - Global step 1300 Train loss 0.30 EM 0.0625 on epoch=649
02/28/2022 22:57:15 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.29 on epoch=654
02/28/2022 22:57:18 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=659
02/28/2022 22:57:20 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.28 on epoch=664
02/28/2022 22:57:22 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.27 on epoch=669
02/28/2022 22:57:24 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=674
02/28/2022 22:57:30 - INFO - __main__ - Global step 1350 Train loss 0.29 EM 0.03125 on epoch=674
02/28/2022 22:57:32 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.27 on epoch=679
02/28/2022 22:57:35 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.31 on epoch=684
02/28/2022 22:57:37 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=689
02/28/2022 22:57:39 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.27 on epoch=694
02/28/2022 22:57:41 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.29 on epoch=699
02/28/2022 22:57:48 - INFO - __main__ - Global step 1400 Train loss 0.29 EM 0.0625 on epoch=699
02/28/2022 22:57:50 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.25 on epoch=704
02/28/2022 22:57:52 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.26 on epoch=709
02/28/2022 22:57:54 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=714
02/28/2022 22:57:57 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.30 on epoch=719
02/28/2022 22:57:59 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.27 on epoch=724
02/28/2022 22:58:05 - INFO - __main__ - Global step 1450 Train loss 0.27 EM 0.0625 on epoch=724
02/28/2022 22:58:07 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.29 on epoch=729
02/28/2022 22:58:10 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.25 on epoch=734
02/28/2022 22:58:12 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.26 on epoch=739
02/28/2022 22:58:14 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.26 on epoch=744
02/28/2022 22:58:16 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.26 on epoch=749
02/28/2022 22:58:22 - INFO - __main__ - Global step 1500 Train loss 0.26 EM 0.0625 on epoch=749
02/28/2022 22:58:24 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.25 on epoch=754
02/28/2022 22:58:27 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
02/28/2022 22:58:29 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=764
02/28/2022 22:58:31 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.27 on epoch=769
02/28/2022 22:58:33 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.26 on epoch=774
02/28/2022 22:58:39 - INFO - __main__ - Global step 1550 Train loss 0.26 EM 0.0625 on epoch=774
02/28/2022 22:58:42 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.27 on epoch=779
02/28/2022 22:58:44 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.26 on epoch=784
02/28/2022 22:58:46 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.26 on epoch=789
02/28/2022 22:58:48 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=794
02/28/2022 22:58:50 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.25 on epoch=799
02/28/2022 22:58:58 - INFO - __main__ - Global step 1600 Train loss 0.26 EM 0.09375 on epoch=799
02/28/2022 22:59:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.25 on epoch=804
02/28/2022 22:59:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.25 on epoch=809
02/28/2022 22:59:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.27 on epoch=814
02/28/2022 22:59:06 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.25 on epoch=819
02/28/2022 22:59:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.26 on epoch=824
02/28/2022 22:59:15 - INFO - __main__ - Global step 1650 Train loss 0.25 EM 0.0625 on epoch=824
02/28/2022 22:59:17 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
02/28/2022 22:59:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.25 on epoch=834
02/28/2022 22:59:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.25 on epoch=839
02/28/2022 22:59:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.24 on epoch=844
02/28/2022 22:59:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.25 on epoch=849
02/28/2022 22:59:33 - INFO - __main__ - Global step 1700 Train loss 0.24 EM 0.03125 on epoch=849
02/28/2022 22:59:35 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.24 on epoch=854
02/28/2022 22:59:37 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.24 on epoch=859
02/28/2022 22:59:39 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.23 on epoch=864
02/28/2022 22:59:42 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=869
02/28/2022 22:59:44 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.24 on epoch=874
02/28/2022 22:59:51 - INFO - __main__ - Global step 1750 Train loss 0.24 EM 0.03125 on epoch=874
02/28/2022 22:59:53 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.26 on epoch=879
02/28/2022 22:59:55 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=884
02/28/2022 22:59:57 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.22 on epoch=889
02/28/2022 23:00:00 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.23 on epoch=894
02/28/2022 23:00:02 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=899
02/28/2022 23:00:09 - INFO - __main__ - Global step 1800 Train loss 0.23 EM 0.0625 on epoch=899
02/28/2022 23:00:11 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=904
02/28/2022 23:00:13 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.22 on epoch=909
02/28/2022 23:00:15 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.25 on epoch=914
02/28/2022 23:00:18 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.22 on epoch=919
02/28/2022 23:00:20 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.22 on epoch=924
02/28/2022 23:00:27 - INFO - __main__ - Global step 1850 Train loss 0.23 EM 0.03125 on epoch=924
02/28/2022 23:00:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.23 on epoch=929
02/28/2022 23:00:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.24 on epoch=934
02/28/2022 23:00:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.24 on epoch=939
02/28/2022 23:00:35 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.22 on epoch=944
02/28/2022 23:00:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.23 on epoch=949
02/28/2022 23:00:49 - INFO - __main__ - Global step 1900 Train loss 0.23 EM 0.03125 on epoch=949
02/28/2022 23:00:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.22 on epoch=954
02/28/2022 23:00:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.24 on epoch=959
02/28/2022 23:00:56 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.22 on epoch=964
02/28/2022 23:00:58 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.23 on epoch=969
02/28/2022 23:01:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.22 on epoch=974
02/28/2022 23:01:06 - INFO - __main__ - Global step 1950 Train loss 0.23 EM 0.03125 on epoch=974
02/28/2022 23:01:09 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.21 on epoch=979
02/28/2022 23:01:11 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.21 on epoch=984
02/28/2022 23:01:13 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=989
02/28/2022 23:01:15 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.21 on epoch=994
02/28/2022 23:01:18 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.22 on epoch=999
02/28/2022 23:01:19 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 23:01:19 - INFO - __main__ - Printing 3 examples
02/28/2022 23:01:19 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
02/28/2022 23:01:19 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
02/28/2022 23:01:19 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
02/28/2022 23:01:19 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
02/28/2022 23:01:19 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
02/28/2022 23:01:19 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
02/28/2022 23:01:19 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
02/28/2022 23:01:19 - INFO - __main__ - Tokenizing Output ...
02/28/2022 23:01:19 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 23:01:19 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 23:01:19 - INFO - __main__ - Printing 3 examples
02/28/2022 23:01:19 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
02/28/2022 23:01:19 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
02/28/2022 23:01:19 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
02/28/2022 23:01:19 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
02/28/2022 23:01:19 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
02/28/2022 23:01:19 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
02/28/2022 23:01:19 - INFO - __main__ - Tokenizing Input ...
02/28/2022 23:01:19 - INFO - __main__ - Tokenizing Output ...
02/28/2022 23:01:19 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 23:01:25 - INFO - __main__ - Global step 2000 Train loss 0.21 EM 0.09375 on epoch=999
02/28/2022 23:01:25 - INFO - __main__ - save last model!
02/28/2022 23:01:26 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
02/28/2022 23:01:26 - INFO - __main__ - Start tokenizing ... 7760 instances
02/28/2022 23:01:26 - INFO - __main__ - Printing 3 examples
02/28/2022 23:01:26 - INFO - __main__ -  [break-QDMR] question: what flights are available tomorrow from denver to philadelphia 
02/28/2022 23:01:26 - INFO - __main__ - ['return flights ;return #1 from  denver ;return #2 to philadelphia ;return #3 if  available']
02/28/2022 23:01:26 - INFO - __main__ -  [break-QDMR] question: show me the afternoon flights from washington to boston 
02/28/2022 23:01:26 - INFO - __main__ - ['return flights ;return #1 from  washington ;return #2 to boston ;return #3 in the afternoon']
02/28/2022 23:01:26 - INFO - __main__ -  [break-QDMR] question: show me the flights from atlanta to baltimore 
02/28/2022 23:01:26 - INFO - __main__ - ['return flights ;return #1 from  atlanta ;return #2 to baltimore']
02/28/2022 23:01:26 - INFO - __main__ - Tokenizing Input ...
02/28/2022 23:01:29 - INFO - __main__ - Tokenizing Output ...
02/28/2022 23:01:31 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 23:01:32 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 23:01:32 - INFO - __main__ - Starting training!
02/28/2022 23:01:37 - INFO - __main__ - Loaded 7760 examples from test data
[E ProcessGroupNCCL.cpp:566] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1804905 milliseconds before timing out.
02/28/2022 23:33:03 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR/break-QDMR_32_100_0.4_8_predictions.txt
02/28/2022 23:33:03 - INFO - __main__ - EM on test data: 0.0323
02/28/2022 23:33:04 - INFO - __main__ - prefix=break-QDMR_32_100, lr=0.4, bsz=8, dev_performance=0.09375, test_performance=0.03234536082474227
02/28/2022 23:33:04 - INFO - __main__ - Running ... prefix=break-QDMR_32_100, lr=0.3, bsz=8 ...
02/28/2022 23:33:04 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 23:33:04 - INFO - __main__ - Printing 3 examples
02/28/2022 23:33:04 - INFO - __main__ -  [break-QDMR] question: In which movies with music by John Debney did Taylor Lautner star?
02/28/2022 23:33:04 - INFO - __main__ - ['return Taylor Lautner ;return movies of #1 ;return #2 with music by John Debney']
02/28/2022 23:33:04 - INFO - __main__ -  [break-QDMR] question: If the right image has a dog on a gray floor mat and green walls
02/28/2022 23:33:04 - INFO - __main__ - ['return right image ;return dog in  #1 ;return floor mat ;return #3 that is gray ;return #2 that is on #4 ;return number of  #5 ;return if  #5 is at least one ;return walls in  #1 ;return if  #8 are green ;return if  both  #7 and #9 are true']
02/28/2022 23:33:04 - INFO - __main__ -  [break-QDMR] question: What are the details and star ratings of the three hotels with the lowest price ranges?
02/28/2022 23:33:04 - INFO - __main__ - ['return hotels ;return price ranges of #1 ;return the  three lowest of #2 ;return #1 where #2 is equal to any of #3 ;return details of #4 ;return star ratings of #4 ;return #5 ,  #6']
02/28/2022 23:33:04 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
02/28/2022 23:33:04 - INFO - __main__ - Tokenizing Output ...
02/28/2022 23:33:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
02/28/2022 23:33:04 - INFO - __main__ - Start tokenizing ... 32 instances
02/28/2022 23:33:04 - INFO - __main__ - Printing 3 examples
02/28/2022 23:33:04 - INFO - __main__ -  [break-QDMR] question: If people are seated outside in a shopping area.
02/28/2022 23:33:04 - INFO - __main__ - ['return people ;return #1 that are seated outside ;return a  shopping area ;return if  #2 are in  #3']
02/28/2022 23:33:04 - INFO - __main__ -  [break-QDMR] question: If an image shows exactly two collie dogs posed outdoors, with one reclining at the left of a dog sitting upright.
02/28/2022 23:33:04 - INFO - __main__ - ['return collie dogs ;return #1 that are posed outdoors ;return #2 that are reclining ;return #2 that is sitting upright ;return #3 that is at the  left of #4 ;return images ;return number of  #1 for each  #6 ;return #6 where  #7 is equal to  two ;return number of  #5 for each  #8 ;return #8 where  #9 is equal to  one ;return number of  #10 ;return if  #11 is at least one']
02/28/2022 23:33:04 - INFO - __main__ -  [break-QDMR] question: How many locations and territories are in the Central Western Time Zone?
02/28/2022 23:33:04 - INFO - __main__ - ['return the  Central Western Time Zone ;return locations in  #1 ;return territories in  #1 ;return number of  #2 ;return number of  #3 ;return sum of #4 and  #5']
02/28/2022 23:33:04 - INFO - __main__ - Tokenizing Input ...
02/28/2022 23:33:04 - INFO - __main__ - Tokenizing Output ...
02/28/2022 23:33:05 - INFO - __main__ - Loaded 32 examples from dev data
02/28/2022 23:33:19 - INFO - __main__ - load prompt embedding from ckpt
02/28/2022 23:33:19 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
02/28/2022 23:33:19 - INFO - __main__ - Starting training!
[E ProcessGroupNCCL.cpp:325] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data. To avoid this inconsistency, we are taking the entire process down.
terminate called after throwing an instance of 'std::runtime_error'
  what():  [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(OpType=BROADCAST, Timeout(ms)=1800000) ran for 1804905 milliseconds before timing out.
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -6) local_rank: 1 (pid: 2755) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_sc17shdk/none_toayta73/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_sc17shdk/none_toayta73/attempt_1/1/error.json
Output directory () already exists and is not empty.
02/28/2022 23:35:57 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 23:35:57 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
02/28/2022 23:35:57 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
02/28/2022 23:35:57 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
02/28/2022 23:35:57 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
02/28/2022 23:35:57 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
02/28/2022 23:36:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:36:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:37:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:38:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:39:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:40:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:41:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:42:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:43:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:07 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:17 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:27 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:37 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:47 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:44:57 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:45:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:46:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:47:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:48:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:49:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:50:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:51:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:52:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:53:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:54:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:55:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:56:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:57:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:58:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
02/28/2022 23:59:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:00:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:01:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:02:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:03:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:04:58 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:08 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:18 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:28 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:38 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
03/01/2022 00:05:48 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=4, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2790) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_sc17shdk/none_toayta73/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_sc17shdk/none_toayta73/attempt_2/1/error.json
Output directory () already exists and is not empty.
03/01/2022 00:06:03 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 00:06:03 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
03/01/2022 00:06:03 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 00:06:03 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
03/01/2022 00:06:04 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 00:06:04 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 00:06:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:06:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:06:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:06:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:06:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:06:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:06:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:06:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:06:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:06:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:07:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:08:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:09:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:10:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:11:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:12:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:13:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:14:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:15:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:16:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:17:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:18:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:14 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:24 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:34 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:44 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:54 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:19:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:04 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:20:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:21:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:22:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:23:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:24:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:25:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:26:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:27:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:28:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:29:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:30:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:31:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:32:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:33:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:34:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:05 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:15 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:25 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:35 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:45 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
03/01/2022 00:35:55 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=6, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2814) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=3
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_sc17shdk/none_toayta73/attempt_3/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_sc17shdk/none_toayta73/attempt_3/1/error.json
03/01/2022 00:36:10 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 00:36:10 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
Output directory () already exists and is not empty.
03/01/2022 00:36:10 - INFO - __main__ - Namespace(task_dir='data/break-QDMR/', task_name='break-QDMR', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 00:36:10 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-break-QDMR
03/01/2022 00:36:11 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 00:36:11 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 00:36:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:36:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:36:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:36:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:36:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:36:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:36:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:36:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:37:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:38:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:39:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:40:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:41:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:42:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:43:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:44:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:45:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:46:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:47:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:48:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:49:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:50:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:51:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:52:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:01 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:11 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:21 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:31 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:41 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:53:51 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:54:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:55:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:56:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:57:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:58:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 00:59:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:00:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:01:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:02:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:03:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:04:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:12 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:22 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:32 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:42 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:05:52 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:06:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
03/01/2022 01:06:02 - INFO - torch.distributed.distributed_c10d - Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 227, in <module>
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    main()
  File "/export/share/sjoty/continual-learning/MetaPromptTuning/CrossFit_maml/singletask_from_meta.py", line 156, in main
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 1, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
    torch.distributed.init_process_group(backend="nccl")
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 219, in _store_based_barrier
    raise RuntimeError(
RuntimeError: Timed out initializing process group in store based barrier on rank: 0, for key: store_based_barrier_key:1 (world_size=2, worker_count=8, timeout=0:30:00)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2869) of binary: /opt/conda/envs/meta/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00036787986755371094 seconds
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "2869", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 10381, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.FAILED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "2870", "role": "default", "hostname": "sjoty-torch-gpu8", "state": "FAILED", "total_run_time": 10381, "rdzv_backend": "static", "raw_error": "{\"message\": \"<NONE>\"}", "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 3}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "sjoty-torch-gpu8", "state": "SUCCEEDED", "total_run_time": 10381, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python\"}", "agent_restarts": 3}}
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:354: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 2869 (local_rank 0) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/meta/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 173, in <module>
    main()
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py", line 169, in main
    run(args)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/run.py", line 621, in run
    elastic_launch(
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
***************************************
     singletask_from_meta.py FAILED    
=======================================
Root Cause:
[0]:
  time: 2022-03-01_01:06:15
  rank: 0 (local_rank: 0)
  exitcode: 1 (pid: 2869)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=======================================
Other Failures:
[1]:
  time: 2022-03-01_01:06:15
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 2870)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
***************************************

*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
++++++++++++++++++++++++++++++
kill: (2914): No such process
Task: freebase_qa, Checkpoint: models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt, Identifier: T5-large-maml-random-3e-5-2-5000-5e-1
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : singletask_from_meta.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29548
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_geaurtn1/none_08_3awac
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29548
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_geaurtn1/none_08_3awac/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_geaurtn1/none_08_3awac/attempt_0/1/error.json
Output directory () already exists and is not empty.
03/01/2022 01:06:19 - INFO - __main__ - Namespace(task_dir='data/freebase_qa/', task_name='freebase_qa', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=1, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 01:06:19 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa
03/01/2022 01:06:19 - INFO - __main__ - Namespace(task_dir='data/freebase_qa/', task_name='freebase_qa', identifier='T5-large-maml-random-3e-5-2-5000-5e-1', train_file='data', dev_file='data', test_file='data', dataset='nlp_forest_single', output_dir='models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa', do_train=True, do_predict=True, predict_checkpoint='best-model.pt', checkpoint='models/upstream-maml-random-3e-5-2-5000-5e-1/last-model.pt', do_lowercase=False, freeze_embeds=False, max_input_length=512, max_output_length=128, num_beams=4, append_another_bos=False, train_batch_size=4, predict_batch_size=16, learning_rate=0.5, weight_decay=1e-05, adam_epsilon=1e-08, max_grad_norm=1.0, gradient_accumulation_steps=1, num_train_epochs=1000.0, warmup_steps=50, total_steps=3000, wait_step=10000000000, quiet=False, eval_period=50, prefix='', debug=False, seed=42, learning_rate_list=[0.5, 0.4, 0.3, 0.2], bsz_list=[8], cache_dir='/export/share/sjoty/continual-learning/cache/', local_rank=0, log_step=10, lm_adapted_path='/export/share/sjoty/continual-learning/lm_adapted_model/torch_ckpt/large/pytorch_model.bin', model='google/t5-v1_1-large', prompt_number=100, cuda='2,3')
03/01/2022 01:06:19 - INFO - __main__ - models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa
03/01/2022 01:06:21 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 1
03/01/2022 01:06:21 - INFO - torch.distributed.distributed_c10d - Added key: store_based_barrier_key:1 to store for rank: 0
03/01/2022 01:06:21 - INFO - torch.distributed.distributed_c10d - Rank 0: Completed store-based barrier for 2 nodes.
03/01/2022 01:06:21 - INFO - __main__ - args.device: cuda:0
03/01/2022 01:06:21 - INFO - __main__ - Using 2 gpus
03/01/2022 01:06:21 - INFO - torch.distributed.distributed_c10d - Rank 1: Completed store-based barrier for 2 nodes.
03/01/2022 01:06:21 - INFO - __main__ - args.device: cuda:1
03/01/2022 01:06:21 - INFO - __main__ - Using 2 gpus
03/01/2022 01:06:21 - INFO - __main__ - Fine-tuning the following samples: ['freebase_qa_32_100', 'freebase_qa_32_13', 'freebase_qa_32_21', 'freebase_qa_32_42', 'freebase_qa_32_87']
03/01/2022 01:06:21 - INFO - __main__ - Fine-tuning the following samples: ['freebase_qa_32_100', 'freebase_qa_32_13', 'freebase_qa_32_21', 'freebase_qa_32_42', 'freebase_qa_32_87']
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
03/01/2022 01:06:28 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.5, bsz=8 ...
03/01/2022 01:06:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:06:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:06:29 - INFO - __main__ - Printing 3 examples
03/01/2022 01:06:29 - INFO - __main__ - Printing 3 examples
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 01:06:29 - INFO - __main__ - ['orange is the new black']
03/01/2022 01:06:29 - INFO - __main__ - ['orange is the new black']
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 01:06:29 - INFO - __main__ - ['western australia']
03/01/2022 01:06:29 - INFO - __main__ - ['western australia']
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 01:06:29 - INFO - __main__ - ['turkey']
03/01/2022 01:06:29 - INFO - __main__ - ['turkey']
03/01/2022 01:06:29 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:06:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 01:06:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:06:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:06:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:06:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:06:29 - INFO - __main__ - Printing 3 examples
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 01:06:29 - INFO - __main__ - ['benito mussolini']
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 01:06:29 - INFO - __main__ - ['the kinks']
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 01:06:29 - INFO - __main__ - ['saigon']
03/01/2022 01:06:29 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:06:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:06:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:06:29 - INFO - __main__ - Printing 3 examples
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 01:06:29 - INFO - __main__ - ['benito mussolini']
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 01:06:29 - INFO - __main__ - ['the kinks']
03/01/2022 01:06:29 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 01:06:29 - INFO - __main__ - ['saigon']
03/01/2022 01:06:29 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:06:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:06:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:06:29 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:06:29 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:06:44 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:06:44 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:06:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:06:44 - INFO - __main__ - Starting training!
03/01/2022 01:06:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:06:52 - INFO - __main__ - Starting training!
03/01/2022 01:06:56 - INFO - __main__ - Step 10 Global step 10 Train loss 3.18 on epoch=4
03/01/2022 01:06:58 - INFO - __main__ - Step 20 Global step 20 Train loss 2.53 on epoch=9
03/01/2022 01:07:01 - INFO - __main__ - Step 30 Global step 30 Train loss 2.39 on epoch=14
03/01/2022 01:07:03 - INFO - __main__ - Step 40 Global step 40 Train loss 2.31 on epoch=19
03/01/2022 01:07:05 - INFO - __main__ - Step 50 Global step 50 Train loss 2.19 on epoch=24
/opt/conda/envs/meta/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)
  return torch.floor_divide(self, other)
03/01/2022 01:07:06 - INFO - __main__ - Global step 50 Train loss 2.52 EM 0.0 on epoch=24
03/01/2022 01:07:06 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 01:07:09 - INFO - __main__ - Step 60 Global step 60 Train loss 2.20 on epoch=29
03/01/2022 01:07:11 - INFO - __main__ - Step 70 Global step 70 Train loss 2.13 on epoch=34
03/01/2022 01:07:13 - INFO - __main__ - Step 80 Global step 80 Train loss 2.07 on epoch=39
03/01/2022 01:07:15 - INFO - __main__ - Step 90 Global step 90 Train loss 1.94 on epoch=44
03/01/2022 01:07:17 - INFO - __main__ - Step 100 Global step 100 Train loss 1.96 on epoch=49
03/01/2022 01:07:19 - INFO - __main__ - Global step 100 Train loss 2.06 EM 0.0 on epoch=49
03/01/2022 01:07:21 - INFO - __main__ - Step 110 Global step 110 Train loss 1.89 on epoch=54
03/01/2022 01:07:23 - INFO - __main__ - Step 120 Global step 120 Train loss 1.83 on epoch=59
03/01/2022 01:07:25 - INFO - __main__ - Step 130 Global step 130 Train loss 1.80 on epoch=64
03/01/2022 01:07:27 - INFO - __main__ - Step 140 Global step 140 Train loss 1.75 on epoch=69
03/01/2022 01:07:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.67 on epoch=74
03/01/2022 01:07:31 - INFO - __main__ - Global step 150 Train loss 1.79 EM 0.0 on epoch=74
03/01/2022 01:07:33 - INFO - __main__ - Step 160 Global step 160 Train loss 1.70 on epoch=79
03/01/2022 01:07:35 - INFO - __main__ - Step 170 Global step 170 Train loss 1.63 on epoch=84
03/01/2022 01:07:37 - INFO - __main__ - Step 180 Global step 180 Train loss 1.54 on epoch=89
03/01/2022 01:07:40 - INFO - __main__ - Step 190 Global step 190 Train loss 1.46 on epoch=94
03/01/2022 01:07:42 - INFO - __main__ - Step 200 Global step 200 Train loss 1.38 on epoch=99
03/01/2022 01:07:43 - INFO - __main__ - Global step 200 Train loss 1.54 EM 0.0 on epoch=99
03/01/2022 01:07:45 - INFO - __main__ - Step 210 Global step 210 Train loss 1.40 on epoch=104
03/01/2022 01:07:47 - INFO - __main__ - Step 220 Global step 220 Train loss 1.31 on epoch=109
03/01/2022 01:07:49 - INFO - __main__ - Step 230 Global step 230 Train loss 1.30 on epoch=114
03/01/2022 01:07:52 - INFO - __main__ - Step 240 Global step 240 Train loss 1.31 on epoch=119
03/01/2022 01:07:54 - INFO - __main__ - Step 250 Global step 250 Train loss 1.28 on epoch=124
03/01/2022 01:07:55 - INFO - __main__ - Global step 250 Train loss 1.32 EM 0.0 on epoch=124
03/01/2022 01:07:57 - INFO - __main__ - Step 260 Global step 260 Train loss 1.22 on epoch=129
03/01/2022 01:07:59 - INFO - __main__ - Step 270 Global step 270 Train loss 1.22 on epoch=134
03/01/2022 01:08:01 - INFO - __main__ - Step 280 Global step 280 Train loss 1.14 on epoch=139
03/01/2022 01:08:04 - INFO - __main__ - Step 290 Global step 290 Train loss 1.20 on epoch=144
03/01/2022 01:08:06 - INFO - __main__ - Step 300 Global step 300 Train loss 1.22 on epoch=149
03/01/2022 01:08:07 - INFO - __main__ - Global step 300 Train loss 1.20 EM 0.0 on epoch=149
03/01/2022 01:08:09 - INFO - __main__ - Step 310 Global step 310 Train loss 1.17 on epoch=154
03/01/2022 01:08:11 - INFO - __main__ - Step 320 Global step 320 Train loss 1.10 on epoch=159
03/01/2022 01:08:13 - INFO - __main__ - Step 330 Global step 330 Train loss 1.02 on epoch=164
03/01/2022 01:08:16 - INFO - __main__ - Step 340 Global step 340 Train loss 1.04 on epoch=169
03/01/2022 01:08:18 - INFO - __main__ - Step 350 Global step 350 Train loss 1.11 on epoch=174
03/01/2022 01:08:19 - INFO - __main__ - Global step 350 Train loss 1.09 EM 0.0 on epoch=174
03/01/2022 01:08:21 - INFO - __main__ - Step 360 Global step 360 Train loss 1.04 on epoch=179
03/01/2022 01:08:23 - INFO - __main__ - Step 370 Global step 370 Train loss 1.05 on epoch=184
03/01/2022 01:08:26 - INFO - __main__ - Step 380 Global step 380 Train loss 0.88 on epoch=189
03/01/2022 01:08:28 - INFO - __main__ - Step 390 Global step 390 Train loss 0.94 on epoch=194
03/01/2022 01:08:30 - INFO - __main__ - Step 400 Global step 400 Train loss 0.85 on epoch=199
03/01/2022 01:08:31 - INFO - __main__ - Global step 400 Train loss 0.95 EM 0.0 on epoch=199
03/01/2022 01:08:33 - INFO - __main__ - Step 410 Global step 410 Train loss 0.84 on epoch=204
03/01/2022 01:08:35 - INFO - __main__ - Step 420 Global step 420 Train loss 0.91 on epoch=209
03/01/2022 01:08:38 - INFO - __main__ - Step 430 Global step 430 Train loss 0.86 on epoch=214
03/01/2022 01:08:40 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=219
03/01/2022 01:08:42 - INFO - __main__ - Step 450 Global step 450 Train loss 0.77 on epoch=224
03/01/2022 01:08:43 - INFO - __main__ - Global step 450 Train loss 0.85 EM 0.0 on epoch=224
03/01/2022 01:08:45 - INFO - __main__ - Step 460 Global step 460 Train loss 0.84 on epoch=229
03/01/2022 01:08:48 - INFO - __main__ - Step 470 Global step 470 Train loss 0.83 on epoch=234
03/01/2022 01:08:50 - INFO - __main__ - Step 480 Global step 480 Train loss 0.75 on epoch=239
03/01/2022 01:08:52 - INFO - __main__ - Step 490 Global step 490 Train loss 0.66 on epoch=244
03/01/2022 01:08:54 - INFO - __main__ - Step 500 Global step 500 Train loss 0.72 on epoch=249
03/01/2022 01:08:55 - INFO - __main__ - Global step 500 Train loss 0.76 EM 0.0 on epoch=249
03/01/2022 01:08:58 - INFO - __main__ - Step 510 Global step 510 Train loss 0.78 on epoch=254
03/01/2022 01:09:00 - INFO - __main__ - Step 520 Global step 520 Train loss 0.64 on epoch=259
03/01/2022 01:09:02 - INFO - __main__ - Step 530 Global step 530 Train loss 0.72 on epoch=264
03/01/2022 01:09:04 - INFO - __main__ - Step 540 Global step 540 Train loss 0.63 on epoch=269
03/01/2022 01:09:06 - INFO - __main__ - Step 550 Global step 550 Train loss 0.63 on epoch=274
03/01/2022 01:09:08 - INFO - __main__ - Global step 550 Train loss 0.68 EM 0.0 on epoch=274
03/01/2022 01:09:10 - INFO - __main__ - Step 560 Global step 560 Train loss 0.63 on epoch=279
03/01/2022 01:09:12 - INFO - __main__ - Step 570 Global step 570 Train loss 0.67 on epoch=284
03/01/2022 01:09:14 - INFO - __main__ - Step 580 Global step 580 Train loss 0.54 on epoch=289
03/01/2022 01:09:16 - INFO - __main__ - Step 590 Global step 590 Train loss 0.59 on epoch=294
03/01/2022 01:09:19 - INFO - __main__ - Step 600 Global step 600 Train loss 0.58 on epoch=299
03/01/2022 01:09:20 - INFO - __main__ - Global step 600 Train loss 0.60 EM 0.0 on epoch=299
03/01/2022 01:09:22 - INFO - __main__ - Step 610 Global step 610 Train loss 0.60 on epoch=304
03/01/2022 01:09:24 - INFO - __main__ - Step 620 Global step 620 Train loss 0.57 on epoch=309
03/01/2022 01:09:27 - INFO - __main__ - Step 630 Global step 630 Train loss 0.51 on epoch=314
03/01/2022 01:09:29 - INFO - __main__ - Step 640 Global step 640 Train loss 0.50 on epoch=319
03/01/2022 01:09:31 - INFO - __main__ - Step 650 Global step 650 Train loss 0.51 on epoch=324
03/01/2022 01:09:32 - INFO - __main__ - Global step 650 Train loss 0.54 EM 0.0 on epoch=324
03/01/2022 01:09:35 - INFO - __main__ - Step 660 Global step 660 Train loss 0.50 on epoch=329
03/01/2022 01:09:37 - INFO - __main__ - Step 670 Global step 670 Train loss 0.49 on epoch=334
03/01/2022 01:09:39 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=339
03/01/2022 01:09:41 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=344
03/01/2022 01:09:43 - INFO - __main__ - Step 700 Global step 700 Train loss 0.41 on epoch=349
03/01/2022 01:09:44 - INFO - __main__ - Global step 700 Train loss 0.45 EM 0.0 on epoch=349
03/01/2022 01:09:47 - INFO - __main__ - Step 710 Global step 710 Train loss 0.48 on epoch=354
03/01/2022 01:09:49 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=359
03/01/2022 01:09:51 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
03/01/2022 01:09:53 - INFO - __main__ - Step 740 Global step 740 Train loss 0.38 on epoch=369
03/01/2022 01:09:55 - INFO - __main__ - Step 750 Global step 750 Train loss 0.37 on epoch=374
03/01/2022 01:09:57 - INFO - __main__ - Global step 750 Train loss 0.42 EM 0.0 on epoch=374
03/01/2022 01:09:59 - INFO - __main__ - Step 760 Global step 760 Train loss 0.40 on epoch=379
03/01/2022 01:10:01 - INFO - __main__ - Step 770 Global step 770 Train loss 0.42 on epoch=384
03/01/2022 01:10:03 - INFO - __main__ - Step 780 Global step 780 Train loss 0.39 on epoch=389
03/01/2022 01:10:06 - INFO - __main__ - Step 790 Global step 790 Train loss 0.36 on epoch=394
03/01/2022 01:10:08 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=399
03/01/2022 01:10:09 - INFO - __main__ - Global step 800 Train loss 0.38 EM 0.0 on epoch=399
03/01/2022 01:10:11 - INFO - __main__ - Step 810 Global step 810 Train loss 0.36 on epoch=404
03/01/2022 01:10:13 - INFO - __main__ - Step 820 Global step 820 Train loss 0.34 on epoch=409
03/01/2022 01:10:16 - INFO - __main__ - Step 830 Global step 830 Train loss 0.33 on epoch=414
03/01/2022 01:10:18 - INFO - __main__ - Step 840 Global step 840 Train loss 0.28 on epoch=419
03/01/2022 01:10:20 - INFO - __main__ - Step 850 Global step 850 Train loss 0.30 on epoch=424
03/01/2022 01:10:21 - INFO - __main__ - Global step 850 Train loss 0.32 EM 0.0 on epoch=424
03/01/2022 01:10:23 - INFO - __main__ - Step 860 Global step 860 Train loss 0.40 on epoch=429
03/01/2022 01:10:26 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=434
03/01/2022 01:10:28 - INFO - __main__ - Step 880 Global step 880 Train loss 0.32 on epoch=439
03/01/2022 01:10:30 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
03/01/2022 01:10:32 - INFO - __main__ - Step 900 Global step 900 Train loss 0.31 on epoch=449
03/01/2022 01:10:33 - INFO - __main__ - Global step 900 Train loss 0.32 EM 0.0 on epoch=449
03/01/2022 01:10:36 - INFO - __main__ - Step 910 Global step 910 Train loss 0.33 on epoch=454
03/01/2022 01:10:38 - INFO - __main__ - Step 920 Global step 920 Train loss 0.31 on epoch=459
03/01/2022 01:10:40 - INFO - __main__ - Step 930 Global step 930 Train loss 0.32 on epoch=464
03/01/2022 01:10:42 - INFO - __main__ - Step 940 Global step 940 Train loss 0.29 on epoch=469
03/01/2022 01:10:44 - INFO - __main__ - Step 950 Global step 950 Train loss 0.32 on epoch=474
03/01/2022 01:10:46 - INFO - __main__ - Global step 950 Train loss 0.32 EM 0.0 on epoch=474
03/01/2022 01:10:48 - INFO - __main__ - Step 960 Global step 960 Train loss 0.27 on epoch=479
03/01/2022 01:10:50 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=484
03/01/2022 01:10:52 - INFO - __main__ - Step 980 Global step 980 Train loss 0.22 on epoch=489
03/01/2022 01:10:55 - INFO - __main__ - Step 990 Global step 990 Train loss 0.28 on epoch=494
03/01/2022 01:10:57 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.31 on epoch=499
03/01/2022 01:10:58 - INFO - __main__ - Global step 1000 Train loss 0.28 EM 0.0 on epoch=499
03/01/2022 01:11:00 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.27 on epoch=504
03/01/2022 01:11:03 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=509
03/01/2022 01:11:05 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.26 on epoch=514
03/01/2022 01:11:07 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.26 on epoch=519
03/01/2022 01:11:09 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.27 on epoch=524
03/01/2022 01:11:10 - INFO - __main__ - Global step 1050 Train loss 0.25 EM 0.0 on epoch=524
03/01/2022 01:11:13 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
03/01/2022 01:11:15 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.23 on epoch=534
03/01/2022 01:11:17 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.21 on epoch=539
03/01/2022 01:11:19 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
03/01/2022 01:11:21 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.19 on epoch=549
03/01/2022 01:11:23 - INFO - __main__ - Global step 1100 Train loss 0.22 EM 0.0 on epoch=549
03/01/2022 01:11:25 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.24 on epoch=554
03/01/2022 01:11:27 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.19 on epoch=559
03/01/2022 01:11:29 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.21 on epoch=564
03/01/2022 01:11:32 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.16 on epoch=569
03/01/2022 01:11:34 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.19 on epoch=574
03/01/2022 01:11:35 - INFO - __main__ - Global step 1150 Train loss 0.20 EM 0.0 on epoch=574
03/01/2022 01:11:37 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.21 on epoch=579
03/01/2022 01:11:39 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.21 on epoch=584
03/01/2022 01:11:42 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.14 on epoch=589
03/01/2022 01:11:44 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.23 on epoch=594
03/01/2022 01:11:46 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.21 on epoch=599
03/01/2022 01:11:47 - INFO - __main__ - Global step 1200 Train loss 0.20 EM 0.0 on epoch=599
03/01/2022 01:11:50 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.25 on epoch=604
03/01/2022 01:11:52 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.22 on epoch=609
03/01/2022 01:11:54 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.23 on epoch=614
03/01/2022 01:11:56 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=619
03/01/2022 01:11:58 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/01/2022 01:12:00 - INFO - __main__ - Global step 1250 Train loss 0.23 EM 0.0 on epoch=624
03/01/2022 01:12:02 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.21 on epoch=629
03/01/2022 01:12:04 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.17 on epoch=634
03/01/2022 01:12:06 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.16 on epoch=639
03/01/2022 01:12:08 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.16 on epoch=644
03/01/2022 01:12:11 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.17 on epoch=649
03/01/2022 01:12:12 - INFO - __main__ - Global step 1300 Train loss 0.17 EM 0.0 on epoch=649
03/01/2022 01:12:14 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.21 on epoch=654
03/01/2022 01:12:16 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.15 on epoch=659
03/01/2022 01:12:18 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.21 on epoch=664
03/01/2022 01:12:21 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.20 on epoch=669
03/01/2022 01:12:23 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=674
03/01/2022 01:12:24 - INFO - __main__ - Global step 1350 Train loss 0.18 EM 0.0 on epoch=674
03/01/2022 01:12:26 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=679
03/01/2022 01:12:29 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.16 on epoch=684
03/01/2022 01:12:31 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.14 on epoch=689
03/01/2022 01:12:33 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
03/01/2022 01:12:35 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.16 on epoch=699
03/01/2022 01:12:36 - INFO - __main__ - Global step 1400 Train loss 0.16 EM 0.0 on epoch=699
03/01/2022 01:12:39 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.16 on epoch=704
03/01/2022 01:12:41 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.17 on epoch=709
03/01/2022 01:12:43 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.13 on epoch=714
03/01/2022 01:12:45 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=719
03/01/2022 01:12:47 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.13 on epoch=724
03/01/2022 01:12:49 - INFO - __main__ - Global step 1450 Train loss 0.15 EM 0.0 on epoch=724
03/01/2022 01:12:51 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.16 on epoch=729
03/01/2022 01:12:53 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.14 on epoch=734
03/01/2022 01:12:55 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.15 on epoch=739
03/01/2022 01:12:58 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.13 on epoch=744
03/01/2022 01:13:00 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.13 on epoch=749
03/01/2022 01:13:01 - INFO - __main__ - Global step 1500 Train loss 0.14 EM 0.03125 on epoch=749
03/01/2022 01:13:01 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=749, global_step=1500
03/01/2022 01:13:03 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.15 on epoch=754
03/01/2022 01:13:05 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=759
03/01/2022 01:13:08 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.14 on epoch=764
03/01/2022 01:13:10 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.12 on epoch=769
03/01/2022 01:13:12 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.18 on epoch=774
03/01/2022 01:13:13 - INFO - __main__ - Global step 1550 Train loss 0.14 EM 0.0 on epoch=774
03/01/2022 01:13:15 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
03/01/2022 01:13:17 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
03/01/2022 01:13:20 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.10 on epoch=789
03/01/2022 01:13:22 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.12 on epoch=794
03/01/2022 01:13:24 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
03/01/2022 01:13:25 - INFO - __main__ - Global step 1600 Train loss 0.12 EM 0.0 on epoch=799
03/01/2022 01:13:28 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.14 on epoch=804
03/01/2022 01:13:30 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.13 on epoch=809
03/01/2022 01:13:32 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.10 on epoch=814
03/01/2022 01:13:34 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.10 on epoch=819
03/01/2022 01:13:36 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.12 on epoch=824
03/01/2022 01:13:38 - INFO - __main__ - Global step 1650 Train loss 0.12 EM 0.0 on epoch=824
03/01/2022 01:13:40 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=829
03/01/2022 01:13:42 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.07 on epoch=834
03/01/2022 01:13:44 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.11 on epoch=839
03/01/2022 01:13:46 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.11 on epoch=844
03/01/2022 01:13:49 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.14 on epoch=849
03/01/2022 01:13:50 - INFO - __main__ - Global step 1700 Train loss 0.10 EM 0.0 on epoch=849
03/01/2022 01:13:52 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.13 on epoch=854
03/01/2022 01:13:54 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=859
03/01/2022 01:13:56 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.11 on epoch=864
03/01/2022 01:13:59 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=869
03/01/2022 01:14:01 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=874
03/01/2022 01:14:02 - INFO - __main__ - Global step 1750 Train loss 0.11 EM 0.03125 on epoch=874
03/01/2022 01:14:04 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.11 on epoch=879
03/01/2022 01:14:07 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.11 on epoch=884
03/01/2022 01:14:09 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.09 on epoch=889
03/01/2022 01:14:11 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=894
03/01/2022 01:14:13 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=899
03/01/2022 01:14:15 - INFO - __main__ - Global step 1800 Train loss 0.10 EM 0.0 on epoch=899
03/01/2022 01:14:17 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.11 on epoch=904
03/01/2022 01:14:19 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.09 on epoch=909
03/01/2022 01:14:21 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.06 on epoch=914
03/01/2022 01:14:23 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=919
03/01/2022 01:14:25 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.08 on epoch=924
03/01/2022 01:14:27 - INFO - __main__ - Global step 1850 Train loss 0.09 EM 0.0 on epoch=924
03/01/2022 01:14:29 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=929
03/01/2022 01:14:31 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/01/2022 01:14:33 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=939
03/01/2022 01:14:36 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.07 on epoch=944
03/01/2022 01:14:38 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.12 on epoch=949
03/01/2022 01:14:39 - INFO - __main__ - Global step 1900 Train loss 0.10 EM 0.03125 on epoch=949
03/01/2022 01:14:41 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.08 on epoch=954
03/01/2022 01:14:43 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.08 on epoch=959
03/01/2022 01:14:46 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=964
03/01/2022 01:14:48 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=969
03/01/2022 01:14:50 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=974
03/01/2022 01:14:51 - INFO - __main__ - Global step 1950 Train loss 0.09 EM 0.0 on epoch=974
03/01/2022 01:14:53 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.12 on epoch=979
03/01/2022 01:14:56 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.10 on epoch=984
03/01/2022 01:14:58 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=989
03/01/2022 01:15:00 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.07 on epoch=994
03/01/2022 01:15:02 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.07 on epoch=999
03/01/2022 01:15:03 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:15:03 - INFO - __main__ - Printing 3 examples
03/01/2022 01:15:03 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 01:15:03 - INFO - __main__ - ['orange is the new black']
03/01/2022 01:15:03 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 01:15:03 - INFO - __main__ - ['western australia']
03/01/2022 01:15:03 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 01:15:03 - INFO - __main__ - ['turkey']
03/01/2022 01:15:03 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 01:15:03 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:15:04 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:15:04 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:15:04 - INFO - __main__ - Printing 3 examples
03/01/2022 01:15:04 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 01:15:04 - INFO - __main__ - ['benito mussolini']
03/01/2022 01:15:04 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 01:15:04 - INFO - __main__ - ['the kinks']
03/01/2022 01:15:04 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 01:15:04 - INFO - __main__ - ['saigon']
03/01/2022 01:15:04 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:15:04 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:15:04 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:15:04 - INFO - __main__ - Global step 2000 Train loss 0.08 EM 0.0 on epoch=999
03/01/2022 01:15:04 - INFO - __main__ - save last model!
03/01/2022 01:15:04 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 01:15:04 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 01:15:04 - INFO - __main__ - Printing 3 examples
03/01/2022 01:15:04 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 01:15:04 - INFO - __main__ - ['taming of the shrew']
03/01/2022 01:15:04 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 01:15:04 - INFO - __main__ - ['henry fonda']
03/01/2022 01:15:04 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 01:15:04 - INFO - __main__ - ['tchaikovsky']
03/01/2022 01:15:04 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:15:05 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:15:09 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 01:15:17 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:15:18 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:15:18 - INFO - __main__ - Starting training!
03/01/2022 01:18:10 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_100_0.5_8_predictions.txt
03/01/2022 01:18:10 - INFO - __main__ - EM on test data: 0.0095
03/01/2022 01:18:10 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.5, bsz=8, dev_performance=0.03125, test_performance=0.009514271407110666
03/01/2022 01:18:10 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.4, bsz=8 ...
03/01/2022 01:18:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:18:11 - INFO - __main__ - Printing 3 examples
03/01/2022 01:18:11 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 01:18:11 - INFO - __main__ - ['orange is the new black']
03/01/2022 01:18:11 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 01:18:11 - INFO - __main__ - ['western australia']
03/01/2022 01:18:11 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 01:18:11 - INFO - __main__ - ['turkey']
03/01/2022 01:18:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 01:18:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:18:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:18:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:18:11 - INFO - __main__ - Printing 3 examples
03/01/2022 01:18:11 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 01:18:11 - INFO - __main__ - ['benito mussolini']
03/01/2022 01:18:11 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 01:18:11 - INFO - __main__ - ['the kinks']
03/01/2022 01:18:11 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 01:18:11 - INFO - __main__ - ['saigon']
03/01/2022 01:18:11 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:18:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:18:11 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:18:25 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:18:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:18:25 - INFO - __main__ - Starting training!
03/01/2022 01:18:28 - INFO - __main__ - Step 10 Global step 10 Train loss 3.17 on epoch=4
03/01/2022 01:18:30 - INFO - __main__ - Step 20 Global step 20 Train loss 2.67 on epoch=9
03/01/2022 01:18:33 - INFO - __main__ - Step 30 Global step 30 Train loss 2.36 on epoch=14
03/01/2022 01:18:35 - INFO - __main__ - Step 40 Global step 40 Train loss 2.37 on epoch=19
03/01/2022 01:18:37 - INFO - __main__ - Step 50 Global step 50 Train loss 2.30 on epoch=24
03/01/2022 01:18:39 - INFO - __main__ - Global step 50 Train loss 2.58 EM 0.0 on epoch=24
03/01/2022 01:18:39 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 01:18:41 - INFO - __main__ - Step 60 Global step 60 Train loss 2.26 on epoch=29
03/01/2022 01:18:43 - INFO - __main__ - Step 70 Global step 70 Train loss 2.21 on epoch=34
03/01/2022 01:18:45 - INFO - __main__ - Step 80 Global step 80 Train loss 2.16 on epoch=39
03/01/2022 01:18:47 - INFO - __main__ - Step 90 Global step 90 Train loss 2.07 on epoch=44
03/01/2022 01:18:50 - INFO - __main__ - Step 100 Global step 100 Train loss 2.07 on epoch=49
03/01/2022 01:18:51 - INFO - __main__ - Global step 100 Train loss 2.15 EM 0.0 on epoch=49
03/01/2022 01:18:53 - INFO - __main__ - Step 110 Global step 110 Train loss 2.03 on epoch=54
03/01/2022 01:18:56 - INFO - __main__ - Step 120 Global step 120 Train loss 1.95 on epoch=59
03/01/2022 01:18:58 - INFO - __main__ - Step 130 Global step 130 Train loss 1.97 on epoch=64
03/01/2022 01:19:00 - INFO - __main__ - Step 140 Global step 140 Train loss 1.88 on epoch=69
03/01/2022 01:19:02 - INFO - __main__ - Step 150 Global step 150 Train loss 1.85 on epoch=74
03/01/2022 01:19:04 - INFO - __main__ - Global step 150 Train loss 1.94 EM 0.0 on epoch=74
03/01/2022 01:19:06 - INFO - __main__ - Step 160 Global step 160 Train loss 1.79 on epoch=79
03/01/2022 01:19:08 - INFO - __main__ - Step 170 Global step 170 Train loss 1.77 on epoch=84
03/01/2022 01:19:10 - INFO - __main__ - Step 180 Global step 180 Train loss 1.66 on epoch=89
03/01/2022 01:19:13 - INFO - __main__ - Step 190 Global step 190 Train loss 1.77 on epoch=94
03/01/2022 01:19:15 - INFO - __main__ - Step 200 Global step 200 Train loss 1.67 on epoch=99
03/01/2022 01:19:16 - INFO - __main__ - Global step 200 Train loss 1.73 EM 0.0 on epoch=99
03/01/2022 01:19:18 - INFO - __main__ - Step 210 Global step 210 Train loss 1.55 on epoch=104
03/01/2022 01:19:20 - INFO - __main__ - Step 220 Global step 220 Train loss 1.55 on epoch=109
03/01/2022 01:19:23 - INFO - __main__ - Step 230 Global step 230 Train loss 1.63 on epoch=114
03/01/2022 01:19:25 - INFO - __main__ - Step 240 Global step 240 Train loss 1.50 on epoch=119
03/01/2022 01:19:27 - INFO - __main__ - Step 250 Global step 250 Train loss 1.42 on epoch=124
03/01/2022 01:19:28 - INFO - __main__ - Global step 250 Train loss 1.53 EM 0.0 on epoch=124
03/01/2022 01:19:30 - INFO - __main__ - Step 260 Global step 260 Train loss 1.44 on epoch=129
03/01/2022 01:19:33 - INFO - __main__ - Step 270 Global step 270 Train loss 1.33 on epoch=134
03/01/2022 01:19:35 - INFO - __main__ - Step 280 Global step 280 Train loss 1.44 on epoch=139
03/01/2022 01:19:37 - INFO - __main__ - Step 290 Global step 290 Train loss 1.35 on epoch=144
03/01/2022 01:19:39 - INFO - __main__ - Step 300 Global step 300 Train loss 1.32 on epoch=149
03/01/2022 01:19:40 - INFO - __main__ - Global step 300 Train loss 1.37 EM 0.0 on epoch=149
03/01/2022 01:19:43 - INFO - __main__ - Step 310 Global step 310 Train loss 1.37 on epoch=154
03/01/2022 01:19:45 - INFO - __main__ - Step 320 Global step 320 Train loss 1.34 on epoch=159
03/01/2022 01:19:47 - INFO - __main__ - Step 330 Global step 330 Train loss 1.26 on epoch=164
03/01/2022 01:19:49 - INFO - __main__ - Step 340 Global step 340 Train loss 1.19 on epoch=169
03/01/2022 01:19:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.17 on epoch=174
03/01/2022 01:19:53 - INFO - __main__ - Global step 350 Train loss 1.27 EM 0.0 on epoch=174
03/01/2022 01:19:55 - INFO - __main__ - Step 360 Global step 360 Train loss 1.19 on epoch=179
03/01/2022 01:19:57 - INFO - __main__ - Step 370 Global step 370 Train loss 1.16 on epoch=184
03/01/2022 01:19:59 - INFO - __main__ - Step 380 Global step 380 Train loss 1.21 on epoch=189
03/01/2022 01:20:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.07 on epoch=194
03/01/2022 01:20:04 - INFO - __main__ - Step 400 Global step 400 Train loss 1.06 on epoch=199
03/01/2022 01:20:05 - INFO - __main__ - Global step 400 Train loss 1.14 EM 0.0 on epoch=199
03/01/2022 01:20:07 - INFO - __main__ - Step 410 Global step 410 Train loss 1.11 on epoch=204
03/01/2022 01:20:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.12 on epoch=209
03/01/2022 01:20:12 - INFO - __main__ - Step 430 Global step 430 Train loss 1.10 on epoch=214
03/01/2022 01:20:14 - INFO - __main__ - Step 440 Global step 440 Train loss 1.02 on epoch=219
03/01/2022 01:20:16 - INFO - __main__ - Step 450 Global step 450 Train loss 1.04 on epoch=224
03/01/2022 01:20:17 - INFO - __main__ - Global step 450 Train loss 1.08 EM 0.0 on epoch=224
03/01/2022 01:20:20 - INFO - __main__ - Step 460 Global step 460 Train loss 1.04 on epoch=229
03/01/2022 01:20:22 - INFO - __main__ - Step 470 Global step 470 Train loss 0.96 on epoch=234
03/01/2022 01:20:24 - INFO - __main__ - Step 480 Global step 480 Train loss 0.94 on epoch=239
03/01/2022 01:20:26 - INFO - __main__ - Step 490 Global step 490 Train loss 0.93 on epoch=244
03/01/2022 01:20:29 - INFO - __main__ - Step 500 Global step 500 Train loss 0.94 on epoch=249
03/01/2022 01:20:30 - INFO - __main__ - Global step 500 Train loss 0.96 EM 0.0 on epoch=249
03/01/2022 01:20:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.91 on epoch=254
03/01/2022 01:20:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.86 on epoch=259
03/01/2022 01:20:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.86 on epoch=264
03/01/2022 01:20:39 - INFO - __main__ - Step 540 Global step 540 Train loss 0.80 on epoch=269
03/01/2022 01:20:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=274
03/01/2022 01:20:42 - INFO - __main__ - Global step 550 Train loss 0.85 EM 0.0 on epoch=274
03/01/2022 01:20:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.85 on epoch=279
03/01/2022 01:20:47 - INFO - __main__ - Step 570 Global step 570 Train loss 0.85 on epoch=284
03/01/2022 01:20:49 - INFO - __main__ - Step 580 Global step 580 Train loss 0.79 on epoch=289
03/01/2022 01:20:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.83 on epoch=294
03/01/2022 01:20:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.77 on epoch=299
03/01/2022 01:20:54 - INFO - __main__ - Global step 600 Train loss 0.82 EM 0.0 on epoch=299
03/01/2022 01:20:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.84 on epoch=304
03/01/2022 01:20:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.74 on epoch=309
03/01/2022 01:21:01 - INFO - __main__ - Step 630 Global step 630 Train loss 0.74 on epoch=314
03/01/2022 01:21:03 - INFO - __main__ - Step 640 Global step 640 Train loss 0.65 on epoch=319
03/01/2022 01:21:06 - INFO - __main__ - Step 650 Global step 650 Train loss 0.71 on epoch=324
03/01/2022 01:21:07 - INFO - __main__ - Global step 650 Train loss 0.74 EM 0.0 on epoch=324
03/01/2022 01:21:09 - INFO - __main__ - Step 660 Global step 660 Train loss 0.64 on epoch=329
03/01/2022 01:21:11 - INFO - __main__ - Step 670 Global step 670 Train loss 0.66 on epoch=334
03/01/2022 01:21:13 - INFO - __main__ - Step 680 Global step 680 Train loss 0.60 on epoch=339
03/01/2022 01:21:16 - INFO - __main__ - Step 690 Global step 690 Train loss 0.66 on epoch=344
03/01/2022 01:21:18 - INFO - __main__ - Step 700 Global step 700 Train loss 0.65 on epoch=349
03/01/2022 01:21:19 - INFO - __main__ - Global step 700 Train loss 0.64 EM 0.0 on epoch=349
03/01/2022 01:21:21 - INFO - __main__ - Step 710 Global step 710 Train loss 0.57 on epoch=354
03/01/2022 01:21:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.64 on epoch=359
03/01/2022 01:21:26 - INFO - __main__ - Step 730 Global step 730 Train loss 0.57 on epoch=364
03/01/2022 01:21:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.62 on epoch=369
03/01/2022 01:21:30 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
03/01/2022 01:21:32 - INFO - __main__ - Global step 750 Train loss 0.59 EM 0.0 on epoch=374
03/01/2022 01:21:34 - INFO - __main__ - Step 760 Global step 760 Train loss 0.61 on epoch=379
03/01/2022 01:21:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=384
03/01/2022 01:21:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.52 on epoch=389
03/01/2022 01:21:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=394
03/01/2022 01:21:43 - INFO - __main__ - Step 800 Global step 800 Train loss 0.49 on epoch=399
03/01/2022 01:21:44 - INFO - __main__ - Global step 800 Train loss 0.53 EM 0.0 on epoch=399
03/01/2022 01:21:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.52 on epoch=404
03/01/2022 01:21:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.47 on epoch=409
03/01/2022 01:21:51 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=414
03/01/2022 01:21:53 - INFO - __main__ - Step 840 Global step 840 Train loss 0.50 on epoch=419
03/01/2022 01:21:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.51 on epoch=424
03/01/2022 01:21:56 - INFO - __main__ - Global step 850 Train loss 0.49 EM 0.0 on epoch=424
03/01/2022 01:21:59 - INFO - __main__ - Step 860 Global step 860 Train loss 0.51 on epoch=429
03/01/2022 01:22:01 - INFO - __main__ - Step 870 Global step 870 Train loss 0.44 on epoch=434
03/01/2022 01:22:03 - INFO - __main__ - Step 880 Global step 880 Train loss 0.43 on epoch=439
03/01/2022 01:22:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.46 on epoch=444
03/01/2022 01:22:08 - INFO - __main__ - Step 900 Global step 900 Train loss 0.45 on epoch=449
03/01/2022 01:22:09 - INFO - __main__ - Global step 900 Train loss 0.46 EM 0.0 on epoch=449
03/01/2022 01:22:11 - INFO - __main__ - Step 910 Global step 910 Train loss 0.45 on epoch=454
03/01/2022 01:22:13 - INFO - __main__ - Step 920 Global step 920 Train loss 0.43 on epoch=459
03/01/2022 01:22:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.42 on epoch=464
03/01/2022 01:22:18 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=469
03/01/2022 01:22:20 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
03/01/2022 01:22:21 - INFO - __main__ - Global step 950 Train loss 0.41 EM 0.0 on epoch=474
03/01/2022 01:22:23 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=479
03/01/2022 01:22:26 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
03/01/2022 01:22:28 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=489
03/01/2022 01:22:30 - INFO - __main__ - Step 990 Global step 990 Train loss 0.34 on epoch=494
03/01/2022 01:22:32 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.41 on epoch=499
03/01/2022 01:22:34 - INFO - __main__ - Global step 1000 Train loss 0.39 EM 0.0 on epoch=499
03/01/2022 01:22:36 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.32 on epoch=504
03/01/2022 01:22:38 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.37 on epoch=509
03/01/2022 01:22:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.32 on epoch=514
03/01/2022 01:22:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.32 on epoch=519
03/01/2022 01:22:45 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.34 on epoch=524
03/01/2022 01:22:46 - INFO - __main__ - Global step 1050 Train loss 0.33 EM 0.0 on epoch=524
03/01/2022 01:22:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.32 on epoch=529
03/01/2022 01:22:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.31 on epoch=534
03/01/2022 01:22:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.29 on epoch=539
03/01/2022 01:22:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.31 on epoch=544
03/01/2022 01:22:57 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=549
03/01/2022 01:22:59 - INFO - __main__ - Global step 1100 Train loss 0.31 EM 0.0 on epoch=549
03/01/2022 01:23:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=554
03/01/2022 01:23:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.30 on epoch=559
03/01/2022 01:23:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.33 on epoch=564
03/01/2022 01:23:08 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.36 on epoch=569
03/01/2022 01:23:10 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.28 on epoch=574
03/01/2022 01:23:11 - INFO - __main__ - Global step 1150 Train loss 0.31 EM 0.0 on epoch=574
03/01/2022 01:23:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.28 on epoch=579
03/01/2022 01:23:16 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=584
03/01/2022 01:23:18 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.31 on epoch=589
03/01/2022 01:23:20 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.29 on epoch=594
03/01/2022 01:23:22 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.29 on epoch=599
03/01/2022 01:23:24 - INFO - __main__ - Global step 1200 Train loss 0.29 EM 0.0 on epoch=599
03/01/2022 01:23:26 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.31 on epoch=604
03/01/2022 01:23:28 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.27 on epoch=609
03/01/2022 01:23:30 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.24 on epoch=614
03/01/2022 01:23:32 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.26 on epoch=619
03/01/2022 01:23:35 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.24 on epoch=624
03/01/2022 01:23:36 - INFO - __main__ - Global step 1250 Train loss 0.26 EM 0.0 on epoch=624
03/01/2022 01:23:38 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.29 on epoch=629
03/01/2022 01:23:40 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.27 on epoch=634
03/01/2022 01:23:43 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=639
03/01/2022 01:23:45 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.23 on epoch=644
03/01/2022 01:23:47 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.21 on epoch=649
03/01/2022 01:23:48 - INFO - __main__ - Global step 1300 Train loss 0.25 EM 0.0 on epoch=649
03/01/2022 01:23:50 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.26 on epoch=654
03/01/2022 01:23:53 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.23 on epoch=659
03/01/2022 01:23:55 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.22 on epoch=664
03/01/2022 01:23:57 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.21 on epoch=669
03/01/2022 01:23:59 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.30 on epoch=674
03/01/2022 01:24:01 - INFO - __main__ - Global step 1350 Train loss 0.25 EM 0.0 on epoch=674
03/01/2022 01:24:03 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.23 on epoch=679
03/01/2022 01:24:05 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.19 on epoch=684
03/01/2022 01:24:07 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.26 on epoch=689
03/01/2022 01:24:09 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=694
03/01/2022 01:24:12 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.22 on epoch=699
03/01/2022 01:24:13 - INFO - __main__ - Global step 1400 Train loss 0.24 EM 0.0 on epoch=699
03/01/2022 01:24:15 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=704
03/01/2022 01:24:17 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.21 on epoch=709
03/01/2022 01:24:19 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.24 on epoch=714
03/01/2022 01:24:22 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.22 on epoch=719
03/01/2022 01:24:24 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.22 on epoch=724
03/01/2022 01:24:25 - INFO - __main__ - Global step 1450 Train loss 0.22 EM 0.0 on epoch=724
03/01/2022 01:24:28 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.23 on epoch=729
03/01/2022 01:24:30 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/01/2022 01:24:32 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.21 on epoch=739
03/01/2022 01:24:34 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.19 on epoch=744
03/01/2022 01:24:36 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.19 on epoch=749
03/01/2022 01:24:38 - INFO - __main__ - Global step 1500 Train loss 0.20 EM 0.0 on epoch=749
03/01/2022 01:24:40 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.18 on epoch=754
03/01/2022 01:24:42 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.21 on epoch=759
03/01/2022 01:24:44 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=764
03/01/2022 01:24:47 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.21 on epoch=769
03/01/2022 01:24:49 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=774
03/01/2022 01:24:50 - INFO - __main__ - Global step 1550 Train loss 0.20 EM 0.0 on epoch=774
03/01/2022 01:24:52 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=779
03/01/2022 01:24:55 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.18 on epoch=784
03/01/2022 01:24:57 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.17 on epoch=789
03/01/2022 01:24:59 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=794
03/01/2022 01:25:01 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.22 on epoch=799
03/01/2022 01:25:03 - INFO - __main__ - Global step 1600 Train loss 0.19 EM 0.0 on epoch=799
03/01/2022 01:25:05 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
03/01/2022 01:25:07 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=809
03/01/2022 01:25:09 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=814
03/01/2022 01:25:12 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=819
03/01/2022 01:25:14 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.23 on epoch=824
03/01/2022 01:25:15 - INFO - __main__ - Global step 1650 Train loss 0.19 EM 0.0 on epoch=824
03/01/2022 01:25:18 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.17 on epoch=829
03/01/2022 01:25:20 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=834
03/01/2022 01:25:22 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.15 on epoch=839
03/01/2022 01:25:24 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.17 on epoch=844
03/01/2022 01:25:26 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.17 on epoch=849
03/01/2022 01:25:28 - INFO - __main__ - Global step 1700 Train loss 0.16 EM 0.0 on epoch=849
03/01/2022 01:25:30 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.19 on epoch=854
03/01/2022 01:25:32 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=859
03/01/2022 01:25:34 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.17 on epoch=864
03/01/2022 01:25:37 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=869
03/01/2022 01:25:39 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=874
03/01/2022 01:25:40 - INFO - __main__ - Global step 1750 Train loss 0.16 EM 0.0 on epoch=874
03/01/2022 01:25:42 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.14 on epoch=879
03/01/2022 01:25:45 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=884
03/01/2022 01:25:47 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/01/2022 01:25:49 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=894
03/01/2022 01:25:51 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.14 on epoch=899
03/01/2022 01:25:53 - INFO - __main__ - Global step 1800 Train loss 0.16 EM 0.0 on epoch=899
03/01/2022 01:25:55 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.15 on epoch=904
03/01/2022 01:25:57 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=909
03/01/2022 01:26:00 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=914
03/01/2022 01:26:02 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.18 on epoch=919
03/01/2022 01:26:04 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=924
03/01/2022 01:26:05 - INFO - __main__ - Global step 1850 Train loss 0.16 EM 0.0 on epoch=924
03/01/2022 01:26:07 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.12 on epoch=929
03/01/2022 01:26:10 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/01/2022 01:26:12 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=939
03/01/2022 01:26:14 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/01/2022 01:26:16 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.13 on epoch=949
03/01/2022 01:26:18 - INFO - __main__ - Global step 1900 Train loss 0.12 EM 0.0 on epoch=949
03/01/2022 01:26:20 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=954
03/01/2022 01:26:22 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.12 on epoch=959
03/01/2022 01:26:24 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.11 on epoch=964
03/01/2022 01:26:26 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.13 on epoch=969
03/01/2022 01:26:29 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.11 on epoch=974
03/01/2022 01:26:30 - INFO - __main__ - Global step 1950 Train loss 0.11 EM 0.0 on epoch=974
03/01/2022 01:26:32 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.15 on epoch=979
03/01/2022 01:26:34 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.17 on epoch=984
03/01/2022 01:26:37 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.13 on epoch=989
03/01/2022 01:26:39 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
03/01/2022 01:26:41 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=999
03/01/2022 01:26:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:26:42 - INFO - __main__ - Printing 3 examples
03/01/2022 01:26:42 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 01:26:42 - INFO - __main__ - ['orange is the new black']
03/01/2022 01:26:42 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 01:26:42 - INFO - __main__ - ['western australia']
03/01/2022 01:26:42 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 01:26:42 - INFO - __main__ - ['turkey']
03/01/2022 01:26:42 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 01:26:42 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:26:42 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:26:42 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:26:42 - INFO - __main__ - Printing 3 examples
03/01/2022 01:26:42 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 01:26:42 - INFO - __main__ - ['benito mussolini']
03/01/2022 01:26:42 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 01:26:42 - INFO - __main__ - ['the kinks']
03/01/2022 01:26:42 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 01:26:42 - INFO - __main__ - ['saigon']
03/01/2022 01:26:42 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:26:42 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:26:42 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:26:42 - INFO - __main__ - Global step 2000 Train loss 0.14 EM 0.0 on epoch=999
03/01/2022 01:26:42 - INFO - __main__ - save last model!
03/01/2022 01:26:42 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 01:26:42 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 01:26:42 - INFO - __main__ - Printing 3 examples
03/01/2022 01:26:42 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 01:26:42 - INFO - __main__ - ['taming of the shrew']
03/01/2022 01:26:42 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 01:26:42 - INFO - __main__ - ['henry fonda']
03/01/2022 01:26:42 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 01:26:42 - INFO - __main__ - ['tchaikovsky']
03/01/2022 01:26:42 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:26:44 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:26:48 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 01:26:55 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:26:55 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:26:55 - INFO - __main__ - Starting training!
03/01/2022 01:29:25 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_100_0.4_8_predictions.txt
03/01/2022 01:29:25 - INFO - __main__ - EM on test data: 0.0085
03/01/2022 01:29:25 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.4, bsz=8, dev_performance=0.0, test_performance=0.008512769153730596
03/01/2022 01:29:25 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.3, bsz=8 ...
03/01/2022 01:29:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:29:26 - INFO - __main__ - Printing 3 examples
03/01/2022 01:29:26 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 01:29:26 - INFO - __main__ - ['orange is the new black']
03/01/2022 01:29:26 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 01:29:26 - INFO - __main__ - ['western australia']
03/01/2022 01:29:26 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 01:29:26 - INFO - __main__ - ['turkey']
03/01/2022 01:29:26 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 01:29:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:29:26 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:29:26 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:29:26 - INFO - __main__ - Printing 3 examples
03/01/2022 01:29:26 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 01:29:26 - INFO - __main__ - ['benito mussolini']
03/01/2022 01:29:26 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 01:29:26 - INFO - __main__ - ['the kinks']
03/01/2022 01:29:26 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 01:29:26 - INFO - __main__ - ['saigon']
03/01/2022 01:29:26 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:29:26 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:29:26 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:29:40 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:29:41 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:29:41 - INFO - __main__ - Starting training!
03/01/2022 01:29:44 - INFO - __main__ - Step 10 Global step 10 Train loss 3.23 on epoch=4
03/01/2022 01:29:46 - INFO - __main__ - Step 20 Global step 20 Train loss 2.74 on epoch=9
03/01/2022 01:29:48 - INFO - __main__ - Step 30 Global step 30 Train loss 2.49 on epoch=14
03/01/2022 01:29:50 - INFO - __main__ - Step 40 Global step 40 Train loss 2.48 on epoch=19
03/01/2022 01:29:52 - INFO - __main__ - Step 50 Global step 50 Train loss 2.31 on epoch=24
03/01/2022 01:29:54 - INFO - __main__ - Global step 50 Train loss 2.65 EM 0.0 on epoch=24
03/01/2022 01:29:54 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 01:29:56 - INFO - __main__ - Step 60 Global step 60 Train loss 2.31 on epoch=29
03/01/2022 01:29:59 - INFO - __main__ - Step 70 Global step 70 Train loss 2.32 on epoch=34
03/01/2022 01:30:01 - INFO - __main__ - Step 80 Global step 80 Train loss 2.23 on epoch=39
03/01/2022 01:30:03 - INFO - __main__ - Step 90 Global step 90 Train loss 2.19 on epoch=44
03/01/2022 01:30:05 - INFO - __main__ - Step 100 Global step 100 Train loss 2.13 on epoch=49
03/01/2022 01:30:07 - INFO - __main__ - Global step 100 Train loss 2.24 EM 0.0 on epoch=49
03/01/2022 01:30:09 - INFO - __main__ - Step 110 Global step 110 Train loss 2.11 on epoch=54
03/01/2022 01:30:11 - INFO - __main__ - Step 120 Global step 120 Train loss 2.09 on epoch=59
03/01/2022 01:30:13 - INFO - __main__ - Step 130 Global step 130 Train loss 1.99 on epoch=64
03/01/2022 01:30:15 - INFO - __main__ - Step 140 Global step 140 Train loss 1.98 on epoch=69
03/01/2022 01:30:18 - INFO - __main__ - Step 150 Global step 150 Train loss 1.97 on epoch=74
03/01/2022 01:30:19 - INFO - __main__ - Global step 150 Train loss 2.03 EM 0.0 on epoch=74
03/01/2022 01:30:21 - INFO - __main__ - Step 160 Global step 160 Train loss 1.99 on epoch=79
03/01/2022 01:30:23 - INFO - __main__ - Step 170 Global step 170 Train loss 1.85 on epoch=84
03/01/2022 01:30:26 - INFO - __main__ - Step 180 Global step 180 Train loss 1.80 on epoch=89
03/01/2022 01:30:28 - INFO - __main__ - Step 190 Global step 190 Train loss 1.82 on epoch=94
03/01/2022 01:30:30 - INFO - __main__ - Step 200 Global step 200 Train loss 1.76 on epoch=99
03/01/2022 01:30:31 - INFO - __main__ - Global step 200 Train loss 1.84 EM 0.0 on epoch=99
03/01/2022 01:30:34 - INFO - __main__ - Step 210 Global step 210 Train loss 1.73 on epoch=104
03/01/2022 01:30:36 - INFO - __main__ - Step 220 Global step 220 Train loss 1.70 on epoch=109
03/01/2022 01:30:38 - INFO - __main__ - Step 230 Global step 230 Train loss 1.67 on epoch=114
03/01/2022 01:30:40 - INFO - __main__ - Step 240 Global step 240 Train loss 1.61 on epoch=119
03/01/2022 01:30:42 - INFO - __main__ - Step 250 Global step 250 Train loss 1.65 on epoch=124
03/01/2022 01:30:44 - INFO - __main__ - Global step 250 Train loss 1.67 EM 0.0 on epoch=124
03/01/2022 01:30:46 - INFO - __main__ - Step 260 Global step 260 Train loss 1.56 on epoch=129
03/01/2022 01:30:48 - INFO - __main__ - Step 270 Global step 270 Train loss 1.58 on epoch=134
03/01/2022 01:30:50 - INFO - __main__ - Step 280 Global step 280 Train loss 1.48 on epoch=139
03/01/2022 01:30:52 - INFO - __main__ - Step 290 Global step 290 Train loss 1.55 on epoch=144
03/01/2022 01:30:55 - INFO - __main__ - Step 300 Global step 300 Train loss 1.54 on epoch=149
03/01/2022 01:30:56 - INFO - __main__ - Global step 300 Train loss 1.54 EM 0.0 on epoch=149
03/01/2022 01:30:58 - INFO - __main__ - Step 310 Global step 310 Train loss 1.49 on epoch=154
03/01/2022 01:31:00 - INFO - __main__ - Step 320 Global step 320 Train loss 1.47 on epoch=159
03/01/2022 01:31:02 - INFO - __main__ - Step 330 Global step 330 Train loss 1.34 on epoch=164
03/01/2022 01:31:05 - INFO - __main__ - Step 340 Global step 340 Train loss 1.37 on epoch=169
03/01/2022 01:31:07 - INFO - __main__ - Step 350 Global step 350 Train loss 1.34 on epoch=174
03/01/2022 01:31:08 - INFO - __main__ - Global step 350 Train loss 1.40 EM 0.0 on epoch=174
03/01/2022 01:31:10 - INFO - __main__ - Step 360 Global step 360 Train loss 1.32 on epoch=179
03/01/2022 01:31:12 - INFO - __main__ - Step 370 Global step 370 Train loss 1.32 on epoch=184
03/01/2022 01:31:15 - INFO - __main__ - Step 380 Global step 380 Train loss 1.25 on epoch=189
03/01/2022 01:31:17 - INFO - __main__ - Step 390 Global step 390 Train loss 1.27 on epoch=194
03/01/2022 01:31:19 - INFO - __main__ - Step 400 Global step 400 Train loss 1.27 on epoch=199
03/01/2022 01:31:20 - INFO - __main__ - Global step 400 Train loss 1.29 EM 0.0 on epoch=199
03/01/2022 01:31:22 - INFO - __main__ - Step 410 Global step 410 Train loss 1.21 on epoch=204
03/01/2022 01:31:24 - INFO - __main__ - Step 420 Global step 420 Train loss 1.17 on epoch=209
03/01/2022 01:31:27 - INFO - __main__ - Step 430 Global step 430 Train loss 1.24 on epoch=214
03/01/2022 01:31:29 - INFO - __main__ - Step 440 Global step 440 Train loss 1.19 on epoch=219
03/01/2022 01:31:31 - INFO - __main__ - Step 450 Global step 450 Train loss 1.09 on epoch=224
03/01/2022 01:31:32 - INFO - __main__ - Global step 450 Train loss 1.18 EM 0.0 on epoch=224
03/01/2022 01:31:34 - INFO - __main__ - Step 460 Global step 460 Train loss 1.20 on epoch=229
03/01/2022 01:31:37 - INFO - __main__ - Step 470 Global step 470 Train loss 1.21 on epoch=234
03/01/2022 01:31:39 - INFO - __main__ - Step 480 Global step 480 Train loss 1.11 on epoch=239
03/01/2022 01:31:41 - INFO - __main__ - Step 490 Global step 490 Train loss 1.07 on epoch=244
03/01/2022 01:31:43 - INFO - __main__ - Step 500 Global step 500 Train loss 1.10 on epoch=249
03/01/2022 01:31:44 - INFO - __main__ - Global step 500 Train loss 1.14 EM 0.0 on epoch=249
03/01/2022 01:31:47 - INFO - __main__ - Step 510 Global step 510 Train loss 1.05 on epoch=254
03/01/2022 01:31:49 - INFO - __main__ - Step 520 Global step 520 Train loss 1.06 on epoch=259
03/01/2022 01:31:51 - INFO - __main__ - Step 530 Global step 530 Train loss 1.03 on epoch=264
03/01/2022 01:31:53 - INFO - __main__ - Step 540 Global step 540 Train loss 1.15 on epoch=269
03/01/2022 01:31:55 - INFO - __main__ - Step 550 Global step 550 Train loss 1.00 on epoch=274
03/01/2022 01:31:56 - INFO - __main__ - Global step 550 Train loss 1.06 EM 0.0 on epoch=274
03/01/2022 01:31:59 - INFO - __main__ - Step 560 Global step 560 Train loss 1.04 on epoch=279
03/01/2022 01:32:01 - INFO - __main__ - Step 570 Global step 570 Train loss 0.98 on epoch=284
03/01/2022 01:32:03 - INFO - __main__ - Step 580 Global step 580 Train loss 1.08 on epoch=289
03/01/2022 01:32:05 - INFO - __main__ - Step 590 Global step 590 Train loss 0.97 on epoch=294
03/01/2022 01:32:07 - INFO - __main__ - Step 600 Global step 600 Train loss 1.03 on epoch=299
03/01/2022 01:32:09 - INFO - __main__ - Global step 600 Train loss 1.02 EM 0.0 on epoch=299
03/01/2022 01:32:11 - INFO - __main__ - Step 610 Global step 610 Train loss 0.95 on epoch=304
03/01/2022 01:32:13 - INFO - __main__ - Step 620 Global step 620 Train loss 0.87 on epoch=309
03/01/2022 01:32:15 - INFO - __main__ - Step 630 Global step 630 Train loss 0.82 on epoch=314
03/01/2022 01:32:17 - INFO - __main__ - Step 640 Global step 640 Train loss 0.91 on epoch=319
03/01/2022 01:32:20 - INFO - __main__ - Step 650 Global step 650 Train loss 0.84 on epoch=324
03/01/2022 01:32:21 - INFO - __main__ - Global step 650 Train loss 0.88 EM 0.0 on epoch=324
03/01/2022 01:32:23 - INFO - __main__ - Step 660 Global step 660 Train loss 0.84 on epoch=329
03/01/2022 01:32:25 - INFO - __main__ - Step 670 Global step 670 Train loss 0.83 on epoch=334
03/01/2022 01:32:27 - INFO - __main__ - Step 680 Global step 680 Train loss 0.93 on epoch=339
03/01/2022 01:32:30 - INFO - __main__ - Step 690 Global step 690 Train loss 0.76 on epoch=344
03/01/2022 01:32:32 - INFO - __main__ - Step 700 Global step 700 Train loss 0.76 on epoch=349
03/01/2022 01:32:33 - INFO - __main__ - Global step 700 Train loss 0.82 EM 0.0 on epoch=349
03/01/2022 01:32:35 - INFO - __main__ - Step 710 Global step 710 Train loss 0.82 on epoch=354
03/01/2022 01:32:38 - INFO - __main__ - Step 720 Global step 720 Train loss 0.78 on epoch=359
03/01/2022 01:32:40 - INFO - __main__ - Step 730 Global step 730 Train loss 0.77 on epoch=364
03/01/2022 01:32:42 - INFO - __main__ - Step 740 Global step 740 Train loss 0.83 on epoch=369
03/01/2022 01:32:44 - INFO - __main__ - Step 750 Global step 750 Train loss 0.74 on epoch=374
03/01/2022 01:32:45 - INFO - __main__ - Global step 750 Train loss 0.79 EM 0.0 on epoch=374
03/01/2022 01:32:48 - INFO - __main__ - Step 760 Global step 760 Train loss 0.75 on epoch=379
03/01/2022 01:32:50 - INFO - __main__ - Step 770 Global step 770 Train loss 0.65 on epoch=384
03/01/2022 01:32:52 - INFO - __main__ - Step 780 Global step 780 Train loss 0.73 on epoch=389
03/01/2022 01:32:54 - INFO - __main__ - Step 790 Global step 790 Train loss 0.67 on epoch=394
03/01/2022 01:32:57 - INFO - __main__ - Step 800 Global step 800 Train loss 0.61 on epoch=399
03/01/2022 01:32:58 - INFO - __main__ - Global step 800 Train loss 0.68 EM 0.0 on epoch=399
03/01/2022 01:33:00 - INFO - __main__ - Step 810 Global step 810 Train loss 0.70 on epoch=404
03/01/2022 01:33:02 - INFO - __main__ - Step 820 Global step 820 Train loss 0.66 on epoch=409
03/01/2022 01:33:04 - INFO - __main__ - Step 830 Global step 830 Train loss 0.69 on epoch=414
03/01/2022 01:33:06 - INFO - __main__ - Step 840 Global step 840 Train loss 0.68 on epoch=419
03/01/2022 01:33:09 - INFO - __main__ - Step 850 Global step 850 Train loss 0.58 on epoch=424
03/01/2022 01:33:10 - INFO - __main__ - Global step 850 Train loss 0.66 EM 0.0 on epoch=424
03/01/2022 01:33:12 - INFO - __main__ - Step 860 Global step 860 Train loss 0.69 on epoch=429
03/01/2022 01:33:14 - INFO - __main__ - Step 870 Global step 870 Train loss 0.60 on epoch=434
03/01/2022 01:33:17 - INFO - __main__ - Step 880 Global step 880 Train loss 0.67 on epoch=439
03/01/2022 01:33:19 - INFO - __main__ - Step 890 Global step 890 Train loss 0.58 on epoch=444
03/01/2022 01:33:21 - INFO - __main__ - Step 900 Global step 900 Train loss 0.54 on epoch=449
03/01/2022 01:33:22 - INFO - __main__ - Global step 900 Train loss 0.62 EM 0.0 on epoch=449
03/01/2022 01:33:24 - INFO - __main__ - Step 910 Global step 910 Train loss 0.58 on epoch=454
03/01/2022 01:33:27 - INFO - __main__ - Step 920 Global step 920 Train loss 0.60 on epoch=459
03/01/2022 01:33:29 - INFO - __main__ - Step 930 Global step 930 Train loss 0.58 on epoch=464
03/01/2022 01:33:31 - INFO - __main__ - Step 940 Global step 940 Train loss 0.54 on epoch=469
03/01/2022 01:33:33 - INFO - __main__ - Step 950 Global step 950 Train loss 0.53 on epoch=474
03/01/2022 01:33:35 - INFO - __main__ - Global step 950 Train loss 0.57 EM 0.0 on epoch=474
03/01/2022 01:33:37 - INFO - __main__ - Step 960 Global step 960 Train loss 0.53 on epoch=479
03/01/2022 01:33:39 - INFO - __main__ - Step 970 Global step 970 Train loss 0.52 on epoch=484
03/01/2022 01:33:41 - INFO - __main__ - Step 980 Global step 980 Train loss 0.52 on epoch=489
03/01/2022 01:33:43 - INFO - __main__ - Step 990 Global step 990 Train loss 0.50 on epoch=494
03/01/2022 01:33:46 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.51 on epoch=499
03/01/2022 01:33:47 - INFO - __main__ - Global step 1000 Train loss 0.52 EM 0.0 on epoch=499
03/01/2022 01:33:49 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.49 on epoch=504
03/01/2022 01:33:51 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.48 on epoch=509
03/01/2022 01:33:54 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.50 on epoch=514
03/01/2022 01:33:56 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.42 on epoch=519
03/01/2022 01:33:58 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.46 on epoch=524
03/01/2022 01:33:59 - INFO - __main__ - Global step 1050 Train loss 0.47 EM 0.0 on epoch=524
03/01/2022 01:34:01 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.44 on epoch=529
03/01/2022 01:34:04 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.46 on epoch=534
03/01/2022 01:34:06 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.43 on epoch=539
03/01/2022 01:34:08 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=544
03/01/2022 01:34:10 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.38 on epoch=549
03/01/2022 01:34:11 - INFO - __main__ - Global step 1100 Train loss 0.42 EM 0.0 on epoch=549
03/01/2022 01:34:14 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.40 on epoch=554
03/01/2022 01:34:16 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.41 on epoch=559
03/01/2022 01:34:18 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=564
03/01/2022 01:34:20 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.44 on epoch=569
03/01/2022 01:34:22 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.42 on epoch=574
03/01/2022 01:34:24 - INFO - __main__ - Global step 1150 Train loss 0.42 EM 0.0 on epoch=574
03/01/2022 01:34:26 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.48 on epoch=579
03/01/2022 01:34:28 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.37 on epoch=584
03/01/2022 01:34:30 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.38 on epoch=589
03/01/2022 01:34:33 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.44 on epoch=594
03/01/2022 01:34:35 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.32 on epoch=599
03/01/2022 01:34:36 - INFO - __main__ - Global step 1200 Train loss 0.40 EM 0.0 on epoch=599
03/01/2022 01:34:38 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.41 on epoch=604
03/01/2022 01:34:41 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.41 on epoch=609
03/01/2022 01:34:43 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.34 on epoch=614
03/01/2022 01:34:45 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.36 on epoch=619
03/01/2022 01:34:47 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.36 on epoch=624
03/01/2022 01:34:48 - INFO - __main__ - Global step 1250 Train loss 0.37 EM 0.0 on epoch=624
03/01/2022 01:34:51 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.33 on epoch=629
03/01/2022 01:34:53 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.37 on epoch=634
03/01/2022 01:34:55 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.43 on epoch=639
03/01/2022 01:34:57 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.32 on epoch=644
03/01/2022 01:35:00 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.32 on epoch=649
03/01/2022 01:35:01 - INFO - __main__ - Global step 1300 Train loss 0.35 EM 0.0 on epoch=649
03/01/2022 01:35:03 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.37 on epoch=654
03/01/2022 01:35:05 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.30 on epoch=659
03/01/2022 01:35:07 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
03/01/2022 01:35:10 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.36 on epoch=669
03/01/2022 01:35:12 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=674
03/01/2022 01:35:13 - INFO - __main__ - Global step 1350 Train loss 0.34 EM 0.0 on epoch=674
03/01/2022 01:35:15 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.37 on epoch=679
03/01/2022 01:35:18 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.29 on epoch=684
03/01/2022 01:35:20 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.30 on epoch=689
03/01/2022 01:35:22 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.30 on epoch=694
03/01/2022 01:35:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.34 on epoch=699
03/01/2022 01:35:26 - INFO - __main__ - Global step 1400 Train loss 0.32 EM 0.0 on epoch=699
03/01/2022 01:35:28 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.30 on epoch=704
03/01/2022 01:35:30 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.31 on epoch=709
03/01/2022 01:35:32 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=714
03/01/2022 01:35:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.25 on epoch=719
03/01/2022 01:35:37 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=724
03/01/2022 01:35:38 - INFO - __main__ - Global step 1450 Train loss 0.28 EM 0.0 on epoch=724
03/01/2022 01:35:40 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.30 on epoch=729
03/01/2022 01:35:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.29 on epoch=734
03/01/2022 01:35:45 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.22 on epoch=739
03/01/2022 01:35:47 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.31 on epoch=744
03/01/2022 01:35:49 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.24 on epoch=749
03/01/2022 01:35:50 - INFO - __main__ - Global step 1500 Train loss 0.27 EM 0.0 on epoch=749
03/01/2022 01:35:53 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.28 on epoch=754
03/01/2022 01:35:55 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.24 on epoch=759
03/01/2022 01:35:57 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.29 on epoch=764
03/01/2022 01:35:59 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.24 on epoch=769
03/01/2022 01:36:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=774
03/01/2022 01:36:03 - INFO - __main__ - Global step 1550 Train loss 0.26 EM 0.0 on epoch=774
03/01/2022 01:36:05 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=779
03/01/2022 01:36:07 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.22 on epoch=784
03/01/2022 01:36:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.35 on epoch=789
03/01/2022 01:36:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.21 on epoch=794
03/01/2022 01:36:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.24 on epoch=799
03/01/2022 01:36:15 - INFO - __main__ - Global step 1600 Train loss 0.25 EM 0.0 on epoch=799
03/01/2022 01:36:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.23 on epoch=804
03/01/2022 01:36:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.29 on epoch=809
03/01/2022 01:36:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=814
03/01/2022 01:36:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.21 on epoch=819
03/01/2022 01:36:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=824
03/01/2022 01:36:28 - INFO - __main__ - Global step 1650 Train loss 0.23 EM 0.0 on epoch=824
03/01/2022 01:36:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.22 on epoch=829
03/01/2022 01:36:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.30 on epoch=834
03/01/2022 01:36:35 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.23 on epoch=839
03/01/2022 01:36:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.20 on epoch=844
03/01/2022 01:36:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=849
03/01/2022 01:36:40 - INFO - __main__ - Global step 1700 Train loss 0.23 EM 0.0 on epoch=849
03/01/2022 01:36:43 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.20 on epoch=854
03/01/2022 01:36:45 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.23 on epoch=859
03/01/2022 01:36:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.20 on epoch=864
03/01/2022 01:36:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.25 on epoch=869
03/01/2022 01:36:52 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.25 on epoch=874
03/01/2022 01:36:53 - INFO - __main__ - Global step 1750 Train loss 0.23 EM 0.0 on epoch=874
03/01/2022 01:36:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.18 on epoch=879
03/01/2022 01:36:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.22 on epoch=884
03/01/2022 01:37:00 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.21 on epoch=889
03/01/2022 01:37:02 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.20 on epoch=894
03/01/2022 01:37:04 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.23 on epoch=899
03/01/2022 01:37:05 - INFO - __main__ - Global step 1800 Train loss 0.21 EM 0.0 on epoch=899
03/01/2022 01:37:08 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=904
03/01/2022 01:37:10 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.23 on epoch=909
03/01/2022 01:37:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.19 on epoch=914
03/01/2022 01:37:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.20 on epoch=919
03/01/2022 01:37:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=924
03/01/2022 01:37:18 - INFO - __main__ - Global step 1850 Train loss 0.21 EM 0.0 on epoch=924
03/01/2022 01:37:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=929
03/01/2022 01:37:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=934
03/01/2022 01:37:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.23 on epoch=939
03/01/2022 01:37:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.19 on epoch=944
03/01/2022 01:37:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.19 on epoch=949
03/01/2022 01:37:30 - INFO - __main__ - Global step 1900 Train loss 0.19 EM 0.0 on epoch=949
03/01/2022 01:37:32 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.20 on epoch=954
03/01/2022 01:37:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.17 on epoch=959
03/01/2022 01:37:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.20 on epoch=964
03/01/2022 01:37:39 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
03/01/2022 01:37:41 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.18 on epoch=974
03/01/2022 01:37:43 - INFO - __main__ - Global step 1950 Train loss 0.17 EM 0.0 on epoch=974
03/01/2022 01:37:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=979
03/01/2022 01:37:47 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.13 on epoch=984
03/01/2022 01:37:49 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.15 on epoch=989
03/01/2022 01:37:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.19 on epoch=994
03/01/2022 01:37:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.12 on epoch=999
03/01/2022 01:37:55 - INFO - __main__ - Global step 2000 Train loss 0.16 EM 0.0 on epoch=999
03/01/2022 01:37:55 - INFO - __main__ - save last model!
03/01/2022 01:37:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:37:55 - INFO - __main__ - Printing 3 examples
03/01/2022 01:37:55 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 01:37:55 - INFO - __main__ - ['orange is the new black']
03/01/2022 01:37:55 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 01:37:55 - INFO - __main__ - ['western australia']
03/01/2022 01:37:55 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 01:37:55 - INFO - __main__ - ['turkey']
03/01/2022 01:37:55 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 01:37:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:37:55 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 01:37:55 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 01:37:55 - INFO - __main__ - Printing 3 examples
03/01/2022 01:37:55 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 01:37:55 - INFO - __main__ - ['taming of the shrew']
03/01/2022 01:37:55 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 01:37:55 - INFO - __main__ - ['henry fonda']
03/01/2022 01:37:55 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 01:37:55 - INFO - __main__ - ['tchaikovsky']
03/01/2022 01:37:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:37:55 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:37:55 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:37:55 - INFO - __main__ - Printing 3 examples
03/01/2022 01:37:55 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 01:37:55 - INFO - __main__ - ['benito mussolini']
03/01/2022 01:37:55 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 01:37:55 - INFO - __main__ - ['the kinks']
03/01/2022 01:37:55 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 01:37:55 - INFO - __main__ - ['saigon']
03/01/2022 01:37:55 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:37:55 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:37:55 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:37:57 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:38:01 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 01:38:09 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:38:10 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:38:10 - INFO - __main__ - Starting training!
03/01/2022 01:40:30 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_100_0.3_8_predictions.txt
03/01/2022 01:40:30 - INFO - __main__ - EM on test data: 0.0078
03/01/2022 01:40:30 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.3, bsz=8, dev_performance=0.0, test_performance=0.007761642463695543
03/01/2022 01:40:30 - INFO - __main__ - Running ... prefix=freebase_qa_32_100, lr=0.2, bsz=8 ...
03/01/2022 01:40:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:40:31 - INFO - __main__ - Printing 3 examples
03/01/2022 01:40:31 - INFO - __main__ -  [freebase_qa] What Netflix exclusive programme chronicles the life of Piper Chapman and her experiences in an American State Prison?
03/01/2022 01:40:31 - INFO - __main__ - ['orange is the new black']
03/01/2022 01:40:31 - INFO - __main__ -  [freebase_qa] The Gibson Desert is in the central area of which Australian state?
03/01/2022 01:40:31 - INFO - __main__ - ['western australia']
03/01/2022 01:40:31 - INFO - __main__ -  [freebase_qa] In which country are the Taurus Mountains?
03/01/2022 01:40:31 - INFO - __main__ - ['turkey']
03/01/2022 01:40:31 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 01:40:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:40:31 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:40:31 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:40:31 - INFO - __main__ - Printing 3 examples
03/01/2022 01:40:31 - INFO - __main__ -  [freebase_qa] Who was Italy's Fascist lender from 1925-43?
03/01/2022 01:40:31 - INFO - __main__ - ['benito mussolini']
03/01/2022 01:40:31 - INFO - __main__ -  [freebase_qa] Waterloo Sunset was a 1967 hit for which band?
03/01/2022 01:40:31 - INFO - __main__ - ['the kinks']
03/01/2022 01:40:31 - INFO - __main__ -  [freebase_qa] What was S Vietnam's Ho Chi Minh City called before 1976?
03/01/2022 01:40:31 - INFO - __main__ - ['saigon']
03/01/2022 01:40:31 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:40:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:40:31 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:40:43 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:40:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:40:44 - INFO - __main__ - Starting training!
03/01/2022 01:40:46 - INFO - __main__ - Step 10 Global step 10 Train loss 3.44 on epoch=4
03/01/2022 01:40:49 - INFO - __main__ - Step 20 Global step 20 Train loss 2.98 on epoch=9
03/01/2022 01:40:51 - INFO - __main__ - Step 30 Global step 30 Train loss 2.71 on epoch=14
03/01/2022 01:40:53 - INFO - __main__ - Step 40 Global step 40 Train loss 2.54 on epoch=19
03/01/2022 01:40:55 - INFO - __main__ - Step 50 Global step 50 Train loss 2.48 on epoch=24
03/01/2022 01:40:57 - INFO - __main__ - Global step 50 Train loss 2.83 EM 0.0 on epoch=24
03/01/2022 01:40:57 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 01:40:59 - INFO - __main__ - Step 60 Global step 60 Train loss 2.44 on epoch=29
03/01/2022 01:41:01 - INFO - __main__ - Step 70 Global step 70 Train loss 2.36 on epoch=34
03/01/2022 01:41:03 - INFO - __main__ - Step 80 Global step 80 Train loss 2.33 on epoch=39
03/01/2022 01:41:05 - INFO - __main__ - Step 90 Global step 90 Train loss 2.29 on epoch=44
03/01/2022 01:41:08 - INFO - __main__ - Step 100 Global step 100 Train loss 2.27 on epoch=49
03/01/2022 01:41:09 - INFO - __main__ - Global step 100 Train loss 2.33 EM 0.0 on epoch=49
03/01/2022 01:41:11 - INFO - __main__ - Step 110 Global step 110 Train loss 2.22 on epoch=54
03/01/2022 01:41:13 - INFO - __main__ - Step 120 Global step 120 Train loss 2.19 on epoch=59
03/01/2022 01:41:16 - INFO - __main__ - Step 130 Global step 130 Train loss 2.15 on epoch=64
03/01/2022 01:41:18 - INFO - __main__ - Step 140 Global step 140 Train loss 2.13 on epoch=69
03/01/2022 01:41:20 - INFO - __main__ - Step 150 Global step 150 Train loss 2.13 on epoch=74
03/01/2022 01:41:22 - INFO - __main__ - Global step 150 Train loss 2.16 EM 0.0 on epoch=74
03/01/2022 01:41:24 - INFO - __main__ - Step 160 Global step 160 Train loss 2.08 on epoch=79
03/01/2022 01:41:26 - INFO - __main__ - Step 170 Global step 170 Train loss 2.02 on epoch=84
03/01/2022 01:41:28 - INFO - __main__ - Step 180 Global step 180 Train loss 2.08 on epoch=89
03/01/2022 01:41:30 - INFO - __main__ - Step 190 Global step 190 Train loss 2.01 on epoch=94
03/01/2022 01:41:32 - INFO - __main__ - Step 200 Global step 200 Train loss 2.06 on epoch=99
03/01/2022 01:41:34 - INFO - __main__ - Global step 200 Train loss 2.05 EM 0.0 on epoch=99
03/01/2022 01:41:36 - INFO - __main__ - Step 210 Global step 210 Train loss 1.96 on epoch=104
03/01/2022 01:41:38 - INFO - __main__ - Step 220 Global step 220 Train loss 1.95 on epoch=109
03/01/2022 01:41:40 - INFO - __main__ - Step 230 Global step 230 Train loss 1.87 on epoch=114
03/01/2022 01:41:43 - INFO - __main__ - Step 240 Global step 240 Train loss 1.91 on epoch=119
03/01/2022 01:41:45 - INFO - __main__ - Step 250 Global step 250 Train loss 1.90 on epoch=124
03/01/2022 01:41:46 - INFO - __main__ - Global step 250 Train loss 1.92 EM 0.0 on epoch=124
03/01/2022 01:41:48 - INFO - __main__ - Step 260 Global step 260 Train loss 1.87 on epoch=129
03/01/2022 01:41:51 - INFO - __main__ - Step 270 Global step 270 Train loss 1.90 on epoch=134
03/01/2022 01:41:53 - INFO - __main__ - Step 280 Global step 280 Train loss 1.80 on epoch=139
03/01/2022 01:41:55 - INFO - __main__ - Step 290 Global step 290 Train loss 1.72 on epoch=144
03/01/2022 01:41:57 - INFO - __main__ - Step 300 Global step 300 Train loss 1.71 on epoch=149
03/01/2022 01:41:58 - INFO - __main__ - Global step 300 Train loss 1.80 EM 0.0 on epoch=149
03/01/2022 01:42:01 - INFO - __main__ - Step 310 Global step 310 Train loss 1.69 on epoch=154
03/01/2022 01:42:03 - INFO - __main__ - Step 320 Global step 320 Train loss 1.71 on epoch=159
03/01/2022 01:42:05 - INFO - __main__ - Step 330 Global step 330 Train loss 1.69 on epoch=164
03/01/2022 01:42:07 - INFO - __main__ - Step 340 Global step 340 Train loss 1.73 on epoch=169
03/01/2022 01:42:09 - INFO - __main__ - Step 350 Global step 350 Train loss 1.61 on epoch=174
03/01/2022 01:42:11 - INFO - __main__ - Global step 350 Train loss 1.69 EM 0.0 on epoch=174
03/01/2022 01:42:13 - INFO - __main__ - Step 360 Global step 360 Train loss 1.55 on epoch=179
03/01/2022 01:42:15 - INFO - __main__ - Step 370 Global step 370 Train loss 1.60 on epoch=184
03/01/2022 01:42:17 - INFO - __main__ - Step 380 Global step 380 Train loss 1.59 on epoch=189
03/01/2022 01:42:19 - INFO - __main__ - Step 390 Global step 390 Train loss 1.52 on epoch=194
03/01/2022 01:42:21 - INFO - __main__ - Step 400 Global step 400 Train loss 1.56 on epoch=199
03/01/2022 01:42:23 - INFO - __main__ - Global step 400 Train loss 1.57 EM 0.0 on epoch=199
03/01/2022 01:42:25 - INFO - __main__ - Step 410 Global step 410 Train loss 1.57 on epoch=204
03/01/2022 01:42:27 - INFO - __main__ - Step 420 Global step 420 Train loss 1.49 on epoch=209
03/01/2022 01:42:29 - INFO - __main__ - Step 430 Global step 430 Train loss 1.36 on epoch=214
03/01/2022 01:42:31 - INFO - __main__ - Step 440 Global step 440 Train loss 1.43 on epoch=219
03/01/2022 01:42:34 - INFO - __main__ - Step 450 Global step 450 Train loss 1.38 on epoch=224
03/01/2022 01:42:35 - INFO - __main__ - Global step 450 Train loss 1.45 EM 0.0 on epoch=224
03/01/2022 01:42:37 - INFO - __main__ - Step 460 Global step 460 Train loss 1.40 on epoch=229
03/01/2022 01:42:39 - INFO - __main__ - Step 470 Global step 470 Train loss 1.39 on epoch=234
03/01/2022 01:42:41 - INFO - __main__ - Step 480 Global step 480 Train loss 1.39 on epoch=239
03/01/2022 01:42:43 - INFO - __main__ - Step 490 Global step 490 Train loss 1.36 on epoch=244
03/01/2022 01:42:46 - INFO - __main__ - Step 500 Global step 500 Train loss 1.40 on epoch=249
03/01/2022 01:42:47 - INFO - __main__ - Global step 500 Train loss 1.39 EM 0.0 on epoch=249
03/01/2022 01:42:49 - INFO - __main__ - Step 510 Global step 510 Train loss 1.32 on epoch=254
03/01/2022 01:42:51 - INFO - __main__ - Step 520 Global step 520 Train loss 1.31 on epoch=259
03/01/2022 01:42:53 - INFO - __main__ - Step 530 Global step 530 Train loss 1.31 on epoch=264
03/01/2022 01:42:56 - INFO - __main__ - Step 540 Global step 540 Train loss 1.24 on epoch=269
03/01/2022 01:42:58 - INFO - __main__ - Step 550 Global step 550 Train loss 1.23 on epoch=274
03/01/2022 01:42:59 - INFO - __main__ - Global step 550 Train loss 1.28 EM 0.0 on epoch=274
03/01/2022 01:43:01 - INFO - __main__ - Step 560 Global step 560 Train loss 1.26 on epoch=279
03/01/2022 01:43:03 - INFO - __main__ - Step 570 Global step 570 Train loss 1.23 on epoch=284
03/01/2022 01:43:05 - INFO - __main__ - Step 580 Global step 580 Train loss 1.16 on epoch=289
03/01/2022 01:43:08 - INFO - __main__ - Step 590 Global step 590 Train loss 1.17 on epoch=294
03/01/2022 01:43:10 - INFO - __main__ - Step 600 Global step 600 Train loss 1.27 on epoch=299
03/01/2022 01:43:11 - INFO - __main__ - Global step 600 Train loss 1.22 EM 0.0 on epoch=299
03/01/2022 01:43:13 - INFO - __main__ - Step 610 Global step 610 Train loss 1.22 on epoch=304
03/01/2022 01:43:15 - INFO - __main__ - Step 620 Global step 620 Train loss 1.14 on epoch=309
03/01/2022 01:43:18 - INFO - __main__ - Step 630 Global step 630 Train loss 1.19 on epoch=314
03/01/2022 01:43:20 - INFO - __main__ - Step 640 Global step 640 Train loss 1.09 on epoch=319
03/01/2022 01:43:22 - INFO - __main__ - Step 650 Global step 650 Train loss 1.08 on epoch=324
03/01/2022 01:43:23 - INFO - __main__ - Global step 650 Train loss 1.15 EM 0.0 on epoch=324
03/01/2022 01:43:25 - INFO - __main__ - Step 660 Global step 660 Train loss 1.20 on epoch=329
03/01/2022 01:43:28 - INFO - __main__ - Step 670 Global step 670 Train loss 1.01 on epoch=334
03/01/2022 01:43:30 - INFO - __main__ - Step 680 Global step 680 Train loss 1.13 on epoch=339
03/01/2022 01:43:32 - INFO - __main__ - Step 690 Global step 690 Train loss 1.01 on epoch=344
03/01/2022 01:43:34 - INFO - __main__ - Step 700 Global step 700 Train loss 1.02 on epoch=349
03/01/2022 01:43:35 - INFO - __main__ - Global step 700 Train loss 1.08 EM 0.0 on epoch=349
03/01/2022 01:43:38 - INFO - __main__ - Step 710 Global step 710 Train loss 0.98 on epoch=354
03/01/2022 01:43:40 - INFO - __main__ - Step 720 Global step 720 Train loss 1.03 on epoch=359
03/01/2022 01:43:42 - INFO - __main__ - Step 730 Global step 730 Train loss 1.04 on epoch=364
03/01/2022 01:43:44 - INFO - __main__ - Step 740 Global step 740 Train loss 1.00 on epoch=369
03/01/2022 01:43:46 - INFO - __main__ - Step 750 Global step 750 Train loss 1.04 on epoch=374
03/01/2022 01:43:48 - INFO - __main__ - Global step 750 Train loss 1.02 EM 0.0 on epoch=374
03/01/2022 01:43:50 - INFO - __main__ - Step 760 Global step 760 Train loss 0.96 on epoch=379
03/01/2022 01:43:52 - INFO - __main__ - Step 770 Global step 770 Train loss 0.96 on epoch=384
03/01/2022 01:43:54 - INFO - __main__ - Step 780 Global step 780 Train loss 0.95 on epoch=389
03/01/2022 01:43:56 - INFO - __main__ - Step 790 Global step 790 Train loss 0.87 on epoch=394
03/01/2022 01:43:59 - INFO - __main__ - Step 800 Global step 800 Train loss 0.86 on epoch=399
03/01/2022 01:44:00 - INFO - __main__ - Global step 800 Train loss 0.92 EM 0.0 on epoch=399
03/01/2022 01:44:02 - INFO - __main__ - Step 810 Global step 810 Train loss 0.84 on epoch=404
03/01/2022 01:44:04 - INFO - __main__ - Step 820 Global step 820 Train loss 1.01 on epoch=409
03/01/2022 01:44:06 - INFO - __main__ - Step 830 Global step 830 Train loss 0.88 on epoch=414
03/01/2022 01:44:08 - INFO - __main__ - Step 840 Global step 840 Train loss 0.92 on epoch=419
03/01/2022 01:44:11 - INFO - __main__ - Step 850 Global step 850 Train loss 0.82 on epoch=424
03/01/2022 01:44:12 - INFO - __main__ - Global step 850 Train loss 0.89 EM 0.0 on epoch=424
03/01/2022 01:44:14 - INFO - __main__ - Step 860 Global step 860 Train loss 0.80 on epoch=429
03/01/2022 01:44:16 - INFO - __main__ - Step 870 Global step 870 Train loss 0.84 on epoch=434
03/01/2022 01:44:18 - INFO - __main__ - Step 880 Global step 880 Train loss 0.84 on epoch=439
03/01/2022 01:44:21 - INFO - __main__ - Step 890 Global step 890 Train loss 0.78 on epoch=444
03/01/2022 01:44:23 - INFO - __main__ - Step 900 Global step 900 Train loss 0.88 on epoch=449
03/01/2022 01:44:24 - INFO - __main__ - Global step 900 Train loss 0.83 EM 0.0 on epoch=449
03/01/2022 01:44:26 - INFO - __main__ - Step 910 Global step 910 Train loss 0.83 on epoch=454
03/01/2022 01:44:29 - INFO - __main__ - Step 920 Global step 920 Train loss 0.80 on epoch=459
03/01/2022 01:44:31 - INFO - __main__ - Step 930 Global step 930 Train loss 0.77 on epoch=464
03/01/2022 01:44:33 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=469
03/01/2022 01:44:35 - INFO - __main__ - Step 950 Global step 950 Train loss 0.79 on epoch=474
03/01/2022 01:44:36 - INFO - __main__ - Global step 950 Train loss 0.79 EM 0.0 on epoch=474
03/01/2022 01:44:39 - INFO - __main__ - Step 960 Global step 960 Train loss 0.74 on epoch=479
03/01/2022 01:44:41 - INFO - __main__ - Step 970 Global step 970 Train loss 0.77 on epoch=484
03/01/2022 01:44:43 - INFO - __main__ - Step 980 Global step 980 Train loss 0.72 on epoch=489
03/01/2022 01:44:45 - INFO - __main__ - Step 990 Global step 990 Train loss 0.78 on epoch=494
03/01/2022 01:44:47 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.71 on epoch=499
03/01/2022 01:44:49 - INFO - __main__ - Global step 1000 Train loss 0.75 EM 0.0 on epoch=499
03/01/2022 01:44:51 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.72 on epoch=504
03/01/2022 01:44:53 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.69 on epoch=509
03/01/2022 01:44:55 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.66 on epoch=514
03/01/2022 01:44:57 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.70 on epoch=519
03/01/2022 01:45:00 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.77 on epoch=524
03/01/2022 01:45:01 - INFO - __main__ - Global step 1050 Train loss 0.71 EM 0.0 on epoch=524
03/01/2022 01:45:03 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.65 on epoch=529
03/01/2022 01:45:05 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.72 on epoch=534
03/01/2022 01:45:07 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.63 on epoch=539
03/01/2022 01:45:10 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.61 on epoch=544
03/01/2022 01:45:12 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.69 on epoch=549
03/01/2022 01:45:13 - INFO - __main__ - Global step 1100 Train loss 0.66 EM 0.0 on epoch=549
03/01/2022 01:45:15 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.61 on epoch=554
03/01/2022 01:45:17 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.64 on epoch=559
03/01/2022 01:45:20 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.61 on epoch=564
03/01/2022 01:45:22 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.63 on epoch=569
03/01/2022 01:45:24 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.72 on epoch=574
03/01/2022 01:45:26 - INFO - __main__ - Global step 1150 Train loss 0.64 EM 0.0 on epoch=574
03/01/2022 01:45:28 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.59 on epoch=579
03/01/2022 01:45:30 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.59 on epoch=584
03/01/2022 01:45:32 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.69 on epoch=589
03/01/2022 01:45:34 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.55 on epoch=594
03/01/2022 01:45:36 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.62 on epoch=599
03/01/2022 01:45:38 - INFO - __main__ - Global step 1200 Train loss 0.61 EM 0.0 on epoch=599
03/01/2022 01:45:40 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.56 on epoch=604
03/01/2022 01:45:42 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.59 on epoch=609
03/01/2022 01:45:44 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.62 on epoch=614
03/01/2022 01:45:46 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.55 on epoch=619
03/01/2022 01:45:49 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.61 on epoch=624
03/01/2022 01:45:50 - INFO - __main__ - Global step 1250 Train loss 0.59 EM 0.0 on epoch=624
03/01/2022 01:45:52 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.61 on epoch=629
03/01/2022 01:45:54 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.59 on epoch=634
03/01/2022 01:45:57 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.54 on epoch=639
03/01/2022 01:45:59 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.64 on epoch=644
03/01/2022 01:46:01 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.54 on epoch=649
03/01/2022 01:46:02 - INFO - __main__ - Global step 1300 Train loss 0.58 EM 0.0 on epoch=649
03/01/2022 01:46:04 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.60 on epoch=654
03/01/2022 01:46:07 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.56 on epoch=659
03/01/2022 01:46:09 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.48 on epoch=664
03/01/2022 01:46:11 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.47 on epoch=669
03/01/2022 01:46:13 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.48 on epoch=674
03/01/2022 01:46:14 - INFO - __main__ - Global step 1350 Train loss 0.52 EM 0.0 on epoch=674
03/01/2022 01:46:17 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.55 on epoch=679
03/01/2022 01:46:19 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.52 on epoch=684
03/01/2022 01:46:21 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.47 on epoch=689
03/01/2022 01:46:23 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.51 on epoch=694
03/01/2022 01:46:25 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.53 on epoch=699
03/01/2022 01:46:27 - INFO - __main__ - Global step 1400 Train loss 0.52 EM 0.0 on epoch=699
03/01/2022 01:46:29 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.48 on epoch=704
03/01/2022 01:46:31 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.49 on epoch=709
03/01/2022 01:46:33 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.46 on epoch=714
03/01/2022 01:46:35 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.48 on epoch=719
03/01/2022 01:46:38 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.51 on epoch=724
03/01/2022 01:46:39 - INFO - __main__ - Global step 1450 Train loss 0.48 EM 0.0 on epoch=724
03/01/2022 01:46:41 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.47 on epoch=729
03/01/2022 01:46:43 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.41 on epoch=734
03/01/2022 01:46:46 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.49 on epoch=739
03/01/2022 01:46:48 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.49 on epoch=744
03/01/2022 01:46:50 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.44 on epoch=749
03/01/2022 01:46:51 - INFO - __main__ - Global step 1500 Train loss 0.46 EM 0.0 on epoch=749
03/01/2022 01:46:54 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.43 on epoch=754
03/01/2022 01:46:56 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.43 on epoch=759
03/01/2022 01:46:58 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.43 on epoch=764
03/01/2022 01:47:00 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.44 on epoch=769
03/01/2022 01:47:02 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.49 on epoch=774
03/01/2022 01:47:04 - INFO - __main__ - Global step 1550 Train loss 0.44 EM 0.0 on epoch=774
03/01/2022 01:47:06 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.38 on epoch=779
03/01/2022 01:47:08 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.37 on epoch=784
03/01/2022 01:47:10 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
03/01/2022 01:47:12 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.42 on epoch=794
03/01/2022 01:47:14 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.48 on epoch=799
03/01/2022 01:47:16 - INFO - __main__ - Global step 1600 Train loss 0.41 EM 0.0 on epoch=799
03/01/2022 01:47:18 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.48 on epoch=804
03/01/2022 01:47:20 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.40 on epoch=809
03/01/2022 01:47:22 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.45 on epoch=814
03/01/2022 01:47:24 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.39 on epoch=819
03/01/2022 01:47:27 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.39 on epoch=824
03/01/2022 01:47:28 - INFO - __main__ - Global step 1650 Train loss 0.42 EM 0.0 on epoch=824
03/01/2022 01:47:30 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.38 on epoch=829
03/01/2022 01:47:32 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.35 on epoch=834
03/01/2022 01:47:34 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.40 on epoch=839
03/01/2022 01:47:37 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.41 on epoch=844
03/01/2022 01:47:39 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.39 on epoch=849
03/01/2022 01:47:40 - INFO - __main__ - Global step 1700 Train loss 0.39 EM 0.0 on epoch=849
03/01/2022 01:47:42 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.39 on epoch=854
03/01/2022 01:47:44 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.44 on epoch=859
03/01/2022 01:47:47 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=864
03/01/2022 01:47:49 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.40 on epoch=869
03/01/2022 01:47:51 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.43 on epoch=874
03/01/2022 01:47:52 - INFO - __main__ - Global step 1750 Train loss 0.40 EM 0.0 on epoch=874
03/01/2022 01:47:55 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=879
03/01/2022 01:47:57 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.40 on epoch=884
03/01/2022 01:47:59 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.42 on epoch=889
03/01/2022 01:48:01 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.36 on epoch=894
03/01/2022 01:48:03 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.36 on epoch=899
03/01/2022 01:48:05 - INFO - __main__ - Global step 1800 Train loss 0.37 EM 0.0 on epoch=899
03/01/2022 01:48:07 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.40 on epoch=904
03/01/2022 01:48:09 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.41 on epoch=909
03/01/2022 01:48:12 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.35 on epoch=914
03/01/2022 01:48:14 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.35 on epoch=919
03/01/2022 01:48:16 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.37 on epoch=924
03/01/2022 01:48:18 - INFO - __main__ - Global step 1850 Train loss 0.38 EM 0.0 on epoch=924
03/01/2022 01:48:20 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.40 on epoch=929
03/01/2022 01:48:22 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.28 on epoch=934
03/01/2022 01:48:24 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.38 on epoch=939
03/01/2022 01:48:27 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.40 on epoch=944
03/01/2022 01:48:29 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.32 on epoch=949
03/01/2022 01:48:30 - INFO - __main__ - Global step 1900 Train loss 0.36 EM 0.0 on epoch=949
03/01/2022 01:48:33 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.39 on epoch=954
03/01/2022 01:48:35 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.37 on epoch=959
03/01/2022 01:48:37 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
03/01/2022 01:48:40 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.32 on epoch=969
03/01/2022 01:48:42 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.30 on epoch=974
03/01/2022 01:48:43 - INFO - __main__ - Global step 1950 Train loss 0.34 EM 0.0 on epoch=974
03/01/2022 01:48:45 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.32 on epoch=979
03/01/2022 01:48:48 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=984
03/01/2022 01:48:50 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.30 on epoch=989
03/01/2022 01:48:52 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.33 on epoch=994
03/01/2022 01:48:54 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.31 on epoch=999
03/01/2022 01:48:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:48:56 - INFO - __main__ - Printing 3 examples
03/01/2022 01:48:56 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 01:48:56 - INFO - __main__ - ['kieren fallon']
03/01/2022 01:48:56 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 01:48:56 - INFO - __main__ - ['uranus']
03/01/2022 01:48:56 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 01:48:56 - INFO - __main__ - ['rob thomas']
03/01/2022 01:48:56 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 01:48:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:48:56 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:48:56 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:48:56 - INFO - __main__ - Printing 3 examples
03/01/2022 01:48:56 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 01:48:56 - INFO - __main__ - ['terence rattigan']
03/01/2022 01:48:56 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 01:48:56 - INFO - __main__ - ['casino royale']
03/01/2022 01:48:56 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 01:48:56 - INFO - __main__ - ['offenbach']
03/01/2022 01:48:56 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:48:56 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:48:56 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:48:56 - INFO - __main__ - Global step 2000 Train loss 0.32 EM 0.0 on epoch=999
03/01/2022 01:48:56 - INFO - __main__ - save last model!
03/01/2022 01:48:56 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 01:48:56 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 01:48:56 - INFO - __main__ - Printing 3 examples
03/01/2022 01:48:56 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 01:48:56 - INFO - __main__ - ['taming of the shrew']
03/01/2022 01:48:56 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 01:48:56 - INFO - __main__ - ['henry fonda']
03/01/2022 01:48:56 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 01:48:56 - INFO - __main__ - ['tchaikovsky']
03/01/2022 01:48:56 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:48:57 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:49:01 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 01:49:08 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:49:09 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:49:09 - INFO - __main__ - Starting training!
03/01/2022 01:51:36 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_100_0.2_8_predictions.txt
03/01/2022 01:51:36 - INFO - __main__ - EM on test data: 0.0083
03/01/2022 01:51:36 - INFO - __main__ - prefix=freebase_qa_32_100, lr=0.2, bsz=8, dev_performance=0.0, test_performance=0.008262393590385579
03/01/2022 01:51:36 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.5, bsz=8 ...
03/01/2022 01:51:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:51:37 - INFO - __main__ - Printing 3 examples
03/01/2022 01:51:37 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 01:51:37 - INFO - __main__ - ['kieren fallon']
03/01/2022 01:51:37 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 01:51:37 - INFO - __main__ - ['uranus']
03/01/2022 01:51:37 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 01:51:37 - INFO - __main__ - ['rob thomas']
03/01/2022 01:51:37 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 01:51:37 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:51:37 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 01:51:37 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 01:51:37 - INFO - __main__ - Printing 3 examples
03/01/2022 01:51:37 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 01:51:37 - INFO - __main__ - ['terence rattigan']
03/01/2022 01:51:37 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 01:51:37 - INFO - __main__ - ['casino royale']
03/01/2022 01:51:37 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 01:51:37 - INFO - __main__ - ['offenbach']
03/01/2022 01:51:37 - INFO - __main__ - Tokenizing Input ...
03/01/2022 01:51:37 - INFO - __main__ - Tokenizing Output ...
03/01/2022 01:51:37 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 01:51:51 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 01:51:52 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 01:51:52 - INFO - __main__ - Starting training!
03/01/2022 01:51:57 - INFO - __main__ - Step 10 Global step 10 Train loss 3.22 on epoch=4
03/01/2022 01:51:59 - INFO - __main__ - Step 20 Global step 20 Train loss 2.59 on epoch=9
03/01/2022 01:52:01 - INFO - __main__ - Step 30 Global step 30 Train loss 2.43 on epoch=14
03/01/2022 01:52:03 - INFO - __main__ - Step 40 Global step 40 Train loss 2.40 on epoch=19
03/01/2022 01:52:05 - INFO - __main__ - Step 50 Global step 50 Train loss 2.28 on epoch=24
03/01/2022 01:52:07 - INFO - __main__ - Global step 50 Train loss 2.58 EM 0.03125 on epoch=24
03/01/2022 01:52:07 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
03/01/2022 01:52:09 - INFO - __main__ - Step 60 Global step 60 Train loss 2.24 on epoch=29
03/01/2022 01:52:11 - INFO - __main__ - Step 70 Global step 70 Train loss 2.10 on epoch=34
03/01/2022 01:52:13 - INFO - __main__ - Step 80 Global step 80 Train loss 2.11 on epoch=39
03/01/2022 01:52:16 - INFO - __main__ - Step 90 Global step 90 Train loss 1.90 on epoch=44
03/01/2022 01:52:18 - INFO - __main__ - Step 100 Global step 100 Train loss 1.87 on epoch=49
03/01/2022 01:52:19 - INFO - __main__ - Global step 100 Train loss 2.05 EM 0.0 on epoch=49
03/01/2022 01:52:21 - INFO - __main__ - Step 110 Global step 110 Train loss 1.82 on epoch=54
03/01/2022 01:52:23 - INFO - __main__ - Step 120 Global step 120 Train loss 1.74 on epoch=59
03/01/2022 01:52:26 - INFO - __main__ - Step 130 Global step 130 Train loss 1.70 on epoch=64
03/01/2022 01:52:28 - INFO - __main__ - Step 140 Global step 140 Train loss 1.70 on epoch=69
03/01/2022 01:52:30 - INFO - __main__ - Step 150 Global step 150 Train loss 1.60 on epoch=74
03/01/2022 01:52:31 - INFO - __main__ - Global step 150 Train loss 1.71 EM 0.0 on epoch=74
03/01/2022 01:52:34 - INFO - __main__ - Step 160 Global step 160 Train loss 1.58 on epoch=79
03/01/2022 01:52:36 - INFO - __main__ - Step 170 Global step 170 Train loss 1.56 on epoch=84
03/01/2022 01:52:38 - INFO - __main__ - Step 180 Global step 180 Train loss 1.45 on epoch=89
03/01/2022 01:52:40 - INFO - __main__ - Step 190 Global step 190 Train loss 1.44 on epoch=94
03/01/2022 01:52:43 - INFO - __main__ - Step 200 Global step 200 Train loss 1.34 on epoch=99
03/01/2022 01:52:44 - INFO - __main__ - Global step 200 Train loss 1.47 EM 0.0 on epoch=99
03/01/2022 01:52:46 - INFO - __main__ - Step 210 Global step 210 Train loss 1.34 on epoch=104
03/01/2022 01:52:48 - INFO - __main__ - Step 220 Global step 220 Train loss 1.25 on epoch=109
03/01/2022 01:52:50 - INFO - __main__ - Step 230 Global step 230 Train loss 1.23 on epoch=114
03/01/2022 01:52:53 - INFO - __main__ - Step 240 Global step 240 Train loss 1.20 on epoch=119
03/01/2022 01:52:55 - INFO - __main__ - Step 250 Global step 250 Train loss 1.18 on epoch=124
03/01/2022 01:52:56 - INFO - __main__ - Global step 250 Train loss 1.24 EM 0.0 on epoch=124
03/01/2022 01:52:58 - INFO - __main__ - Step 260 Global step 260 Train loss 1.13 on epoch=129
03/01/2022 01:53:00 - INFO - __main__ - Step 270 Global step 270 Train loss 1.13 on epoch=134
03/01/2022 01:53:03 - INFO - __main__ - Step 280 Global step 280 Train loss 1.11 on epoch=139
03/01/2022 01:53:05 - INFO - __main__ - Step 290 Global step 290 Train loss 1.11 on epoch=144
03/01/2022 01:53:07 - INFO - __main__ - Step 300 Global step 300 Train loss 1.06 on epoch=149
03/01/2022 01:53:08 - INFO - __main__ - Global step 300 Train loss 1.11 EM 0.0 on epoch=149
03/01/2022 01:53:10 - INFO - __main__ - Step 310 Global step 310 Train loss 0.97 on epoch=154
03/01/2022 01:53:13 - INFO - __main__ - Step 320 Global step 320 Train loss 0.98 on epoch=159
03/01/2022 01:53:15 - INFO - __main__ - Step 330 Global step 330 Train loss 0.95 on epoch=164
03/01/2022 01:53:17 - INFO - __main__ - Step 340 Global step 340 Train loss 0.84 on epoch=169
03/01/2022 01:53:19 - INFO - __main__ - Step 350 Global step 350 Train loss 0.95 on epoch=174
03/01/2022 01:53:21 - INFO - __main__ - Global step 350 Train loss 0.94 EM 0.0 on epoch=174
03/01/2022 01:53:23 - INFO - __main__ - Step 360 Global step 360 Train loss 0.94 on epoch=179
03/01/2022 01:53:25 - INFO - __main__ - Step 370 Global step 370 Train loss 0.85 on epoch=184
03/01/2022 01:53:27 - INFO - __main__ - Step 380 Global step 380 Train loss 0.76 on epoch=189
03/01/2022 01:53:29 - INFO - __main__ - Step 390 Global step 390 Train loss 0.84 on epoch=194
03/01/2022 01:53:32 - INFO - __main__ - Step 400 Global step 400 Train loss 0.75 on epoch=199
03/01/2022 01:53:33 - INFO - __main__ - Global step 400 Train loss 0.83 EM 0.0 on epoch=199
03/01/2022 01:53:35 - INFO - __main__ - Step 410 Global step 410 Train loss 0.68 on epoch=204
03/01/2022 01:53:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.78 on epoch=209
03/01/2022 01:53:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.77 on epoch=214
03/01/2022 01:53:42 - INFO - __main__ - Step 440 Global step 440 Train loss 0.68 on epoch=219
03/01/2022 01:53:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.71 on epoch=224
03/01/2022 01:53:45 - INFO - __main__ - Global step 450 Train loss 0.72 EM 0.0 on epoch=224
03/01/2022 01:53:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.71 on epoch=229
03/01/2022 01:53:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.58 on epoch=234
03/01/2022 01:53:52 - INFO - __main__ - Step 480 Global step 480 Train loss 0.63 on epoch=239
03/01/2022 01:53:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.56 on epoch=244
03/01/2022 01:53:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.59 on epoch=249
03/01/2022 01:53:57 - INFO - __main__ - Global step 500 Train loss 0.62 EM 0.0 on epoch=249
03/01/2022 01:54:00 - INFO - __main__ - Step 510 Global step 510 Train loss 0.58 on epoch=254
03/01/2022 01:54:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.52 on epoch=259
03/01/2022 01:54:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.55 on epoch=264
03/01/2022 01:54:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.60 on epoch=269
03/01/2022 01:54:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.53 on epoch=274
03/01/2022 01:54:10 - INFO - __main__ - Global step 550 Train loss 0.55 EM 0.0 on epoch=274
03/01/2022 01:54:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.50 on epoch=279
03/01/2022 01:54:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.45 on epoch=284
03/01/2022 01:54:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.47 on epoch=289
03/01/2022 01:54:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.54 on epoch=294
03/01/2022 01:54:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.41 on epoch=299
03/01/2022 01:54:22 - INFO - __main__ - Global step 600 Train loss 0.47 EM 0.0 on epoch=299
03/01/2022 01:54:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.44 on epoch=304
03/01/2022 01:54:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.51 on epoch=309
03/01/2022 01:54:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.46 on epoch=314
03/01/2022 01:54:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.43 on epoch=319
03/01/2022 01:54:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.41 on epoch=324
03/01/2022 01:54:34 - INFO - __main__ - Global step 650 Train loss 0.45 EM 0.0 on epoch=324
03/01/2022 01:54:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.41 on epoch=329
03/01/2022 01:54:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.38 on epoch=334
03/01/2022 01:54:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=339
03/01/2022 01:54:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.36 on epoch=344
03/01/2022 01:54:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=349
03/01/2022 01:54:46 - INFO - __main__ - Global step 700 Train loss 0.39 EM 0.0 on epoch=349
03/01/2022 01:54:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.38 on epoch=354
03/01/2022 01:54:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.38 on epoch=359
03/01/2022 01:54:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.38 on epoch=364
03/01/2022 01:54:55 - INFO - __main__ - Step 740 Global step 740 Train loss 0.37 on epoch=369
03/01/2022 01:54:57 - INFO - __main__ - Step 750 Global step 750 Train loss 0.31 on epoch=374
03/01/2022 01:54:59 - INFO - __main__ - Global step 750 Train loss 0.36 EM 0.0 on epoch=374
03/01/2022 01:55:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.33 on epoch=379
03/01/2022 01:55:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.29 on epoch=384
03/01/2022 01:55:05 - INFO - __main__ - Step 780 Global step 780 Train loss 0.33 on epoch=389
03/01/2022 01:55:07 - INFO - __main__ - Step 790 Global step 790 Train loss 0.33 on epoch=394
03/01/2022 01:55:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.32 on epoch=399
03/01/2022 01:55:11 - INFO - __main__ - Global step 800 Train loss 0.32 EM 0.0 on epoch=399
03/01/2022 01:55:13 - INFO - __main__ - Step 810 Global step 810 Train loss 0.26 on epoch=404
03/01/2022 01:55:15 - INFO - __main__ - Step 820 Global step 820 Train loss 0.29 on epoch=409
03/01/2022 01:55:17 - INFO - __main__ - Step 830 Global step 830 Train loss 0.30 on epoch=414
03/01/2022 01:55:19 - INFO - __main__ - Step 840 Global step 840 Train loss 0.33 on epoch=419
03/01/2022 01:55:22 - INFO - __main__ - Step 850 Global step 850 Train loss 0.29 on epoch=424
03/01/2022 01:55:23 - INFO - __main__ - Global step 850 Train loss 0.29 EM 0.0 on epoch=424
03/01/2022 01:55:25 - INFO - __main__ - Step 860 Global step 860 Train loss 0.24 on epoch=429
03/01/2022 01:55:27 - INFO - __main__ - Step 870 Global step 870 Train loss 0.30 on epoch=434
03/01/2022 01:55:30 - INFO - __main__ - Step 880 Global step 880 Train loss 0.26 on epoch=439
03/01/2022 01:55:32 - INFO - __main__ - Step 890 Global step 890 Train loss 0.28 on epoch=444
03/01/2022 01:55:34 - INFO - __main__ - Step 900 Global step 900 Train loss 0.19 on epoch=449
03/01/2022 01:55:35 - INFO - __main__ - Global step 900 Train loss 0.25 EM 0.0 on epoch=449
03/01/2022 01:55:37 - INFO - __main__ - Step 910 Global step 910 Train loss 0.27 on epoch=454
03/01/2022 01:55:40 - INFO - __main__ - Step 920 Global step 920 Train loss 0.30 on epoch=459
03/01/2022 01:55:42 - INFO - __main__ - Step 930 Global step 930 Train loss 0.25 on epoch=464
03/01/2022 01:55:44 - INFO - __main__ - Step 940 Global step 940 Train loss 0.28 on epoch=469
03/01/2022 01:55:46 - INFO - __main__ - Step 950 Global step 950 Train loss 0.19 on epoch=474
03/01/2022 01:55:48 - INFO - __main__ - Global step 950 Train loss 0.26 EM 0.0 on epoch=474
03/01/2022 01:55:50 - INFO - __main__ - Step 960 Global step 960 Train loss 0.23 on epoch=479
03/01/2022 01:55:52 - INFO - __main__ - Step 970 Global step 970 Train loss 0.25 on epoch=484
03/01/2022 01:55:54 - INFO - __main__ - Step 980 Global step 980 Train loss 0.30 on epoch=489
03/01/2022 01:55:56 - INFO - __main__ - Step 990 Global step 990 Train loss 0.24 on epoch=494
03/01/2022 01:55:58 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.21 on epoch=499
03/01/2022 01:56:00 - INFO - __main__ - Global step 1000 Train loss 0.25 EM 0.0 on epoch=499
03/01/2022 01:56:02 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.18 on epoch=504
03/01/2022 01:56:04 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.23 on epoch=509
03/01/2022 01:56:06 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.23 on epoch=514
03/01/2022 01:56:09 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.17 on epoch=519
03/01/2022 01:56:11 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.22 on epoch=524
03/01/2022 01:56:12 - INFO - __main__ - Global step 1050 Train loss 0.21 EM 0.0 on epoch=524
03/01/2022 01:56:14 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.18 on epoch=529
03/01/2022 01:56:17 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.19 on epoch=534
03/01/2022 01:56:19 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.24 on epoch=539
03/01/2022 01:56:21 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.21 on epoch=544
03/01/2022 01:56:23 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.16 on epoch=549
03/01/2022 01:56:25 - INFO - __main__ - Global step 1100 Train loss 0.20 EM 0.0 on epoch=549
03/01/2022 01:56:27 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.22 on epoch=554
03/01/2022 01:56:29 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.21 on epoch=559
03/01/2022 01:56:31 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.19 on epoch=564
03/01/2022 01:56:34 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.24 on epoch=569
03/01/2022 01:56:36 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.21 on epoch=574
03/01/2022 01:56:37 - INFO - __main__ - Global step 1150 Train loss 0.21 EM 0.0 on epoch=574
03/01/2022 01:56:39 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.19 on epoch=579
03/01/2022 01:56:42 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.17 on epoch=584
03/01/2022 01:56:44 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.20 on epoch=589
03/01/2022 01:56:46 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=594
03/01/2022 01:56:48 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.19 on epoch=599
03/01/2022 01:56:50 - INFO - __main__ - Global step 1200 Train loss 0.19 EM 0.0 on epoch=599
03/01/2022 01:56:52 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.16 on epoch=604
03/01/2022 01:56:54 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.19 on epoch=609
03/01/2022 01:56:56 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.15 on epoch=614
03/01/2022 01:56:58 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.22 on epoch=619
03/01/2022 01:57:01 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.21 on epoch=624
03/01/2022 01:57:02 - INFO - __main__ - Global step 1250 Train loss 0.19 EM 0.0 on epoch=624
03/01/2022 01:57:04 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.17 on epoch=629
03/01/2022 01:57:06 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.23 on epoch=634
03/01/2022 01:57:09 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.20 on epoch=639
03/01/2022 01:57:11 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.14 on epoch=644
03/01/2022 01:57:13 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=649
03/01/2022 01:57:14 - INFO - __main__ - Global step 1300 Train loss 0.19 EM 0.0 on epoch=649
03/01/2022 01:57:17 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.15 on epoch=654
03/01/2022 01:57:19 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.14 on epoch=659
03/01/2022 01:57:21 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.14 on epoch=664
03/01/2022 01:57:23 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.13 on epoch=669
03/01/2022 01:57:25 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.15 on epoch=674
03/01/2022 01:57:27 - INFO - __main__ - Global step 1350 Train loss 0.14 EM 0.0 on epoch=674
03/01/2022 01:57:29 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.18 on epoch=679
03/01/2022 01:57:31 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.23 on epoch=684
03/01/2022 01:57:33 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.15 on epoch=689
03/01/2022 01:57:36 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.13 on epoch=694
03/01/2022 01:57:38 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.11 on epoch=699
03/01/2022 01:57:39 - INFO - __main__ - Global step 1400 Train loss 0.16 EM 0.0 on epoch=699
03/01/2022 01:57:41 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.18 on epoch=704
03/01/2022 01:57:43 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=709
03/01/2022 01:57:46 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.10 on epoch=714
03/01/2022 01:57:48 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.15 on epoch=719
03/01/2022 01:57:50 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.15 on epoch=724
03/01/2022 01:57:51 - INFO - __main__ - Global step 1450 Train loss 0.14 EM 0.0 on epoch=724
03/01/2022 01:57:54 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.14 on epoch=729
03/01/2022 01:57:56 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.18 on epoch=734
03/01/2022 01:57:58 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/01/2022 01:58:00 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.16 on epoch=744
03/01/2022 01:58:02 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.12 on epoch=749
03/01/2022 01:58:04 - INFO - __main__ - Global step 1500 Train loss 0.15 EM 0.0 on epoch=749
03/01/2022 01:58:06 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.11 on epoch=754
03/01/2022 01:58:08 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.13 on epoch=759
03/01/2022 01:58:10 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.15 on epoch=764
03/01/2022 01:58:13 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.14 on epoch=769
03/01/2022 01:58:15 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.12 on epoch=774
03/01/2022 01:58:16 - INFO - __main__ - Global step 1550 Train loss 0.13 EM 0.0 on epoch=774
03/01/2022 01:58:18 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.13 on epoch=779
03/01/2022 01:58:20 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.11 on epoch=784
03/01/2022 01:58:23 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.13 on epoch=789
03/01/2022 01:58:25 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=794
03/01/2022 01:58:27 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=799
03/01/2022 01:58:28 - INFO - __main__ - Global step 1600 Train loss 0.14 EM 0.0 on epoch=799
03/01/2022 01:58:31 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.15 on epoch=804
03/01/2022 01:58:33 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.12 on epoch=809
03/01/2022 01:58:35 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.09 on epoch=814
03/01/2022 01:58:37 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=819
03/01/2022 01:58:39 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=824
03/01/2022 01:58:41 - INFO - __main__ - Global step 1650 Train loss 0.12 EM 0.0 on epoch=824
03/01/2022 01:58:43 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.09 on epoch=829
03/01/2022 01:58:45 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=834
03/01/2022 01:58:47 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.10 on epoch=839
03/01/2022 01:58:49 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.09 on epoch=844
03/01/2022 01:58:52 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.10 on epoch=849
03/01/2022 01:58:53 - INFO - __main__ - Global step 1700 Train loss 0.10 EM 0.0 on epoch=849
03/01/2022 01:58:55 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.09 on epoch=854
03/01/2022 01:58:57 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.11 on epoch=859
03/01/2022 01:58:59 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.12 on epoch=864
03/01/2022 01:59:02 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.11 on epoch=869
03/01/2022 01:59:04 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.12 on epoch=874
03/01/2022 01:59:05 - INFO - __main__ - Global step 1750 Train loss 0.11 EM 0.0 on epoch=874
03/01/2022 01:59:07 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
03/01/2022 01:59:10 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=884
03/01/2022 01:59:12 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.08 on epoch=889
03/01/2022 01:59:14 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.07 on epoch=894
03/01/2022 01:59:16 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.09 on epoch=899
03/01/2022 01:59:18 - INFO - __main__ - Global step 1800 Train loss 0.09 EM 0.0 on epoch=899
03/01/2022 01:59:20 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.12 on epoch=904
03/01/2022 01:59:22 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.12 on epoch=909
03/01/2022 01:59:24 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.10 on epoch=914
03/01/2022 01:59:26 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.09 on epoch=919
03/01/2022 01:59:29 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.13 on epoch=924
03/01/2022 01:59:30 - INFO - __main__ - Global step 1850 Train loss 0.11 EM 0.0 on epoch=924
03/01/2022 01:59:32 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.06 on epoch=929
03/01/2022 01:59:34 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.11 on epoch=934
03/01/2022 01:59:37 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.13 on epoch=939
03/01/2022 01:59:39 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.08 on epoch=944
03/01/2022 01:59:41 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.08 on epoch=949
03/01/2022 01:59:42 - INFO - __main__ - Global step 1900 Train loss 0.09 EM 0.0 on epoch=949
03/01/2022 01:59:45 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.05 on epoch=954
03/01/2022 01:59:47 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.09 on epoch=959
03/01/2022 01:59:49 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.07 on epoch=964
03/01/2022 01:59:51 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.08 on epoch=969
03/01/2022 01:59:53 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.10 on epoch=974
03/01/2022 01:59:55 - INFO - __main__ - Global step 1950 Train loss 0.08 EM 0.0 on epoch=974
03/01/2022 01:59:57 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.08 on epoch=979
03/01/2022 01:59:59 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.08 on epoch=984
03/01/2022 02:00:01 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.07 on epoch=989
03/01/2022 02:00:04 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.06 on epoch=994
03/01/2022 02:00:06 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.09 on epoch=999
03/01/2022 02:00:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:00:07 - INFO - __main__ - Printing 3 examples
03/01/2022 02:00:07 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 02:00:07 - INFO - __main__ - ['kieren fallon']
03/01/2022 02:00:07 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 02:00:07 - INFO - __main__ - ['uranus']
03/01/2022 02:00:07 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 02:00:07 - INFO - __main__ - ['rob thomas']
03/01/2022 02:00:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 02:00:07 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:00:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:00:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:00:07 - INFO - __main__ - Printing 3 examples
03/01/2022 02:00:07 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 02:00:07 - INFO - __main__ - ['terence rattigan']
03/01/2022 02:00:07 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 02:00:07 - INFO - __main__ - ['casino royale']
03/01/2022 02:00:07 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 02:00:07 - INFO - __main__ - ['offenbach']
03/01/2022 02:00:07 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:00:07 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:00:07 - INFO - __main__ - Global step 2000 Train loss 0.08 EM 0.0 on epoch=999
03/01/2022 02:00:07 - INFO - __main__ - save last model!
03/01/2022 02:00:07 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:00:07 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 02:00:07 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 02:00:07 - INFO - __main__ - Printing 3 examples
03/01/2022 02:00:07 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 02:00:07 - INFO - __main__ - ['taming of the shrew']
03/01/2022 02:00:07 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 02:00:07 - INFO - __main__ - ['henry fonda']
03/01/2022 02:00:07 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 02:00:07 - INFO - __main__ - ['tchaikovsky']
03/01/2022 02:00:07 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:00:09 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:00:13 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 02:00:20 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:00:21 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:00:21 - INFO - __main__ - Starting training!
03/01/2022 02:02:49 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_13_0.5_8_predictions.txt
03/01/2022 02:02:49 - INFO - __main__ - EM on test data: 0.0065
03/01/2022 02:02:50 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.5, bsz=8, dev_performance=0.03125, test_performance=0.006509764646970456
03/01/2022 02:02:50 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.4, bsz=8 ...
03/01/2022 02:02:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:02:51 - INFO - __main__ - Printing 3 examples
03/01/2022 02:02:51 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 02:02:51 - INFO - __main__ - ['kieren fallon']
03/01/2022 02:02:51 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 02:02:51 - INFO - __main__ - ['uranus']
03/01/2022 02:02:51 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 02:02:51 - INFO - __main__ - ['rob thomas']
03/01/2022 02:02:51 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 02:02:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:02:51 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:02:51 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:02:51 - INFO - __main__ - Printing 3 examples
03/01/2022 02:02:51 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 02:02:51 - INFO - __main__ - ['terence rattigan']
03/01/2022 02:02:51 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 02:02:51 - INFO - __main__ - ['casino royale']
03/01/2022 02:02:51 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 02:02:51 - INFO - __main__ - ['offenbach']
03/01/2022 02:02:51 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:02:51 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:02:51 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:03:05 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:03:06 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:03:06 - INFO - __main__ - Starting training!
03/01/2022 02:03:08 - INFO - __main__ - Step 10 Global step 10 Train loss 3.23 on epoch=4
03/01/2022 02:03:11 - INFO - __main__ - Step 20 Global step 20 Train loss 2.62 on epoch=9
03/01/2022 02:03:13 - INFO - __main__ - Step 30 Global step 30 Train loss 2.51 on epoch=14
03/01/2022 02:03:15 - INFO - __main__ - Step 40 Global step 40 Train loss 2.40 on epoch=19
03/01/2022 02:03:17 - INFO - __main__ - Step 50 Global step 50 Train loss 2.30 on epoch=24
03/01/2022 02:03:19 - INFO - __main__ - Global step 50 Train loss 2.61 EM 0.0 on epoch=24
03/01/2022 02:03:19 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 02:03:21 - INFO - __main__ - Step 60 Global step 60 Train loss 2.24 on epoch=29
03/01/2022 02:03:23 - INFO - __main__ - Step 70 Global step 70 Train loss 2.25 on epoch=34
03/01/2022 02:03:25 - INFO - __main__ - Step 80 Global step 80 Train loss 2.19 on epoch=39
03/01/2022 02:03:27 - INFO - __main__ - Step 90 Global step 90 Train loss 2.04 on epoch=44
03/01/2022 02:03:30 - INFO - __main__ - Step 100 Global step 100 Train loss 1.99 on epoch=49
03/01/2022 02:03:31 - INFO - __main__ - Global step 100 Train loss 2.14 EM 0.03125 on epoch=49
03/01/2022 02:03:31 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=49, global_step=100
03/01/2022 02:03:33 - INFO - __main__ - Step 110 Global step 110 Train loss 1.89 on epoch=54
03/01/2022 02:03:35 - INFO - __main__ - Step 120 Global step 120 Train loss 1.93 on epoch=59
03/01/2022 02:03:37 - INFO - __main__ - Step 130 Global step 130 Train loss 1.92 on epoch=64
03/01/2022 02:03:39 - INFO - __main__ - Step 140 Global step 140 Train loss 1.80 on epoch=69
03/01/2022 02:03:42 - INFO - __main__ - Step 150 Global step 150 Train loss 1.86 on epoch=74
03/01/2022 02:03:43 - INFO - __main__ - Global step 150 Train loss 1.88 EM 0.0 on epoch=74
03/01/2022 02:03:45 - INFO - __main__ - Step 160 Global step 160 Train loss 1.75 on epoch=79
03/01/2022 02:03:47 - INFO - __main__ - Step 170 Global step 170 Train loss 1.67 on epoch=84
03/01/2022 02:03:49 - INFO - __main__ - Step 180 Global step 180 Train loss 1.71 on epoch=89
03/01/2022 02:03:51 - INFO - __main__ - Step 190 Global step 190 Train loss 1.51 on epoch=94
03/01/2022 02:03:53 - INFO - __main__ - Step 200 Global step 200 Train loss 1.52 on epoch=99
03/01/2022 02:03:55 - INFO - __main__ - Global step 200 Train loss 1.63 EM 0.0 on epoch=99
03/01/2022 02:03:57 - INFO - __main__ - Step 210 Global step 210 Train loss 1.53 on epoch=104
03/01/2022 02:03:59 - INFO - __main__ - Step 220 Global step 220 Train loss 1.42 on epoch=109
03/01/2022 02:04:01 - INFO - __main__ - Step 230 Global step 230 Train loss 1.41 on epoch=114
03/01/2022 02:04:03 - INFO - __main__ - Step 240 Global step 240 Train loss 1.42 on epoch=119
03/01/2022 02:04:05 - INFO - __main__ - Step 250 Global step 250 Train loss 1.37 on epoch=124
03/01/2022 02:04:07 - INFO - __main__ - Global step 250 Train loss 1.43 EM 0.0 on epoch=124
03/01/2022 02:04:09 - INFO - __main__ - Step 260 Global step 260 Train loss 1.32 on epoch=129
03/01/2022 02:04:11 - INFO - __main__ - Step 270 Global step 270 Train loss 1.24 on epoch=134
03/01/2022 02:04:13 - INFO - __main__ - Step 280 Global step 280 Train loss 1.26 on epoch=139
03/01/2022 02:04:15 - INFO - __main__ - Step 290 Global step 290 Train loss 1.25 on epoch=144
03/01/2022 02:04:17 - INFO - __main__ - Step 300 Global step 300 Train loss 1.07 on epoch=149
03/01/2022 02:04:18 - INFO - __main__ - Global step 300 Train loss 1.23 EM 0.0 on epoch=149
03/01/2022 02:04:20 - INFO - __main__ - Step 310 Global step 310 Train loss 1.12 on epoch=154
03/01/2022 02:04:23 - INFO - __main__ - Step 320 Global step 320 Train loss 1.12 on epoch=159
03/01/2022 02:04:25 - INFO - __main__ - Step 330 Global step 330 Train loss 1.10 on epoch=164
03/01/2022 02:04:27 - INFO - __main__ - Step 340 Global step 340 Train loss 1.07 on epoch=169
03/01/2022 02:04:29 - INFO - __main__ - Step 350 Global step 350 Train loss 1.01 on epoch=174
03/01/2022 02:04:30 - INFO - __main__ - Global step 350 Train loss 1.08 EM 0.0 on epoch=174
03/01/2022 02:04:32 - INFO - __main__ - Step 360 Global step 360 Train loss 1.06 on epoch=179
03/01/2022 02:04:34 - INFO - __main__ - Step 370 Global step 370 Train loss 1.03 on epoch=184
03/01/2022 02:04:37 - INFO - __main__ - Step 380 Global step 380 Train loss 1.01 on epoch=189
03/01/2022 02:04:39 - INFO - __main__ - Step 390 Global step 390 Train loss 1.00 on epoch=194
03/01/2022 02:04:41 - INFO - __main__ - Step 400 Global step 400 Train loss 0.96 on epoch=199
03/01/2022 02:04:42 - INFO - __main__ - Global step 400 Train loss 1.01 EM 0.0 on epoch=199
03/01/2022 02:04:44 - INFO - __main__ - Step 410 Global step 410 Train loss 0.94 on epoch=204
03/01/2022 02:04:46 - INFO - __main__ - Step 420 Global step 420 Train loss 0.90 on epoch=209
03/01/2022 02:04:48 - INFO - __main__ - Step 430 Global step 430 Train loss 0.92 on epoch=214
03/01/2022 02:04:50 - INFO - __main__ - Step 440 Global step 440 Train loss 0.88 on epoch=219
03/01/2022 02:04:53 - INFO - __main__ - Step 450 Global step 450 Train loss 0.92 on epoch=224
03/01/2022 02:04:54 - INFO - __main__ - Global step 450 Train loss 0.91 EM 0.0 on epoch=224
03/01/2022 02:04:56 - INFO - __main__ - Step 460 Global step 460 Train loss 0.83 on epoch=229
03/01/2022 02:04:58 - INFO - __main__ - Step 470 Global step 470 Train loss 0.83 on epoch=234
03/01/2022 02:05:00 - INFO - __main__ - Step 480 Global step 480 Train loss 0.78 on epoch=239
03/01/2022 02:05:02 - INFO - __main__ - Step 490 Global step 490 Train loss 0.76 on epoch=244
03/01/2022 02:05:04 - INFO - __main__ - Step 500 Global step 500 Train loss 0.76 on epoch=249
03/01/2022 02:05:06 - INFO - __main__ - Global step 500 Train loss 0.79 EM 0.0 on epoch=249
03/01/2022 02:05:08 - INFO - __main__ - Step 510 Global step 510 Train loss 0.76 on epoch=254
03/01/2022 02:05:10 - INFO - __main__ - Step 520 Global step 520 Train loss 0.79 on epoch=259
03/01/2022 02:05:12 - INFO - __main__ - Step 530 Global step 530 Train loss 0.77 on epoch=264
03/01/2022 02:05:14 - INFO - __main__ - Step 540 Global step 540 Train loss 0.72 on epoch=269
03/01/2022 02:05:16 - INFO - __main__ - Step 550 Global step 550 Train loss 0.67 on epoch=274
03/01/2022 02:05:18 - INFO - __main__ - Global step 550 Train loss 0.74 EM 0.0 on epoch=274
03/01/2022 02:05:20 - INFO - __main__ - Step 560 Global step 560 Train loss 0.64 on epoch=279
03/01/2022 02:05:22 - INFO - __main__ - Step 570 Global step 570 Train loss 0.65 on epoch=284
03/01/2022 02:05:24 - INFO - __main__ - Step 580 Global step 580 Train loss 0.66 on epoch=289
03/01/2022 02:05:26 - INFO - __main__ - Step 590 Global step 590 Train loss 0.56 on epoch=294
03/01/2022 02:05:28 - INFO - __main__ - Step 600 Global step 600 Train loss 0.57 on epoch=299
03/01/2022 02:05:30 - INFO - __main__ - Global step 600 Train loss 0.62 EM 0.0 on epoch=299
03/01/2022 02:05:32 - INFO - __main__ - Step 610 Global step 610 Train loss 0.61 on epoch=304
03/01/2022 02:05:34 - INFO - __main__ - Step 620 Global step 620 Train loss 0.61 on epoch=309
03/01/2022 02:05:36 - INFO - __main__ - Step 630 Global step 630 Train loss 0.55 on epoch=314
03/01/2022 02:05:38 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=319
03/01/2022 02:05:40 - INFO - __main__ - Step 650 Global step 650 Train loss 0.57 on epoch=324
03/01/2022 02:05:42 - INFO - __main__ - Global step 650 Train loss 0.58 EM 0.0 on epoch=324
03/01/2022 02:05:44 - INFO - __main__ - Step 660 Global step 660 Train loss 0.53 on epoch=329
03/01/2022 02:05:46 - INFO - __main__ - Step 670 Global step 670 Train loss 0.53 on epoch=334
03/01/2022 02:05:48 - INFO - __main__ - Step 680 Global step 680 Train loss 0.48 on epoch=339
03/01/2022 02:05:50 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=344
03/01/2022 02:05:52 - INFO - __main__ - Step 700 Global step 700 Train loss 0.50 on epoch=349
03/01/2022 02:05:54 - INFO - __main__ - Global step 700 Train loss 0.52 EM 0.0 on epoch=349
03/01/2022 02:05:56 - INFO - __main__ - Step 710 Global step 710 Train loss 0.64 on epoch=354
03/01/2022 02:05:58 - INFO - __main__ - Step 720 Global step 720 Train loss 0.51 on epoch=359
03/01/2022 02:06:00 - INFO - __main__ - Step 730 Global step 730 Train loss 0.54 on epoch=364
03/01/2022 02:06:02 - INFO - __main__ - Step 740 Global step 740 Train loss 0.57 on epoch=369
03/01/2022 02:06:04 - INFO - __main__ - Step 750 Global step 750 Train loss 0.44 on epoch=374
03/01/2022 02:06:05 - INFO - __main__ - Global step 750 Train loss 0.54 EM 0.0 on epoch=374
03/01/2022 02:06:07 - INFO - __main__ - Step 760 Global step 760 Train loss 0.47 on epoch=379
03/01/2022 02:06:10 - INFO - __main__ - Step 770 Global step 770 Train loss 0.45 on epoch=384
03/01/2022 02:06:12 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=389
03/01/2022 02:06:14 - INFO - __main__ - Step 790 Global step 790 Train loss 0.48 on epoch=394
03/01/2022 02:06:16 - INFO - __main__ - Step 800 Global step 800 Train loss 0.53 on epoch=399
03/01/2022 02:06:18 - INFO - __main__ - Global step 800 Train loss 0.48 EM 0.0 on epoch=399
03/01/2022 02:06:20 - INFO - __main__ - Step 810 Global step 810 Train loss 0.49 on epoch=404
03/01/2022 02:06:22 - INFO - __main__ - Step 820 Global step 820 Train loss 0.42 on epoch=409
03/01/2022 02:06:24 - INFO - __main__ - Step 830 Global step 830 Train loss 0.42 on epoch=414
03/01/2022 02:06:26 - INFO - __main__ - Step 840 Global step 840 Train loss 0.48 on epoch=419
03/01/2022 02:06:29 - INFO - __main__ - Step 850 Global step 850 Train loss 0.38 on epoch=424
03/01/2022 02:06:30 - INFO - __main__ - Global step 850 Train loss 0.44 EM 0.0 on epoch=424
03/01/2022 02:06:32 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
03/01/2022 02:06:34 - INFO - __main__ - Step 870 Global step 870 Train loss 0.41 on epoch=434
03/01/2022 02:06:36 - INFO - __main__ - Step 880 Global step 880 Train loss 0.40 on epoch=439
03/01/2022 02:06:39 - INFO - __main__ - Step 890 Global step 890 Train loss 0.42 on epoch=444
03/01/2022 02:06:41 - INFO - __main__ - Step 900 Global step 900 Train loss 0.40 on epoch=449
03/01/2022 02:06:42 - INFO - __main__ - Global step 900 Train loss 0.41 EM 0.0 on epoch=449
03/01/2022 02:06:44 - INFO - __main__ - Step 910 Global step 910 Train loss 0.36 on epoch=454
03/01/2022 02:06:46 - INFO - __main__ - Step 920 Global step 920 Train loss 0.37 on epoch=459
03/01/2022 02:06:49 - INFO - __main__ - Step 930 Global step 930 Train loss 0.40 on epoch=464
03/01/2022 02:06:51 - INFO - __main__ - Step 940 Global step 940 Train loss 0.38 on epoch=469
03/01/2022 02:06:53 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
03/01/2022 02:06:54 - INFO - __main__ - Global step 950 Train loss 0.38 EM 0.0 on epoch=474
03/01/2022 02:06:57 - INFO - __main__ - Step 960 Global step 960 Train loss 0.35 on epoch=479
03/01/2022 02:06:59 - INFO - __main__ - Step 970 Global step 970 Train loss 0.33 on epoch=484
03/01/2022 02:07:01 - INFO - __main__ - Step 980 Global step 980 Train loss 0.38 on epoch=489
03/01/2022 02:07:03 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=494
03/01/2022 02:07:05 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.35 on epoch=499
03/01/2022 02:07:07 - INFO - __main__ - Global step 1000 Train loss 0.35 EM 0.0 on epoch=499
03/01/2022 02:07:09 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.35 on epoch=504
03/01/2022 02:07:11 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.31 on epoch=509
03/01/2022 02:07:13 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.30 on epoch=514
03/01/2022 02:07:16 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.31 on epoch=519
03/01/2022 02:07:18 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.32 on epoch=524
03/01/2022 02:07:19 - INFO - __main__ - Global step 1050 Train loss 0.32 EM 0.0 on epoch=524
03/01/2022 02:07:21 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.29 on epoch=529
03/01/2022 02:07:23 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.29 on epoch=534
03/01/2022 02:07:26 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.30 on epoch=539
03/01/2022 02:07:28 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.23 on epoch=544
03/01/2022 02:07:30 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.29 on epoch=549
03/01/2022 02:07:31 - INFO - __main__ - Global step 1100 Train loss 0.28 EM 0.0 on epoch=549
03/01/2022 02:07:34 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.27 on epoch=554
03/01/2022 02:07:36 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.28 on epoch=559
03/01/2022 02:07:38 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.23 on epoch=564
03/01/2022 02:07:40 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.31 on epoch=569
03/01/2022 02:07:42 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.23 on epoch=574
03/01/2022 02:07:44 - INFO - __main__ - Global step 1150 Train loss 0.27 EM 0.0 on epoch=574
03/01/2022 02:07:46 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=579
03/01/2022 02:07:48 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=584
03/01/2022 02:07:50 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=589
03/01/2022 02:07:53 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.28 on epoch=594
03/01/2022 02:07:55 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.22 on epoch=599
03/01/2022 02:07:56 - INFO - __main__ - Global step 1200 Train loss 0.27 EM 0.0 on epoch=599
03/01/2022 02:07:58 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
03/01/2022 02:08:01 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=609
03/01/2022 02:08:03 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.26 on epoch=614
03/01/2022 02:08:05 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.21 on epoch=619
03/01/2022 02:08:07 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=624
03/01/2022 02:08:09 - INFO - __main__ - Global step 1250 Train loss 0.22 EM 0.0 on epoch=624
03/01/2022 02:08:11 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.24 on epoch=629
03/01/2022 02:08:13 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.26 on epoch=634
03/01/2022 02:08:15 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=639
03/01/2022 02:08:17 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.20 on epoch=644
03/01/2022 02:08:19 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.24 on epoch=649
03/01/2022 02:08:21 - INFO - __main__ - Global step 1300 Train loss 0.25 EM 0.0 on epoch=649
03/01/2022 02:08:23 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.25 on epoch=654
03/01/2022 02:08:25 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
03/01/2022 02:08:27 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=664
03/01/2022 02:08:30 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.22 on epoch=669
03/01/2022 02:08:32 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.22 on epoch=674
03/01/2022 02:08:33 - INFO - __main__ - Global step 1350 Train loss 0.24 EM 0.0 on epoch=674
03/01/2022 02:08:35 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.22 on epoch=679
03/01/2022 02:08:38 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=684
03/01/2022 02:08:40 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.19 on epoch=689
03/01/2022 02:08:42 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.17 on epoch=694
03/01/2022 02:08:44 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.18 on epoch=699
03/01/2022 02:08:46 - INFO - __main__ - Global step 1400 Train loss 0.19 EM 0.0 on epoch=699
03/01/2022 02:08:48 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.22 on epoch=704
03/01/2022 02:08:50 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.18 on epoch=709
03/01/2022 02:08:52 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.27 on epoch=714
03/01/2022 02:08:54 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.19 on epoch=719
03/01/2022 02:08:57 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.21 on epoch=724
03/01/2022 02:08:58 - INFO - __main__ - Global step 1450 Train loss 0.21 EM 0.0 on epoch=724
03/01/2022 02:09:00 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=729
03/01/2022 02:09:02 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=734
03/01/2022 02:09:05 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/01/2022 02:09:07 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.17 on epoch=744
03/01/2022 02:09:09 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.21 on epoch=749
03/01/2022 02:09:10 - INFO - __main__ - Global step 1500 Train loss 0.20 EM 0.0 on epoch=749
03/01/2022 02:09:12 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.22 on epoch=754
03/01/2022 02:09:15 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.22 on epoch=759
03/01/2022 02:09:17 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.19 on epoch=764
03/01/2022 02:09:19 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.20 on epoch=769
03/01/2022 02:09:21 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.19 on epoch=774
03/01/2022 02:09:23 - INFO - __main__ - Global step 1550 Train loss 0.20 EM 0.0 on epoch=774
03/01/2022 02:09:25 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.18 on epoch=779
03/01/2022 02:09:27 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=784
03/01/2022 02:09:29 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.14 on epoch=789
03/01/2022 02:09:31 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.20 on epoch=794
03/01/2022 02:09:33 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.16 on epoch=799
03/01/2022 02:09:35 - INFO - __main__ - Global step 1600 Train loss 0.18 EM 0.0 on epoch=799
03/01/2022 02:09:37 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.20 on epoch=804
03/01/2022 02:09:39 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.19 on epoch=809
03/01/2022 02:09:41 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.19 on epoch=814
03/01/2022 02:09:44 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.17 on epoch=819
03/01/2022 02:09:46 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=824
03/01/2022 02:09:47 - INFO - __main__ - Global step 1650 Train loss 0.18 EM 0.0 on epoch=824
03/01/2022 02:09:49 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.15 on epoch=829
03/01/2022 02:09:51 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.12 on epoch=834
03/01/2022 02:09:53 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=839
03/01/2022 02:09:56 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.15 on epoch=844
03/01/2022 02:09:58 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.15 on epoch=849
03/01/2022 02:09:59 - INFO - __main__ - Global step 1700 Train loss 0.15 EM 0.0 on epoch=849
03/01/2022 02:10:01 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.16 on epoch=854
03/01/2022 02:10:04 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=859
03/01/2022 02:10:06 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=864
03/01/2022 02:10:08 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=869
03/01/2022 02:10:10 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.18 on epoch=874
03/01/2022 02:10:11 - INFO - __main__ - Global step 1750 Train loss 0.15 EM 0.0 on epoch=874
03/01/2022 02:10:13 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.15 on epoch=879
03/01/2022 02:10:16 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.10 on epoch=884
03/01/2022 02:10:18 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.18 on epoch=889
03/01/2022 02:10:20 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.14 on epoch=894
03/01/2022 02:10:22 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.15 on epoch=899
03/01/2022 02:10:24 - INFO - __main__ - Global step 1800 Train loss 0.14 EM 0.0 on epoch=899
03/01/2022 02:10:26 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=904
03/01/2022 02:10:28 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=909
03/01/2022 02:10:30 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.13 on epoch=914
03/01/2022 02:10:33 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.13 on epoch=919
03/01/2022 02:10:35 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.15 on epoch=924
03/01/2022 02:10:36 - INFO - __main__ - Global step 1850 Train loss 0.14 EM 0.0 on epoch=924
03/01/2022 02:10:38 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.13 on epoch=929
03/01/2022 02:10:40 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.16 on epoch=934
03/01/2022 02:10:43 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.11 on epoch=939
03/01/2022 02:10:45 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.15 on epoch=944
03/01/2022 02:10:47 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.14 on epoch=949
03/01/2022 02:10:48 - INFO - __main__ - Global step 1900 Train loss 0.14 EM 0.0 on epoch=949
03/01/2022 02:10:51 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=954
03/01/2022 02:10:53 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.11 on epoch=959
03/01/2022 02:10:55 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=964
03/01/2022 02:10:57 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.12 on epoch=969
03/01/2022 02:11:00 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.13 on epoch=974
03/01/2022 02:11:01 - INFO - __main__ - Global step 1950 Train loss 0.12 EM 0.0 on epoch=974
03/01/2022 02:11:03 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.16 on epoch=979
03/01/2022 02:11:05 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.15 on epoch=984
03/01/2022 02:11:08 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.12 on epoch=989
03/01/2022 02:11:10 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=994
03/01/2022 02:11:12 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.17 on epoch=999
03/01/2022 02:11:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:11:13 - INFO - __main__ - Printing 3 examples
03/01/2022 02:11:13 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 02:11:13 - INFO - __main__ - ['kieren fallon']
03/01/2022 02:11:13 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 02:11:13 - INFO - __main__ - ['uranus']
03/01/2022 02:11:13 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 02:11:13 - INFO - __main__ - ['rob thomas']
03/01/2022 02:11:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 02:11:13 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:11:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:11:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:11:13 - INFO - __main__ - Printing 3 examples
03/01/2022 02:11:13 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 02:11:13 - INFO - __main__ - ['terence rattigan']
03/01/2022 02:11:13 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 02:11:13 - INFO - __main__ - ['casino royale']
03/01/2022 02:11:13 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 02:11:13 - INFO - __main__ - ['offenbach']
03/01/2022 02:11:13 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:11:13 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:11:13 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:11:13 - INFO - __main__ - Global step 2000 Train loss 0.14 EM 0.0 on epoch=999
03/01/2022 02:11:13 - INFO - __main__ - save last model!
03/01/2022 02:11:13 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 02:11:13 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 02:11:13 - INFO - __main__ - Printing 3 examples
03/01/2022 02:11:13 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 02:11:13 - INFO - __main__ - ['taming of the shrew']
03/01/2022 02:11:13 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 02:11:13 - INFO - __main__ - ['henry fonda']
03/01/2022 02:11:13 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 02:11:13 - INFO - __main__ - ['tchaikovsky']
03/01/2022 02:11:13 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:11:15 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:11:19 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 02:11:26 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:11:27 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:11:27 - INFO - __main__ - Starting training!
03/01/2022 02:14:01 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_13_0.4_8_predictions.txt
03/01/2022 02:14:01 - INFO - __main__ - EM on test data: 0.0070
03/01/2022 02:14:01 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.4, bsz=8, dev_performance=0.03125, test_performance=0.007010515773660491
03/01/2022 02:14:01 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.3, bsz=8 ...
03/01/2022 02:14:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:14:02 - INFO - __main__ - Printing 3 examples
03/01/2022 02:14:02 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 02:14:02 - INFO - __main__ - ['kieren fallon']
03/01/2022 02:14:02 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 02:14:02 - INFO - __main__ - ['uranus']
03/01/2022 02:14:02 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 02:14:02 - INFO - __main__ - ['rob thomas']
03/01/2022 02:14:02 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 02:14:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:14:02 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:14:02 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:14:02 - INFO - __main__ - Printing 3 examples
03/01/2022 02:14:02 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 02:14:02 - INFO - __main__ - ['terence rattigan']
03/01/2022 02:14:02 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 02:14:02 - INFO - __main__ - ['casino royale']
03/01/2022 02:14:02 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 02:14:02 - INFO - __main__ - ['offenbach']
03/01/2022 02:14:02 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:14:02 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:14:02 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:14:14 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:14:15 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:14:15 - INFO - __main__ - Starting training!
03/01/2022 02:14:18 - INFO - __main__ - Step 10 Global step 10 Train loss 3.33 on epoch=4
03/01/2022 02:14:21 - INFO - __main__ - Step 20 Global step 20 Train loss 2.73 on epoch=9
03/01/2022 02:14:23 - INFO - __main__ - Step 30 Global step 30 Train loss 2.48 on epoch=14
03/01/2022 02:14:25 - INFO - __main__ - Step 40 Global step 40 Train loss 2.47 on epoch=19
03/01/2022 02:14:28 - INFO - __main__ - Step 50 Global step 50 Train loss 2.40 on epoch=24
03/01/2022 02:14:29 - INFO - __main__ - Global step 50 Train loss 2.68 EM 0.03125 on epoch=24
03/01/2022 02:14:29 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.03125 on epoch=24, global_step=50
03/01/2022 02:14:31 - INFO - __main__ - Step 60 Global step 60 Train loss 2.37 on epoch=29
03/01/2022 02:14:34 - INFO - __main__ - Step 70 Global step 70 Train loss 2.34 on epoch=34
03/01/2022 02:14:36 - INFO - __main__ - Step 80 Global step 80 Train loss 2.24 on epoch=39
03/01/2022 02:14:38 - INFO - __main__ - Step 90 Global step 90 Train loss 2.23 on epoch=44
03/01/2022 02:14:40 - INFO - __main__ - Step 100 Global step 100 Train loss 2.05 on epoch=49
03/01/2022 02:14:41 - INFO - __main__ - Global step 100 Train loss 2.25 EM 0.03125 on epoch=49
03/01/2022 02:14:44 - INFO - __main__ - Step 110 Global step 110 Train loss 2.15 on epoch=54
03/01/2022 02:14:46 - INFO - __main__ - Step 120 Global step 120 Train loss 2.06 on epoch=59
03/01/2022 02:14:48 - INFO - __main__ - Step 130 Global step 130 Train loss 2.00 on epoch=64
03/01/2022 02:14:50 - INFO - __main__ - Step 140 Global step 140 Train loss 2.02 on epoch=69
03/01/2022 02:14:52 - INFO - __main__ - Step 150 Global step 150 Train loss 1.93 on epoch=74
03/01/2022 02:14:54 - INFO - __main__ - Global step 150 Train loss 2.03 EM 0.0 on epoch=74
03/01/2022 02:14:56 - INFO - __main__ - Step 160 Global step 160 Train loss 1.79 on epoch=79
03/01/2022 02:14:58 - INFO - __main__ - Step 170 Global step 170 Train loss 1.82 on epoch=84
03/01/2022 02:15:00 - INFO - __main__ - Step 180 Global step 180 Train loss 1.75 on epoch=89
03/01/2022 02:15:02 - INFO - __main__ - Step 190 Global step 190 Train loss 1.69 on epoch=94
03/01/2022 02:15:05 - INFO - __main__ - Step 200 Global step 200 Train loss 1.63 on epoch=99
03/01/2022 02:15:06 - INFO - __main__ - Global step 200 Train loss 1.74 EM 0.0 on epoch=99
03/01/2022 02:15:08 - INFO - __main__ - Step 210 Global step 210 Train loss 1.58 on epoch=104
03/01/2022 02:15:10 - INFO - __main__ - Step 220 Global step 220 Train loss 1.53 on epoch=109
03/01/2022 02:15:13 - INFO - __main__ - Step 230 Global step 230 Train loss 1.59 on epoch=114
03/01/2022 02:15:15 - INFO - __main__ - Step 240 Global step 240 Train loss 1.49 on epoch=119
03/01/2022 02:15:17 - INFO - __main__ - Step 250 Global step 250 Train loss 1.51 on epoch=124
03/01/2022 02:15:18 - INFO - __main__ - Global step 250 Train loss 1.54 EM 0.0 on epoch=124
03/01/2022 02:15:20 - INFO - __main__ - Step 260 Global step 260 Train loss 1.44 on epoch=129
03/01/2022 02:15:23 - INFO - __main__ - Step 270 Global step 270 Train loss 1.41 on epoch=134
03/01/2022 02:15:25 - INFO - __main__ - Step 280 Global step 280 Train loss 1.36 on epoch=139
03/01/2022 02:15:27 - INFO - __main__ - Step 290 Global step 290 Train loss 1.30 on epoch=144
03/01/2022 02:15:29 - INFO - __main__ - Step 300 Global step 300 Train loss 1.32 on epoch=149
03/01/2022 02:15:30 - INFO - __main__ - Global step 300 Train loss 1.37 EM 0.0 on epoch=149
03/01/2022 02:15:33 - INFO - __main__ - Step 310 Global step 310 Train loss 1.27 on epoch=154
03/01/2022 02:15:35 - INFO - __main__ - Step 320 Global step 320 Train loss 1.28 on epoch=159
03/01/2022 02:15:37 - INFO - __main__ - Step 330 Global step 330 Train loss 1.20 on epoch=164
03/01/2022 02:15:39 - INFO - __main__ - Step 340 Global step 340 Train loss 1.20 on epoch=169
03/01/2022 02:15:41 - INFO - __main__ - Step 350 Global step 350 Train loss 1.24 on epoch=174
03/01/2022 02:15:43 - INFO - __main__ - Global step 350 Train loss 1.24 EM 0.0 on epoch=174
03/01/2022 02:15:45 - INFO - __main__ - Step 360 Global step 360 Train loss 1.21 on epoch=179
03/01/2022 02:15:47 - INFO - __main__ - Step 370 Global step 370 Train loss 1.18 on epoch=184
03/01/2022 02:15:49 - INFO - __main__ - Step 380 Global step 380 Train loss 1.13 on epoch=189
03/01/2022 02:15:52 - INFO - __main__ - Step 390 Global step 390 Train loss 1.12 on epoch=194
03/01/2022 02:15:54 - INFO - __main__ - Step 400 Global step 400 Train loss 1.17 on epoch=199
03/01/2022 02:15:55 - INFO - __main__ - Global step 400 Train loss 1.16 EM 0.0 on epoch=199
03/01/2022 02:15:57 - INFO - __main__ - Step 410 Global step 410 Train loss 1.06 on epoch=204
03/01/2022 02:15:59 - INFO - __main__ - Step 420 Global step 420 Train loss 1.11 on epoch=209
03/01/2022 02:16:02 - INFO - __main__ - Step 430 Global step 430 Train loss 1.14 on epoch=214
03/01/2022 02:16:04 - INFO - __main__ - Step 440 Global step 440 Train loss 1.08 on epoch=219
03/01/2022 02:16:06 - INFO - __main__ - Step 450 Global step 450 Train loss 1.06 on epoch=224
03/01/2022 02:16:07 - INFO - __main__ - Global step 450 Train loss 1.09 EM 0.0 on epoch=224
03/01/2022 02:16:10 - INFO - __main__ - Step 460 Global step 460 Train loss 1.09 on epoch=229
03/01/2022 02:16:12 - INFO - __main__ - Step 470 Global step 470 Train loss 0.99 on epoch=234
03/01/2022 02:16:14 - INFO - __main__ - Step 480 Global step 480 Train loss 0.99 on epoch=239
03/01/2022 02:16:16 - INFO - __main__ - Step 490 Global step 490 Train loss 1.04 on epoch=244
03/01/2022 02:16:18 - INFO - __main__ - Step 500 Global step 500 Train loss 0.91 on epoch=249
03/01/2022 02:16:20 - INFO - __main__ - Global step 500 Train loss 1.01 EM 0.0 on epoch=249
03/01/2022 02:16:22 - INFO - __main__ - Step 510 Global step 510 Train loss 0.97 on epoch=254
03/01/2022 02:16:24 - INFO - __main__ - Step 520 Global step 520 Train loss 0.99 on epoch=259
03/01/2022 02:16:26 - INFO - __main__ - Step 530 Global step 530 Train loss 0.86 on epoch=264
03/01/2022 02:16:28 - INFO - __main__ - Step 540 Global step 540 Train loss 0.97 on epoch=269
03/01/2022 02:16:31 - INFO - __main__ - Step 550 Global step 550 Train loss 0.83 on epoch=274
03/01/2022 02:16:32 - INFO - __main__ - Global step 550 Train loss 0.92 EM 0.0 on epoch=274
03/01/2022 02:16:34 - INFO - __main__ - Step 560 Global step 560 Train loss 0.85 on epoch=279
03/01/2022 02:16:36 - INFO - __main__ - Step 570 Global step 570 Train loss 0.88 on epoch=284
03/01/2022 02:16:39 - INFO - __main__ - Step 580 Global step 580 Train loss 0.92 on epoch=289
03/01/2022 02:16:41 - INFO - __main__ - Step 590 Global step 590 Train loss 0.75 on epoch=294
03/01/2022 02:16:43 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=299
03/01/2022 02:16:44 - INFO - __main__ - Global step 600 Train loss 0.85 EM 0.0 on epoch=299
03/01/2022 02:16:46 - INFO - __main__ - Step 610 Global step 610 Train loss 0.80 on epoch=304
03/01/2022 02:16:49 - INFO - __main__ - Step 620 Global step 620 Train loss 0.78 on epoch=309
03/01/2022 02:16:51 - INFO - __main__ - Step 630 Global step 630 Train loss 0.79 on epoch=314
03/01/2022 02:16:53 - INFO - __main__ - Step 640 Global step 640 Train loss 0.76 on epoch=319
03/01/2022 02:16:55 - INFO - __main__ - Step 650 Global step 650 Train loss 0.75 on epoch=324
03/01/2022 02:16:56 - INFO - __main__ - Global step 650 Train loss 0.78 EM 0.0 on epoch=324
03/01/2022 02:16:59 - INFO - __main__ - Step 660 Global step 660 Train loss 0.72 on epoch=329
03/01/2022 02:17:01 - INFO - __main__ - Step 670 Global step 670 Train loss 0.76 on epoch=334
03/01/2022 02:17:03 - INFO - __main__ - Step 680 Global step 680 Train loss 0.75 on epoch=339
03/01/2022 02:17:05 - INFO - __main__ - Step 690 Global step 690 Train loss 0.70 on epoch=344
03/01/2022 02:17:08 - INFO - __main__ - Step 700 Global step 700 Train loss 0.70 on epoch=349
03/01/2022 02:17:09 - INFO - __main__ - Global step 700 Train loss 0.73 EM 0.0 on epoch=349
03/01/2022 02:17:11 - INFO - __main__ - Step 710 Global step 710 Train loss 0.64 on epoch=354
03/01/2022 02:17:13 - INFO - __main__ - Step 720 Global step 720 Train loss 0.74 on epoch=359
03/01/2022 02:17:15 - INFO - __main__ - Step 730 Global step 730 Train loss 0.67 on epoch=364
03/01/2022 02:17:18 - INFO - __main__ - Step 740 Global step 740 Train loss 0.62 on epoch=369
03/01/2022 02:17:20 - INFO - __main__ - Step 750 Global step 750 Train loss 0.62 on epoch=374
03/01/2022 02:17:21 - INFO - __main__ - Global step 750 Train loss 0.66 EM 0.0 on epoch=374
03/01/2022 02:17:23 - INFO - __main__ - Step 760 Global step 760 Train loss 0.68 on epoch=379
03/01/2022 02:17:25 - INFO - __main__ - Step 770 Global step 770 Train loss 0.70 on epoch=384
03/01/2022 02:17:28 - INFO - __main__ - Step 780 Global step 780 Train loss 0.68 on epoch=389
03/01/2022 02:17:30 - INFO - __main__ - Step 790 Global step 790 Train loss 0.61 on epoch=394
03/01/2022 02:17:32 - INFO - __main__ - Step 800 Global step 800 Train loss 0.55 on epoch=399
03/01/2022 02:17:33 - INFO - __main__ - Global step 800 Train loss 0.64 EM 0.0 on epoch=399
03/01/2022 02:17:35 - INFO - __main__ - Step 810 Global step 810 Train loss 0.58 on epoch=404
03/01/2022 02:17:38 - INFO - __main__ - Step 820 Global step 820 Train loss 0.58 on epoch=409
03/01/2022 02:17:40 - INFO - __main__ - Step 830 Global step 830 Train loss 0.56 on epoch=414
03/01/2022 02:17:42 - INFO - __main__ - Step 840 Global step 840 Train loss 0.55 on epoch=419
03/01/2022 02:17:44 - INFO - __main__ - Step 850 Global step 850 Train loss 0.56 on epoch=424
03/01/2022 02:17:46 - INFO - __main__ - Global step 850 Train loss 0.57 EM 0.0 on epoch=424
03/01/2022 02:17:48 - INFO - __main__ - Step 860 Global step 860 Train loss 0.49 on epoch=429
03/01/2022 02:17:50 - INFO - __main__ - Step 870 Global step 870 Train loss 0.53 on epoch=434
03/01/2022 02:17:52 - INFO - __main__ - Step 880 Global step 880 Train loss 0.60 on epoch=439
03/01/2022 02:17:54 - INFO - __main__ - Step 890 Global step 890 Train loss 0.56 on epoch=444
03/01/2022 02:17:57 - INFO - __main__ - Step 900 Global step 900 Train loss 0.50 on epoch=449
03/01/2022 02:17:58 - INFO - __main__ - Global step 900 Train loss 0.54 EM 0.0 on epoch=449
03/01/2022 02:18:00 - INFO - __main__ - Step 910 Global step 910 Train loss 0.55 on epoch=454
03/01/2022 02:18:02 - INFO - __main__ - Step 920 Global step 920 Train loss 0.44 on epoch=459
03/01/2022 02:18:05 - INFO - __main__ - Step 930 Global step 930 Train loss 0.50 on epoch=464
03/01/2022 02:18:07 - INFO - __main__ - Step 940 Global step 940 Train loss 0.55 on epoch=469
03/01/2022 02:18:09 - INFO - __main__ - Step 950 Global step 950 Train loss 0.51 on epoch=474
03/01/2022 02:18:10 - INFO - __main__ - Global step 950 Train loss 0.51 EM 0.0 on epoch=474
03/01/2022 02:18:12 - INFO - __main__ - Step 960 Global step 960 Train loss 0.49 on epoch=479
03/01/2022 02:18:15 - INFO - __main__ - Step 970 Global step 970 Train loss 0.50 on epoch=484
03/01/2022 02:18:17 - INFO - __main__ - Step 980 Global step 980 Train loss 0.55 on epoch=489
03/01/2022 02:18:19 - INFO - __main__ - Step 990 Global step 990 Train loss 0.45 on epoch=494
03/01/2022 02:18:21 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.48 on epoch=499
03/01/2022 02:18:23 - INFO - __main__ - Global step 1000 Train loss 0.49 EM 0.0 on epoch=499
03/01/2022 02:18:25 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.53 on epoch=504
03/01/2022 02:18:27 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.45 on epoch=509
03/01/2022 02:18:29 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.44 on epoch=514
03/01/2022 02:18:31 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.41 on epoch=519
03/01/2022 02:18:34 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.40 on epoch=524
03/01/2022 02:18:35 - INFO - __main__ - Global step 1050 Train loss 0.45 EM 0.0 on epoch=524
03/01/2022 02:18:37 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.43 on epoch=529
03/01/2022 02:18:39 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.48 on epoch=534
03/01/2022 02:18:41 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.42 on epoch=539
03/01/2022 02:18:44 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.46 on epoch=544
03/01/2022 02:18:46 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.43 on epoch=549
03/01/2022 02:18:47 - INFO - __main__ - Global step 1100 Train loss 0.44 EM 0.0 on epoch=549
03/01/2022 02:18:49 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.48 on epoch=554
03/01/2022 02:18:51 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.40 on epoch=559
03/01/2022 02:18:54 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.40 on epoch=564
03/01/2022 02:18:56 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.42 on epoch=569
03/01/2022 02:18:58 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.39 on epoch=574
03/01/2022 02:18:59 - INFO - __main__ - Global step 1150 Train loss 0.42 EM 0.0 on epoch=574
03/01/2022 02:19:02 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.38 on epoch=579
03/01/2022 02:19:04 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.40 on epoch=584
03/01/2022 02:19:06 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.39 on epoch=589
03/01/2022 02:19:08 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.34 on epoch=594
03/01/2022 02:19:10 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.42 on epoch=599
03/01/2022 02:19:12 - INFO - __main__ - Global step 1200 Train loss 0.38 EM 0.0 on epoch=599
03/01/2022 02:19:14 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.32 on epoch=604
03/01/2022 02:19:16 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.42 on epoch=609
03/01/2022 02:19:18 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.35 on epoch=614
03/01/2022 02:19:20 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.40 on epoch=619
03/01/2022 02:19:23 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.39 on epoch=624
03/01/2022 02:19:24 - INFO - __main__ - Global step 1250 Train loss 0.38 EM 0.0 on epoch=624
03/01/2022 02:19:26 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.34 on epoch=629
03/01/2022 02:19:28 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.38 on epoch=634
03/01/2022 02:19:30 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.33 on epoch=639
03/01/2022 02:19:33 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.30 on epoch=644
03/01/2022 02:19:35 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.34 on epoch=649
03/01/2022 02:19:36 - INFO - __main__ - Global step 1300 Train loss 0.34 EM 0.0 on epoch=649
03/01/2022 02:19:38 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.36 on epoch=654
03/01/2022 02:19:41 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.29 on epoch=659
03/01/2022 02:19:43 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.33 on epoch=664
03/01/2022 02:19:45 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.39 on epoch=669
03/01/2022 02:19:47 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.34 on epoch=674
03/01/2022 02:19:49 - INFO - __main__ - Global step 1350 Train loss 0.34 EM 0.0 on epoch=674
03/01/2022 02:19:51 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.33 on epoch=679
03/01/2022 02:19:53 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.33 on epoch=684
03/01/2022 02:19:55 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.34 on epoch=689
03/01/2022 02:19:58 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.28 on epoch=694
03/01/2022 02:20:00 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.35 on epoch=699
03/01/2022 02:20:01 - INFO - __main__ - Global step 1400 Train loss 0.33 EM 0.0 on epoch=699
03/01/2022 02:20:03 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.31 on epoch=704
03/01/2022 02:20:05 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.35 on epoch=709
03/01/2022 02:20:08 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.31 on epoch=714
03/01/2022 02:20:10 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.32 on epoch=719
03/01/2022 02:20:12 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.28 on epoch=724
03/01/2022 02:20:13 - INFO - __main__ - Global step 1450 Train loss 0.31 EM 0.0 on epoch=724
03/01/2022 02:20:16 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.27 on epoch=729
03/01/2022 02:20:18 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.38 on epoch=734
03/01/2022 02:20:20 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.28 on epoch=739
03/01/2022 02:20:22 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.32 on epoch=744
03/01/2022 02:20:25 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.29 on epoch=749
03/01/2022 02:20:26 - INFO - __main__ - Global step 1500 Train loss 0.31 EM 0.0 on epoch=749
03/01/2022 02:20:28 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.29 on epoch=754
03/01/2022 02:20:30 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.28 on epoch=759
03/01/2022 02:20:32 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.27 on epoch=764
03/01/2022 02:20:35 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.29 on epoch=769
03/01/2022 02:20:37 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.27 on epoch=774
03/01/2022 02:20:38 - INFO - __main__ - Global step 1550 Train loss 0.28 EM 0.0 on epoch=774
03/01/2022 02:20:40 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.24 on epoch=779
03/01/2022 02:20:43 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.28 on epoch=784
03/01/2022 02:20:45 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.29 on epoch=789
03/01/2022 02:20:47 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.27 on epoch=794
03/01/2022 02:20:49 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.32 on epoch=799
03/01/2022 02:20:50 - INFO - __main__ - Global step 1600 Train loss 0.28 EM 0.0 on epoch=799
03/01/2022 02:20:53 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.22 on epoch=804
03/01/2022 02:20:55 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.21 on epoch=809
03/01/2022 02:20:57 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.28 on epoch=814
03/01/2022 02:20:59 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.30 on epoch=819
03/01/2022 02:21:02 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.24 on epoch=824
03/01/2022 02:21:03 - INFO - __main__ - Global step 1650 Train loss 0.25 EM 0.0 on epoch=824
03/01/2022 02:21:05 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.27 on epoch=829
03/01/2022 02:21:07 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.24 on epoch=834
03/01/2022 02:21:09 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.29 on epoch=839
03/01/2022 02:21:12 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.28 on epoch=844
03/01/2022 02:21:14 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=849
03/01/2022 02:21:15 - INFO - __main__ - Global step 1700 Train loss 0.26 EM 0.0 on epoch=849
03/01/2022 02:21:17 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.22 on epoch=854
03/01/2022 02:21:20 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.29 on epoch=859
03/01/2022 02:21:22 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.26 on epoch=864
03/01/2022 02:21:24 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.24 on epoch=869
03/01/2022 02:21:26 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.20 on epoch=874
03/01/2022 02:21:28 - INFO - __main__ - Global step 1750 Train loss 0.24 EM 0.0 on epoch=874
03/01/2022 02:21:30 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.24 on epoch=879
03/01/2022 02:21:32 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.21 on epoch=884
03/01/2022 02:21:34 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.19 on epoch=889
03/01/2022 02:21:36 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.22 on epoch=894
03/01/2022 02:21:39 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.21 on epoch=899
03/01/2022 02:21:40 - INFO - __main__ - Global step 1800 Train loss 0.22 EM 0.0 on epoch=899
03/01/2022 02:21:42 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.23 on epoch=904
03/01/2022 02:21:44 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.17 on epoch=909
03/01/2022 02:21:47 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.23 on epoch=914
03/01/2022 02:21:49 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.21 on epoch=919
03/01/2022 02:21:51 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.19 on epoch=924
03/01/2022 02:21:52 - INFO - __main__ - Global step 1850 Train loss 0.21 EM 0.0 on epoch=924
03/01/2022 02:21:55 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.20 on epoch=929
03/01/2022 02:21:57 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.21 on epoch=934
03/01/2022 02:21:59 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.19 on epoch=939
03/01/2022 02:22:01 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.21 on epoch=944
03/01/2022 02:22:03 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.26 on epoch=949
03/01/2022 02:22:05 - INFO - __main__ - Global step 1900 Train loss 0.21 EM 0.0 on epoch=949
03/01/2022 02:22:07 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.18 on epoch=954
03/01/2022 02:22:09 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.20 on epoch=959
03/01/2022 02:22:11 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.16 on epoch=964
03/01/2022 02:22:14 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.19 on epoch=969
03/01/2022 02:22:16 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.19 on epoch=974
03/01/2022 02:22:17 - INFO - __main__ - Global step 1950 Train loss 0.18 EM 0.0 on epoch=974
03/01/2022 02:22:19 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.20 on epoch=979
03/01/2022 02:22:22 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.18 on epoch=984
03/01/2022 02:22:24 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.22 on epoch=989
03/01/2022 02:22:26 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.18 on epoch=994
03/01/2022 02:22:28 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.19 on epoch=999
03/01/2022 02:22:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:22:29 - INFO - __main__ - Printing 3 examples
03/01/2022 02:22:29 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 02:22:29 - INFO - __main__ - ['kieren fallon']
03/01/2022 02:22:29 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 02:22:29 - INFO - __main__ - ['uranus']
03/01/2022 02:22:29 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 02:22:29 - INFO - __main__ - ['rob thomas']
03/01/2022 02:22:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 02:22:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:22:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:22:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:22:29 - INFO - __main__ - Printing 3 examples
03/01/2022 02:22:29 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 02:22:29 - INFO - __main__ - ['terence rattigan']
03/01/2022 02:22:29 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 02:22:29 - INFO - __main__ - ['casino royale']
03/01/2022 02:22:29 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 02:22:29 - INFO - __main__ - ['offenbach']
03/01/2022 02:22:29 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:22:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:22:29 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:22:30 - INFO - __main__ - Global step 2000 Train loss 0.19 EM 0.0 on epoch=999
03/01/2022 02:22:30 - INFO - __main__ - save last model!
03/01/2022 02:22:30 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 02:22:30 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 02:22:30 - INFO - __main__ - Printing 3 examples
03/01/2022 02:22:30 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 02:22:30 - INFO - __main__ - ['taming of the shrew']
03/01/2022 02:22:30 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 02:22:30 - INFO - __main__ - ['henry fonda']
03/01/2022 02:22:30 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 02:22:30 - INFO - __main__ - ['tchaikovsky']
03/01/2022 02:22:30 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:22:31 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:22:35 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 02:22:42 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:22:43 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:22:43 - INFO - __main__ - Starting training!
03/01/2022 02:25:11 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_13_0.3_8_predictions.txt
03/01/2022 02:25:11 - INFO - __main__ - EM on test data: 0.0045
03/01/2022 02:25:12 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.3, bsz=8, dev_performance=0.03125, test_performance=0.004506760140210316
03/01/2022 02:25:12 - INFO - __main__ - Running ... prefix=freebase_qa_32_13, lr=0.2, bsz=8 ...
03/01/2022 02:25:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:25:13 - INFO - __main__ - Printing 3 examples
03/01/2022 02:25:13 - INFO - __main__ -  [freebase_qa] Who rode Kris Kin to success in the Epsom Derby in 2003?
03/01/2022 02:25:13 - INFO - __main__ - ['kieren fallon']
03/01/2022 02:25:13 - INFO - __main__ -  [freebase_qa] Miranda is a moon that orbits which planet?
03/01/2022 02:25:13 - INFO - __main__ - ['uranus']
03/01/2022 02:25:13 - INFO - __main__ -  [freebase_qa] Which of these if the correct name for the singer who released Lonely No More in 2005?
03/01/2022 02:25:13 - INFO - __main__ - ['rob thomas']
03/01/2022 02:25:13 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 02:25:13 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:25:13 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:25:13 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:25:13 - INFO - __main__ - Printing 3 examples
03/01/2022 02:25:13 - INFO - __main__ -  [freebase_qa] Which British playwright sprang to fame in 1936 with his comedy, French Without Tears?
03/01/2022 02:25:13 - INFO - __main__ - ['terence rattigan']
03/01/2022 02:25:13 - INFO - __main__ -  [freebase_qa] Although not making it as an official EON production until the 21st film, what was the first James Bond novel published in April, 1953?
03/01/2022 02:25:13 - INFO - __main__ - ['casino royale']
03/01/2022 02:25:13 - INFO - __main__ -  [freebase_qa] Who wrote the music of the light opera Orpheus in the Underworld ?
03/01/2022 02:25:13 - INFO - __main__ - ['offenbach']
03/01/2022 02:25:13 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:25:13 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:25:13 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:25:27 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:25:28 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:25:28 - INFO - __main__ - Starting training!
03/01/2022 02:25:30 - INFO - __main__ - Step 10 Global step 10 Train loss 3.41 on epoch=4
03/01/2022 02:25:33 - INFO - __main__ - Step 20 Global step 20 Train loss 3.04 on epoch=9
03/01/2022 02:25:35 - INFO - __main__ - Step 30 Global step 30 Train loss 2.68 on epoch=14
03/01/2022 02:25:37 - INFO - __main__ - Step 40 Global step 40 Train loss 2.58 on epoch=19
03/01/2022 02:25:39 - INFO - __main__ - Step 50 Global step 50 Train loss 2.47 on epoch=24
03/01/2022 02:25:41 - INFO - __main__ - Global step 50 Train loss 2.84 EM 0.0 on epoch=24
03/01/2022 02:25:41 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 02:25:43 - INFO - __main__ - Step 60 Global step 60 Train loss 2.42 on epoch=29
03/01/2022 02:25:45 - INFO - __main__ - Step 70 Global step 70 Train loss 2.38 on epoch=34
03/01/2022 02:25:47 - INFO - __main__ - Step 80 Global step 80 Train loss 2.36 on epoch=39
03/01/2022 02:25:50 - INFO - __main__ - Step 90 Global step 90 Train loss 2.31 on epoch=44
03/01/2022 02:25:52 - INFO - __main__ - Step 100 Global step 100 Train loss 2.30 on epoch=49
03/01/2022 02:25:53 - INFO - __main__ - Global step 100 Train loss 2.35 EM 0.0 on epoch=49
03/01/2022 02:25:55 - INFO - __main__ - Step 110 Global step 110 Train loss 2.20 on epoch=54
03/01/2022 02:25:58 - INFO - __main__ - Step 120 Global step 120 Train loss 2.26 on epoch=59
03/01/2022 02:26:00 - INFO - __main__ - Step 130 Global step 130 Train loss 2.17 on epoch=64
03/01/2022 02:26:02 - INFO - __main__ - Step 140 Global step 140 Train loss 2.18 on epoch=69
03/01/2022 02:26:04 - INFO - __main__ - Step 150 Global step 150 Train loss 2.17 on epoch=74
03/01/2022 02:26:06 - INFO - __main__ - Global step 150 Train loss 2.19 EM 0.0 on epoch=74
03/01/2022 02:26:08 - INFO - __main__ - Step 160 Global step 160 Train loss 2.12 on epoch=79
03/01/2022 02:26:10 - INFO - __main__ - Step 170 Global step 170 Train loss 2.05 on epoch=84
03/01/2022 02:26:12 - INFO - __main__ - Step 180 Global step 180 Train loss 2.05 on epoch=89
03/01/2022 02:26:15 - INFO - __main__ - Step 190 Global step 190 Train loss 1.98 on epoch=94
03/01/2022 02:26:17 - INFO - __main__ - Step 200 Global step 200 Train loss 1.98 on epoch=99
03/01/2022 02:26:18 - INFO - __main__ - Global step 200 Train loss 2.04 EM 0.0 on epoch=99
03/01/2022 02:26:20 - INFO - __main__ - Step 210 Global step 210 Train loss 1.97 on epoch=104
03/01/2022 02:26:22 - INFO - __main__ - Step 220 Global step 220 Train loss 1.90 on epoch=109
03/01/2022 02:26:25 - INFO - __main__ - Step 230 Global step 230 Train loss 1.90 on epoch=114
03/01/2022 02:26:27 - INFO - __main__ - Step 240 Global step 240 Train loss 1.90 on epoch=119
03/01/2022 02:26:29 - INFO - __main__ - Step 250 Global step 250 Train loss 1.82 on epoch=124
03/01/2022 02:26:30 - INFO - __main__ - Global step 250 Train loss 1.90 EM 0.0 on epoch=124
03/01/2022 02:26:32 - INFO - __main__ - Step 260 Global step 260 Train loss 1.83 on epoch=129
03/01/2022 02:26:35 - INFO - __main__ - Step 270 Global step 270 Train loss 1.92 on epoch=134
03/01/2022 02:26:37 - INFO - __main__ - Step 280 Global step 280 Train loss 1.78 on epoch=139
03/01/2022 02:26:39 - INFO - __main__ - Step 290 Global step 290 Train loss 1.81 on epoch=144
03/01/2022 02:26:41 - INFO - __main__ - Step 300 Global step 300 Train loss 1.74 on epoch=149
03/01/2022 02:26:42 - INFO - __main__ - Global step 300 Train loss 1.82 EM 0.0 on epoch=149
03/01/2022 02:26:45 - INFO - __main__ - Step 310 Global step 310 Train loss 1.80 on epoch=154
03/01/2022 02:26:47 - INFO - __main__ - Step 320 Global step 320 Train loss 1.56 on epoch=159
03/01/2022 02:26:49 - INFO - __main__ - Step 330 Global step 330 Train loss 1.71 on epoch=164
03/01/2022 02:26:51 - INFO - __main__ - Step 340 Global step 340 Train loss 1.69 on epoch=169
03/01/2022 02:26:53 - INFO - __main__ - Step 350 Global step 350 Train loss 1.68 on epoch=174
03/01/2022 02:26:55 - INFO - __main__ - Global step 350 Train loss 1.69 EM 0.0 on epoch=174
03/01/2022 02:26:57 - INFO - __main__ - Step 360 Global step 360 Train loss 1.62 on epoch=179
03/01/2022 02:26:59 - INFO - __main__ - Step 370 Global step 370 Train loss 1.55 on epoch=184
03/01/2022 02:27:01 - INFO - __main__ - Step 380 Global step 380 Train loss 1.58 on epoch=189
03/01/2022 02:27:03 - INFO - __main__ - Step 390 Global step 390 Train loss 1.53 on epoch=194
03/01/2022 02:27:05 - INFO - __main__ - Step 400 Global step 400 Train loss 1.52 on epoch=199
03/01/2022 02:27:07 - INFO - __main__ - Global step 400 Train loss 1.56 EM 0.0 on epoch=199
03/01/2022 02:27:09 - INFO - __main__ - Step 410 Global step 410 Train loss 1.47 on epoch=204
03/01/2022 02:27:11 - INFO - __main__ - Step 420 Global step 420 Train loss 1.50 on epoch=209
03/01/2022 02:27:13 - INFO - __main__ - Step 430 Global step 430 Train loss 1.53 on epoch=214
03/01/2022 02:27:15 - INFO - __main__ - Step 440 Global step 440 Train loss 1.41 on epoch=219
03/01/2022 02:27:18 - INFO - __main__ - Step 450 Global step 450 Train loss 1.40 on epoch=224
03/01/2022 02:27:19 - INFO - __main__ - Global step 450 Train loss 1.46 EM 0.0 on epoch=224
03/01/2022 02:27:21 - INFO - __main__ - Step 460 Global step 460 Train loss 1.39 on epoch=229
03/01/2022 02:27:23 - INFO - __main__ - Step 470 Global step 470 Train loss 1.46 on epoch=234
03/01/2022 02:27:25 - INFO - __main__ - Step 480 Global step 480 Train loss 1.44 on epoch=239
03/01/2022 02:27:28 - INFO - __main__ - Step 490 Global step 490 Train loss 1.34 on epoch=244
03/01/2022 02:27:30 - INFO - __main__ - Step 500 Global step 500 Train loss 1.38 on epoch=249
03/01/2022 02:27:31 - INFO - __main__ - Global step 500 Train loss 1.40 EM 0.0 on epoch=249
03/01/2022 02:27:33 - INFO - __main__ - Step 510 Global step 510 Train loss 1.32 on epoch=254
03/01/2022 02:27:35 - INFO - __main__ - Step 520 Global step 520 Train loss 1.32 on epoch=259
03/01/2022 02:27:38 - INFO - __main__ - Step 530 Global step 530 Train loss 1.26 on epoch=264
03/01/2022 02:27:40 - INFO - __main__ - Step 540 Global step 540 Train loss 1.28 on epoch=269
03/01/2022 02:27:42 - INFO - __main__ - Step 550 Global step 550 Train loss 1.29 on epoch=274
03/01/2022 02:27:43 - INFO - __main__ - Global step 550 Train loss 1.29 EM 0.0 on epoch=274
03/01/2022 02:27:45 - INFO - __main__ - Step 560 Global step 560 Train loss 1.33 on epoch=279
03/01/2022 02:27:47 - INFO - __main__ - Step 570 Global step 570 Train loss 1.26 on epoch=284
03/01/2022 02:27:50 - INFO - __main__ - Step 580 Global step 580 Train loss 1.33 on epoch=289
03/01/2022 02:27:52 - INFO - __main__ - Step 590 Global step 590 Train loss 1.13 on epoch=294
03/01/2022 02:27:54 - INFO - __main__ - Step 600 Global step 600 Train loss 1.18 on epoch=299
03/01/2022 02:27:55 - INFO - __main__ - Global step 600 Train loss 1.24 EM 0.0 on epoch=299
03/01/2022 02:27:57 - INFO - __main__ - Step 610 Global step 610 Train loss 1.17 on epoch=304
03/01/2022 02:28:00 - INFO - __main__ - Step 620 Global step 620 Train loss 1.21 on epoch=309
03/01/2022 02:28:02 - INFO - __main__ - Step 630 Global step 630 Train loss 1.15 on epoch=314
03/01/2022 02:28:04 - INFO - __main__ - Step 640 Global step 640 Train loss 1.14 on epoch=319
03/01/2022 02:28:06 - INFO - __main__ - Step 650 Global step 650 Train loss 1.20 on epoch=324
03/01/2022 02:28:07 - INFO - __main__ - Global step 650 Train loss 1.17 EM 0.0 on epoch=324
03/01/2022 02:28:09 - INFO - __main__ - Step 660 Global step 660 Train loss 1.09 on epoch=329
03/01/2022 02:28:12 - INFO - __main__ - Step 670 Global step 670 Train loss 1.07 on epoch=334
03/01/2022 02:28:14 - INFO - __main__ - Step 680 Global step 680 Train loss 1.06 on epoch=339
03/01/2022 02:28:16 - INFO - __main__ - Step 690 Global step 690 Train loss 1.06 on epoch=344
03/01/2022 02:28:18 - INFO - __main__ - Step 700 Global step 700 Train loss 1.09 on epoch=349
03/01/2022 02:28:19 - INFO - __main__ - Global step 700 Train loss 1.07 EM 0.0 on epoch=349
03/01/2022 02:28:22 - INFO - __main__ - Step 710 Global step 710 Train loss 1.06 on epoch=354
03/01/2022 02:28:24 - INFO - __main__ - Step 720 Global step 720 Train loss 0.95 on epoch=359
03/01/2022 02:28:26 - INFO - __main__ - Step 730 Global step 730 Train loss 1.07 on epoch=364
03/01/2022 02:28:28 - INFO - __main__ - Step 740 Global step 740 Train loss 0.97 on epoch=369
03/01/2022 02:28:30 - INFO - __main__ - Step 750 Global step 750 Train loss 1.00 on epoch=374
03/01/2022 02:28:31 - INFO - __main__ - Global step 750 Train loss 1.01 EM 0.0 on epoch=374
03/01/2022 02:28:34 - INFO - __main__ - Step 760 Global step 760 Train loss 1.05 on epoch=379
03/01/2022 02:28:36 - INFO - __main__ - Step 770 Global step 770 Train loss 0.98 on epoch=384
03/01/2022 02:28:38 - INFO - __main__ - Step 780 Global step 780 Train loss 0.98 on epoch=389
03/01/2022 02:28:40 - INFO - __main__ - Step 790 Global step 790 Train loss 0.98 on epoch=394
03/01/2022 02:28:42 - INFO - __main__ - Step 800 Global step 800 Train loss 0.97 on epoch=399
03/01/2022 02:28:44 - INFO - __main__ - Global step 800 Train loss 0.99 EM 0.0 on epoch=399
03/01/2022 02:28:46 - INFO - __main__ - Step 810 Global step 810 Train loss 0.99 on epoch=404
03/01/2022 02:28:48 - INFO - __main__ - Step 820 Global step 820 Train loss 0.99 on epoch=409
03/01/2022 02:28:50 - INFO - __main__ - Step 830 Global step 830 Train loss 0.90 on epoch=414
03/01/2022 02:28:52 - INFO - __main__ - Step 840 Global step 840 Train loss 0.88 on epoch=419
03/01/2022 02:28:55 - INFO - __main__ - Step 850 Global step 850 Train loss 0.88 on epoch=424
03/01/2022 02:28:56 - INFO - __main__ - Global step 850 Train loss 0.93 EM 0.0 on epoch=424
03/01/2022 02:28:58 - INFO - __main__ - Step 860 Global step 860 Train loss 0.85 on epoch=429
03/01/2022 02:29:00 - INFO - __main__ - Step 870 Global step 870 Train loss 0.81 on epoch=434
03/01/2022 02:29:02 - INFO - __main__ - Step 880 Global step 880 Train loss 0.87 on epoch=439
03/01/2022 02:29:05 - INFO - __main__ - Step 890 Global step 890 Train loss 0.88 on epoch=444
03/01/2022 02:29:07 - INFO - __main__ - Step 900 Global step 900 Train loss 0.79 on epoch=449
03/01/2022 02:29:08 - INFO - __main__ - Global step 900 Train loss 0.84 EM 0.0 on epoch=449
03/01/2022 02:29:10 - INFO - __main__ - Step 910 Global step 910 Train loss 0.77 on epoch=454
03/01/2022 02:29:12 - INFO - __main__ - Step 920 Global step 920 Train loss 0.79 on epoch=459
03/01/2022 02:29:15 - INFO - __main__ - Step 930 Global step 930 Train loss 0.86 on epoch=464
03/01/2022 02:29:17 - INFO - __main__ - Step 940 Global step 940 Train loss 0.80 on epoch=469
03/01/2022 02:29:19 - INFO - __main__ - Step 950 Global step 950 Train loss 0.83 on epoch=474
03/01/2022 02:29:20 - INFO - __main__ - Global step 950 Train loss 0.81 EM 0.0 on epoch=474
03/01/2022 02:29:22 - INFO - __main__ - Step 960 Global step 960 Train loss 0.72 on epoch=479
03/01/2022 02:29:25 - INFO - __main__ - Step 970 Global step 970 Train loss 0.75 on epoch=484
03/01/2022 02:29:27 - INFO - __main__ - Step 980 Global step 980 Train loss 0.77 on epoch=489
03/01/2022 02:29:29 - INFO - __main__ - Step 990 Global step 990 Train loss 0.73 on epoch=494
03/01/2022 02:29:31 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.72 on epoch=499
03/01/2022 02:29:32 - INFO - __main__ - Global step 1000 Train loss 0.74 EM 0.0 on epoch=499
03/01/2022 02:29:35 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.72 on epoch=504
03/01/2022 02:29:37 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.71 on epoch=509
03/01/2022 02:29:39 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.75 on epoch=514
03/01/2022 02:29:41 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.74 on epoch=519
03/01/2022 02:29:43 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.71 on epoch=524
03/01/2022 02:29:44 - INFO - __main__ - Global step 1050 Train loss 0.73 EM 0.0 on epoch=524
03/01/2022 02:29:47 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.68 on epoch=529
03/01/2022 02:29:49 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.66 on epoch=534
03/01/2022 02:29:51 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.66 on epoch=539
03/01/2022 02:29:53 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.64 on epoch=544
03/01/2022 02:29:55 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.71 on epoch=549
03/01/2022 02:29:57 - INFO - __main__ - Global step 1100 Train loss 0.67 EM 0.0 on epoch=549
03/01/2022 02:29:59 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.71 on epoch=554
03/01/2022 02:30:01 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.71 on epoch=559
03/01/2022 02:30:03 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.71 on epoch=564
03/01/2022 02:30:05 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.70 on epoch=569
03/01/2022 02:30:08 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.66 on epoch=574
03/01/2022 02:30:09 - INFO - __main__ - Global step 1150 Train loss 0.70 EM 0.0 on epoch=574
03/01/2022 02:30:11 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.69 on epoch=579
03/01/2022 02:30:13 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.67 on epoch=584
03/01/2022 02:30:15 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.59 on epoch=589
03/01/2022 02:30:17 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.65 on epoch=594
03/01/2022 02:30:20 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.66 on epoch=599
03/01/2022 02:30:21 - INFO - __main__ - Global step 1200 Train loss 0.65 EM 0.0 on epoch=599
03/01/2022 02:30:23 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.63 on epoch=604
03/01/2022 02:30:25 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.62 on epoch=609
03/01/2022 02:30:27 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.57 on epoch=614
03/01/2022 02:30:30 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.61 on epoch=619
03/01/2022 02:30:32 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.62 on epoch=624
03/01/2022 02:30:33 - INFO - __main__ - Global step 1250 Train loss 0.61 EM 0.0 on epoch=624
03/01/2022 02:30:35 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.62 on epoch=629
03/01/2022 02:30:37 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.60 on epoch=634
03/01/2022 02:30:40 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.55 on epoch=639
03/01/2022 02:30:42 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.60 on epoch=644
03/01/2022 02:30:44 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.53 on epoch=649
03/01/2022 02:30:45 - INFO - __main__ - Global step 1300 Train loss 0.58 EM 0.0 on epoch=649
03/01/2022 02:30:47 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.58 on epoch=654
03/01/2022 02:30:50 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.51 on epoch=659
03/01/2022 02:30:52 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.55 on epoch=664
03/01/2022 02:30:54 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.50 on epoch=669
03/01/2022 02:30:56 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.55 on epoch=674
03/01/2022 02:30:57 - INFO - __main__ - Global step 1350 Train loss 0.54 EM 0.0 on epoch=674
03/01/2022 02:31:00 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.57 on epoch=679
03/01/2022 02:31:02 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.48 on epoch=684
03/01/2022 02:31:04 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.50 on epoch=689
03/01/2022 02:31:06 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.52 on epoch=694
03/01/2022 02:31:08 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.50 on epoch=699
03/01/2022 02:31:10 - INFO - __main__ - Global step 1400 Train loss 0.52 EM 0.0 on epoch=699
03/01/2022 02:31:12 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.52 on epoch=704
03/01/2022 02:31:14 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.50 on epoch=709
03/01/2022 02:31:16 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.55 on epoch=714
03/01/2022 02:31:18 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.45 on epoch=719
03/01/2022 02:31:20 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.52 on epoch=724
03/01/2022 02:31:22 - INFO - __main__ - Global step 1450 Train loss 0.51 EM 0.0 on epoch=724
03/01/2022 02:31:24 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.49 on epoch=729
03/01/2022 02:31:26 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.51 on epoch=734
03/01/2022 02:31:28 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.56 on epoch=739
03/01/2022 02:31:30 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.52 on epoch=744
03/01/2022 02:31:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.45 on epoch=749
03/01/2022 02:31:34 - INFO - __main__ - Global step 1500 Train loss 0.50 EM 0.0 on epoch=749
03/01/2022 02:31:36 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.44 on epoch=754
03/01/2022 02:31:38 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.60 on epoch=759
03/01/2022 02:31:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.50 on epoch=764
03/01/2022 02:31:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.47 on epoch=769
03/01/2022 02:31:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.42 on epoch=774
03/01/2022 02:31:46 - INFO - __main__ - Global step 1550 Train loss 0.49 EM 0.0 on epoch=774
03/01/2022 02:31:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.42 on epoch=779
03/01/2022 02:31:50 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.48 on epoch=784
03/01/2022 02:31:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.42 on epoch=789
03/01/2022 02:31:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.46 on epoch=794
03/01/2022 02:31:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.40 on epoch=799
03/01/2022 02:31:58 - INFO - __main__ - Global step 1600 Train loss 0.44 EM 0.0 on epoch=799
03/01/2022 02:32:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.40 on epoch=804
03/01/2022 02:32:03 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.39 on epoch=809
03/01/2022 02:32:05 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.41 on epoch=814
03/01/2022 02:32:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.44 on epoch=819
03/01/2022 02:32:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.43 on epoch=824
03/01/2022 02:32:10 - INFO - __main__ - Global step 1650 Train loss 0.41 EM 0.0 on epoch=824
03/01/2022 02:32:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.44 on epoch=829
03/01/2022 02:32:15 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.43 on epoch=834
03/01/2022 02:32:17 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.41 on epoch=839
03/01/2022 02:32:19 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.33 on epoch=844
03/01/2022 02:32:21 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.38 on epoch=849
03/01/2022 02:32:22 - INFO - __main__ - Global step 1700 Train loss 0.40 EM 0.0 on epoch=849
03/01/2022 02:32:25 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.43 on epoch=854
03/01/2022 02:32:27 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.41 on epoch=859
03/01/2022 02:32:29 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.37 on epoch=864
03/01/2022 02:32:31 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.37 on epoch=869
03/01/2022 02:32:33 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.35 on epoch=874
03/01/2022 02:32:34 - INFO - __main__ - Global step 1750 Train loss 0.39 EM 0.0 on epoch=874
03/01/2022 02:32:37 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.34 on epoch=879
03/01/2022 02:32:39 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.34 on epoch=884
03/01/2022 02:32:41 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.35 on epoch=889
03/01/2022 02:32:43 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.38 on epoch=894
03/01/2022 02:32:45 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.35 on epoch=899
03/01/2022 02:32:47 - INFO - __main__ - Global step 1800 Train loss 0.35 EM 0.0 on epoch=899
03/01/2022 02:32:49 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.41 on epoch=904
03/01/2022 02:32:51 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.38 on epoch=909
03/01/2022 02:32:53 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.38 on epoch=914
03/01/2022 02:32:55 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.34 on epoch=919
03/01/2022 02:32:58 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.35 on epoch=924
03/01/2022 02:32:59 - INFO - __main__ - Global step 1850 Train loss 0.37 EM 0.0 on epoch=924
03/01/2022 02:33:01 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.35 on epoch=929
03/01/2022 02:33:03 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.34 on epoch=934
03/01/2022 02:33:06 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.32 on epoch=939
03/01/2022 02:33:08 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.29 on epoch=944
03/01/2022 02:33:10 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.34 on epoch=949
03/01/2022 02:33:11 - INFO - __main__ - Global step 1900 Train loss 0.33 EM 0.0 on epoch=949
03/01/2022 02:33:13 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.28 on epoch=954
03/01/2022 02:33:16 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.35 on epoch=959
03/01/2022 02:33:18 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.30 on epoch=964
03/01/2022 02:33:20 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.27 on epoch=969
03/01/2022 02:33:22 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.29 on epoch=974
03/01/2022 02:33:23 - INFO - __main__ - Global step 1950 Train loss 0.30 EM 0.0 on epoch=974
03/01/2022 02:33:26 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.29 on epoch=979
03/01/2022 02:33:28 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.32 on epoch=984
03/01/2022 02:33:30 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.29 on epoch=989
03/01/2022 02:33:32 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.31 on epoch=994
03/01/2022 02:33:34 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.36 on epoch=999
03/01/2022 02:33:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:33:36 - INFO - __main__ - Printing 3 examples
03/01/2022 02:33:36 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 02:33:36 - INFO - __main__ - ['camille saint-saens']
03/01/2022 02:33:36 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 02:33:36 - INFO - __main__ - ['madness']
03/01/2022 02:33:36 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 02:33:36 - INFO - __main__ - ['genevieve']
03/01/2022 02:33:36 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 02:33:36 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:33:36 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:33:36 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:33:36 - INFO - __main__ - Printing 3 examples
03/01/2022 02:33:36 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 02:33:36 - INFO - __main__ - ['will hay']
03/01/2022 02:33:36 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 02:33:36 - INFO - __main__ - ['alan sugar']
03/01/2022 02:33:36 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 02:33:36 - INFO - __main__ - ['cleopatra']
03/01/2022 02:33:36 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:33:36 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:33:36 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:33:36 - INFO - __main__ - Global step 2000 Train loss 0.31 EM 0.0 on epoch=999
03/01/2022 02:33:36 - INFO - __main__ - save last model!
03/01/2022 02:33:36 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 02:33:36 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 02:33:36 - INFO - __main__ - Printing 3 examples
03/01/2022 02:33:36 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 02:33:36 - INFO - __main__ - ['taming of the shrew']
03/01/2022 02:33:36 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 02:33:36 - INFO - __main__ - ['henry fonda']
03/01/2022 02:33:36 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 02:33:36 - INFO - __main__ - ['tchaikovsky']
03/01/2022 02:33:36 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:33:37 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:33:41 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 02:33:48 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:33:49 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:33:49 - INFO - __main__ - Starting training!
03/01/2022 02:36:14 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_13_0.2_8_predictions.txt
03/01/2022 02:36:14 - INFO - __main__ - EM on test data: 0.0050
03/01/2022 02:36:15 - INFO - __main__ - prefix=freebase_qa_32_13, lr=0.2, bsz=8, dev_performance=0.0, test_performance=0.005007511266900351
03/01/2022 02:36:15 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.5, bsz=8 ...
03/01/2022 02:36:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:36:15 - INFO - __main__ - Printing 3 examples
03/01/2022 02:36:15 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 02:36:15 - INFO - __main__ - ['camille saint-saens']
03/01/2022 02:36:15 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 02:36:15 - INFO - __main__ - ['madness']
03/01/2022 02:36:15 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 02:36:15 - INFO - __main__ - ['genevieve']
03/01/2022 02:36:15 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 02:36:15 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:36:15 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:36:15 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:36:15 - INFO - __main__ - Printing 3 examples
03/01/2022 02:36:15 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 02:36:15 - INFO - __main__ - ['will hay']
03/01/2022 02:36:15 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 02:36:15 - INFO - __main__ - ['alan sugar']
03/01/2022 02:36:15 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 02:36:15 - INFO - __main__ - ['cleopatra']
03/01/2022 02:36:15 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:36:15 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:36:16 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:36:29 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:36:30 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:36:30 - INFO - __main__ - Starting training!
03/01/2022 02:36:33 - INFO - __main__ - Step 10 Global step 10 Train loss 4.06 on epoch=4
03/01/2022 02:36:35 - INFO - __main__ - Step 20 Global step 20 Train loss 3.23 on epoch=9
03/01/2022 02:36:38 - INFO - __main__ - Step 30 Global step 30 Train loss 3.05 on epoch=14
03/01/2022 02:36:40 - INFO - __main__ - Step 40 Global step 40 Train loss 2.79 on epoch=19
03/01/2022 02:36:42 - INFO - __main__ - Step 50 Global step 50 Train loss 2.77 on epoch=24
03/01/2022 02:36:43 - INFO - __main__ - Global step 50 Train loss 3.18 EM 0.0 on epoch=24
03/01/2022 02:36:43 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 02:36:45 - INFO - __main__ - Step 60 Global step 60 Train loss 2.64 on epoch=29
03/01/2022 02:36:48 - INFO - __main__ - Step 70 Global step 70 Train loss 2.56 on epoch=34
03/01/2022 02:36:50 - INFO - __main__ - Step 80 Global step 80 Train loss 2.39 on epoch=39
03/01/2022 02:36:52 - INFO - __main__ - Step 90 Global step 90 Train loss 2.35 on epoch=44
03/01/2022 02:36:54 - INFO - __main__ - Step 100 Global step 100 Train loss 2.26 on epoch=49
03/01/2022 02:36:55 - INFO - __main__ - Global step 100 Train loss 2.44 EM 0.0 on epoch=49
03/01/2022 02:36:58 - INFO - __main__ - Step 110 Global step 110 Train loss 2.15 on epoch=54
03/01/2022 02:37:00 - INFO - __main__ - Step 120 Global step 120 Train loss 2.06 on epoch=59
03/01/2022 02:37:02 - INFO - __main__ - Step 130 Global step 130 Train loss 1.94 on epoch=64
03/01/2022 02:37:04 - INFO - __main__ - Step 140 Global step 140 Train loss 1.89 on epoch=69
03/01/2022 02:37:06 - INFO - __main__ - Step 150 Global step 150 Train loss 1.76 on epoch=74
03/01/2022 02:37:08 - INFO - __main__ - Global step 150 Train loss 1.96 EM 0.0 on epoch=74
03/01/2022 02:37:10 - INFO - __main__ - Step 160 Global step 160 Train loss 1.80 on epoch=79
03/01/2022 02:37:12 - INFO - __main__ - Step 170 Global step 170 Train loss 1.60 on epoch=84
03/01/2022 02:37:14 - INFO - __main__ - Step 180 Global step 180 Train loss 1.54 on epoch=89
03/01/2022 02:37:16 - INFO - __main__ - Step 190 Global step 190 Train loss 1.48 on epoch=94
03/01/2022 02:37:19 - INFO - __main__ - Step 200 Global step 200 Train loss 1.45 on epoch=99
03/01/2022 02:37:20 - INFO - __main__ - Global step 200 Train loss 1.57 EM 0.0 on epoch=99
03/01/2022 02:37:22 - INFO - __main__ - Step 210 Global step 210 Train loss 1.43 on epoch=104
03/01/2022 02:37:24 - INFO - __main__ - Step 220 Global step 220 Train loss 1.29 on epoch=109
03/01/2022 02:37:26 - INFO - __main__ - Step 230 Global step 230 Train loss 1.31 on epoch=114
03/01/2022 02:37:29 - INFO - __main__ - Step 240 Global step 240 Train loss 1.27 on epoch=119
03/01/2022 02:37:31 - INFO - __main__ - Step 250 Global step 250 Train loss 1.27 on epoch=124
03/01/2022 02:37:32 - INFO - __main__ - Global step 250 Train loss 1.31 EM 0.0 on epoch=124
03/01/2022 02:37:34 - INFO - __main__ - Step 260 Global step 260 Train loss 1.20 on epoch=129
03/01/2022 02:37:36 - INFO - __main__ - Step 270 Global step 270 Train loss 1.21 on epoch=134
03/01/2022 02:37:39 - INFO - __main__ - Step 280 Global step 280 Train loss 1.21 on epoch=139
03/01/2022 02:37:41 - INFO - __main__ - Step 290 Global step 290 Train loss 1.13 on epoch=144
03/01/2022 02:37:43 - INFO - __main__ - Step 300 Global step 300 Train loss 1.07 on epoch=149
03/01/2022 02:37:44 - INFO - __main__ - Global step 300 Train loss 1.16 EM 0.0 on epoch=149
03/01/2022 02:37:46 - INFO - __main__ - Step 310 Global step 310 Train loss 1.07 on epoch=154
03/01/2022 02:37:48 - INFO - __main__ - Step 320 Global step 320 Train loss 1.01 on epoch=159
03/01/2022 02:37:51 - INFO - __main__ - Step 330 Global step 330 Train loss 0.96 on epoch=164
03/01/2022 02:37:53 - INFO - __main__ - Step 340 Global step 340 Train loss 0.92 on epoch=169
03/01/2022 02:37:55 - INFO - __main__ - Step 350 Global step 350 Train loss 0.97 on epoch=174
03/01/2022 02:37:57 - INFO - __main__ - Global step 350 Train loss 0.99 EM 0.03125 on epoch=174
03/01/2022 02:37:57 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=174, global_step=350
03/01/2022 02:37:59 - INFO - __main__ - Step 360 Global step 360 Train loss 0.94 on epoch=179
03/01/2022 02:38:01 - INFO - __main__ - Step 370 Global step 370 Train loss 0.88 on epoch=184
03/01/2022 02:38:03 - INFO - __main__ - Step 380 Global step 380 Train loss 0.89 on epoch=189
03/01/2022 02:38:05 - INFO - __main__ - Step 390 Global step 390 Train loss 0.82 on epoch=194
03/01/2022 02:38:07 - INFO - __main__ - Step 400 Global step 400 Train loss 0.82 on epoch=199
03/01/2022 02:38:09 - INFO - __main__ - Global step 400 Train loss 0.87 EM 0.03125 on epoch=199
03/01/2022 02:38:11 - INFO - __main__ - Step 410 Global step 410 Train loss 0.79 on epoch=204
03/01/2022 02:38:13 - INFO - __main__ - Step 420 Global step 420 Train loss 0.84 on epoch=209
03/01/2022 02:38:15 - INFO - __main__ - Step 430 Global step 430 Train loss 0.80 on epoch=214
03/01/2022 02:38:18 - INFO - __main__ - Step 440 Global step 440 Train loss 0.80 on epoch=219
03/01/2022 02:38:20 - INFO - __main__ - Step 450 Global step 450 Train loss 0.71 on epoch=224
03/01/2022 02:38:21 - INFO - __main__ - Global step 450 Train loss 0.79 EM 0.03125 on epoch=224
03/01/2022 02:38:24 - INFO - __main__ - Step 460 Global step 460 Train loss 0.74 on epoch=229
03/01/2022 02:38:26 - INFO - __main__ - Step 470 Global step 470 Train loss 0.78 on epoch=234
03/01/2022 02:38:28 - INFO - __main__ - Step 480 Global step 480 Train loss 0.76 on epoch=239
03/01/2022 02:38:30 - INFO - __main__ - Step 490 Global step 490 Train loss 0.71 on epoch=244
03/01/2022 02:38:32 - INFO - __main__ - Step 500 Global step 500 Train loss 0.64 on epoch=249
03/01/2022 02:38:34 - INFO - __main__ - Global step 500 Train loss 0.72 EM 0.03125 on epoch=249
03/01/2022 02:38:36 - INFO - __main__ - Step 510 Global step 510 Train loss 0.66 on epoch=254
03/01/2022 02:38:38 - INFO - __main__ - Step 520 Global step 520 Train loss 0.66 on epoch=259
03/01/2022 02:38:40 - INFO - __main__ - Step 530 Global step 530 Train loss 0.67 on epoch=264
03/01/2022 02:38:42 - INFO - __main__ - Step 540 Global step 540 Train loss 0.64 on epoch=269
03/01/2022 02:38:45 - INFO - __main__ - Step 550 Global step 550 Train loss 0.68 on epoch=274
03/01/2022 02:38:46 - INFO - __main__ - Global step 550 Train loss 0.66 EM 0.0 on epoch=274
03/01/2022 02:38:48 - INFO - __main__ - Step 560 Global step 560 Train loss 0.61 on epoch=279
03/01/2022 02:38:50 - INFO - __main__ - Step 570 Global step 570 Train loss 0.59 on epoch=284
03/01/2022 02:38:52 - INFO - __main__ - Step 580 Global step 580 Train loss 0.57 on epoch=289
03/01/2022 02:38:55 - INFO - __main__ - Step 590 Global step 590 Train loss 0.47 on epoch=294
03/01/2022 02:38:57 - INFO - __main__ - Step 600 Global step 600 Train loss 0.51 on epoch=299
03/01/2022 02:38:58 - INFO - __main__ - Global step 600 Train loss 0.55 EM 0.0 on epoch=299
03/01/2022 02:39:00 - INFO - __main__ - Step 610 Global step 610 Train loss 0.57 on epoch=304
03/01/2022 02:39:02 - INFO - __main__ - Step 620 Global step 620 Train loss 0.48 on epoch=309
03/01/2022 02:39:05 - INFO - __main__ - Step 630 Global step 630 Train loss 0.50 on epoch=314
03/01/2022 02:39:07 - INFO - __main__ - Step 640 Global step 640 Train loss 0.54 on epoch=319
03/01/2022 02:39:09 - INFO - __main__ - Step 650 Global step 650 Train loss 0.48 on epoch=324
03/01/2022 02:39:10 - INFO - __main__ - Global step 650 Train loss 0.51 EM 0.03125 on epoch=324
03/01/2022 02:39:13 - INFO - __main__ - Step 660 Global step 660 Train loss 0.48 on epoch=329
03/01/2022 02:39:15 - INFO - __main__ - Step 670 Global step 670 Train loss 0.51 on epoch=334
03/01/2022 02:39:17 - INFO - __main__ - Step 680 Global step 680 Train loss 0.43 on epoch=339
03/01/2022 02:39:19 - INFO - __main__ - Step 690 Global step 690 Train loss 0.39 on epoch=344
03/01/2022 02:39:21 - INFO - __main__ - Step 700 Global step 700 Train loss 0.40 on epoch=349
03/01/2022 02:39:23 - INFO - __main__ - Global step 700 Train loss 0.44 EM 0.03125 on epoch=349
03/01/2022 02:39:25 - INFO - __main__ - Step 710 Global step 710 Train loss 0.39 on epoch=354
03/01/2022 02:39:27 - INFO - __main__ - Step 720 Global step 720 Train loss 0.44 on epoch=359
03/01/2022 02:39:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.41 on epoch=364
03/01/2022 02:39:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.47 on epoch=369
03/01/2022 02:39:34 - INFO - __main__ - Step 750 Global step 750 Train loss 0.47 on epoch=374
03/01/2022 02:39:35 - INFO - __main__ - Global step 750 Train loss 0.44 EM 0.03125 on epoch=374
03/01/2022 02:39:37 - INFO - __main__ - Step 760 Global step 760 Train loss 0.42 on epoch=379
03/01/2022 02:39:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.40 on epoch=384
03/01/2022 02:39:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.44 on epoch=389
03/01/2022 02:39:44 - INFO - __main__ - Step 790 Global step 790 Train loss 0.35 on epoch=394
03/01/2022 02:39:46 - INFO - __main__ - Step 800 Global step 800 Train loss 0.36 on epoch=399
03/01/2022 02:39:47 - INFO - __main__ - Global step 800 Train loss 0.39 EM 0.0 on epoch=399
03/01/2022 02:39:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.33 on epoch=404
03/01/2022 02:39:52 - INFO - __main__ - Step 820 Global step 820 Train loss 0.38 on epoch=409
03/01/2022 02:39:54 - INFO - __main__ - Step 830 Global step 830 Train loss 0.44 on epoch=414
03/01/2022 02:39:56 - INFO - __main__ - Step 840 Global step 840 Train loss 0.36 on epoch=419
03/01/2022 02:39:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.34 on epoch=424
03/01/2022 02:40:00 - INFO - __main__ - Global step 850 Train loss 0.37 EM 0.0 on epoch=424
03/01/2022 02:40:02 - INFO - __main__ - Step 860 Global step 860 Train loss 0.35 on epoch=429
03/01/2022 02:40:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.29 on epoch=434
03/01/2022 02:40:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.30 on epoch=439
03/01/2022 02:40:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.34 on epoch=444
03/01/2022 02:40:11 - INFO - __main__ - Step 900 Global step 900 Train loss 0.30 on epoch=449
03/01/2022 02:40:12 - INFO - __main__ - Global step 900 Train loss 0.32 EM 0.03125 on epoch=449
03/01/2022 02:40:14 - INFO - __main__ - Step 910 Global step 910 Train loss 0.35 on epoch=454
03/01/2022 02:40:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.33 on epoch=459
03/01/2022 02:40:19 - INFO - __main__ - Step 930 Global step 930 Train loss 0.29 on epoch=464
03/01/2022 02:40:21 - INFO - __main__ - Step 940 Global step 940 Train loss 0.34 on epoch=469
03/01/2022 02:40:23 - INFO - __main__ - Step 950 Global step 950 Train loss 0.30 on epoch=474
03/01/2022 02:40:24 - INFO - __main__ - Global step 950 Train loss 0.32 EM 0.03125 on epoch=474
03/01/2022 02:40:27 - INFO - __main__ - Step 960 Global step 960 Train loss 0.26 on epoch=479
03/01/2022 02:40:29 - INFO - __main__ - Step 970 Global step 970 Train loss 0.28 on epoch=484
03/01/2022 02:40:31 - INFO - __main__ - Step 980 Global step 980 Train loss 0.29 on epoch=489
03/01/2022 02:40:33 - INFO - __main__ - Step 990 Global step 990 Train loss 0.26 on epoch=494
03/01/2022 02:40:35 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.28 on epoch=499
03/01/2022 02:40:37 - INFO - __main__ - Global step 1000 Train loss 0.27 EM 0.0 on epoch=499
03/01/2022 02:40:39 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.20 on epoch=504
03/01/2022 02:40:41 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.21 on epoch=509
03/01/2022 02:40:43 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.29 on epoch=514
03/01/2022 02:40:46 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.22 on epoch=519
03/01/2022 02:40:48 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.26 on epoch=524
03/01/2022 02:40:49 - INFO - __main__ - Global step 1050 Train loss 0.24 EM 0.03125 on epoch=524
03/01/2022 02:40:52 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.25 on epoch=529
03/01/2022 02:40:54 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.24 on epoch=534
03/01/2022 02:40:56 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.25 on epoch=539
03/01/2022 02:40:58 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.24 on epoch=544
03/01/2022 02:41:00 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.20 on epoch=549
03/01/2022 02:41:02 - INFO - __main__ - Global step 1100 Train loss 0.24 EM 0.03125 on epoch=549
03/01/2022 02:41:04 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.21 on epoch=554
03/01/2022 02:41:06 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.24 on epoch=559
03/01/2022 02:41:08 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.20 on epoch=564
03/01/2022 02:41:10 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=569
03/01/2022 02:41:13 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=574
03/01/2022 02:41:14 - INFO - __main__ - Global step 1150 Train loss 0.23 EM 0.0 on epoch=574
03/01/2022 02:41:16 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.18 on epoch=579
03/01/2022 02:41:19 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.26 on epoch=584
03/01/2022 02:41:21 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.30 on epoch=589
03/01/2022 02:41:23 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.20 on epoch=594
03/01/2022 02:41:25 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.16 on epoch=599
03/01/2022 02:41:26 - INFO - __main__ - Global step 1200 Train loss 0.22 EM 0.03125 on epoch=599
03/01/2022 02:41:29 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.22 on epoch=604
03/01/2022 02:41:31 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.15 on epoch=609
03/01/2022 02:41:33 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.17 on epoch=614
03/01/2022 02:41:35 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.19 on epoch=619
03/01/2022 02:41:37 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.20 on epoch=624
03/01/2022 02:41:39 - INFO - __main__ - Global step 1250 Train loss 0.19 EM 0.0 on epoch=624
03/01/2022 02:41:41 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.22 on epoch=629
03/01/2022 02:41:43 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.20 on epoch=634
03/01/2022 02:41:45 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.23 on epoch=639
03/01/2022 02:41:48 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.13 on epoch=644
03/01/2022 02:41:50 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.20 on epoch=649
03/01/2022 02:41:51 - INFO - __main__ - Global step 1300 Train loss 0.20 EM 0.03125 on epoch=649
03/01/2022 02:41:53 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.20 on epoch=654
03/01/2022 02:41:56 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.16 on epoch=659
03/01/2022 02:41:58 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.18 on epoch=664
03/01/2022 02:42:00 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=669
03/01/2022 02:42:02 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.14 on epoch=674
03/01/2022 02:42:04 - INFO - __main__ - Global step 1350 Train loss 0.17 EM 0.03125 on epoch=674
03/01/2022 02:42:06 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.20 on epoch=679
03/01/2022 02:42:08 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.21 on epoch=684
03/01/2022 02:42:10 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.18 on epoch=689
03/01/2022 02:42:12 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.11 on epoch=694
03/01/2022 02:42:15 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.17 on epoch=699
03/01/2022 02:42:16 - INFO - __main__ - Global step 1400 Train loss 0.17 EM 0.0 on epoch=699
03/01/2022 02:42:18 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.17 on epoch=704
03/01/2022 02:42:20 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.12 on epoch=709
03/01/2022 02:42:23 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.14 on epoch=714
03/01/2022 02:42:25 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.13 on epoch=719
03/01/2022 02:42:27 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.17 on epoch=724
03/01/2022 02:42:28 - INFO - __main__ - Global step 1450 Train loss 0.14 EM 0.03125 on epoch=724
03/01/2022 02:42:30 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.19 on epoch=729
03/01/2022 02:42:33 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.15 on epoch=734
03/01/2022 02:42:35 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.13 on epoch=739
03/01/2022 02:42:37 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=744
03/01/2022 02:42:39 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=749
03/01/2022 02:42:40 - INFO - __main__ - Global step 1500 Train loss 0.15 EM 0.0 on epoch=749
03/01/2022 02:42:43 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
03/01/2022 02:42:45 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.12 on epoch=759
03/01/2022 02:42:47 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.12 on epoch=764
03/01/2022 02:42:49 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.13 on epoch=769
03/01/2022 02:42:52 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.14 on epoch=774
03/01/2022 02:42:53 - INFO - __main__ - Global step 1550 Train loss 0.14 EM 0.0 on epoch=774
03/01/2022 02:42:55 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.11 on epoch=779
03/01/2022 02:42:57 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.15 on epoch=784
03/01/2022 02:42:59 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.15 on epoch=789
03/01/2022 02:43:02 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.15 on epoch=794
03/01/2022 02:43:04 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.14 on epoch=799
03/01/2022 02:43:05 - INFO - __main__ - Global step 1600 Train loss 0.14 EM 0.0 on epoch=799
03/01/2022 02:43:07 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.07 on epoch=804
03/01/2022 02:43:10 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.10 on epoch=809
03/01/2022 02:43:12 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.12 on epoch=814
03/01/2022 02:43:14 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.09 on epoch=819
03/01/2022 02:43:16 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.09 on epoch=824
03/01/2022 02:43:18 - INFO - __main__ - Global step 1650 Train loss 0.10 EM 0.0 on epoch=824
03/01/2022 02:43:20 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.08 on epoch=829
03/01/2022 02:43:22 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=834
03/01/2022 02:43:24 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.09 on epoch=839
03/01/2022 02:43:26 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
03/01/2022 02:43:29 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.12 on epoch=849
03/01/2022 02:43:30 - INFO - __main__ - Global step 1700 Train loss 0.11 EM 0.0 on epoch=849
03/01/2022 02:43:32 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.18 on epoch=854
03/01/2022 02:43:34 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.13 on epoch=859
03/01/2022 02:43:36 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=864
03/01/2022 02:43:39 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.10 on epoch=869
03/01/2022 02:43:41 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.13 on epoch=874
03/01/2022 02:43:42 - INFO - __main__ - Global step 1750 Train loss 0.14 EM 0.0 on epoch=874
03/01/2022 02:43:44 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.07 on epoch=879
03/01/2022 02:43:47 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.08 on epoch=884
03/01/2022 02:43:49 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.13 on epoch=889
03/01/2022 02:43:51 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.11 on epoch=894
03/01/2022 02:43:53 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.08 on epoch=899
03/01/2022 02:43:54 - INFO - __main__ - Global step 1800 Train loss 0.09 EM 0.03125 on epoch=899
03/01/2022 02:43:57 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.16 on epoch=904
03/01/2022 02:43:59 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.10 on epoch=909
03/01/2022 02:44:01 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.07 on epoch=914
03/01/2022 02:44:03 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.11 on epoch=919
03/01/2022 02:44:06 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 02:44:07 - INFO - __main__ - Global step 1850 Train loss 0.11 EM 0.03125 on epoch=924
03/01/2022 02:44:09 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.07 on epoch=929
03/01/2022 02:44:11 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.08 on epoch=934
03/01/2022 02:44:13 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.08 on epoch=939
03/01/2022 02:44:16 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/01/2022 02:44:18 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.09 on epoch=949
03/01/2022 02:44:19 - INFO - __main__ - Global step 1900 Train loss 0.09 EM 0.0 on epoch=949
03/01/2022 02:44:21 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.09 on epoch=954
03/01/2022 02:44:24 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.10 on epoch=959
03/01/2022 02:44:26 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.06 on epoch=964
03/01/2022 02:44:28 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.05 on epoch=969
03/01/2022 02:44:30 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.09 on epoch=974
03/01/2022 02:44:32 - INFO - __main__ - Global step 1950 Train loss 0.08 EM 0.0 on epoch=974
03/01/2022 02:44:34 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.14 on epoch=979
03/01/2022 02:44:36 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=984
03/01/2022 02:44:38 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.10 on epoch=989
03/01/2022 02:44:40 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.09 on epoch=994
03/01/2022 02:44:43 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.10 on epoch=999
03/01/2022 02:44:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:44:44 - INFO - __main__ - Printing 3 examples
03/01/2022 02:44:44 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 02:44:44 - INFO - __main__ - ['camille saint-saens']
03/01/2022 02:44:44 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 02:44:44 - INFO - __main__ - ['madness']
03/01/2022 02:44:44 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 02:44:44 - INFO - __main__ - ['genevieve']
03/01/2022 02:44:44 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 02:44:44 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:44:44 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:44:44 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:44:44 - INFO - __main__ - Printing 3 examples
03/01/2022 02:44:44 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 02:44:44 - INFO - __main__ - ['will hay']
03/01/2022 02:44:44 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 02:44:44 - INFO - __main__ - ['alan sugar']
03/01/2022 02:44:44 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 02:44:44 - INFO - __main__ - ['cleopatra']
03/01/2022 02:44:44 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:44:44 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:44:44 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:44:44 - INFO - __main__ - Global step 2000 Train loss 0.11 EM 0.0 on epoch=999
03/01/2022 02:44:44 - INFO - __main__ - save last model!
03/01/2022 02:44:44 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 02:44:44 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 02:44:44 - INFO - __main__ - Printing 3 examples
03/01/2022 02:44:44 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 02:44:44 - INFO - __main__ - ['taming of the shrew']
03/01/2022 02:44:44 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 02:44:44 - INFO - __main__ - ['henry fonda']
03/01/2022 02:44:44 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 02:44:44 - INFO - __main__ - ['tchaikovsky']
03/01/2022 02:44:44 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:44:46 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:44:49 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 02:44:58 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:44:59 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:44:59 - INFO - __main__ - Starting training!
03/01/2022 02:47:38 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_21_0.5_8_predictions.txt
03/01/2022 02:47:38 - INFO - __main__ - EM on test data: 0.0053
03/01/2022 02:47:38 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.5, bsz=8, dev_performance=0.03125, test_performance=0.005257886830245368
03/01/2022 02:47:38 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.4, bsz=8 ...
03/01/2022 02:47:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:47:39 - INFO - __main__ - Printing 3 examples
03/01/2022 02:47:39 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 02:47:39 - INFO - __main__ - ['camille saint-saens']
03/01/2022 02:47:39 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 02:47:39 - INFO - __main__ - ['madness']
03/01/2022 02:47:39 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 02:47:39 - INFO - __main__ - ['genevieve']
03/01/2022 02:47:39 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 02:47:39 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:47:39 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:47:39 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:47:39 - INFO - __main__ - Printing 3 examples
03/01/2022 02:47:39 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 02:47:39 - INFO - __main__ - ['will hay']
03/01/2022 02:47:39 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 02:47:39 - INFO - __main__ - ['alan sugar']
03/01/2022 02:47:39 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 02:47:39 - INFO - __main__ - ['cleopatra']
03/01/2022 02:47:39 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:47:39 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:47:39 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:47:53 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:47:54 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:47:54 - INFO - __main__ - Starting training!
03/01/2022 02:47:57 - INFO - __main__ - Step 10 Global step 10 Train loss 4.12 on epoch=4
03/01/2022 02:47:59 - INFO - __main__ - Step 20 Global step 20 Train loss 3.42 on epoch=9
03/01/2022 02:48:01 - INFO - __main__ - Step 30 Global step 30 Train loss 3.15 on epoch=14
03/01/2022 02:48:03 - INFO - __main__ - Step 40 Global step 40 Train loss 3.03 on epoch=19
03/01/2022 02:48:05 - INFO - __main__ - Step 50 Global step 50 Train loss 2.90 on epoch=24
03/01/2022 02:48:07 - INFO - __main__ - Global step 50 Train loss 3.33 EM 0.0 on epoch=24
03/01/2022 02:48:07 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 02:48:09 - INFO - __main__ - Step 60 Global step 60 Train loss 2.78 on epoch=29
03/01/2022 02:48:11 - INFO - __main__ - Step 70 Global step 70 Train loss 2.67 on epoch=34
03/01/2022 02:48:13 - INFO - __main__ - Step 80 Global step 80 Train loss 2.51 on epoch=39
03/01/2022 02:48:15 - INFO - __main__ - Step 90 Global step 90 Train loss 2.52 on epoch=44
03/01/2022 02:48:17 - INFO - __main__ - Step 100 Global step 100 Train loss 2.37 on epoch=49
03/01/2022 02:48:18 - INFO - __main__ - Global step 100 Train loss 2.57 EM 0.0 on epoch=49
03/01/2022 02:48:21 - INFO - __main__ - Step 110 Global step 110 Train loss 2.28 on epoch=54
03/01/2022 02:48:23 - INFO - __main__ - Step 120 Global step 120 Train loss 2.29 on epoch=59
03/01/2022 02:48:25 - INFO - __main__ - Step 130 Global step 130 Train loss 2.15 on epoch=64
03/01/2022 02:48:27 - INFO - __main__ - Step 140 Global step 140 Train loss 2.11 on epoch=69
03/01/2022 02:48:29 - INFO - __main__ - Step 150 Global step 150 Train loss 1.99 on epoch=74
03/01/2022 02:48:31 - INFO - __main__ - Global step 150 Train loss 2.16 EM 0.0 on epoch=74
03/01/2022 02:48:33 - INFO - __main__ - Step 160 Global step 160 Train loss 1.88 on epoch=79
03/01/2022 02:48:35 - INFO - __main__ - Step 170 Global step 170 Train loss 1.91 on epoch=84
03/01/2022 02:48:37 - INFO - __main__ - Step 180 Global step 180 Train loss 1.91 on epoch=89
03/01/2022 02:48:39 - INFO - __main__ - Step 190 Global step 190 Train loss 1.81 on epoch=94
03/01/2022 02:48:42 - INFO - __main__ - Step 200 Global step 200 Train loss 1.72 on epoch=99
03/01/2022 02:48:43 - INFO - __main__ - Global step 200 Train loss 1.85 EM 0.0 on epoch=99
03/01/2022 02:48:45 - INFO - __main__ - Step 210 Global step 210 Train loss 1.70 on epoch=104
03/01/2022 02:48:48 - INFO - __main__ - Step 220 Global step 220 Train loss 1.67 on epoch=109
03/01/2022 02:48:50 - INFO - __main__ - Step 230 Global step 230 Train loss 1.55 on epoch=114
03/01/2022 02:48:52 - INFO - __main__ - Step 240 Global step 240 Train loss 1.52 on epoch=119
03/01/2022 02:48:54 - INFO - __main__ - Step 250 Global step 250 Train loss 1.38 on epoch=124
03/01/2022 02:48:56 - INFO - __main__ - Global step 250 Train loss 1.56 EM 0.0 on epoch=124
03/01/2022 02:48:58 - INFO - __main__ - Step 260 Global step 260 Train loss 1.37 on epoch=129
03/01/2022 02:49:00 - INFO - __main__ - Step 270 Global step 270 Train loss 1.42 on epoch=134
03/01/2022 02:49:02 - INFO - __main__ - Step 280 Global step 280 Train loss 1.41 on epoch=139
03/01/2022 02:49:04 - INFO - __main__ - Step 290 Global step 290 Train loss 1.22 on epoch=144
03/01/2022 02:49:06 - INFO - __main__ - Step 300 Global step 300 Train loss 1.26 on epoch=149
03/01/2022 02:49:08 - INFO - __main__ - Global step 300 Train loss 1.34 EM 0.0 on epoch=149
03/01/2022 02:49:10 - INFO - __main__ - Step 310 Global step 310 Train loss 1.13 on epoch=154
03/01/2022 02:49:12 - INFO - __main__ - Step 320 Global step 320 Train loss 1.21 on epoch=159
03/01/2022 02:49:14 - INFO - __main__ - Step 330 Global step 330 Train loss 1.27 on epoch=164
03/01/2022 02:49:17 - INFO - __main__ - Step 340 Global step 340 Train loss 1.20 on epoch=169
03/01/2022 02:49:19 - INFO - __main__ - Step 350 Global step 350 Train loss 1.08 on epoch=174
03/01/2022 02:49:20 - INFO - __main__ - Global step 350 Train loss 1.18 EM 0.0 on epoch=174
03/01/2022 02:49:22 - INFO - __main__ - Step 360 Global step 360 Train loss 1.03 on epoch=179
03/01/2022 02:49:24 - INFO - __main__ - Step 370 Global step 370 Train loss 1.00 on epoch=184
03/01/2022 02:49:27 - INFO - __main__ - Step 380 Global step 380 Train loss 1.09 on epoch=189
03/01/2022 02:49:29 - INFO - __main__ - Step 390 Global step 390 Train loss 1.05 on epoch=194
03/01/2022 02:49:31 - INFO - __main__ - Step 400 Global step 400 Train loss 1.02 on epoch=199
03/01/2022 02:49:32 - INFO - __main__ - Global step 400 Train loss 1.04 EM 0.0 on epoch=199
03/01/2022 02:49:35 - INFO - __main__ - Step 410 Global step 410 Train loss 1.08 on epoch=204
03/01/2022 02:49:37 - INFO - __main__ - Step 420 Global step 420 Train loss 0.97 on epoch=209
03/01/2022 02:49:39 - INFO - __main__ - Step 430 Global step 430 Train loss 0.90 on epoch=214
03/01/2022 02:49:41 - INFO - __main__ - Step 440 Global step 440 Train loss 0.89 on epoch=219
03/01/2022 02:49:44 - INFO - __main__ - Step 450 Global step 450 Train loss 0.93 on epoch=224
03/01/2022 02:49:45 - INFO - __main__ - Global step 450 Train loss 0.95 EM 0.0 on epoch=224
03/01/2022 02:49:47 - INFO - __main__ - Step 460 Global step 460 Train loss 0.86 on epoch=229
03/01/2022 02:49:49 - INFO - __main__ - Step 470 Global step 470 Train loss 0.89 on epoch=234
03/01/2022 02:49:51 - INFO - __main__ - Step 480 Global step 480 Train loss 0.86 on epoch=239
03/01/2022 02:49:54 - INFO - __main__ - Step 490 Global step 490 Train loss 0.78 on epoch=244
03/01/2022 02:49:56 - INFO - __main__ - Step 500 Global step 500 Train loss 0.82 on epoch=249
03/01/2022 02:49:57 - INFO - __main__ - Global step 500 Train loss 0.84 EM 0.0 on epoch=249
03/01/2022 02:49:59 - INFO - __main__ - Step 510 Global step 510 Train loss 0.76 on epoch=254
03/01/2022 02:50:02 - INFO - __main__ - Step 520 Global step 520 Train loss 0.84 on epoch=259
03/01/2022 02:50:04 - INFO - __main__ - Step 530 Global step 530 Train loss 0.88 on epoch=264
03/01/2022 02:50:06 - INFO - __main__ - Step 540 Global step 540 Train loss 0.71 on epoch=269
03/01/2022 02:50:08 - INFO - __main__ - Step 550 Global step 550 Train loss 0.72 on epoch=274
03/01/2022 02:50:10 - INFO - __main__ - Global step 550 Train loss 0.78 EM 0.03125 on epoch=274
03/01/2022 02:50:10 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=274, global_step=550
03/01/2022 02:50:12 - INFO - __main__ - Step 560 Global step 560 Train loss 0.74 on epoch=279
03/01/2022 02:50:14 - INFO - __main__ - Step 570 Global step 570 Train loss 0.67 on epoch=284
03/01/2022 02:50:16 - INFO - __main__ - Step 580 Global step 580 Train loss 0.70 on epoch=289
03/01/2022 02:50:18 - INFO - __main__ - Step 590 Global step 590 Train loss 0.65 on epoch=294
03/01/2022 02:50:21 - INFO - __main__ - Step 600 Global step 600 Train loss 0.66 on epoch=299
03/01/2022 02:50:22 - INFO - __main__ - Global step 600 Train loss 0.68 EM 0.0 on epoch=299
03/01/2022 02:50:24 - INFO - __main__ - Step 610 Global step 610 Train loss 0.64 on epoch=304
03/01/2022 02:50:26 - INFO - __main__ - Step 620 Global step 620 Train loss 0.60 on epoch=309
03/01/2022 02:50:29 - INFO - __main__ - Step 630 Global step 630 Train loss 0.67 on epoch=314
03/01/2022 02:50:31 - INFO - __main__ - Step 640 Global step 640 Train loss 0.57 on epoch=319
03/01/2022 02:50:33 - INFO - __main__ - Step 650 Global step 650 Train loss 0.60 on epoch=324
03/01/2022 02:50:34 - INFO - __main__ - Global step 650 Train loss 0.62 EM 0.0 on epoch=324
03/01/2022 02:50:36 - INFO - __main__ - Step 660 Global step 660 Train loss 0.63 on epoch=329
03/01/2022 02:50:39 - INFO - __main__ - Step 670 Global step 670 Train loss 0.62 on epoch=334
03/01/2022 02:50:41 - INFO - __main__ - Step 680 Global step 680 Train loss 0.72 on epoch=339
03/01/2022 02:50:43 - INFO - __main__ - Step 690 Global step 690 Train loss 0.58 on epoch=344
03/01/2022 02:50:45 - INFO - __main__ - Step 700 Global step 700 Train loss 0.51 on epoch=349
03/01/2022 02:50:47 - INFO - __main__ - Global step 700 Train loss 0.61 EM 0.0 on epoch=349
03/01/2022 02:50:49 - INFO - __main__ - Step 710 Global step 710 Train loss 0.57 on epoch=354
03/01/2022 02:50:51 - INFO - __main__ - Step 720 Global step 720 Train loss 0.52 on epoch=359
03/01/2022 02:50:53 - INFO - __main__ - Step 730 Global step 730 Train loss 0.51 on epoch=364
03/01/2022 02:50:56 - INFO - __main__ - Step 740 Global step 740 Train loss 0.54 on epoch=369
03/01/2022 02:50:58 - INFO - __main__ - Step 750 Global step 750 Train loss 0.55 on epoch=374
03/01/2022 02:50:59 - INFO - __main__ - Global step 750 Train loss 0.54 EM 0.03125 on epoch=374
03/01/2022 02:51:01 - INFO - __main__ - Step 760 Global step 760 Train loss 0.55 on epoch=379
03/01/2022 02:51:03 - INFO - __main__ - Step 770 Global step 770 Train loss 0.50 on epoch=384
03/01/2022 02:51:06 - INFO - __main__ - Step 780 Global step 780 Train loss 0.47 on epoch=389
03/01/2022 02:51:08 - INFO - __main__ - Step 790 Global step 790 Train loss 0.52 on epoch=394
03/01/2022 02:51:10 - INFO - __main__ - Step 800 Global step 800 Train loss 0.43 on epoch=399
03/01/2022 02:51:12 - INFO - __main__ - Global step 800 Train loss 0.49 EM 0.0 on epoch=399
03/01/2022 02:51:14 - INFO - __main__ - Step 810 Global step 810 Train loss 0.39 on epoch=404
03/01/2022 02:51:16 - INFO - __main__ - Step 820 Global step 820 Train loss 0.51 on epoch=409
03/01/2022 02:51:18 - INFO - __main__ - Step 830 Global step 830 Train loss 0.43 on epoch=414
03/01/2022 02:51:20 - INFO - __main__ - Step 840 Global step 840 Train loss 0.40 on epoch=419
03/01/2022 02:51:23 - INFO - __main__ - Step 850 Global step 850 Train loss 0.50 on epoch=424
03/01/2022 02:51:24 - INFO - __main__ - Global step 850 Train loss 0.45 EM 0.0 on epoch=424
03/01/2022 02:51:26 - INFO - __main__ - Step 860 Global step 860 Train loss 0.41 on epoch=429
03/01/2022 02:51:29 - INFO - __main__ - Step 870 Global step 870 Train loss 0.46 on epoch=434
03/01/2022 02:51:31 - INFO - __main__ - Step 880 Global step 880 Train loss 0.37 on epoch=439
03/01/2022 02:51:33 - INFO - __main__ - Step 890 Global step 890 Train loss 0.47 on epoch=444
03/01/2022 02:51:35 - INFO - __main__ - Step 900 Global step 900 Train loss 0.47 on epoch=449
03/01/2022 02:51:37 - INFO - __main__ - Global step 900 Train loss 0.44 EM 0.0 on epoch=449
03/01/2022 02:51:39 - INFO - __main__ - Step 910 Global step 910 Train loss 0.43 on epoch=454
03/01/2022 02:51:41 - INFO - __main__ - Step 920 Global step 920 Train loss 0.46 on epoch=459
03/01/2022 02:51:43 - INFO - __main__ - Step 930 Global step 930 Train loss 0.41 on epoch=464
03/01/2022 02:51:45 - INFO - __main__ - Step 940 Global step 940 Train loss 0.39 on epoch=469
03/01/2022 02:51:48 - INFO - __main__ - Step 950 Global step 950 Train loss 0.33 on epoch=474
03/01/2022 02:51:49 - INFO - __main__ - Global step 950 Train loss 0.41 EM 0.03125 on epoch=474
03/01/2022 02:51:51 - INFO - __main__ - Step 960 Global step 960 Train loss 0.32 on epoch=479
03/01/2022 02:51:54 - INFO - __main__ - Step 970 Global step 970 Train loss 0.31 on epoch=484
03/01/2022 02:51:56 - INFO - __main__ - Step 980 Global step 980 Train loss 0.40 on epoch=489
03/01/2022 02:51:58 - INFO - __main__ - Step 990 Global step 990 Train loss 0.33 on epoch=494
03/01/2022 02:52:00 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.30 on epoch=499
03/01/2022 02:52:02 - INFO - __main__ - Global step 1000 Train loss 0.33 EM 0.0 on epoch=499
03/01/2022 02:52:04 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.33 on epoch=504
03/01/2022 02:52:06 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.33 on epoch=509
03/01/2022 02:52:08 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.36 on epoch=514
03/01/2022 02:52:11 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.27 on epoch=519
03/01/2022 02:52:13 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.29 on epoch=524
03/01/2022 02:52:14 - INFO - __main__ - Global step 1050 Train loss 0.31 EM 0.03125 on epoch=524
03/01/2022 02:52:16 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.34 on epoch=529
03/01/2022 02:52:19 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.25 on epoch=534
03/01/2022 02:52:21 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.28 on epoch=539
03/01/2022 02:52:23 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.32 on epoch=544
03/01/2022 02:52:25 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.32 on epoch=549
03/01/2022 02:52:27 - INFO - __main__ - Global step 1100 Train loss 0.30 EM 0.0 on epoch=549
03/01/2022 02:52:29 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.29 on epoch=554
03/01/2022 02:52:31 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.25 on epoch=559
03/01/2022 02:52:33 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.22 on epoch=564
03/01/2022 02:52:36 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.28 on epoch=569
03/01/2022 02:52:38 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.26 on epoch=574
03/01/2022 02:52:39 - INFO - __main__ - Global step 1150 Train loss 0.26 EM 0.0 on epoch=574
03/01/2022 02:52:41 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.25 on epoch=579
03/01/2022 02:52:44 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.28 on epoch=584
03/01/2022 02:52:46 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.21 on epoch=589
03/01/2022 02:52:48 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.24 on epoch=594
03/01/2022 02:52:50 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.20 on epoch=599
03/01/2022 02:52:52 - INFO - __main__ - Global step 1200 Train loss 0.24 EM 0.03125 on epoch=599
03/01/2022 02:52:54 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.21 on epoch=604
03/01/2022 02:52:56 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.20 on epoch=609
03/01/2022 02:52:58 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.22 on epoch=614
03/01/2022 02:53:00 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.20 on epoch=619
03/01/2022 02:53:03 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.23 on epoch=624
03/01/2022 02:53:04 - INFO - __main__ - Global step 1250 Train loss 0.21 EM 0.0 on epoch=624
03/01/2022 02:53:06 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.18 on epoch=629
03/01/2022 02:53:09 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.22 on epoch=634
03/01/2022 02:53:11 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.28 on epoch=639
03/01/2022 02:53:13 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.25 on epoch=644
03/01/2022 02:53:15 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.25 on epoch=649
03/01/2022 02:53:17 - INFO - __main__ - Global step 1300 Train loss 0.23 EM 0.03125 on epoch=649
03/01/2022 02:53:19 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.22 on epoch=654
03/01/2022 02:53:21 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.25 on epoch=659
03/01/2022 02:53:23 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.25 on epoch=664
03/01/2022 02:53:25 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.19 on epoch=669
03/01/2022 02:53:28 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.24 on epoch=674
03/01/2022 02:53:29 - INFO - __main__ - Global step 1350 Train loss 0.23 EM 0.03125 on epoch=674
03/01/2022 02:53:31 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.17 on epoch=679
03/01/2022 02:53:33 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.17 on epoch=684
03/01/2022 02:53:36 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.20 on epoch=689
03/01/2022 02:53:38 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.16 on epoch=694
03/01/2022 02:53:40 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.20 on epoch=699
03/01/2022 02:53:42 - INFO - __main__ - Global step 1400 Train loss 0.18 EM 0.03125 on epoch=699
03/01/2022 02:53:44 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.20 on epoch=704
03/01/2022 02:53:46 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.15 on epoch=709
03/01/2022 02:53:48 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.19 on epoch=714
03/01/2022 02:53:50 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.16 on epoch=719
03/01/2022 02:53:53 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.20 on epoch=724
03/01/2022 02:53:54 - INFO - __main__ - Global step 1450 Train loss 0.18 EM 0.0 on epoch=724
03/01/2022 02:53:56 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.18 on epoch=729
03/01/2022 02:53:58 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.21 on epoch=734
03/01/2022 02:54:01 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.18 on epoch=739
03/01/2022 02:54:03 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.14 on epoch=744
03/01/2022 02:54:05 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.16 on epoch=749
03/01/2022 02:54:07 - INFO - __main__ - Global step 1500 Train loss 0.18 EM 0.03125 on epoch=749
03/01/2022 02:54:09 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.17 on epoch=754
03/01/2022 02:54:11 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.15 on epoch=759
03/01/2022 02:54:13 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.16 on epoch=764
03/01/2022 02:54:15 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.15 on epoch=769
03/01/2022 02:54:18 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.17 on epoch=774
03/01/2022 02:54:19 - INFO - __main__ - Global step 1550 Train loss 0.16 EM 0.03125 on epoch=774
03/01/2022 02:54:21 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.14 on epoch=779
03/01/2022 02:54:23 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.21 on epoch=784
03/01/2022 02:54:26 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.12 on epoch=789
03/01/2022 02:54:28 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.14 on epoch=794
03/01/2022 02:54:30 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.19 on epoch=799
03/01/2022 02:54:31 - INFO - __main__ - Global step 1600 Train loss 0.16 EM 0.0 on epoch=799
03/01/2022 02:54:34 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.18 on epoch=804
03/01/2022 02:54:36 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.14 on epoch=809
03/01/2022 02:54:38 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.13 on epoch=814
03/01/2022 02:54:40 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.13 on epoch=819
03/01/2022 02:54:43 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.16 on epoch=824
03/01/2022 02:54:44 - INFO - __main__ - Global step 1650 Train loss 0.15 EM 0.03125 on epoch=824
03/01/2022 02:54:46 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.13 on epoch=829
03/01/2022 02:54:48 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.13 on epoch=834
03/01/2022 02:54:51 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.13 on epoch=839
03/01/2022 02:54:53 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.13 on epoch=844
03/01/2022 02:54:55 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.13 on epoch=849
03/01/2022 02:54:56 - INFO - __main__ - Global step 1700 Train loss 0.13 EM 0.03125 on epoch=849
03/01/2022 02:54:58 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.12 on epoch=854
03/01/2022 02:55:01 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.12 on epoch=859
03/01/2022 02:55:03 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.14 on epoch=864
03/01/2022 02:55:05 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.13 on epoch=869
03/01/2022 02:55:07 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.14 on epoch=874
03/01/2022 02:55:09 - INFO - __main__ - Global step 1750 Train loss 0.13 EM 0.03125 on epoch=874
03/01/2022 02:55:11 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.12 on epoch=879
03/01/2022 02:55:13 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.14 on epoch=884
03/01/2022 02:55:15 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/01/2022 02:55:18 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.12 on epoch=894
03/01/2022 02:55:20 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.10 on epoch=899
03/01/2022 02:55:21 - INFO - __main__ - Global step 1800 Train loss 0.13 EM 0.03125 on epoch=899
03/01/2022 02:55:23 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.14 on epoch=904
03/01/2022 02:55:26 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.15 on epoch=909
03/01/2022 02:55:28 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.14 on epoch=914
03/01/2022 02:55:30 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.12 on epoch=919
03/01/2022 02:55:32 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 02:55:34 - INFO - __main__ - Global step 1850 Train loss 0.13 EM 0.03125 on epoch=924
03/01/2022 02:55:36 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.10 on epoch=929
03/01/2022 02:55:38 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.12 on epoch=934
03/01/2022 02:55:40 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.10 on epoch=939
03/01/2022 02:55:42 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.11 on epoch=944
03/01/2022 02:55:45 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.07 on epoch=949
03/01/2022 02:55:46 - INFO - __main__ - Global step 1900 Train loss 0.10 EM 0.0 on epoch=949
03/01/2022 02:55:48 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.14 on epoch=954
03/01/2022 02:55:50 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.14 on epoch=959
03/01/2022 02:55:53 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.09 on epoch=964
03/01/2022 02:55:55 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.10 on epoch=969
03/01/2022 02:55:57 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.14 on epoch=974
03/01/2022 02:55:58 - INFO - __main__ - Global step 1950 Train loss 0.12 EM 0.0 on epoch=974
03/01/2022 02:56:01 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=979
03/01/2022 02:56:03 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.11 on epoch=984
03/01/2022 02:56:05 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.09 on epoch=989
03/01/2022 02:56:07 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.12 on epoch=994
03/01/2022 02:56:09 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.11 on epoch=999
03/01/2022 02:56:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:56:11 - INFO - __main__ - Printing 3 examples
03/01/2022 02:56:11 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 02:56:11 - INFO - __main__ - ['camille saint-saens']
03/01/2022 02:56:11 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 02:56:11 - INFO - __main__ - ['madness']
03/01/2022 02:56:11 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 02:56:11 - INFO - __main__ - ['genevieve']
03/01/2022 02:56:11 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 02:56:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:56:11 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:56:11 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:56:11 - INFO - __main__ - Printing 3 examples
03/01/2022 02:56:11 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 02:56:11 - INFO - __main__ - ['will hay']
03/01/2022 02:56:11 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 02:56:11 - INFO - __main__ - ['alan sugar']
03/01/2022 02:56:11 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 02:56:11 - INFO - __main__ - ['cleopatra']
03/01/2022 02:56:11 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:56:11 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:56:11 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:56:11 - INFO - __main__ - Global step 2000 Train loss 0.11 EM 0.03125 on epoch=999
03/01/2022 02:56:11 - INFO - __main__ - save last model!
03/01/2022 02:56:11 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 02:56:11 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 02:56:11 - INFO - __main__ - Printing 3 examples
03/01/2022 02:56:11 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 02:56:11 - INFO - __main__ - ['taming of the shrew']
03/01/2022 02:56:11 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 02:56:11 - INFO - __main__ - ['henry fonda']
03/01/2022 02:56:11 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 02:56:11 - INFO - __main__ - ['tchaikovsky']
03/01/2022 02:56:11 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:56:12 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:56:16 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 02:56:25 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:56:25 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:56:25 - INFO - __main__ - Starting training!
03/01/2022 02:59:06 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_21_0.4_8_predictions.txt
03/01/2022 02:59:06 - INFO - __main__ - EM on test data: 0.0060
03/01/2022 02:59:06 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.4, bsz=8, dev_performance=0.03125, test_performance=0.006009013520280421
03/01/2022 02:59:06 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.3, bsz=8 ...
03/01/2022 02:59:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:59:07 - INFO - __main__ - Printing 3 examples
03/01/2022 02:59:07 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 02:59:07 - INFO - __main__ - ['camille saint-saens']
03/01/2022 02:59:07 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 02:59:07 - INFO - __main__ - ['madness']
03/01/2022 02:59:07 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 02:59:07 - INFO - __main__ - ['genevieve']
03/01/2022 02:59:07 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 02:59:07 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:59:07 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 02:59:07 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 02:59:07 - INFO - __main__ - Printing 3 examples
03/01/2022 02:59:07 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 02:59:07 - INFO - __main__ - ['will hay']
03/01/2022 02:59:07 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 02:59:07 - INFO - __main__ - ['alan sugar']
03/01/2022 02:59:07 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 02:59:07 - INFO - __main__ - ['cleopatra']
03/01/2022 02:59:07 - INFO - __main__ - Tokenizing Input ...
03/01/2022 02:59:07 - INFO - __main__ - Tokenizing Output ...
03/01/2022 02:59:07 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 02:59:21 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 02:59:22 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 02:59:22 - INFO - __main__ - Starting training!
03/01/2022 02:59:26 - INFO - __main__ - Step 10 Global step 10 Train loss 4.25 on epoch=4
03/01/2022 02:59:28 - INFO - __main__ - Step 20 Global step 20 Train loss 3.64 on epoch=9
03/01/2022 02:59:31 - INFO - __main__ - Step 30 Global step 30 Train loss 3.25 on epoch=14
03/01/2022 02:59:33 - INFO - __main__ - Step 40 Global step 40 Train loss 3.15 on epoch=19
03/01/2022 02:59:35 - INFO - __main__ - Step 50 Global step 50 Train loss 2.99 on epoch=24
03/01/2022 02:59:37 - INFO - __main__ - Global step 50 Train loss 3.46 EM 0.0 on epoch=24
03/01/2022 02:59:37 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 02:59:39 - INFO - __main__ - Step 60 Global step 60 Train loss 2.92 on epoch=29
03/01/2022 02:59:41 - INFO - __main__ - Step 70 Global step 70 Train loss 2.78 on epoch=34
03/01/2022 02:59:43 - INFO - __main__ - Step 80 Global step 80 Train loss 2.65 on epoch=39
03/01/2022 02:59:45 - INFO - __main__ - Step 90 Global step 90 Train loss 2.57 on epoch=44
03/01/2022 02:59:48 - INFO - __main__ - Step 100 Global step 100 Train loss 2.56 on epoch=49
03/01/2022 02:59:49 - INFO - __main__ - Global step 100 Train loss 2.70 EM 0.0 on epoch=49
03/01/2022 02:59:51 - INFO - __main__ - Step 110 Global step 110 Train loss 2.45 on epoch=54
03/01/2022 02:59:53 - INFO - __main__ - Step 120 Global step 120 Train loss 2.41 on epoch=59
03/01/2022 02:59:56 - INFO - __main__ - Step 130 Global step 130 Train loss 2.37 on epoch=64
03/01/2022 02:59:58 - INFO - __main__ - Step 140 Global step 140 Train loss 2.29 on epoch=69
03/01/2022 03:00:00 - INFO - __main__ - Step 150 Global step 150 Train loss 2.28 on epoch=74
03/01/2022 03:00:01 - INFO - __main__ - Global step 150 Train loss 2.36 EM 0.0 on epoch=74
03/01/2022 03:00:04 - INFO - __main__ - Step 160 Global step 160 Train loss 2.11 on epoch=79
03/01/2022 03:00:06 - INFO - __main__ - Step 170 Global step 170 Train loss 2.00 on epoch=84
03/01/2022 03:00:08 - INFO - __main__ - Step 180 Global step 180 Train loss 2.13 on epoch=89
03/01/2022 03:00:10 - INFO - __main__ - Step 190 Global step 190 Train loss 2.09 on epoch=94
03/01/2022 03:00:12 - INFO - __main__ - Step 200 Global step 200 Train loss 1.97 on epoch=99
03/01/2022 03:00:14 - INFO - __main__ - Global step 200 Train loss 2.06 EM 0.0 on epoch=99
03/01/2022 03:00:16 - INFO - __main__ - Step 210 Global step 210 Train loss 1.94 on epoch=104
03/01/2022 03:00:19 - INFO - __main__ - Step 220 Global step 220 Train loss 1.89 on epoch=109
03/01/2022 03:00:21 - INFO - __main__ - Step 230 Global step 230 Train loss 1.76 on epoch=114
03/01/2022 03:00:23 - INFO - __main__ - Step 240 Global step 240 Train loss 1.83 on epoch=119
03/01/2022 03:00:25 - INFO - __main__ - Step 250 Global step 250 Train loss 1.73 on epoch=124
03/01/2022 03:00:27 - INFO - __main__ - Global step 250 Train loss 1.83 EM 0.0 on epoch=124
03/01/2022 03:00:29 - INFO - __main__ - Step 260 Global step 260 Train loss 1.74 on epoch=129
03/01/2022 03:00:31 - INFO - __main__ - Step 270 Global step 270 Train loss 1.65 on epoch=134
03/01/2022 03:00:33 - INFO - __main__ - Step 280 Global step 280 Train loss 1.57 on epoch=139
03/01/2022 03:00:36 - INFO - __main__ - Step 290 Global step 290 Train loss 1.64 on epoch=144
03/01/2022 03:00:38 - INFO - __main__ - Step 300 Global step 300 Train loss 1.55 on epoch=149
03/01/2022 03:00:40 - INFO - __main__ - Global step 300 Train loss 1.63 EM 0.0 on epoch=149
03/01/2022 03:00:42 - INFO - __main__ - Step 310 Global step 310 Train loss 1.58 on epoch=154
03/01/2022 03:00:44 - INFO - __main__ - Step 320 Global step 320 Train loss 1.45 on epoch=159
03/01/2022 03:00:46 - INFO - __main__ - Step 330 Global step 330 Train loss 1.48 on epoch=164
03/01/2022 03:00:48 - INFO - __main__ - Step 340 Global step 340 Train loss 1.48 on epoch=169
03/01/2022 03:00:51 - INFO - __main__ - Step 350 Global step 350 Train loss 1.37 on epoch=174
03/01/2022 03:00:52 - INFO - __main__ - Global step 350 Train loss 1.47 EM 0.0 on epoch=174
03/01/2022 03:00:54 - INFO - __main__ - Step 360 Global step 360 Train loss 1.34 on epoch=179
03/01/2022 03:00:57 - INFO - __main__ - Step 370 Global step 370 Train loss 1.45 on epoch=184
03/01/2022 03:00:59 - INFO - __main__ - Step 380 Global step 380 Train loss 1.31 on epoch=189
03/01/2022 03:01:01 - INFO - __main__ - Step 390 Global step 390 Train loss 1.25 on epoch=194
03/01/2022 03:01:03 - INFO - __main__ - Step 400 Global step 400 Train loss 1.28 on epoch=199
03/01/2022 03:01:05 - INFO - __main__ - Global step 400 Train loss 1.32 EM 0.0 on epoch=199
03/01/2022 03:01:07 - INFO - __main__ - Step 410 Global step 410 Train loss 1.19 on epoch=204
03/01/2022 03:01:09 - INFO - __main__ - Step 420 Global step 420 Train loss 1.10 on epoch=209
03/01/2022 03:01:11 - INFO - __main__ - Step 430 Global step 430 Train loss 1.09 on epoch=214
03/01/2022 03:01:14 - INFO - __main__ - Step 440 Global step 440 Train loss 1.09 on epoch=219
03/01/2022 03:01:16 - INFO - __main__ - Step 450 Global step 450 Train loss 1.08 on epoch=224
03/01/2022 03:01:17 - INFO - __main__ - Global step 450 Train loss 1.11 EM 0.0 on epoch=224
03/01/2022 03:01:19 - INFO - __main__ - Step 460 Global step 460 Train loss 1.09 on epoch=229
03/01/2022 03:01:22 - INFO - __main__ - Step 470 Global step 470 Train loss 1.11 on epoch=234
03/01/2022 03:01:24 - INFO - __main__ - Step 480 Global step 480 Train loss 1.02 on epoch=239
03/01/2022 03:01:26 - INFO - __main__ - Step 490 Global step 490 Train loss 1.03 on epoch=244
03/01/2022 03:01:28 - INFO - __main__ - Step 500 Global step 500 Train loss 0.96 on epoch=249
03/01/2022 03:01:29 - INFO - __main__ - Global step 500 Train loss 1.04 EM 0.0 on epoch=249
03/01/2022 03:01:32 - INFO - __main__ - Step 510 Global step 510 Train loss 0.93 on epoch=254
03/01/2022 03:01:34 - INFO - __main__ - Step 520 Global step 520 Train loss 0.90 on epoch=259
03/01/2022 03:01:36 - INFO - __main__ - Step 530 Global step 530 Train loss 0.99 on epoch=264
03/01/2022 03:01:38 - INFO - __main__ - Step 540 Global step 540 Train loss 0.92 on epoch=269
03/01/2022 03:01:41 - INFO - __main__ - Step 550 Global step 550 Train loss 0.82 on epoch=274
03/01/2022 03:01:42 - INFO - __main__ - Global step 550 Train loss 0.91 EM 0.03125 on epoch=274
03/01/2022 03:01:42 - INFO - __main__ - Saving model with best EM: 0.0 -> 0.03125 on epoch=274, global_step=550
03/01/2022 03:01:44 - INFO - __main__ - Step 560 Global step 560 Train loss 0.88 on epoch=279
03/01/2022 03:01:46 - INFO - __main__ - Step 570 Global step 570 Train loss 0.88 on epoch=284
03/01/2022 03:01:48 - INFO - __main__ - Step 580 Global step 580 Train loss 0.85 on epoch=289
03/01/2022 03:01:51 - INFO - __main__ - Step 590 Global step 590 Train loss 0.77 on epoch=294
03/01/2022 03:01:53 - INFO - __main__ - Step 600 Global step 600 Train loss 0.82 on epoch=299
03/01/2022 03:01:54 - INFO - __main__ - Global step 600 Train loss 0.84 EM 0.03125 on epoch=299
03/01/2022 03:01:57 - INFO - __main__ - Step 610 Global step 610 Train loss 0.76 on epoch=304
03/01/2022 03:01:59 - INFO - __main__ - Step 620 Global step 620 Train loss 0.74 on epoch=309
03/01/2022 03:02:02 - INFO - __main__ - Step 630 Global step 630 Train loss 0.80 on epoch=314
03/01/2022 03:02:04 - INFO - __main__ - Step 640 Global step 640 Train loss 0.75 on epoch=319
03/01/2022 03:02:07 - INFO - __main__ - Step 650 Global step 650 Train loss 0.78 on epoch=324
03/01/2022 03:02:08 - INFO - __main__ - Global step 650 Train loss 0.77 EM 0.03125 on epoch=324
03/01/2022 03:02:11 - INFO - __main__ - Step 660 Global step 660 Train loss 0.76 on epoch=329
03/01/2022 03:02:13 - INFO - __main__ - Step 670 Global step 670 Train loss 0.63 on epoch=334
03/01/2022 03:02:15 - INFO - __main__ - Step 680 Global step 680 Train loss 0.58 on epoch=339
03/01/2022 03:02:18 - INFO - __main__ - Step 690 Global step 690 Train loss 0.65 on epoch=344
03/01/2022 03:02:20 - INFO - __main__ - Step 700 Global step 700 Train loss 0.65 on epoch=349
03/01/2022 03:02:22 - INFO - __main__ - Global step 700 Train loss 0.65 EM 0.03125 on epoch=349
03/01/2022 03:02:24 - INFO - __main__ - Step 710 Global step 710 Train loss 0.59 on epoch=354
03/01/2022 03:02:26 - INFO - __main__ - Step 720 Global step 720 Train loss 0.61 on epoch=359
03/01/2022 03:02:29 - INFO - __main__ - Step 730 Global step 730 Train loss 0.61 on epoch=364
03/01/2022 03:02:31 - INFO - __main__ - Step 740 Global step 740 Train loss 0.63 on epoch=369
03/01/2022 03:02:33 - INFO - __main__ - Step 750 Global step 750 Train loss 0.60 on epoch=374
03/01/2022 03:02:34 - INFO - __main__ - Global step 750 Train loss 0.61 EM 0.03125 on epoch=374
03/01/2022 03:02:36 - INFO - __main__ - Step 760 Global step 760 Train loss 0.64 on epoch=379
03/01/2022 03:02:39 - INFO - __main__ - Step 770 Global step 770 Train loss 0.56 on epoch=384
03/01/2022 03:02:41 - INFO - __main__ - Step 780 Global step 780 Train loss 0.51 on epoch=389
03/01/2022 03:02:43 - INFO - __main__ - Step 790 Global step 790 Train loss 0.57 on epoch=394
03/01/2022 03:02:45 - INFO - __main__ - Step 800 Global step 800 Train loss 0.62 on epoch=399
03/01/2022 03:02:47 - INFO - __main__ - Global step 800 Train loss 0.58 EM 0.0 on epoch=399
03/01/2022 03:02:49 - INFO - __main__ - Step 810 Global step 810 Train loss 0.55 on epoch=404
03/01/2022 03:02:51 - INFO - __main__ - Step 820 Global step 820 Train loss 0.58 on epoch=409
03/01/2022 03:02:53 - INFO - __main__ - Step 830 Global step 830 Train loss 0.53 on epoch=414
03/01/2022 03:02:55 - INFO - __main__ - Step 840 Global step 840 Train loss 0.49 on epoch=419
03/01/2022 03:02:58 - INFO - __main__ - Step 850 Global step 850 Train loss 0.48 on epoch=424
03/01/2022 03:02:59 - INFO - __main__ - Global step 850 Train loss 0.52 EM 0.0 on epoch=424
03/01/2022 03:03:01 - INFO - __main__ - Step 860 Global step 860 Train loss 0.53 on epoch=429
03/01/2022 03:03:04 - INFO - __main__ - Step 870 Global step 870 Train loss 0.50 on epoch=434
03/01/2022 03:03:06 - INFO - __main__ - Step 880 Global step 880 Train loss 0.45 on epoch=439
03/01/2022 03:03:08 - INFO - __main__ - Step 890 Global step 890 Train loss 0.55 on epoch=444
03/01/2022 03:03:10 - INFO - __main__ - Step 900 Global step 900 Train loss 0.42 on epoch=449
03/01/2022 03:03:11 - INFO - __main__ - Global step 900 Train loss 0.49 EM 0.03125 on epoch=449
03/01/2022 03:03:13 - INFO - __main__ - Step 910 Global step 910 Train loss 0.52 on epoch=454
03/01/2022 03:03:16 - INFO - __main__ - Step 920 Global step 920 Train loss 0.51 on epoch=459
03/01/2022 03:03:18 - INFO - __main__ - Step 930 Global step 930 Train loss 0.52 on epoch=464
03/01/2022 03:03:20 - INFO - __main__ - Step 940 Global step 940 Train loss 0.48 on epoch=469
03/01/2022 03:03:22 - INFO - __main__ - Step 950 Global step 950 Train loss 0.39 on epoch=474
03/01/2022 03:03:23 - INFO - __main__ - Global step 950 Train loss 0.48 EM 0.03125 on epoch=474
03/01/2022 03:03:25 - INFO - __main__ - Step 960 Global step 960 Train loss 0.37 on epoch=479
03/01/2022 03:03:27 - INFO - __main__ - Step 970 Global step 970 Train loss 0.44 on epoch=484
03/01/2022 03:03:29 - INFO - __main__ - Step 980 Global step 980 Train loss 0.42 on epoch=489
03/01/2022 03:03:32 - INFO - __main__ - Step 990 Global step 990 Train loss 0.36 on epoch=494
03/01/2022 03:03:34 - INFO - __main__ - Step 1000 Global step 1000 Train loss 0.45 on epoch=499
03/01/2022 03:03:35 - INFO - __main__ - Global step 1000 Train loss 0.41 EM 0.03125 on epoch=499
03/01/2022 03:03:37 - INFO - __main__ - Step 1010 Global step 1010 Train loss 0.45 on epoch=504
03/01/2022 03:03:39 - INFO - __main__ - Step 1020 Global step 1020 Train loss 0.40 on epoch=509
03/01/2022 03:03:41 - INFO - __main__ - Step 1030 Global step 1030 Train loss 0.45 on epoch=514
03/01/2022 03:03:43 - INFO - __main__ - Step 1040 Global step 1040 Train loss 0.55 on epoch=519
03/01/2022 03:03:46 - INFO - __main__ - Step 1050 Global step 1050 Train loss 0.35 on epoch=524
03/01/2022 03:03:47 - INFO - __main__ - Global step 1050 Train loss 0.44 EM 0.0 on epoch=524
03/01/2022 03:03:49 - INFO - __main__ - Step 1060 Global step 1060 Train loss 0.35 on epoch=529
03/01/2022 03:03:51 - INFO - __main__ - Step 1070 Global step 1070 Train loss 0.35 on epoch=534
03/01/2022 03:03:53 - INFO - __main__ - Step 1080 Global step 1080 Train loss 0.36 on epoch=539
03/01/2022 03:03:55 - INFO - __main__ - Step 1090 Global step 1090 Train loss 0.40 on epoch=544
03/01/2022 03:03:58 - INFO - __main__ - Step 1100 Global step 1100 Train loss 0.41 on epoch=549
03/01/2022 03:03:59 - INFO - __main__ - Global step 1100 Train loss 0.37 EM 0.03125 on epoch=549
03/01/2022 03:04:01 - INFO - __main__ - Step 1110 Global step 1110 Train loss 0.34 on epoch=554
03/01/2022 03:04:03 - INFO - __main__ - Step 1120 Global step 1120 Train loss 0.37 on epoch=559
03/01/2022 03:04:05 - INFO - __main__ - Step 1130 Global step 1130 Train loss 0.39 on epoch=564
03/01/2022 03:04:07 - INFO - __main__ - Step 1140 Global step 1140 Train loss 0.26 on epoch=569
03/01/2022 03:04:09 - INFO - __main__ - Step 1150 Global step 1150 Train loss 0.33 on epoch=574
03/01/2022 03:04:11 - INFO - __main__ - Global step 1150 Train loss 0.34 EM 0.03125 on epoch=574
03/01/2022 03:04:13 - INFO - __main__ - Step 1160 Global step 1160 Train loss 0.34 on epoch=579
03/01/2022 03:04:15 - INFO - __main__ - Step 1170 Global step 1170 Train loss 0.30 on epoch=584
03/01/2022 03:04:17 - INFO - __main__ - Step 1180 Global step 1180 Train loss 0.34 on epoch=589
03/01/2022 03:04:19 - INFO - __main__ - Step 1190 Global step 1190 Train loss 0.37 on epoch=594
03/01/2022 03:04:21 - INFO - __main__ - Step 1200 Global step 1200 Train loss 0.33 on epoch=599
03/01/2022 03:04:23 - INFO - __main__ - Global step 1200 Train loss 0.34 EM 0.03125 on epoch=599
03/01/2022 03:04:25 - INFO - __main__ - Step 1210 Global step 1210 Train loss 0.30 on epoch=604
03/01/2022 03:04:27 - INFO - __main__ - Step 1220 Global step 1220 Train loss 0.30 on epoch=609
03/01/2022 03:04:29 - INFO - __main__ - Step 1230 Global step 1230 Train loss 0.28 on epoch=614
03/01/2022 03:04:31 - INFO - __main__ - Step 1240 Global step 1240 Train loss 0.32 on epoch=619
03/01/2022 03:04:33 - INFO - __main__ - Step 1250 Global step 1250 Train loss 0.34 on epoch=624
03/01/2022 03:04:35 - INFO - __main__ - Global step 1250 Train loss 0.31 EM 0.0 on epoch=624
03/01/2022 03:04:37 - INFO - __main__ - Step 1260 Global step 1260 Train loss 0.27 on epoch=629
03/01/2022 03:04:39 - INFO - __main__ - Step 1270 Global step 1270 Train loss 0.28 on epoch=634
03/01/2022 03:04:41 - INFO - __main__ - Step 1280 Global step 1280 Train loss 0.29 on epoch=639
03/01/2022 03:04:43 - INFO - __main__ - Step 1290 Global step 1290 Train loss 0.28 on epoch=644
03/01/2022 03:04:46 - INFO - __main__ - Step 1300 Global step 1300 Train loss 0.22 on epoch=649
03/01/2022 03:04:47 - INFO - __main__ - Global step 1300 Train loss 0.27 EM 0.03125 on epoch=649
03/01/2022 03:04:49 - INFO - __main__ - Step 1310 Global step 1310 Train loss 0.33 on epoch=654
03/01/2022 03:04:51 - INFO - __main__ - Step 1320 Global step 1320 Train loss 0.27 on epoch=659
03/01/2022 03:04:53 - INFO - __main__ - Step 1330 Global step 1330 Train loss 0.27 on epoch=664
03/01/2022 03:04:56 - INFO - __main__ - Step 1340 Global step 1340 Train loss 0.25 on epoch=669
03/01/2022 03:04:58 - INFO - __main__ - Step 1350 Global step 1350 Train loss 0.29 on epoch=674
03/01/2022 03:04:59 - INFO - __main__ - Global step 1350 Train loss 0.28 EM 0.03125 on epoch=674
03/01/2022 03:05:01 - INFO - __main__ - Step 1360 Global step 1360 Train loss 0.25 on epoch=679
03/01/2022 03:05:03 - INFO - __main__ - Step 1370 Global step 1370 Train loss 0.26 on epoch=684
03/01/2022 03:05:06 - INFO - __main__ - Step 1380 Global step 1380 Train loss 0.31 on epoch=689
03/01/2022 03:05:08 - INFO - __main__ - Step 1390 Global step 1390 Train loss 0.29 on epoch=694
03/01/2022 03:05:10 - INFO - __main__ - Step 1400 Global step 1400 Train loss 0.26 on epoch=699
03/01/2022 03:05:11 - INFO - __main__ - Global step 1400 Train loss 0.27 EM 0.03125 on epoch=699
03/01/2022 03:05:13 - INFO - __main__ - Step 1410 Global step 1410 Train loss 0.32 on epoch=704
03/01/2022 03:05:15 - INFO - __main__ - Step 1420 Global step 1420 Train loss 0.23 on epoch=709
03/01/2022 03:05:17 - INFO - __main__ - Step 1430 Global step 1430 Train loss 0.28 on epoch=714
03/01/2022 03:05:19 - INFO - __main__ - Step 1440 Global step 1440 Train loss 0.24 on epoch=719
03/01/2022 03:05:21 - INFO - __main__ - Step 1450 Global step 1450 Train loss 0.24 on epoch=724
03/01/2022 03:05:23 - INFO - __main__ - Global step 1450 Train loss 0.26 EM 0.03125 on epoch=724
03/01/2022 03:05:25 - INFO - __main__ - Step 1460 Global step 1460 Train loss 0.22 on epoch=729
03/01/2022 03:05:27 - INFO - __main__ - Step 1470 Global step 1470 Train loss 0.22 on epoch=734
03/01/2022 03:05:29 - INFO - __main__ - Step 1480 Global step 1480 Train loss 0.25 on epoch=739
03/01/2022 03:05:31 - INFO - __main__ - Step 1490 Global step 1490 Train loss 0.27 on epoch=744
03/01/2022 03:05:33 - INFO - __main__ - Step 1500 Global step 1500 Train loss 0.22 on epoch=749
03/01/2022 03:05:34 - INFO - __main__ - Global step 1500 Train loss 0.23 EM 0.03125 on epoch=749
03/01/2022 03:05:37 - INFO - __main__ - Step 1510 Global step 1510 Train loss 0.20 on epoch=754
03/01/2022 03:05:39 - INFO - __main__ - Step 1520 Global step 1520 Train loss 0.25 on epoch=759
03/01/2022 03:05:41 - INFO - __main__ - Step 1530 Global step 1530 Train loss 0.21 on epoch=764
03/01/2022 03:05:43 - INFO - __main__ - Step 1540 Global step 1540 Train loss 0.23 on epoch=769
03/01/2022 03:05:45 - INFO - __main__ - Step 1550 Global step 1550 Train loss 0.23 on epoch=774
03/01/2022 03:05:46 - INFO - __main__ - Global step 1550 Train loss 0.22 EM 0.03125 on epoch=774
03/01/2022 03:05:48 - INFO - __main__ - Step 1560 Global step 1560 Train loss 0.22 on epoch=779
03/01/2022 03:05:51 - INFO - __main__ - Step 1570 Global step 1570 Train loss 0.20 on epoch=784
03/01/2022 03:05:53 - INFO - __main__ - Step 1580 Global step 1580 Train loss 0.22 on epoch=789
03/01/2022 03:05:55 - INFO - __main__ - Step 1590 Global step 1590 Train loss 0.18 on epoch=794
03/01/2022 03:05:57 - INFO - __main__ - Step 1600 Global step 1600 Train loss 0.17 on epoch=799
03/01/2022 03:05:58 - INFO - __main__ - Global step 1600 Train loss 0.20 EM 0.0 on epoch=799
03/01/2022 03:06:00 - INFO - __main__ - Step 1610 Global step 1610 Train loss 0.19 on epoch=804
03/01/2022 03:06:02 - INFO - __main__ - Step 1620 Global step 1620 Train loss 0.23 on epoch=809
03/01/2022 03:06:04 - INFO - __main__ - Step 1630 Global step 1630 Train loss 0.22 on epoch=814
03/01/2022 03:06:07 - INFO - __main__ - Step 1640 Global step 1640 Train loss 0.16 on epoch=819
03/01/2022 03:06:09 - INFO - __main__ - Step 1650 Global step 1650 Train loss 0.18 on epoch=824
03/01/2022 03:06:10 - INFO - __main__ - Global step 1650 Train loss 0.20 EM 0.03125 on epoch=824
03/01/2022 03:06:12 - INFO - __main__ - Step 1660 Global step 1660 Train loss 0.23 on epoch=829
03/01/2022 03:06:14 - INFO - __main__ - Step 1670 Global step 1670 Train loss 0.19 on epoch=834
03/01/2022 03:06:16 - INFO - __main__ - Step 1680 Global step 1680 Train loss 0.19 on epoch=839
03/01/2022 03:06:18 - INFO - __main__ - Step 1690 Global step 1690 Train loss 0.19 on epoch=844
03/01/2022 03:06:20 - INFO - __main__ - Step 1700 Global step 1700 Train loss 0.20 on epoch=849
03/01/2022 03:06:22 - INFO - __main__ - Global step 1700 Train loss 0.20 EM 0.0 on epoch=849
03/01/2022 03:06:24 - INFO - __main__ - Step 1710 Global step 1710 Train loss 0.14 on epoch=854
03/01/2022 03:06:26 - INFO - __main__ - Step 1720 Global step 1720 Train loss 0.16 on epoch=859
03/01/2022 03:06:28 - INFO - __main__ - Step 1730 Global step 1730 Train loss 0.16 on epoch=864
03/01/2022 03:06:30 - INFO - __main__ - Step 1740 Global step 1740 Train loss 0.12 on epoch=869
03/01/2022 03:06:32 - INFO - __main__ - Step 1750 Global step 1750 Train loss 0.15 on epoch=874
03/01/2022 03:06:34 - INFO - __main__ - Global step 1750 Train loss 0.14 EM 0.03125 on epoch=874
03/01/2022 03:06:36 - INFO - __main__ - Step 1760 Global step 1760 Train loss 0.19 on epoch=879
03/01/2022 03:06:38 - INFO - __main__ - Step 1770 Global step 1770 Train loss 0.23 on epoch=884
03/01/2022 03:06:40 - INFO - __main__ - Step 1780 Global step 1780 Train loss 0.15 on epoch=889
03/01/2022 03:06:42 - INFO - __main__ - Step 1790 Global step 1790 Train loss 0.17 on epoch=894
03/01/2022 03:06:44 - INFO - __main__ - Step 1800 Global step 1800 Train loss 0.18 on epoch=899
03/01/2022 03:06:46 - INFO - __main__ - Global step 1800 Train loss 0.18 EM 0.03125 on epoch=899
03/01/2022 03:06:48 - INFO - __main__ - Step 1810 Global step 1810 Train loss 0.22 on epoch=904
03/01/2022 03:06:50 - INFO - __main__ - Step 1820 Global step 1820 Train loss 0.11 on epoch=909
03/01/2022 03:06:52 - INFO - __main__ - Step 1830 Global step 1830 Train loss 0.16 on epoch=914
03/01/2022 03:06:54 - INFO - __main__ - Step 1840 Global step 1840 Train loss 0.16 on epoch=919
03/01/2022 03:06:56 - INFO - __main__ - Step 1850 Global step 1850 Train loss 0.10 on epoch=924
03/01/2022 03:06:58 - INFO - __main__ - Global step 1850 Train loss 0.15 EM 0.0 on epoch=924
03/01/2022 03:07:00 - INFO - __main__ - Step 1860 Global step 1860 Train loss 0.18 on epoch=929
03/01/2022 03:07:02 - INFO - __main__ - Step 1870 Global step 1870 Train loss 0.14 on epoch=934
03/01/2022 03:07:04 - INFO - __main__ - Step 1880 Global step 1880 Train loss 0.14 on epoch=939
03/01/2022 03:07:06 - INFO - __main__ - Step 1890 Global step 1890 Train loss 0.13 on epoch=944
03/01/2022 03:07:09 - INFO - __main__ - Step 1900 Global step 1900 Train loss 0.17 on epoch=949
03/01/2022 03:07:10 - INFO - __main__ - Global step 1900 Train loss 0.15 EM 0.0 on epoch=949
03/01/2022 03:07:12 - INFO - __main__ - Step 1910 Global step 1910 Train loss 0.16 on epoch=954
03/01/2022 03:07:14 - INFO - __main__ - Step 1920 Global step 1920 Train loss 0.16 on epoch=959
03/01/2022 03:07:17 - INFO - __main__ - Step 1930 Global step 1930 Train loss 0.13 on epoch=964
03/01/2022 03:07:19 - INFO - __main__ - Step 1940 Global step 1940 Train loss 0.14 on epoch=969
03/01/2022 03:07:21 - INFO - __main__ - Step 1950 Global step 1950 Train loss 0.15 on epoch=974
03/01/2022 03:07:22 - INFO - __main__ - Global step 1950 Train loss 0.15 EM 0.03125 on epoch=974
03/01/2022 03:07:25 - INFO - __main__ - Step 1960 Global step 1960 Train loss 0.13 on epoch=979
03/01/2022 03:07:27 - INFO - __main__ - Step 1970 Global step 1970 Train loss 0.19 on epoch=984
03/01/2022 03:07:29 - INFO - __main__ - Step 1980 Global step 1980 Train loss 0.11 on epoch=989
03/01/2022 03:07:31 - INFO - __main__ - Step 1990 Global step 1990 Train loss 0.16 on epoch=994
03/01/2022 03:07:33 - INFO - __main__ - Step 2000 Global step 2000 Train loss 0.15 on epoch=999
03/01/2022 03:07:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 03:07:34 - INFO - __main__ - Printing 3 examples
03/01/2022 03:07:34 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 03:07:34 - INFO - __main__ - ['camille saint-saens']
03/01/2022 03:07:34 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 03:07:34 - INFO - __main__ - ['madness']
03/01/2022 03:07:34 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 03:07:34 - INFO - __main__ - ['genevieve']
03/01/2022 03:07:34 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
03/01/2022 03:07:34 - INFO - __main__ - Tokenizing Output ...
03/01/2022 03:07:34 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 03:07:34 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 03:07:34 - INFO - __main__ - Printing 3 examples
03/01/2022 03:07:34 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 03:07:34 - INFO - __main__ - ['will hay']
03/01/2022 03:07:34 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 03:07:34 - INFO - __main__ - ['alan sugar']
03/01/2022 03:07:34 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 03:07:34 - INFO - __main__ - ['cleopatra']
03/01/2022 03:07:34 - INFO - __main__ - Tokenizing Input ...
03/01/2022 03:07:34 - INFO - __main__ - Tokenizing Output ...
03/01/2022 03:07:34 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 03:07:34 - INFO - __main__ - Global step 2000 Train loss 0.15 EM 0.03125 on epoch=999
03/01/2022 03:07:34 - INFO - __main__ - save last model!
03/01/2022 03:07:34 - INFO - __main__ - Loading checkpoint from best ckpt on the fly
03/01/2022 03:07:34 - INFO - __main__ - Start tokenizing ... 3994 instances
03/01/2022 03:07:34 - INFO - __main__ - Printing 3 examples
03/01/2022 03:07:34 - INFO - __main__ -  [freebase_qa] The 1999 film '10 Things I Hate About You' is based on which Shakespeare play?
03/01/2022 03:07:34 - INFO - __main__ - ['taming of the shrew']
03/01/2022 03:07:34 - INFO - __main__ -  [freebase_qa] Who began as a Broadway actor, made his Hollywood debut in 1935, and had lead roles in The Grapes of Wrath, The Ox-Bow Incident, Mister Roberts and 12 Angry Men?
03/01/2022 03:07:34 - INFO - __main__ - ['henry fonda']
03/01/2022 03:07:34 - INFO - __main__ -  [freebase_qa] Who composed the 1812 Overture?
03/01/2022 03:07:34 - INFO - __main__ - ['tchaikovsky']
03/01/2022 03:07:34 - INFO - __main__ - Tokenizing Input ...
03/01/2022 03:07:36 - INFO - __main__ - Tokenizing Output ...
03/01/2022 03:07:40 - INFO - __main__ - Loaded 3994 examples from test data
03/01/2022 03:07:47 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 03:07:48 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 03:07:48 - INFO - __main__ - Starting training!
03/01/2022 03:10:28 - INFO - __main__ - Saved prediction in models/T5-large-maml-random-3e-5-2-5000-5e-1/singletask-freebase_qa/freebase_qa_32_21_0.3_8_predictions.txt
03/01/2022 03:10:28 - INFO - __main__ - EM on test data: 0.0045
03/01/2022 03:10:28 - INFO - __main__ - prefix=freebase_qa_32_21, lr=0.3, bsz=8, dev_performance=0.03125, test_performance=0.004506760140210316
03/01/2022 03:10:28 - INFO - __main__ - Running ... prefix=freebase_qa_32_21, lr=0.2, bsz=8 ...
03/01/2022 03:10:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 03:10:29 - INFO - __main__ - Printing 3 examples
03/01/2022 03:10:29 - INFO - __main__ -  [freebase_qa] Amongst which composer's best known works are Danse Macabre and The Organ Symphony no 3?
03/01/2022 03:10:29 - INFO - __main__ - ['camille saint-saens']
03/01/2022 03:10:29 - INFO - __main__ -  [freebase_qa] The lead singer of which band is known as Suggs?
03/01/2022 03:10:29 - INFO - __main__ - ['madness']
03/01/2022 03:10:29 - INFO - __main__ -  [freebase_qa] In a film of the 1950s, what was the name of the car in which Kenneth Moore and Dinah Sheridan travelled from London to Brighton?
03/01/2022 03:10:29 - INFO - __main__ - ['genevieve']
03/01/2022 03:10:29 - INFO - __main__ - Tokenizing Input ...
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/opt/conda/envs/meta/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
03/01/2022 03:10:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 03:10:29 - INFO - __main__ - Loaded 32 examples from train data
use DistributedSampler
03/01/2022 03:10:29 - INFO - __main__ - Start tokenizing ... 32 instances
03/01/2022 03:10:29 - INFO - __main__ - Printing 3 examples
03/01/2022 03:10:29 - INFO - __main__ -  [freebase_qa] Who played the title character in the 1937 comedy film Oh, Mr Porter!?
03/01/2022 03:10:29 - INFO - __main__ - ['will hay']
03/01/2022 03:10:29 - INFO - __main__ -  [freebase_qa] Which businessman on his elevation to the House of Lords, by Gordon Brown, in 2000, took the title Baron of Clapton?
03/01/2022 03:10:29 - INFO - __main__ - ['alan sugar']
03/01/2022 03:10:29 - INFO - __main__ -  [freebase_qa] Who famously had children with both Julius Caesar and Mark Antony?
03/01/2022 03:10:29 - INFO - __main__ - ['cleopatra']
03/01/2022 03:10:29 - INFO - __main__ - Tokenizing Input ...
03/01/2022 03:10:29 - INFO - __main__ - Tokenizing Output ...
03/01/2022 03:10:29 - INFO - __main__ - Loaded 32 examples from dev data
03/01/2022 03:10:43 - INFO - __main__ - load prompt embedding from ckpt
03/01/2022 03:10:44 - INFO - root - ShardedDDP bucket size: 8.00M parameters, model size 746.97M parameters
03/01/2022 03:10:44 - INFO - __main__ - Starting training!
03/01/2022 03:10:47 - INFO - __main__ - Step 10 Global step 10 Train loss 4.38 on epoch=4
03/01/2022 03:10:49 - INFO - __main__ - Step 20 Global step 20 Train loss 3.96 on epoch=9
03/01/2022 03:10:52 - INFO - __main__ - Step 30 Global step 30 Train loss 3.51 on epoch=14
03/01/2022 03:10:54 - INFO - __main__ - Step 40 Global step 40 Train loss 3.32 on epoch=19
03/01/2022 03:10:56 - INFO - __main__ - Step 50 Global step 50 Train loss 3.19 on epoch=24
03/01/2022 03:10:57 - INFO - __main__ - Global step 50 Train loss 3.67 EM 0.0 on epoch=24
03/01/2022 03:10:57 - INFO - __main__ - Saving model with best EM: -1.0 -> 0.0 on epoch=24, global_step=50
03/01/2022 03:11:00 - INFO - __main__ - Step 60 Global step 60 Train loss 3.02 on epoch=29
03/01/2022 03:11:02 - INFO - __main__ - Step 70 Global step 70 Train loss 3.00 on epoch=34
03/01/2022 03:11:04 - INFO - __main__ - Step 80 Global step 80 Train loss 2.90 on epoch=39
03/01/2022 03:11:06 - INFO - __main__ - Step 90 Global step 90 Train loss 2.90 on epoch=44
03/01/2022 03:11:08 - INFO - __main__ - Step 100 Global step 100 Train loss 2.79 on epoch=49
03/01/2022 03:11:10 - INFO - __main__ - Global step 100 Train loss 2.92 EM 0.0 on epoch=49
03/01/2022 03:11:12 - INFO - __main__ - Step 110 Global step 110 Train loss 2.74 on epoch=54
03/01/2022 03:11:14 - INFO - __main__ - Step 120 Global step 120 Train loss 2.62 on epoch=59
03/01/2022 03:11:16 - INFO - __main__ - Step 130 Global step 130 Train loss 2.62 on epoch=64
03/01/2022 03:11:19 - INFO - __main__ - Step 140 Global step 140 Train loss 2.62 on epoch=69
03/01/2022 03:11:21 - INFO - __main__ - Step 150 Global step 150 Train loss 2.56 on epoch=74
03/01/2022 03:11:22 - INFO - __main__ - Global step 150 Train loss 2.63 EM 0.0 on epoch=74
03/01/2022 03:11:24 - INFO - __main__ - Step 160 Global step 160 Train loss 2.44 on epoch=79
03/01/2022 03:11:27 - INFO - __main__ - Step 170 Global step 170 Train loss 2.38 on epoch=84
03/01/2022 03:11:29 - INFO - __main__ - Step 180 Global step 180 Train loss 2.37 on epoch=89
03/01/2022 03:11:31 - INFO - __main__ - Step 190 Global step 190 Train loss 2.41 on epoch=94
03/01/2022 03:11:33 - INFO - __main__ - Step 200 Global step 200 Train loss 2.23 on epoch=99
03/01/2022 03:11:35 - INFO - __main__ - Global step 200 Train loss 2.36 EM 0.0 on epoch=99
03/01/2022 03:11:37 - INFO - __main__ - Step 210 Global step 210 Train loss 2.27 on epoch=104
03/01/2022 03:11:39 - INFO - __main__ - Step 220 Global step 220 Train loss 2.27 on epoch=109
03/01/2022 03:11:41 - INFO - __main__ - Step 230 Global step 230 Train loss 2.25 on epoch=114
03/01/2022 03:11:43 - INFO - __main__ - Step 240 Global step 240 Train loss 2.17 on epoch=119
03/01/2022 03:11:46 - INFO - __main__ - Step 250 Global step 250 Train loss 2.16 on epoch=124
03/01/2022 03:11:47 - INFO - __main__ - Global step 250 Train loss 2.22 EM 0.0 on epoch=124
03/01/2022 03:11:49 - INFO - __main__ - Step 260 Global step 260 Train loss 2.15 on epoch=129
03/01/2022 03:11:51 - INFO - __main__ - Step 270 Global step 270 Train loss 2.08 on epoch=134
03/01/2022 03:11:54 - INFO - __main__ - Step 280 Global step 280 Train loss 2.07 on epoch=139
03/01/2022 03:11:56 - INFO - __main__ - Step 290 Global step 290 Train loss 1.98 on epoch=144
03/01/2022 03:11:58 - INFO - __main__ - Step 300 Global step 300 Train loss 2.03 on epoch=149
03/01/2022 03:11:59 - INFO - __main__ - Global step 300 Train loss 2.06 EM 0.0 on epoch=149
03/01/2022 03:12:02 - INFO - __main__ - Step 310 Global step 310 Train loss 1.92 on epoch=154
03/01/2022 03:12:04 - INFO - __main__ - Step 320 Global step 320 Train loss 1.94 on epoch=159
03/01/2022 03:12:06 - INFO - __main__ - Step 330 Global step 330 Train loss 1.87 on epoch=164
03/01/2022 03:12:08 - INFO - __main__ - Step 340 Global step 340 Train loss 1.90 on epoch=169
03/01/2022 03:12:10 - INFO - __main__ - Step 350 Global step 350 Train loss 1.88 on epoch=174
03/01/2022 03:12:12 - INFO - __main__ - Global step 350 Train loss 1.90 EM 0.0 on epoch=174
03/01/2022 03:12:14 - INFO - __main__ - Step 360 Global step 360 Train loss 1.84 on epoch=179
03/01/2022 03:12:16 - INFO - __main__ - Step 370 Global step 370 Train loss 1.78 on epoch=184
03/01/2022 03:12:18 - INFO - __main__ - Step 380 Global step 380 Train loss 1.69 on epoch=189
03/01/2022 03:12:21 - INFO - __main__ - Step 390 Global step 390 Train loss 1.68 on epoch=194
03/01/2022 03:12:23 - INFO - __main__ - Step 400 Global step 400 Train loss 1.74 on epoch=199
03/01/2022 03:12:24 - INFO - __main__ - Global step 400 Train loss 1.75 EM 0.0 on epoch=199
03/01/2022 03:12:27 - INFO - __main__ - Step 410 Global step 410 Train loss 1.66 on epoch=204
03/01/2022 03:12:29 - INFO - __main__ - Step 420 Global step 420 Train loss 1.61 on epoch=209
03/01/2022 03:12:31 - INFO - __main__ - Step 430 Global step 430 Train loss 1.65 on epoch=214
03/01/2022 03:12:33 - INFO - __main__ - Step 440 Global step 440 Train loss 1.59 on epoch=219
03/01/2022 03:12:35 - INFO - __main__ - Step 450 Global step 450 Train loss 1.50 on epoch=224
03/01/2022 03:12:37 - INFO - __main__ - Global step 450 Train loss 1.60 EM 0.0 on epoch=224
03/01/2022 03:12:39 - INFO - __main__ - Step 460 Global step 460 Train loss 1.52 on epoch=229
03/01/2022 03:12:41 - INFO - __main__ - Step 470 Global step 470 Train loss 1.51 on epoch=234
03/01/2022 03:12:43 - INFO - __main__ - Step 480 Global step 480 Train loss 1.56 on epoch=239
03/01/2022 03:12:46 - INFO - __main__ - Step 490 Global step 490 Train loss 1.53 on epoch=244
03/01/2022 03:12:48 - INFO - __main__ - Step 500 Global step 500 Train loss 1.41 on epoch=249
03/01/2022 03:12:49 - INFO - __main__ - Global step 500 Train loss 1.51 EM 0.0 on epoch=249
03/01/2022 03:12:52 - INFO - __main__ - Step 510 Global step 510 Train loss 1.34 on epoch=254
03/01/2022 03:12:54 - INFO - __main__ - Step 520 Global step 520 Train loss 1.38 on epoch=259
03/01/2022 03:12:56 - INFO - __main__ - Step 530 Global step 530 Train loss 1.35 on epoch=264
03/01/2022 03:12:58 - INFO - __main__ - Step 540 Global step 540 Train loss 1.28 on epoch=269
03/01/2022 03:13:00 - INFO - __main__ - Step 550 Global step 550 Train loss 1.33 on epoch=274
03/01/2022 03:13:02 - INFO - __main__ - Global step 550 Train loss 1.34 EM 0.0 on epoch=274
03/01/2022 03:13:04 - INFO - __main__ - Step 560 Global step 560 Train loss 1.34 on epoch=279
03/01/2022 03:13:07 - INFO - __main__ - Step 570 Global step 570 Train loss 1.22 on epoch=284
03/01/2022 03:13:09 - INFO - __main__ - Step 580 Global step 580 Train loss 1.28 on epoch=289
03/01/2022 03:13:11 - INFO - __main__ - Step 590 Global step 590 Train loss 1.23 on epoch=294
03/01/2022 03:13:13 - INFO - __main__ - Step 600 Global step 600 Train loss 1.21 on epoch=299
03/01/2022 03:13:15 - INFO - __main__ - Global step 600 Train loss 1.26 EM 0.0 on epoch=299
03/01/2022 03:13:17 - INFO - __main__ - Step 610 Global step 610 Train loss 1.15 on epoch=304
03/01/2022 03:13:19 - INFO - __main__ - Step 620 Global step 620 Train loss 1.21 on epoch=309
03/01/2022 03:13:21 - INFO - __main__ - Step 630 Global step 630 Train loss 1.17 on epoch=314
03/01/2022 03:13:24 - INFO - __main__ - Step 640 Global step 640 Train loss 1.24 on epoch=319
03/01/2022 03:13:26 - INFO - __main__ - Step 650 Global step 650 Train loss 1.15 on epoch=324
03/01/2022 03:13:27 - INFO - __main__ - Global step 650 Train loss 1.18 EM 0.0 on epoch=324
03/01/2022 03:13:30 - INFO - __main__ - Step 660 Global step 660 Train loss 1.04 on epoch=329
03/01/2022 03:13:32 - INFO - __main__ - Step 670 Global step 670 Train loss 1.02 on epoch=334
03/01/2022 03:13:34 - INFO - __main__ - Step 680 Global step 680 Train loss 1.09 on epoch=339
03/01/2022 03:13:36 - INFO - __main__ - Step 690 Global step 690 Train loss 1.06 on epoch=344
03/01/2022 03:13:38 - INFO - __main__ - Step 700 Global step 700 Train loss 1.00 on epoch=349
03/01/2022 03:13:40 - INFO - __main__ - Global step 700 Train loss 1.04 EM 0.0 on epoch=349
03/01/2022 03:13:42 - INFO - __main__ - Step 710 Global step 710 Train loss 1.07 on epoch=354
03/01/2022 03:13:44 - INFO - __main__ - Step 720 Global step 720 Train loss 1.03 on epoch=359
03/01/2022 03:13:47 - INFO - __main__ - Step 730 Global step 730 Train loss 1.07 on epoch=364
03/01/2022 03:13:49 - INFO - __main__ - Step 740 Global step 740 Train loss 1.00 on epoch=369
03/01/2022 03:13:51 - INFO - __main__ - Step 750 Global step 750 Train loss 0.95 on epoch=374
03/01/2022 03:13:52 - INFO - __main__ - Global step 750 Train loss 1.03 EM 0.0 on epoch=374
03/01/2022 03:13:55 - INFO - __main__ - Step 760 Global step 760 Train loss 0.92 on epoch=379
03/01/2022 03:13:57 - INFO - __main__ - Step 770 Global step 770 Train loss 1.02 on epoch=384
03/01/2022 03:13:59 - INFO - __main__ - Step 780 Global step 780 Train loss 0.90 on epoch=389
03/01/2022 03:14:01 - INFO - __main__ - Step 790 Global step 790 Train loss 0.94 on epoch=394
03/01/2022 03:14:03 - INFO - __main__ - Step 800 Global step 800 Train loss 1.00 on epoch=399
03/01/2022 03:14:05 - INFO - __main__ - Global step 800 Train loss 0.96 EM 0.0 on epoch=399
03/01/2022 03:14:07 - INFO - __main__ - Step 810 Global step 810 Train loss 0.91 on epoch=404
03/01/2022 03:14:09 - INFO - __main__ - Step 820 Global step 820 Train loss 0.99 on epoch=409
03/01/2022 03:14:11 - INFO - __main__ - Step 830 Global step 830 Train loss 0.93 on epoch=414
03/01/2022 03:14:14 - INFO - __main__ - Step 840 Global step 840 Train loss 0.88 on epoch=419
03/01/2022 03:14:16 - INFO - __main__ - Step 850 Global step 850 Train loss 0.81 on epoch=424
03/01/2022 03:14:17 - INFO - __main__ - Global step 850 Train loss 0.91 EM 0.0 on epoch=424
03/01/2022 03:14:19 - INFO - __main__ - Step 860 Global step 860 Train loss 0.88 on epoch=429
03/01/2022 03:14:22 - INFO - __main__ - Step 870 Global step 870 Train loss 0.86 on epoch=434
03/01/2022 03:14:24 - INFO - __main__ - Step 880 Global step 880 Train loss 0.79 on epoch=439
03/01/2022 03:14:26 - INFO - __main__ - Step 890 Global step 890 Train loss 0.84 on epoch=444
03/01/2022 03:14:28 - INFO - __main__ - Step 900 Global step 900 Train loss 0.83 on epoch=449
03/01/2022 03:14:30 - INFO - __main__ - Global step 900 Train loss 0.84 EM 0.0 on epoch=449
03/01/2022 03:14:32 - INFO - __main__ - Step 910 Global step 910 Train loss 0.76 on epoch=454
03/01/2022 03:14:34 - INFO - __main__ - Step 920 Global step 920 Train loss 0.86 on epoch=459
03/01/2022 03:14:36 - INFO - __main__ - Step 930 Global step 930 Train loss 0.83 on epoch=464
03/01/2022 03:14:38 - INFO - __main__ - Step 940 Global step 940 Train loss 0.78 on epoch=469
03/01/2022 03:14:41 - INFO - __main__ - Step 950 Global step 950 Train loss 0.82 on epoch=474
03/01/2022 03:14:42 - INFO - __main__ - Global step 950 Train loss 0.81 EM 0.0 on epoch=474
03/01/2022 03:14:44 - INFO - __main__ - Step 960 Global step 960 Train loss 0.76 on epoch=479
03/01/2022 03:14:47 - INFO - __main__ - Step 970 Global step 970 Train loss 0.73 on epoch=484
